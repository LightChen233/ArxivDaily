<!DOCTYPE html>
<html lang="en">

<head>
    <title>ArxivDaily</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <a href="https://github.com/AlongWY/ArxivDaily" style="text-decoration: none;">
                <div class="header-title">
                    <span class="header-title-preffix">AlongWY</span>//ArxivDaily
                </div>
            </a>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-24T00:00:00Z">2022-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Dropping for Efficient <span class="highlight-title">BERT</span> <span class="highlight-title">Pretrain</span>ing <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models generally allocate the same amount of computation
for each token in a given sequence. We develop a simple but effective "token
dropping" method to accelerate the pretraining of transformer models, such as
BERT, without degrading its performance on downstream tasks. In short, we drop
unimportant tokens starting from an intermediate layer in the model to make the
model focus on important tokens; the dropped tokens are later picked up by the
last layer of the model so that the model still produces full-length sequences.
We leverage the already built-in masked language modeling (MLM) loss to
identify unimportant tokens with practically no computational overhead. In our
experiments, this simple approach reduces the pretraining cost of BERT by 25%
while achieving similar overall fine-tuning performance on standard downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMARAGD: Synthesized sMatch for Accurate and Rapid AMR Graph Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juri Opitz, Philipp Meier, Anette Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic similarity of graph-based meaning representations, such as
Abstract Meaning Representation (AMR), is typically assessed using graph
matching algorithms, such as SMATCH (Cai and Knight, 2013). However, SMATCH
suffers from NP-completeness, making its large-scale application, e.g., for AMR
clustering or semantic search, infeasible. To mitigate this issue, we propose
SMARAGD (Synthesized sMatch for accurate and rapid AMR graph distance). We show
the potential of neural networks to approximate the SMATCH scores and graph
alignments, i) in linear time using a machine translation framework to predict
the alignments, or ii) in constant time using a Siamese CNN to directly predict
SMATCH scores. We show that the approximation error can be substantially
reduced by applying data augmentation and AMR graph anonymization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Language Models that Seek for Knowledge: Modular Search & Generation for
  <span class="highlight-title">Dialogue</span> and <span class="highlight-title">Prompt</span> Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, <span class="highlight-author">Jason Weston</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have recently been shown to generate more factual
responses by employing modularity (Zhou et al., 2021) in combination with
retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et
al. (2021) to include internet search as a module. Our SeeKeR (Search
engine->Knowledge->Response) method thus applies a single LM to three modular
tasks in succession: search, generating knowledge, and generating a final
response. We show that, when using SeeKeR as a dialogue model, it outperforms
the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain
knowledge-grounded conversations for the same number of parameters, in terms of
consistency, knowledge and per-turn engagingness. SeeKeR applied to topical
prompt completions as a standard language model outperforms GPT2 (Radford et
al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,
despite GPT3 being a vastly larger model. Our code and models are made publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct parsing to sentiment graphs <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Samuel, Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja Øvrelid, Erik Velldal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper demonstrates how a graph-based semantic parser can be applied to
the task of structured sentiment analysis, directly predicting sentiment graphs
from text. We advance the state of the art on 4 out of 5 standard benchmark
sets. We release the source code, models and predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of hierarchical reference systems in multi-agent communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xenia Ohmer, Marko Duda, Elia Bruni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language, referencing objects at different levels of specificity
is a fundamental pragmatic mechanism for efficient communication in context. We
develop a novel communication game, the hierarchical reference game, to study
the emergence of such reference systems in artificial agents. We consider a
simplified world, in which concepts are abstractions over a set of primitive
attributes (e.g., color, style, shape). Depending on how many attributes are
combined, concepts are more general ("circle") or more specific ("red dotted
circle"). Based on the context, the agents have to communicate at different
levels of this hierarchy. Our results show, that the agents learn to play the
game successfully and can even generalize to novel concepts. To achieve
abstraction, they use implicit (omitting irrelevant information) and explicit
(indicating that attributes are irrelevant) strategies. In addition, the
compositional structure underlying the concept hierarchy is reflected in the
emergent protocols, indicating that the need to develop hierarchical reference
systems supports the emergence of compositionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-armed bandits for online optimization of language model
  <span class="highlight-title">pre-train</span>ing: the use case of dynamic masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iñigo Urteaga, Moulay-Zaïdane Draïdia, Tomer Lancewicki, Shahram Khadivi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models (TLMs) provide state-of-the-art performance
in many modern natural language processing applications. TLM training is
conducted in two phases. First, the model is pre-trained over large volumes of
text to minimize a generic objective function, such as the Masked Language
Model (MLM). Second, the model is fine-tuned in specific downstream tasks.
Pre-training requires large volumes of data and high computational resources,
while introducing many still unresolved design choices. For instance, selecting
hyperparameters for language model pre-training is often carried out based on
heuristics or grid-based searches. In this work, we propose a multi-armed
bandit-based online optimization framework for the sequential selection of
pre-training hyperparameters to optimize language model performance. We pose
the pre-training procedure as a sequential decision-making task, where at each
pre-training step, an agent must determine what hyperparameters to use towards
optimizing the pre-training objective. We propose a Thompson sampling bandit
algorithm, based on a surrogate Gaussian process reward model of the MLM
pre-training objective, for its sequential minimization. We empirically show
how the proposed Gaussian process based Thompson sampling pre-trains robust and
well-performing language models. Namely, by sequentially selecting masking
hyperparameters of the TLM, we achieve satisfactory performance in less epochs,
not only in terms of the pre-training MLM objective, but in diverse downstream
fine-tuning tasks. The proposed bandit-based technique provides an automated
hyperparameter selection method for pre-training TLMs of interest to
practitioners. In addition, our results indicate that, instead of MLM
pre-training with fixed masking probabilities, sequentially adapting the
masking hyperparameters improves both pre-training loss and downstream task
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, <span class="highlight-author">Devi Parikh</span>, Yaniv Taigman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image generation methods provide a simple yet exciting
conversion capability between text and image domains. While these methods have
incrementally improved the generated image fidelity and text relevancy, several
pivotal gaps remain unanswered, limiting applicability and quality. We propose
a novel text-to-image method that addresses these gaps by (i) enabling a simple
control mechanism complementary to text in the form of a scene, (ii)
introducing elements that substantially improve the tokenization process by
employing domain-specific knowledge over key image regions (faces and salient
objects), and (iii) adapting classifier-free guidance for the transformer use
case. Our model achieves state-of-the-art FID and human evaluation results,
unlocking the ability to generate high fidelity images in a resolution of
512x512 pixels, significantly improving visual quality. Through scene
controllability, we introduce several new capabilities: (i) Scene editing, (ii)
text editing with anchor scenes, (iii) overcoming out-of-distribution text
prompts, and (iv) story illustration generation, as demonstrated in the story
we wrote.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ minicons: Enabling Flexible Behavioral and Representational Analyses of
  <span class="highlight-title">Transformer</span> Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanishka Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present minicons, an open source library that provides a standard API for
researchers interested in conducting behavioral and representational analyses
of transformer-based language models (LMs). Specifically, minicons enables
researchers to apply analysis methods at two levels: (1) at the prediction
level -- by providing functions to efficiently extract word/sentence level
probabilities; and (2) at the representational level -- by also facilitating
efficient extraction of word/phrase level vectors from one or more layers. In
this paper, we describe the library and apply it to two motivating case
studies: One focusing on the learning dynamics of the BERT architecture on
relative grammatical judgments, and the other on benchmarking 23 different LMs
on zero-shot abductive reasoning. minicons is available at
https://github.com/kanishkamisra/minicons
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted; Code to reproduce experiments can be found on
  https://github.com/kanishkamisra/minicons-experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Neural Bag of Whole-Words with Col<span class="highlight-title">BERT</span>er: Contextualized
  Late Interactions using Enhanced Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, Allan Hanbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in neural information retrieval has demonstrated large gains
in effectiveness, while often sacrificing the efficiency and interpretability
of the neural model compared to classical approaches. This paper proposes
ColBERTer, a neural retrieval model using contextualized late interaction
(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,
ColBERTer's reductions dramatically lower ColBERT's storage requirements while
simultaneously improving the interpretability of its token-matching scores. To
this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and
optional lexical matching components into one model. For its multi-vector
component, ColBERTer reduces the number of stored vectors per document by
learning unique whole-word representations for the terms in each document and
learning to identify and remove word representations that are not essential to
effective scoring. We employ an explicit multi-task, multi-stage training to
facilitate using very small vector dimensions. Results on the MS MARCO and
TREC-DL collection show that ColBERTer can reduce the storage footprint by up
to 2.5x, while maintaining effectiveness. With just one dimension per token in
its smallest setting, ColBERTer achieves index storage parity with the
plaintext size, with very strong effectiveness results. Finally, we demonstrate
ColBERTer's robustness on seven high-quality out-of-domain collections,
yielding statistically significant gains over traditional retrieval baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensembling and Knowledge Distilling of Large Sequence Taggers for
  Grammatical Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Tarnavskyi, Artem Chernodub, Kostiantyn Omelianchuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate improvements to the GEC sequence tagging
architecture with a focus on ensembling of recent cutting-edge
Transformer-based encoders in Large configurations. We encourage ensembling
models by majority votes on span-level edits because this approach is tolerant
to the model architecture and vocabulary size. Our best ensemble achieves a new
SOTA result with an $F_{0.5}$ score of 76.05 on BEA-2019 (test), even without
pre-training on synthetic datasets. In addition, we perform knowledge
distillation with a trained ensemble to generate new synthetic training
datasets, "Troy-Blogs" and "Troy-1BW". Our best single sequence tagging model
that is pretrained on the generated Troy-datasets in combination with the
publicly available synthetic PIE dataset achieves a near-SOTA (To the best of
our knowledge, our best single model gives way only to much heavier T5 model
result with an $F_{0.5}$ score of 73.21 on BEA-2019 (test). The code, datasets,
and trained models are publicly available).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kratt: Developing an Automatic Subject Indexing Tool for The National
  Library of Estonia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marit Asula, Jane Makke, Linda Freienthal, Hele-Andra Kuulmets, Raul Sirel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual subject indexing in libraries is a time-consuming and costly process
and the quality of the assigned subjects is affected by the cataloguer's
knowledge on the specific topics contained in the book. Trying to solve these
issues, we exploited the opportunities arising from artificial intelligence to
develop Kratt: a prototype of an automatic subject indexing tool. Kratt is able
to subject index a book independent of its extent and genre with a set of
keywords present in the Estonian Subject Thesaurus. It takes Kratt
approximately 1 minute to subject index a book, outperforming humans 10-15
times. Although the resulting keywords were not considered satisfactory by the
cataloguers, the ratings of a small sample of regular library users showed more
promise. We also argue that the results can be enhanced by including a bigger
corpus for training the model and applying more careful preprocessing
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version. It has 12 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Scientific Claims for Zero-Shot Scientific Fact Checking <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated scientific fact checking is difficult due to the complexity of
scientific language and a lack of significant amounts of training data, as
annotation requires domain expertise. To address this challenge, we propose
scientific claim generation, the task of generating one or more atomic and
verifiable claims from scientific sentences, and demonstrate its usefulness in
zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new
supervised method for generating claims supported by the literature, as well as
KBIN, a novel method for generating claim negations. Additionally, we adapt an
existing unsupervised entity-centric method of claim generation to biomedical
claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking
demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN,
achieve up to 90% performance of fully supervised models trained on manually
annotated claims and evidence. A rigorous evaluation study demonstrates
significant improvement in generated claim and negation quality over existing
baselines
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022; 13 pages, 3 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing for Labeled Dependency Trees <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Müller-Eberstein, Rob van der Goot, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probing has become an important tool for analyzing representations in Natural
Language Processing (NLP). For graphical NLP tasks such as dependency parsing,
linear probes are currently limited to extracting undirected or unlabeled parse
trees which do not capture the full task. This work introduces DepProbe, a
linear probe which can extract labeled and directed dependency parse trees from
embeddings while using fewer parameters and compute than prior methods.
Leveraging its full task coverage and lightweight parametrization, we
investigate its predictive power for selecting the best transfer language for
training a full biaffine attention parser. Across 13 languages, our proposed
method identifies the best source treebank 94% of the time, outperforming
competitive baselines and prior work. Finally, we analyze the informativeness
of task-specific subspaces in contextual embeddings as well as which benefits a
full parser's non-linear parametrization provides.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duality-Induced Regularizer for Semantic Matching Knowledge Graph
  Embeddings <span class="chip">NeurIPS'20</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Zhanqiu Zhang, Zhihao Shi, Jianyu Cai, Shuiwang Ji, Feng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic matching models -- which assume that entities with similar semantics
have similar embeddings -- have shown great power in knowledge graph embeddings
(KGE). Many existing semantic matching models use inner products in embedding
spaces to measure the plausibility of triples and quadruples in static and
temporal knowledge graphs. However, vectors that have the same inner products
with another vector can still be orthogonal to each other, which implies that
entities with similar semantics may have dissimilar embeddings. This property
of inner products significantly limits the performance of semantic matching
models. To address this challenge, we propose a novel regularizer -- namely,
DUality-induced RegulArizer (DURA) -- which effectively encourages the entities
with similar semantics to have similar embeddings. The major novelty of DURA is
based on the observation that, for an existing semantic matching KGE model
(primal), there is often another distance based KGE model (dual) closely
associated with it, which can be used as effective constraints for entity
embeddings. Experiments demonstrate that DURA consistently and significantly
improves the performance of state-of-the-art semantic matching models on both
static and temporal knowledge graph benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI. This work is a journal extension of our NeurIPS'20
  paper arXiv:2011.05816</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Data to Mitigate Spurious Correlations in Natural Language
  Inference <span class="highlight-title">Dataset</span>s <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wu, Matt Gardner, Pontus Stenetorp, Pradeep Dasigi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing models often exploit spurious correlations
between task-independent features and labels in datasets to perform well only
within the distributions they are trained on, while not generalising to
different task distributions. We propose to tackle this problem by generating a
debiased version of a dataset, which can then be used to train a debiased,
off-the-shelf model, by simply replacing its training data. Our approach
consists of 1) a method for training data generators to generate high-quality,
label-consistent data samples; and 2) a filtering mechanism for removing data
points that contribute to spurious correlations, measured in terms of
z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and
we evaluate on a large suite of debiased, out-of-distribution, and adversarial
test sets. Results show that models trained on our debiased datasets generalise
better than those trained on the original datasets in all settings. On the
majority of the datasets, our method outperforms or performs comparably to
previous state-of-the-art debiasing strategies, and when combined with an
orthogonal technique, product-of-experts, it improves further and outperforms
previous best results of SNLI-hard and MNLI-hard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mc<span class="highlight-title">BERT</span>: Momentum <span class="highlight-title">Contrastive Learning</span> with <span class="highlight-title">BERT</span> for Zero-Shot Slot
  Filling <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seong-Hwan Heo, WonKee Lee, Jong-Hyeok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot slot filling has received considerable attention to cope with the
problem of limited available data for the target domain. One of the important
factors in zero-shot learning is to make the model learn generalized and
reliable representations. For this purpose, we present mcBERT, which stands for
momentum contrastive learning with BERT, to develop a robust zero-shot slot
filling model. mcBERT uses BERT to initialize the two encoders, the query
encoder and key encoder, and is trained by applying momentum contrastive
learning. Our experimental results on the SNIPS benchmark show that mcBERT
substantially outperforms the previous models, recording a new
state-of-the-art. Besides, we also show that each component composing mcBERT
contributes to the performance improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitasking Framework for Unsupervised Simple Definition Generation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunliang Kong, Yun Chen, Hengyuan Zhang, Liner Yang, Erhong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The definition generation task can help language learners by providing
explanations for unfamiliar words. This task has attracted much attention in
recent years. We propose a novel task of Simple Definition Generation (SDG) to
help language learners and low literacy readers. A significant challenge of
this task is the lack of learner's dictionaries in many languages, and
therefore the lack of data for supervised training. We explore this task and
propose a multitasking framework SimpDefiner that only requires a standard
dictionary with complex definitions and a corpus containing arbitrary simple
texts. We disentangle the complexity factors from the text by carefully
designing a parameter sharing scheme between two decoders. By jointly training
these components, the framework can generate both complex and simple
definitions simultaneously. We demonstrate that the framework can generate
relevant, simple definitions for the target words through automatic and manual
evaluations on English and Chinese datasets. Our method outperforms the
baseline model by a 1.77 SARI score on the English dataset, and raises the
proportion of the low level (HSK level 1-3) words in Chinese definitions by
3.87%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2022 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Rationale-Centric Framework for Human-in-the-loop Machine Learning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghui Lu, Linyi Yang, Brian Mac Namee, <span class="highlight-author">Yue Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel rationale-centric framework with human-in-the-loop --
Rationales-centric Double-robustness Learning (RDL) -- to boost model
out-of-distribution performance in few-shot learning scenarios. By using static
semi-factual generation and dynamic human-intervened correction, RDL exploits
rationales (i.e. phrases that cause the prediction), human interventions and
semi-factual augmentations to decouple spurious associations and bias models
towards generally applicable underlying distributions, which enables fast and
accurate generalisation. Experimental results show that RDL leads to
significant prediction benefits on both in-distribution and out-of-distribution
tests compared to many state-of-the-art benchmarks -- especially for few-shot
learning scenarios. We also perform extensive ablation studies to support
in-depth analyses of each component in our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mono vs Multilingual <span class="highlight-title">BERT</span>: A Case Study in Hindi and Marathi Named
  Entity Recognition <span class="chip">SC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onkar Litake, Maithili Sabane, Parth Patil, Aparna Ranade, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is the process of recognising and classifying
important information (entities) in text. Proper nouns, such as a person's
name, an organization's name, or a location's name, are examples of entities.
The NER is one of the important modules in applications like human resources,
customer support, search engines, content classification, and academia. In this
work, we consider NER for low-resource Indian languages like Hindi and Marathi.
The transformer-based models have been widely used for NER tasks. We consider
different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark
them on publicly available Hindi and Marathi NER datasets. We provide an
exhaustive comparison of different monolingual and multilingual
transformer-based models and establish simple baselines currently missing in
the literature. We show that the monolingual MahaRoBERTa model performs the
best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for
Hindi NER. We also perform cross-language evaluation and present mixed
observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICMISC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some
  benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anssi Moisio, Dejan Porjazovski, Aku Rouhe, Yaroslav Getman, Anja Virkkunen, Tamás Grósz, Krister Lindén, Mikko Kurimo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Donate Speech campaign has so far succeeded in gathering approximately
3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta
(Donate Speech) corpus. The corpus includes over twenty thousand speakers from
all the regions of Finland and from all age brackets. The primary goals of the
collection were to create a representative, large-scale resource to study
spontaneous spoken Finnish and to accelerate the development of language
technology and speech-based services. In this paper, we present the collection
process and the collected corpus, and showcase its versatility through multiple
use cases. The evaluated use cases include: automatic speech recognition of
spontaneous speech, detection of age, gender, dialect and topic and metadata
analysis. We provide benchmarks for the use cases, as well down loadable,
trained baseline systems with open-source code for reproducibility. One further
use case is to verify the metadata and transcripts given in this corpus itself,
and to suggest artificial metadata and transcripts for the part of the corpus
where it is missing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Language Resources and Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech recognition for Speech Assessment of Preschool Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Abaskohi, Fatemeh Mortazavi, Hadi Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acoustic and linguistic features of preschool speech are investigated in
this study to design an automated speech recognition (ASR) system. Acoustic
fluctuation has been highlighted as a significant barrier to developing
high-performance ASR applications for youngsters. Because of the epidemic,
preschool speech assessment should be conducted online. Accordingly, there is a
need for an automatic speech recognition system. We were confronted with new
challenges in our cognitive system, including converting meaningless words from
speech to text and recognizing word sequence. After testing and experimenting
with several models we obtained a 3.1\% phoneme error rate in Persian. Wav2Vec
2.0 is a paradigm that could be used to build a robust end-to-end speech
recognition system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Unsupervised Knowledge Transfer from Social Discussions Help
  Argument Mining? <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Dutta, Jeevesh Juneja, Dipankar Das, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying argument components from unstructured texts and predicting the
relationships expressed among them are two primary steps of argument mining.
The intrinsic complexity of these tasks demands powerful learning models. While
pretrained Transformer-based Language Models (LM) have been shown to provide
state-of-the-art results over different NLP tasks, the scarcity of manually
annotated data and the highly domain-dependent nature of argumentation restrict
the capabilities of such models. In this work, we propose a novel transfer
learning strategy to overcome these challenges. We utilize argumentation-rich
social discussions from the ChangeMyView subreddit as a source of unsupervised,
argumentative discourse-aware knowledge by finetuning pretrained LMs on a
selectively masked language modeling task. Furthermore, we introduce a novel
prompt-based strategy for inter-component relation prediction that compliments
our proposed finetuning method while leveraging on the discourse context.
Exhaustive experiments show the generalization capability of our method on
these two tasks over within-domain as well as out-of-domain datasets,
outperforming several existing and employed strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual CheckList: Generation and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthikeyan K, Shaily Bhatt, Pankaj Singh, Somak Aditya, Sandipan Dandapat, Sunayana Sitaram, Monojit Choudhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation
of NLP systems has revealed high failure rates for basic capabilities for
multiple state-of-the-art and commercial models. However, the CheckList
creation process is manual which creates a bottleneck towards creation of
multilingual CheckLists catering 100s of languages. In this work, we explore
multiple approaches to generate and evaluate the quality of Multilingual
CheckList. We device an algorithm -- Automated Multilingual Checklist
Generation (AMCG) for automatically transferring a CheckList from a source to a
target language that relies on a reasonable machine translation system. We then
compare the CheckList generated by AMCG with CheckLists generated with
different levels of human intervention. Through in-depth crosslingual
experiments between English and Hindi, and broad multilingual experiments
spanning 11 languages, we show that the automatic approach can provide accurate
estimates of failure rates of a model across capabilities, as would a
human-verified CheckList, and better than CheckLists generated by humans from
scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Effects of Leakage on Dependency Parsing <span class="chip">ACL'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Krasner, Miriam Wanner, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work by S{\o}gaard (2020) showed that, treebank size aside, overlap
between training and test graphs (termed leakage) explains more of the observed
variation in dependency parsing performance than other explanations. In this
work we revisit this claim, testing it on more models and languages. We find
that it only holds for zero-shot cross-lingual settings. We then propose a more
fine-grained measure of such leakage which, unlike the original measure, not
only explains but also correlates with observed performance variation. Code and
data are available here: https://github.com/miriamwanner/reu-nlp-project
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be presented at ACL'22 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangleing Content and Fine-grained Prosody Information via Hybrid
  ASR Bottleneck Features for Voice Conversion <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintao Zhao, Feng Liu, Changhe Song, Zhiyong Wu, Shiyin Kang, Deyi Tuo, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-parallel data voice conversion (VC) have achieved considerable
breakthroughs recently through introducing bottleneck features (BNFs) extracted
by the automatic speech recognition(ASR) model. However, selection of BNFs have
a significant impact on VC result. For example, when extracting BNFs from ASR
trained with Cross Entropy loss (CE-BNFs) and feeding into neural network to
train a VC system, the timbre similarity of converted speech is significantly
degraded. If BNFs are extracted from ASR trained using Connectionist Temporal
Classification loss (CTC-BNFs), the naturalness of the converted speech may
decrease. This phenomenon is caused by the difference of information contained
in BNFs. In this paper, we proposed an any-to-one VC method using hybrid
bottleneck features extracted from CTC-BNFs and CE-BNFs to complement each
other advantages. Gradient reversal layer and instance normalization were used
to extract prosody information from CE-BNFs and content information from
CTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate
high-quality waveform. Experimental results show that our proposed method
achieves higher similarity, naturalness, quality than baseline method and
reveals the differences between the information contained in CE-BNFs and
CTC-BNFs as well as the influence they have on the converted speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Distributional Distortion in Neural Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin LeBrun, Alessandro Sordoni, Timothy J. O'Donnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental characteristic of natural language is the high rate at which
speakers produce novel expressions. Because of this novelty, a heavy-tail of
rare events accounts for a significant amount of the total probability mass of
distributions in language (Baayen, 2001). Standard language modeling metrics
such as perplexity quantify the performance of language models (LM) in
aggregate. As a result, we have relatively little understanding of whether
neural LMs accurately estimate the probability of sequences in this heavy-tail
of rare events. To address this gap, we develop a controlled evaluation scheme
which uses generative models trained on natural data as artificial languages
from which we can exactly compute sequence probabilities. Training LMs on
generations from these artificial languages, we compare the sequence-level
probability estimates given by LMs to the true probabilities in the target
language. Our experiments reveal that LSTM and Transformer language models (i)
systematically underestimate the probability of sequences drawn from the target
language, and (ii) do so more severely for less-probable sequences.
Investigating where this probability mass went, (iii) we find that LMs tend to
overestimate the probability of ill formed (perturbed) sequences. In addition,
we find that this underestimation behaviour (iv) is weakened, but not
eliminated by greater amounts of training data, and (v) is exacerbated for
target distributions with lower entropy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying Cyber-Risky Clinical Notes by Employing Natural Language
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suzanna Schmeelk, Martins Samuel Dogo, Yifan Peng, Braja Gopal Patra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical notes, which can be embedded into electronic medical records,
document patient care delivery and summarize interactions between healthcare
providers and patients. These clinical notes directly inform patient care and
can also indirectly inform research and quality/safety metrics, among other
indirect metrics. Recently, some states within the United States of America
require patients to have open access to their clinical notes to improve the
exchange of patient information for patient care. Thus, developing methods to
assess the cyber risks of clinical notes before sharing and exchanging data is
critical. While existing natural language processing techniques are geared to
de-identify clinical notes, to the best of our knowledge, few have focused on
classifying sensitive-information risk, which is a fundamental step toward
developing effective, widespread protection of patient health information. To
bridge this gap, this research investigates methods for identifying
security/privacy risks within clinical notes. The classification either can be
used upstream to identify areas within notes that likely contain sensitive
information or downstream to improve the identification of clinical notes that
have not been entirely de-identified. We develop several models using unigram
and word2vec features with different classifiers to categorize sentence risk.
Experiments on i2b2 de-identification dataset show that the SVM classifier
using word2vec features obtained a maximum F1-score of 0.792. Future research
involves articulation and differentiation of risk in terms of different global
regulatory requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, published in Proceedings of the 55th Hawaii
  International Conference on System Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VL-Adapter: Parameter-Efficient Transfer Learning for
  <span class="highlight-title">Vision-and-Language</span> Tasks <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Sung, Jaemin Cho, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DialFact: A Benchmark for Fact-Checking in <span class="highlight-title">Dialogue</span> <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, <span class="highlight-author">Caiming Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is an essential tool to mitigate the spread of misinformation
and disinformation. We introduce the task of fact-checking in dialogue, which
is a relatively unexplored area. We construct DialFact, a testing benchmark
dataset of 22,245 annotated conversational claims, paired with pieces of
evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable
claim detection task distinguishes whether a response carries verifiable
factual information; 2) Evidence retrieval task retrieves the most relevant
Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue
response to be supported, refuted, or not enough information. We found that
existing fact-checking models trained on non-dialogue data like FEVER fail to
perform well on our task, and thus, we propose a simple yet data-efficient
solution to effectively improve fact-checking performance in dialogue. We point
out unique challenges in DialFact such as handling the colloquialisms,
coreferences and retrieval ambiguities in the error analysis to shed light on
future research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the proper role of linguistically-oriented deep net analysis in
  linguistic theorizing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.08694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.08694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A lively research field has recently emerged that uses experimental methods
to probe the linguistic behavior of modern deep networks. While work in this
tradition often reports intriguing results about the grammatical skills of deep
nets, it is not clear what their implications for linguistic theorizing should
be. As a consequence, linguistically-oriented deep net analysis has had very
little impact on linguistics at large. In this chapter, I suggest that deep
networks should be treated as theories making explicit predictions about the
acceptability of linguistic utterances. I argue that, if we overcome some
obstacles standing in the way of seriously pursuing this idea, we will gain a
powerful new theoretical tool, complementary to mainstream algebraic
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in collective volume on Algebraic Systems and the
  Representation of Linguistic Knowledge, editor: Shalom Lappin, Taylor &
  Francis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Hate Speech with <span class="highlight-title">GPT</span>-3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.12407v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.12407v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke-Li Chiu, Annie Collins, Rohan Alexander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sophisticated language models such as OpenAI's GPT-3 can generate hateful
text that targets marginalized groups. Given this capacity, we are interested
in whether large language models can be used to identify hate speech and
classify text as sexist or racist. We use GPT-3 to identify sexist and racist
text passages with zero-, one-, and few-shot learning. We find that with zero-
and one-shot learning, GPT-3 can identify sexist or racist text with an average
accuracy between 55 per cent and 67 per cent, depending on the category of text
and type of learning. With few-shot learning, the model's accuracy can be as
high as 85 per cent. Large language models have a role to play in hate speech
detection, and with further development they could eventually be used to
counter hate speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 1 figure, 23 tables 24 March 2022: Re-submission changes
  the modelling to occur multiple times and adds standard errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MReD: A Meta-<span class="highlight-title">Review</span> <span class="highlight-title">Dataset</span> for Controllable Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, Luo Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited. A typical example is
when using CNN/Daily Mail dataset for controllable text summarization, there is
no guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and the control signal to guide
the generation, which can only be built with a deep understanding of the domain
knowledge. Motivated by this vision, our paper introduces a new text generation
dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its
45k meta-review sentences are manually annotated with one of the 9 carefully
defined categories, including abstract, strength, decision, etc. We present
experimental results on start-of-the-art summarization models, and propose
methods for structure-controlled generation with both extractive and
abstractive models using our annotated data. By exploring various settings and
analyzing the model behavior with respect to the control signal, we demonstrate
the challenges of our proposed task and the values of our dataset MReD.
Meanwhile, MReD also allows us to have a better understanding of the
meta-review domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepSTL -- From English Requirements to Signal Temporal Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.10294v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.10294v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie He, Ezio Bartocci, Dejan Ničković, Haris Isakovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
  In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ g2pW: A Conditional Weighted Softmax <span class="highlight-title">BERT</span> for Polyphone Disambiguation
  in Mandarin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chang Chen, Yu-Chuan Chang, Yen-Cheng Chang, Yi-Ren Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Insterspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Exploring the Efficacy of Automatically Generated Counterfactuals for
  Sentiment Analysis <span class="chip">ACL-21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.15231v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.15231v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyi Yang, Jiazheng Li, Pádraig Cunningham, <span class="highlight-author">Yue Zhang</span>, Barry Smyth, Ruihai Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While state-of-the-art NLP models have been achieving the excellent
performance of a wide range of tasks in recent years, important questions are
being raised about their robustness and their underlying sensitivity to
systematic biases that may exist in their training and test data. Such issues
come to be manifest in performance problems when faced with out-of-distribution
data in the field. One recent solution has been to use counterfactually
augmented datasets in order to reduce any reliance on spurious patterns that
may exist in the original data. Producing high-quality augmented data can be
costly and time-consuming as it usually needs to involve human feedback and
crowdsourcing efforts. In this work, we propose an alternative by describing
and evaluating an approach to automatically generating counterfactual data for
data augmentation and explanation. A comprehensive evaluation on several
different datasets and using a variety of state-of-the-art benchmarks
demonstrate how our approach can achieve significant improvements in model
performance when compared to models training on the original data and even when
compared to models trained with the benefit of human-generated augmented data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL-21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALRACE: Reading Comprehension <span class="highlight-title">Dataset</span> with Human Data and Labeled
  Rationales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.12373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.12373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Zou, Yuran Zhang, Peiqing Jin, Cheng Luo, Xunyi Pan, Nai Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models achieves high performance on machine reading
comprehension (MRC) tasks but the results are hard to explain. An appealing
approach to make models explainable is to provide rationales for its decision.
To investigate whether human rationales can further improve current models and
to facilitate supervised learning of human rationales, here we present PALRACE
(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for
800 passages selected from the RACE dataset. We further classified the question
to each passage into 6 types. Each passage was read by at least 26 human
readers, who labeled their rationales to answer the question. It is
demonstrated that models such as RoBERTa-large outperforms human readers in all
6 types of questions, including inference questions, but its performance can be
further improved when having access to the human rationales. Simpler models and
pre-trained models that are not fine-tuned based on the task benefit more from
human rationales, and their performance can be boosted by more than 30% by
rationales. With access to human rationales, a simple model based on the GloVe
word embedding can reach the performance of BERT-base.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show and Write: Entity-aware Article Generation with Image Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.05917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.05917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongping Zhang, Yiwen Gu, Bryan A. Plummer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many vision-language applications contain long articles of text paired with
images (e.g., news or Wikipedia articles). Prior work learning to encode and/or
generate these articles has primarily focused on understanding the article
itself and some related metadata like the title or date it was written.
However, the images and their captions or alt-text often contain crucial
information such as named entities that are difficult to be correctly
recognized and predicted by language models. To address this shortcoming, this
paper introduces an ENtity-aware article Generation method with Image
iNformation, ENGIN, to incorporate an article's image information into language
models. ENGIN represents articles that can be conditioned on metadata used by
prior work and information such as captions and named entities extracted from
images. Our key contribution is a novel Entity-aware mechanism to help our
model better recognize and predict the entity names in articles. We perform
experiments on three public datasets, GoodNews, VisualNews, and WikiText.
Quantitative results show that our approach improves generated article
perplexity by 4-5 points over the base models. Qualitative results demonstrate
the text generated by ENGIN is more consistent with embedded article images. We
also perform article quality annotation experiments on the generated articles
to validate that our model produces higher-quality articles. Finally, we
investigate the effect ENGIN has on methods that automatically detect
machine-generated articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewang Zhang, Yibin Zheng, Xinhui Li, Li Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. Both quantitative and qualitative evaluation results demonstrate
the effectiveness of WeSinger in terms of accuracy and naturalness, and
WeSinger achieves state-of-the-art performance on the public corpus Opencpop.
Some synthesized singing samples are available online
(https://zzw922cn.github.io/WeSinger/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to InterSpeech2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IAM: A Comprehensive and Large-Scale <span class="highlight-title">Dataset</span> for Integrated Argument
  Mining Tasks <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liying Cheng, Lidong Bing, Ruidan He, Qian Yu, Yan Zhang, Luo Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, a debate usually requires a manual preparation process,
including reading plenty of articles, selecting the claims, identifying the
stances of the claims, seeking the evidence for the claims, etc. As the AI
debate attracts more attention these years, it is worth exploring the methods
to automate the tedious process involved in the debating system. In this work,
we introduce a comprehensive and large dataset named IAM, which can be applied
to a series of argument mining tasks, including claim extraction, stance
classification, evidence extraction, etc. Our dataset is collected from over 1k
articles related to 123 topics. Near 70k sentences in the dataset are fully
annotated based on their argument properties (e.g., claims, stances, evidence,
etc.). We further propose two new integrated argument mining tasks associated
with the debate preparation process: (1) claim extraction with stance
classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a
pipeline approach and an end-to-end method for each integrated task separately.
Promising experimental results are reported to show the values and challenges
of our proposed tasks, and motivate future research on argument mining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, accepted by ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCP Theorems, SETH and More: Towards Proving Sub-linear Time
  Inapproximability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.02320v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.02320v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengzhao Ma, Jianzhong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose the PCP-like theorem for sub-linear time
inapproximability. Abboud et al. have devised the distributed PCP framework for
sub-quadratic time inapproximability. We show that the distributed PCP theorem
can be generalized for proving arbitrary polynomial time inapproximability, but
fails in the linear case. We prove the sub-linear PCP theorem by adapting from
an MA-protocol for the Set Containment problem, and show how to use the theorem
to prove both existing and new inapproximability results, exhibiting the power
of the sub-linear PCP theorem. Considering the emerging research works on
sub-linear time algorithms, the sub-linear PCP theorem is important in guiding
the research in sub-linear time approximation algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is an old version of another paper submitted to arxiv,
  with id 2107.01520. Moreover, this paper contains mistakes. Theorem 5.1 in
  this paper is wrong. The reason is that Merlin can make Alice believe q\in S
  by sending another q'\ne q but q'\in S. Another player Bob should be included
  in the communication protocol to avoid this situation. The paper 2107.01520
  fixed this problem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning From Human Correction For Data-Centric Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.00225v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.00225v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% accuracy is trained on the corrected dataset, which improve the baseline
from 83.3% to 91.7%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FormNet: Structural Encoding beyond Sequential Modeling in Form Document
  Information Extraction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence modeling has demonstrated state-of-the-art performance on natural
language and document understanding tasks. However, it is challenging to
correctly serialize tokens in form-like documents in practice due to their
variety of layout patterns. We propose FormNet, a structure-aware sequence
model to mitigate the suboptimal serialization of forms. First, we design Rich
Attention that leverages the spatial relationship between tokens in a form for
more precise attention score calculation. Second, we construct Super-Tokens for
each word by embedding representations from their neighboring tokens through
graph convolutions. FormNet therefore explicitly recovers local syntactic
information that may have been lost during serialization. In experiments,
FormNet outperforms existing methods with a more compact model size and less
pre-training data, establishing new state-of-the-art performance on CORD, FUNSD
and Payment benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for
  Monocular Object Pose Estimation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansheng Chen, Pichao Wang, Fan Wang, Wei Tian, Lu Xiong, Hao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is
a long-standing problem in computer vision. Driven by end-to-end deep learning,
recent studies suggest interpreting PnP as a differentiable layer, so that
2D-3D point correspondences can be partly learned by backpropagating the
gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D
points from scratch fails to converge with existing approaches, since the
deterministic pose is inherently non-differentiable. In this paper, we propose
the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,
which outputs a distribution of pose on the SE(3) manifold, essentially
bringing categorical Softmax to the continuous domain. The 2D-3D coordinates
and corresponding weights are treated as intermediate variables learned by
minimizing the KL divergence between the predicted and target pose
distribution. The underlying principle unifies the existing approaches and
resembles the attention mechanism. EPro-PnP significantly outperforms
competitive baselines, closing the gap between PnP-based method and the
task-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D
object detection benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Instance Segmentation via Multi-scale Spatio-temporal Split
  Attention <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omkar Thawakar, Sanath Narayan, Jiale Cao, Hisham Cholakkal, Rao Muhammad Anwer, Muhammad Haris Khan, Salman Khan, Michael Felsberg, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art transformer-based video instance segmentation (VIS)
approaches typically utilize either single-scale spatio-temporal features or
per-frame multi-scale features during the attention computations. We argue that
such an attention computation ignores the multi-scale spatio-temporal feature
relationships that are crucial to tackle target appearance deformations in
videos. To address this issue, we propose a transformer-based VIS framework,
named MS-STS VIS, that comprises a novel multi-scale spatio-temporal split
(MS-STS) attention module in the encoder. The proposed MS-STS module
effectively captures spatio-temporal feature relationships at multiple scales
across frames in a video. We further introduce an attention block in the
decoder to enhance the temporal consistency of the detected instances in
different frames of a video. Moreover, an auxiliary discriminator is introduced
during training to ensure better foreground-background separability within the
multi-scale spatio-temporal feature space. We conduct extensive experiments on
two benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves
state-of-the-art performance on both benchmarks. When using the ResNet50
backbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best
reported results in literature by 2.7 % and by 4.8 % at higher overlap
threshold of AP_75, while being comparable in model size and speed on
Youtube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS
achieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models
are available at https://github.com/OmkarThawakar/MSSTS-VIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient
  Dexterous Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sridhar Pandian Arunachalam, Sneha Silwal, Ben Evans, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Tracking <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyi Zhou, Tianwei Yin, Vladlen Koltun, Phillip Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel transformer-based architecture for global multi-object
tracking. Our network takes a short sequence of frames as input and produces
global trajectories for all objects. The core component is a global tracking
transformer that operates on objects from all frames in the sequence. The
transformer encodes object features from all frames, and uses trajectory
queries to group them into trajectories. The trajectory queries are object
features from a single frame and naturally produce unique trajectories. Our
global tracking transformer does not require intermediate pairwise grouping or
combinatorial association, and can be jointly trained with an object detector.
It achieves competitive performance on the popular MOT17 benchmark, with 75.3
MOTA and 59.1 HOTA. More importantly, our framework seamlessly integrates into
state-of-the-art large-vocabulary detectors to track any objects. Experiments
on the challenging TAO dataset show that our framework consistently improves
upon baselines that are based on pairwise association, outperforming published
works by a significant 7.7 tracking mAP. Code is available at
https://github.com/xingyizhou/GTR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. Code is available at https://github.com/xingyizhou/GTR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BigDetection: A Large-scale Benchmark for Improved Object Detector
  <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Likun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple datasets and open challenges for object detection have been
introduced in recent years. To build more general and powerful object detection
systems, in this paper, we construct a new large-scale benchmark termed
BigDetection. Our goal is to simply leverage the training data from existing
datasets (LVIS, OpenImages and Object365) with carefully designed principles,
and curate a larger dataset for improved detector pre-training. Specifically,
we generate a new taxonomy which unifies the heterogeneous label spaces from
different sources. Our BigDetection dataset has 600 object categories and
contains over 3.4M training images with 36M bounding boxes. It is much larger
in multiple dimensions than previous benchmarks, which offers both
opportunities and challenges. Extensive experiments demonstrate its validity as
a new benchmark for evaluating different object detection methods, and its
effectiveness as a pre-training dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, code is released at
  https://github.com/amazon-research/bigdetection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on StyleGAN show high performance on artistic portrait
generation by transfer learning with limited data. In this paper, we explore
more challenging exemplar-based high-resolution portrait style transfer by
introducing a novel DualStyleGAN with flexible control of dual styles of the
original face domain and the extended artistic portrait domain. Different from
StyleGAN, DualStyleGAN provides a natural way of style transfer by
characterizing the content and style of a portrait with an intrinsic style path
and a new extrinsic style path, respectively. The delicately designed extrinsic
style path enables our model to modulate both the color and complex structural
styles hierarchically to precisely pastiche the style example. Furthermore, a
novel progressive fine-tuning scheme is introduced to smoothly transform the
generative space of the model to the target domain, even with the above
modifications on the network architecture. Experiments demonstrate the
superiority of DualStyleGAN over state-of-the-art methods in high-quality
portrait style transfer and flexible style control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. Code: https://github.com/williamyang1991/DualStyleGAN
  Project page: https://www.mmlab-ntu.com/project/dualstylegan/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRNet: Learning the Rectified Virtual Corresponding Points for 3D Point
  Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Zhang, Jiadai Sun, Yuchao Dai, Bin Fan, Mingyi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud registration is fragile to outliers, which are labeled as the
points without corresponding points. To handle this problem, a widely adopted
strategy is to estimate the relative pose based only on some accurate
correspondences, which is achieved by building correspondences on the
identified inliers or by selecting reliable ones. However, these approaches are
usually complicated and time-consuming. By contrast, the virtual point-based
methods learn the virtual corresponding points (VCPs) for all source points
uniformly without distinguishing the outliers and the inliers. Although this
strategy is time-efficient, the learned VCPs usually exhibit serious collapse
degeneration due to insufficient supervision and the inherent distribution
limitation. In this paper, we propose to exploit the best of both worlds and
present a novel robust 3D point cloud registration framework. We follow the
idea of the virtual point-based methods but learn a new type of virtual points
called rectified virtual corresponding points (RCPs), which are defined as the
point set with the same shape as the source and with the same pose as the
target. Hence, a pair of consistent point clouds, i.e. source and RCPs, is
formed by rectifying VCPs to RCPs (VRNet), through which reliable
correspondences between source and RCPs can be accurately obtained. Since the
relative pose between source and RCPs is the same as the relative pose between
source and target, the input point clouds can be registered naturally.
Specifically, we first construct the initial VCPs by using an estimated soft
matching matrix to perform a weighted average on the target points. Then, we
design a correction-walk module to learn an offset to rectify VCPs to RCPs,
which effectively breaks the distribution limitation of VCPs. Finally, we
develop a hybrid loss function to enforce the shape and geometry structure
consistency ...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Representation Separation Perspective to Correspondences-free
  Unsupervised 3D Point Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Zhang, Jiadai Sun, Yuchao Dai, Dingfu Zhou, Xibin Song, Mingyi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud registration in remote sensing field has been greatly advanced
by deep learning based methods, where the rigid transformation is either
directly regressed from the two point clouds (correspondences-free approaches)
or computed from the learned correspondences (correspondences-based
approaches). Existing correspondences-free methods generally learn the holistic
representation of the entire point cloud, which is fragile for partial and
noisy point clouds. In this paper, we propose a correspondences-free
unsupervised point cloud registration (UPCR) method from the representation
separation perspective. First, we model the input point cloud as a combination
of pose-invariant representation and pose-related representation. Second, the
pose-related representation is used to learn the relative pose wrt a "latent
canonical shape" for the source and target point clouds respectively. Third,
the rigid transformation is obtained from the above two learned relative poses.
Our method not only filters out the disturbance in pose-invariant
representation but also is robust to partial-to-partial point clouds or noise.
Experiments on benchmark datasets demonstrate that our unsupervised method
achieves comparable if not better performance than state-of-the-art supervised
registration methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Geoscience and Remote Sensing Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Open-set Recognition via Augmentation-based Similarity Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepideh Esmaeilpour, Lei shu, <span class="highlight-author">Bing Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary assumption of conventional supervised learning or classification
is that the test samples are drawn from the same distribution as the training
samples, which is called closed set learning or classification. In many
practical scenarios, this is not the case because there are unknowns or unseen
class samples in the test data, which is called the open set scenario, and the
unknowns need to be detected. This problem is referred to as the open set
recognition problem and is important in safety-critical applications. We
propose to detect unknowns (or unseen class samples) through learning pairwise
similarities. The proposed method works in two steps. It first learns a closed
set classifier using the seen classes that have appeared in training and then
learns how to compare seen classes with pseudo-unseen (automatically generated
unseen class samples). The pseudo-unseen generation is carried out by
performing distribution shifting augmentations on the seen or training samples.
We call our method OPG (Open set recognition based on Pseudo unseen data
Generation). The experimental evaluation shows that the learned
similarity-based features can successfully distinguish seen from unseen in
benchmark datasets for open set recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Expression Recognition based on Multi-head Cross Attention
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-Yeop Jeong, Yeong-Gi Hong, Daun Kim, Yuchul Jung, Jin-Woo Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression in-the-wild is essential for various interactive computing
domains. In this paper, we proposed an extended version of DAN model to address
the VA estimation and facial expression challenges introduced in ABAW 2022. Our
method produced preliminary results of 0.44 of mean CCC value for the VA
estimation task, and 0.33 of the average F1 score for the expression
classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Neighbor Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Kolkin, Michal Kucera, Sylvain Paris, Daniel Sykora, Eli Shechtman, Greg Shakhnarovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers
state-of-the-art quality, generalization, and competitive efficiency for
artistic style transfer. Our approach is based on explicitly replacing neural
features extracted from the content input (to be stylized) with those from a
style exemplar, then synthesizing the final output based on these rearranged
features. While the spirit of our approach is similar to prior work, we show
that our design decisions dramatically improve the final visual quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code for NNST-Opt available at
  https://github.com/nkolkin13/NeuralNeighborStyleTransfer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Perturbation Constrained Adversarial Attack for Evaluating the
  Robustness of Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jenny Schmalfuss, Philipp Scholze, Andrés Bruhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent optical flow methods are almost exclusively judged in terms of
accuracy, while analyzing their robustness is often neglected. Although
adversarial attacks offer a useful tool to perform such an analysis, current
attacks on optical flow methods rather focus on real-world attacking scenarios
than on a worst case robustness assessment. Hence, in this work, we propose a
novel adversarial attack - the Perturbation Constrained Flow Attack (PCFA) -
that emphasizes destructivity over applicability as a real-world attack. More
precisely, PCFA is a global attack that optimizes adversarial perturbations to
shift the predicted flow towards a specified target flow, while keeping the L2
norm of the perturbation below a chosen bound. Our experiments not only
demonstrate PCFA's applicability in white- and black-box settings, but also
show that it finds stronger adversarial samples for optical flow than previous
attacking frameworks. Moreover, based on these strong samples, we provide the
first common ranking of optical flow methods in the literature considering both
prediction quality and adversarial robustness, indicating that high quality
methods are not necessarily robust. Our source code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Motion Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federica Arrigoni, Willi Menapace, Marcel Seelbach Benkner, Elisa Ricci, Vladislav Golyanik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion segmentation is a challenging problem that seeks to identify
independent motions in two or several input images. This paper introduces the
first algorithm for motion segmentation that relies on adiabatic quantum
optimization of the objective function. The proposed method achieves on-par
performance with the state of the art on problem instances which can be mapped
to modern quantum annealers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Exemplar-Free Continual Learning in Vision <span class="highlight-title">Transformer</span>s: an
  Account of Attention, Functional and Weight Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Pelosin, Saurav Jha, Andrea Torsello, Bogdan Raducanu, Joost van de Weijer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Video-centralised <span class="highlight-title">Transformer</span> for Video Face Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujiang Wang, Mingzhi Dong, Jie Shen, Yiming Luo, Yiming Lin, Pingchuan Ma, Stavros Petridis, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel method for face clustering in videos using a
video-centralised transformer. Previous works often employed contrastive
learning to learn frame-level representation and used average pooling to
aggregate the features along the temporal dimension. This approach may not
fully capture the complicated video dynamics. In addition, despite the recent
progress in video-based contrastive learning, few have attempted to learn a
self-supervised clustering-friendly face representation that benefits the video
face clustering task. To overcome these limitations, our method employs a
transformer to directly learn video-level representations that can better
reflect the temporally-varying property of faces in videos, while we also
propose a video-centralised self-supervised framework to train the transformer
model. We also investigate face clustering in egocentric videos, a
fast-emerging field that has not been studied yet in works related to face
clustering. To this end, we present and release the first large-scale
egocentric video face clustering dataset named EasyCom-Clustering. We evaluate
our proposed method on both the widely used Big Bang Theory (BBT) dataset and
the new EasyCom-Clustering dataset. Results show the performance of our
video-centralised transformer has surpassed all previous state-of-the-art
methods on both benchmarks, exhibiting a self-attentive understanding of face
videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Hierarchical <span class="highlight-title">Cross-Modal</span> Association for Co-Speech Gesture
  Generation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, <span class="highlight-author">Bolei Zhou</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating speech-consistent body and gesture movements is a long-standing
problem in virtual avatar creation. Previous studies often synthesize pose
movement in a holistic manner, where poses of all joints are generated
simultaneously. Such a straightforward pipeline fails to generate fine-grained
co-speech gestures. One observation is that the hierarchical semantics in
speech and the hierarchical structures of human gestures can be naturally
described into multiple granularities and associated together. To fully utilize
the rich connections between speech audio and human gestures, we propose a
novel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech
gesture generation. In HA2G, a Hierarchical Audio Learner extracts audio
representations across semantic granularities. A Hierarchical Pose Inferer
subsequently renders the entire human pose gradually in a hierarchical manner.
To enhance the quality of synthesized gestures, we develop a contrastive
learning strategy based on audio-text alignment for better audio
representations. Extensive experiments and human evaluation demonstrate that
the proposed method renders realistic co-speech gestures and outperforms
previous methods in a clear margin. Project page:
https://alvinliu0.github.io/projects/HA2G
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2022. Camera-Ready Version, 19 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-based Learning of Parameterized Thermodynamics from Real-time
  Thermography <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza El-Kebir, Joseph Bentsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in automatic control of thermal processes has long been limited by
the difficulty of obtaining high-fidelity thermodynamic models. Traditionally,
in complex thermodynamic systems, it is often infeasible to estimate the
thermophysical parameters of spatiotemporally varying processes, forcing the
adoption of model-free control architectures. This comes at the cost of losing
any robustness guarantees, and implies a need for extensive real-life testing.
In recent years, however, infrared cameras and other thermographic equipment
have become readily applicable to these processes, allowing for a real-time,
non-invasive means of sensing the thermal state of a process. In this work, we
present a novel physics-based approach to learning a thermal process's dynamics
directly from such real-time thermographic data, while focusing attention on
regions with high thermal activity. We call this process, which applies to any
higher-dimensional scalar field, attention-based noise robust averaging (ANRA).
Given a partial-differential equation model structure, we show that our
approach is robust against noise, and can be used to initialize optimization
routines to further refine parameter estimates. We demonstrate our method on
several simulation examples, as well as by applying it to electrosurgical
thermal response data on in vivo porcine skin tissue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Conference in Image Processing (ICIP)
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, <span class="highlight-author">Devi Parikh</span>, Yaniv Taigman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image generation methods provide a simple yet exciting
conversion capability between text and image domains. While these methods have
incrementally improved the generated image fidelity and text relevancy, several
pivotal gaps remain unanswered, limiting applicability and quality. We propose
a novel text-to-image method that addresses these gaps by (i) enabling a simple
control mechanism complementary to text in the form of a scene, (ii)
introducing elements that substantially improve the tokenization process by
employing domain-specific knowledge over key image regions (faces and salient
objects), and (iii) adapting classifier-free guidance for the transformer use
case. Our model achieves state-of-the-art FID and human evaluation results,
unlocking the ability to generate high fidelity images in a resolution of
512x512 pixels, significantly improving visual quality. Through scene
controllability, we introduce several new capabilities: (i) Scene editing, (ii)
text editing with anchor scenes, (iii) overcoming out-of-distribution text
prompts, and (iv) story illustration generation, as demonstrated in the story
we wrote.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moving Window Regression: A Novel Approach to Ordinal Regression <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nyeong-Ho Shin, Seon-Ho Lee, Chang-Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel ordinal regression algorithm, called moving window regression (MWR),
is proposed in this paper. First, we propose the notion of relative rank
($\rho$-rank), which is a new order representation scheme for input and
reference instances. Second, we develop global and local relative regressors
($\rho$-regressors) to predict $\rho$-ranks within entire and specific rank
ranges, respectively. Third, we refine an initial rank estimate iteratively by
selecting two reference instances to form a search window and then estimating
the $\rho$-rank within the window. Extensive experiments results show that the
proposed algorithm achieves the state-of-the-art performances on various
benchmark datasets for facial age estimation and historical color image
classification. The codes are available at https://github.com/nhshin-mcl/MWR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature visualization for convolutional neural network models trained on
  neuroimaging data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Eitel, Anna Melkonyan, Kerstin Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major prerequisite for the application of machine learning models in
clinical decision making is trust and interpretability. Current explainability
studies in the neuroimaging community have mostly focused on explaining
individual decisions of trained models, e.g. obtained by a convolutional neural
network (CNN). Using attribution methods such as layer-wise relevance
propagation or SHAP heatmaps can be created that highlight which regions of an
input are more relevant for the decision than others. While this allows the
detection of potential data set biases and can be used as a guide for a human
expert, it does not allow an understanding of the underlying principles the
model has learned. In this study, we instead show, to the best of our
knowledge, for the first time results using feature visualization of
neuroimaging CNNs. Particularly, we have trained CNNs for different tasks
including sex classification and artificial lesion classification based on
structural magnetic resonance imaging (MRI) data. We have then iteratively
generated images that maximally activate specific neurons, in order to
visualize the patterns they respond to. To improve the visualizations we
compared several regularization strategies. The resulting images reveal the
learned concepts of the artificial lesions, including their shapes, but remain
hard to interpret for abstract features in the sex classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-ray Dissectography Improves Lung Nodule Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Niu, Giridhar Dasegowda, Pingkun Yan, Mannudeep K. Kalra, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although radiographs are the most frequently used worldwide due to their
cost-effectiveness and widespread accessibility, the structural superposition
along the x-ray paths often renders suspicious or concerning lung nodules
difficult to detect. In this study, we apply "X-ray dissectography" to dissect
lungs digitally from a few radiographic projections, suppress the interference
of irrelevant structures, and improve lung nodule detectability. For this
purpose, a collaborative detection network is designed to localize lung nodules
in 2D dissected projections and 3D physical space. Our experimental results
show that our approach can significantly improve the average precision by 20+%
in comparison with the common baseline that detects lung nodules from original
projections using a popular detection network. Potentially, this approach could
help re-design the current X-ray imaging protocols and workflows and improve
the diagnostic performance of chest radiographs in lung diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Egocentric Prediction of Action Target in 3D <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Ziang Cao, Andrew Liang, Benjamin Liang, Luoyao Chen, <span class="highlight-author">Hang Zhao</span>, Chen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R-DFCIL: Relation-Guided Representation Learning for Data-Free Class
  Incremental Learning <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiankun Gao, Chen Zhao, Bernard Ghanem, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) struggles with catastrophic forgetting when
learning new knowledge, and Data-Free CIL (DFCIL) is even more challenging
without access to the training data of previous classes. Though recent DFCIL
works introduce techniques such as model inversion to synthesize data for
previous classes, they fail to overcome forgetting due to the severe domain gap
between the synthetic and real data. To address this issue, this paper proposes
relation-guided representation learning (RRL) for DFCIL, dubbed R-DFCIL. In
RRL, we introduce relational knowledge distillation to flexibly transfer the
structural relation of new data from the old model to the current model. Our
RRL-boosted DFCIL can guide the current model to learn representations of new
classes better compatible with representations of previous classes, which
greatly reduces forgetting while improving plasticity. To avoid the mutual
interference between representation and classifier learning, we employ local
rather than global classification loss during RRL. After RRL, the
classification head is fine-tuned with global class-balanced classification
loss to address the data imbalance issue as well as learn the decision boundary
between new and previous classes. Extensive experiments on CIFAR100,
Tiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL significantly
surpasses previous approaches and achieves a new state-of-the-art performance
for DFCIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures, submitted to the ECCV for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IA-FaceS, Bidirectional Method, Disentangled Attribute Manipulation,
  Flexible Component Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Huang, Shikui Tu, Lei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic face editing has achieved substantial progress in recent years.
Known as a growingly popular method, latent space manipulation performs face
editing by changing the latent code of an input face to liberate users from
painting skills. However, previous latent space manipulation methods usually
encode an entire face into a single low-dimensional embedding, which constrains
the reconstruction capacity and the control flexibility of facial components,
such as eyes and nose. This paper proposes IA-FaceS as a bidirectional method
for disentangled face attribute manipulation as well as flexible, controllable
component editing without the need for segmentation masks or sketches in the
original image. To strike a balance between the reconstruction capacity and the
control flexibility, the encoder is designed as a multi-head structure to yield
embeddings for reconstruction and control, respectively: a high-dimensional
tensor with spatial properties for consistent reconstruction and four
low-dimensional facial component embeddings for semantic face editing.
Manipulating the separate component embeddings can help achieve disentangled
attribute manipulation and flexible control of facial components. To further
disentangle the highly-correlated components, a component adaptive modulation
(CAM) module is proposed for the decoder. The semantic single-eye editing is
developed for the first time without any input visual guidance, such as
segmentation masks or sketches. According to the experimental results, IA-FaceS
establishes a good balance between maintaining image details and performing
flexible face manipulation. Both quantitative and qualitative results indicate
that the proposed method outperforms the other techniques in reconstruction,
face attribute manipulation, and component transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>68 pages, 33 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Preliminary Research on Space Situational Awareness Based on Event
  Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Xiao, Pengju Li, Guohui Wang, Zhi Li, Yi Chen, Yongfeng Xie, Yuqiang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event camera is a new type of sensor that is different from traditional
cameras. Each pixel is triggered asynchronously by an event. The trigger event
is the change of the brightness irradiated on the pixel. If the increment or
decrement is higher than a certain threshold, the event is output. Compared
with traditional cameras, event cameras have the advantages of high temporal
resolution, low latency, high dynamic range, low bandwidth and low power
consumption. We carried out a series of observation experiments in a simulated
space lighting environment. The experimental results show that the event camera
can give full play to the above advantages in space situational awareness. This
article first introduces the basic principles of the event camera, then
analyzes its advantages and disadvantages, then introduces the observation
experiment and analyzes the experimental results, and finally, a workflow of
space situational awareness based on event cameras is given.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:1904.08405 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AziNorm: Exploiting the Radial Symmetry of Point Cloud for
  Azimuth-Normalized 3D Perception <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Wenqiang Zhang, Qian Zhang, Chang Huang, Wenyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying the inherent symmetry of data is of great importance in machine
learning. Point cloud, the most important data format for 3D environmental
perception, is naturally endowed with strong radial symmetry. In this work, we
exploit this radial symmetry via a divide-and-conquer strategy to boost 3D
perception performance and ease optimization. We propose Azimuth Normalization
(AziNorm), which normalizes the point clouds along the radial direction and
eliminates the variability brought by the difference of azimuth. AziNorm can be
flexibly incorporated into most LiDAR-based perception methods. To validate its
effectiveness and generalization ability, we apply AziNorm in both object
detection and semantic segmentation. For detection, we integrate AziNorm into
two representative detection methods, the one-stage SECOND detector and the
state-of-the-art two-stage PV-RCNN detector. Experiments on Waymo Open Dataset
demonstrate that AziNorm improves SECOND and PV-RCNN by 7.03 mAPH and 3.01 mAPH
respectively. For segmentation, we integrate AziNorm into KPConv. On
SemanticKitti dataset, AziNorm improves KPConv by 1.6/1.1 mIoU on val/test set.
Besides, AziNorm remarkably improves data efficiency and accelerates
convergence, reducing the requirement of data amounts or training epochs by an
order of magnitude. SECOND w/ AziNorm can significantly outperform fully
trained vanilla SECOND, even trained with only 10% data or 10% epochs. Code and
models are available at https://github.com/hustvl/AziNorm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitask Emotion Recognition Model with Knowledge <span class="highlight-title">Distillation</span> and Task
  Discriminator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euiseok Jeong, Geesung Oh, Sejoon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the collection of big data and the development of deep learning,
research to predict human emotions in the wild is being actively conducted. We
designed a multi-task model using ABAW dataset to predict valence-arousal,
expression, and action unit through audio data and face images at in real
world. We trained model from the incomplete label by applying the knowledge
distillation technique. The teacher model was trained as a supervised learning
method, and the student model was trained by using the output of the teacher
model as a soft label. As a result we achieved 2.40 in Multi Task Learning task
validation dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIFT and SURF based feature extraction for the anomaly detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Bilik, Karel Horak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we suggest a way, how to use SIFT and SURF algorithms to
extract the image features for anomaly detection. We use those feature vectors
to train various classifiers on a real-world dataset in the semi -supervised
(with a small number of faulty samples) manner with a large number of
classifiers and in the one-class (with no faulty samples) manner using the SVDD
and SVM classifier. We prove, that the SIFT and SURF algorithms could be used
as feature extractors, that they could be used to train a semi-supervised and
one-class classifier with an accuracy around 89\% and that the performance of
the one-class classifier could be comparable to the semi-supervised one. We
also made our dataset and source code publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28th Conference STUDENT EEICT 2022, Brno University of Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bailando: 3D Dance Generation by Actor-Critic <span class="highlight-title">GPT</span> with Choreographic
  Memory <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driving 3D characters to dance following a piece of music is highly
challenging due to the spatial constraints applied to poses by choreography
norms. In addition, the generated dance sequence also needs to maintain
temporal coherency with different music genres. To tackle these challenges, we
propose a novel music-to-dance framework, Bailando, with two powerful
components: 1) a choreographic memory that learns to summarize meaningful
dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic
Generative Pre-trained Transformer (GPT) that composes these units to a fluent
dance coherent to the music. With the learned choreographic memory, dance
generation is realized on the quantized units that meet high choreography
standards, such that the generated dancing sequences are confined within the
spatial constraints. To achieve synchronized alignment between diverse motion
tempos and music beats, we introduce an actor-critic-based reinforcement
learning scheme to the GPT with a newly-designed beat-align reward function.
Extensive experiments on the standard benchmark demonstrate that our proposed
framework achieves state-of-the-art performance both qualitatively and
quantitatively. Notably, the learned choreographic memory is shown to discover
human-interpretable dancing-style poses in an unsupervised manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial
  Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanglei Xue, Zichang Tan, Yu Zhu, Zhongsong Ma, Guodong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition plays an important role in human-computer
interaction. In this paper, we propose the Coarse-to-Fine Cascaded networks
with Smooth Predicting (CFC-SP) to improve the performance of facial expression
recognition. CFC-SP contains two core components, namely Coarse-to-Fine
Cascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups
several similar emotions to form a rough category, and then employs a network
to conduct a coarse but accurate classification. Later, Then, an additional
network for these grouped emotions is further used to obtain fine-grained
predictions. For SP, it improves the recognition capability of the model by
capturing both universal and unique effective features. To be specific, the
universal features denote the general characteristic of facial emotions and the
unique features denote the specific characteristic of each facial expression.
Experiments on Aff-Wild2 show the effectiveness of the proposed CFSP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Temporal Grounding with Structured Variational Cross-Graph
  Correspondence Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang Tang, Fei Wu, Yi Yang, Yueting Zhuang, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal grounding in videos aims to localize one target video segment that
semantically corresponds to a given query sentence. Thanks to the semantic
diversity of natural language descriptions, temporal grounding allows activity
grounding beyond pre-defined classes and has received increasing attention in
recent years. The semantic diversity is rooted in the principle of
compositionality in linguistics, where novel semantics can be systematically
described by combining known words in novel ways (compositional
generalization). However, current temporal grounding datasets do not
specifically test for the compositional generalizability. To systematically
measure the compositional generalizability of temporal grounding models, we
introduce a new Compositional Temporal Grounding task and construct two new
dataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the
state-of-the-art methods on our new dataset splits, we empirically find that
they fail to generalize to queries with novel combinations of seen words. To
tackle this challenge, we propose a variational cross-graph reasoning framework
that explicitly decomposes video and language into multiple structured
hierarchies and learns fine-grained semantic correspondence among them.
Experiments illustrate the superior compositional generalizability of our
approach. The repository of this work is at https://github.com/YYJMJC/
Compositional-Temporal-Grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simulation Benchmark for Vision-based Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauri Suomela, Atakan Dag, Harry Edelman, Joni-Kristian Kämäräinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a simulator benchmark for vision-based autonomous
navigation. The simulator offers control over real world variables such as the
environment, time of day, weather and traffic. The benchmark includes a modular
integration of different components of a full autonomous visual navigation
stack. In the experimental part of the paper, state-of-the-art visual
localization methods are evaluated as a part of the stack in realistic
navigation tasks. To the authors' best knowledge, the proposed benchmark is the
first to study modern visual localization methods as part of a full autonomous
visual navigation stack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Action Unit Recognition With Multi-models Ensembling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiang Jiang, Yannan Wu, Fengsheng Qiao, Liyu Meng, Yuanyuan Deng, Chuanhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition gives
Affective Computing a large promotion. In this paper, we present our method of
AU challenge in this Competition. We use improved IResnet100 as backbone. Then
we train AU dataset in Aff-Wild2 on three pertained models pretrained by our
private au and expression dataset, and Glint360K respectively. Finally, we
ensemble the results of our models. We achieved F1 score (macro) 0.731 on AU
validation set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Prediction of Pulmonary Hypertension in Newborns using
  Echocardiograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Ragnarsdottir, Laura Manduchi, Holger Michel, Fabian Laumer, Sven Wellmann, Ece Ozkan, Julia Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary hypertension (PH) in newborns and infants is a complex condition
associated with several pulmonary, cardiac, and systemic diseases contributing
to morbidity and mortality. Therefore, accurate and early detection of PH is
crucial for successful management. Using echocardiography, the primary
diagnostic tool in pediatrics, human assessment is both time-consuming and
expertise-demanding, raising the need for an automated approach. In this work,
we present an interpretable multi-view video-based deep learning approach to
predict PH for a cohort of 194 newborns using echocardiograms. We use
spatio-temporal convolutional architectures for the prediction of PH from each
view, and aggregate the predictions of the different views using majority
voting. To the best of our knowledge, this is the first work for an automated
assessment of PH in newborns using echocardiograms. Our results show a mean
F1-score of 0.84 for severity prediction and 0.92 for binary detection using
10-fold cross-validation. We complement our predictions with saliency maps and
show that the learned model focuses on clinically relevant cardiac structures,
motivating its usage in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-modal</span> Emotion Estimation for in-the-wild Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyu Meng, Yuchen Liu, Xiaolong Liu, Zhaopei Huang, Wenqiang Jiang, Tenggan Zhang, Yuanyuan Deng, Ruichen Li, Yannan Wu, Jinming Zhao, Fengsheng Qiao, Qin Jin, Chuanhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we briefly introduce our submission to the Valence-Arousal
Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)
competition. Our method utilizes the multi-modal information, i.e., the visual
and audio information, and employs a temporal encoder to model the temporal
context in the videos. Besides, a smooth processor is applied to get more
reasonable predictions, and a model ensemble strategy is used to improve the
performance of our proposed method. The experiment results show that our method
achieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation
set of the Aff-Wild2 dataset, which prove the effectiveness of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Emotion Recognition using Visual-audio-linguistic
  information: A Technical Report for ABAW3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Su Zhang, Ruyi An, Yi Ding, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a cross-modal co-attention model for continuous emotion
recognition using visual-audio-linguistic information. The model consists of
four blocks. The visual, audio, and linguistic blocks are used to learn the
spatial-temporal features of the multimodal input. A co-attention block is
designed to fuse the learned enbeddings with the multihead co-attention
mechanism. The visual encoding from the visual block is concatenated with the
attention feature to emphasize the visual information. To make full use of the
data and alleviate over-fitting, the cross-validation is carried out on the
training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. The achieved CCC on
validation set is 0.450 for valence and 0.651 for arousal, which significantly
outperforms the baseline method with the corresponding CCC of 0.310 and 0.170,
respectively. The code is available at https://github.com/sucv/ABAW3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:2107.01175</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CVF-SID: Cyclic multi-Variate Function for <span class="highlight-title">Self-Supervised</span> Image
  Denoising by Disentangling Noise from Image <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been made on image denoising with strong
supervision from large-scale datasets. However, obtaining well-aligned
noisy-clean training image pairs for each specific scenario is complicated and
costly in practice. Consequently, applying a conventional supervised denoising
network on in-the-wild noisy inputs is not straightforward. Although several
studies have challenged this problem without strong supervision, they rely on
less practical assumptions and cannot be applied to practical situations
directly. To address the aforementioned challenges, we propose a novel and
powerful self-supervised denoising method called CVF-SID based on a Cyclic
multi-Variate Function (CVF) module and a self-supervised image disentangling
(SID) framework. The CVF module can output multiple decomposed variables of the
input and take a combination of the outputs back as an input in a cyclic
manner. Our CVF-SID can disentangle a clean image and noise maps from the input
by leveraging various self-supervised loss terms. Unlike several methods that
only consider the signal-independent noise models, we also deal with
signal-dependent noise components for real-world applications. Furthermore, we
do not rely on any prior assumptions about the underlying noise distribution,
making CVF-SID more generalizable toward realistic noise. Extensive experiments
on real-world datasets show that CVF-SID achieves state-of-the-art
self-supervised image denoising performance and is comparable to other existing
approaches. The code is publicly available from
https://github.com/Reyhanehne/CVF-SID_PyTorch .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Compound Domain Generalization via Meta-Knowledge Encoding <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, Yi<span class="highlight-author">zhou Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to improve the generalization performance for
an unseen target domain by using the knowledge of multiple seen source domains.
Mainstream DG methods typically assume that the domain label of each source
sample is known a priori, which is challenged to be satisfied in many
real-world applications. In this paper, we study a practical problem of
compound DG, which relaxes the discrete domain assumption to the mixed source
domains setting. On the other hand, current DG algorithms prioritize the focus
on semantic invariance across domains (one-vs-one), while paying less attention
to the holistic semantic structure (many-vs-many). Such holistic semantic
structure, referred to as meta-knowledge here, is crucial for learning
generalizable representations. To this end, we present Compound Domain
Generalization via Meta-Knowledge Encoding (COMEN), a general approach to
automatically discover and model latent domains in two steps. Firstly, we
introduce Style-induced Domain-specific Normalization (SDNorm) to re-normalize
the multi-modal underlying distributions, thereby dividing the mixture of
source domains into latent clusters. Secondly, we harness the prototype
representations, the centroids of classes, to perform relational modeling in
the embedding space with two parallel and complementary modules, which
explicitly encode the semantic structure for the out-of-distribution
generalization. Experiments on four standard DG benchmarks reveal that COMEN
exceeds the state-of-the-art performance without the need of domain
supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep-Discrete Learning Framework for Spherical Surface Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed A. Suliman, Logan Z. J. Williams, Abdulah Fawaz, Emma C. Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Nearest Neighbor Graph Embedding for Efficient
  Dimensionality Reduction <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Saquib Sarfraz, Marios Koulakis, Constantin Seibold, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction is crucial both for visualization and preprocessing
high dimensional data for machine learning. We introduce a novel method based
on a hierarchy built on 1-nearest neighbor graphs in the original space which
is used to preserve the grouping properties of the data distribution on
multiple levels. The core of the proposal is an optimization-free projection
that is competitive with the latest versions of t-SNE and UMAP in performance
and visualization quality while being an order of magnitude faster in run-time.
Furthermore, its interpretable mechanics, the ability to project new data, and
the natural separation of data clusters in visualizations make it a general
purpose unsupervised dimension reduction technique. In the paper, we argue
about the soundness of the proposed method and evaluate it on a diverse
collection of datasets with sizes varying from 1K to 11M samples and dimensions
from 28 to 16K. We perform comparisons with other state-of-the-art methods on
multiple metrics and target dimensions highlighting its efficiency and
performance. Code is available at https://github.com/koulakis/h-nne
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Disentangled Representation for One-shot Progressive Face
  Swapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Li, Weining Wang, Chengzhong Xu, Zhenan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although face swapping has attracted much attention in recent years, it
remains a challenging problem. The existing methods leverage a large number of
data samples to explore the intrinsic properties of face swapping without
taking into account the semantic information of face images. Moreover, the
representation of the identity information tends to be fixed, leading to
suboptimal face swapping. In this paper, we present a simple yet efficient
method named FaceSwapper, for one-shot face swapping based on Generative
Adversarial Networks. Our method consists of a disentangled representation
module and a semantic-guided fusion module. The disentangled representation
module is composed of an attribute encoder and an identity encoder, which aims
to achieve the disentanglement of the identity and the attribute information.
The identity encoder is more flexible and the attribute encoder contains more
details of the attributes than its competitors. Benefiting from the
disentangled representation, FaceSwapper can swap face images progressively. In
addition, semantic information is introduced into the semantic-guided fusion
module to control the swapped area and model the pose and expression more
accurately. The experimental results show that our method achieves
state-of-the-art results on benchmark datasets with fewer training samples. Our
code is publicly available at https://github.com/liqi-casia/FaceSwapper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Geometry Enough for Matching in Visual Localization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qunjie Zhou, Sergio Agostinho, Aljosa Osep, Laura Leal-Taixe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose to go beyond the well-established approach to
vision-based localization that relies on visual descriptor matching between a
query image and a 3D point cloud. While matching keypoints via visual
descriptors makes localization highly accurate, it has significant storage
demands, raises privacy concerns and increases map maintenance complexity. To
elegantly address those practical challenges for large-scale localization, we
present GoMatch, an alternative to visual-based matching that solely relies on
geometric information for matching image keypoints to maps, represented as sets
of bearing vectors. Our novel bearing vectors representation of 3D points,
significantly relieves the cross-domain challenge in geometric-based matching
that prevented prior work to tackle localization in a realistic environment.
With additional careful architecture design, GoMatch improves over prior
geometric-based matching work with a reduction of ($10.67m, 95.7^{\circ}$) and
($1.43m$, $34.7^{\circ}$) in average median pose errors on Cambridge Landmarks
and 7-Scenes, while requiring as little as $1.5/1.7\%$ of storage capacity in
comparison to the best visual-based matching methods. This confirms its
potential and feasibility for real-world localization and opens the door to
future efforts in advancing city-scale visual localization methods that do not
require storing visual descriptors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus-and-Detect: A Small Object Detection Framework for Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onur Can Koyun, Reyhan Kevser Keser, İbrahim Batuhan Akkaya, Behçet Uğur Töreyin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances, object detection in aerial images is still a
challenging task. Specific problems in aerial images makes the detection
problem harder, such as small objects, densely packed objects, objects in
different sizes and with different orientations. To address small object
detection problem, we propose a two-stage object detection framework called
"Focus-and-Detect". The first stage which consists of an object detector
network supervised by a Gaussian Mixture Model, generates clusters of objects
constituting the focused regions. The second stage, which is also an object
detector network, predicts objects within the focal regions. Incomplete Box
Suppression (IBS) method is also proposed to overcome the truncation effect of
region search approach. Results indicate that the proposed two-stage framework
achieves an AP score of 42.06 on VisDrone validation dataset, surpassing all
other state-of-the-art small object detection methods reported in the
literature, to the best of authors' knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s Meet Visual Learning Understanding: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Yang, Licheng Jiao, Xu Liu, Fang Liu, Shuyuan Yang, Zhixi Feng, Xu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic attention mechanism and global modeling ability make Transformer show
strong feature learning ability. In recent years, Transformer has become
comparable to CNNs methods in computer vision. This review mainly investigates
the current research progress of Transformer in image and video applications,
which makes a comprehensive overview of Transformer in visual learning
understanding. First, the attention mechanism is reviewed, which plays an
essential part in Transformer. And then, the visual Transformer model and the
principle of each module are introduced. Thirdly, the existing
Transformer-based models are investigated, and their performance is compared in
visual learning understanding applications. Three image tasks and two video
tasks of computer vision are investigated. The former mainly includes image
classification, object detection, and image segmentation. The latter contains
object tracking and video classification. It is significant for comparing
different models' performance in various tasks on several public benchmark data
sets. Finally, ten general problems are summarized, and the developing
prospects of the visual Transformer are given in this review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2010.11929,
  arXiv:1706.03762 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Escaping from Language Bias and OCR Error: Semantics-Centered
  Text Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Fang, Gangyan Zeng, Yu Zhou, Daiqing Wu, Can Ma, Dayong Hu, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texts in scene images convey critical information for scene understanding and
reasoning. The abilities of reading and reasoning matter for the model in the
text-based visual question answering (TextVQA) process. However, current
TextVQA models do not center on the text and suffer from several limitations.
The model is easily dominated by language biases and optical character
recognition (OCR) errors due to the absence of semantic guidance in the answer
prediction process. In this paper, we propose a novel Semantics-Centered
Network (SC-Net) that consists of an instance-level contrastive semantic
prediction module (ICSP) and a semantics-centered transformer module (SCT).
Equipped with the two modules, the semantics-centered model can resist the
language biases and the accumulated errors from OCR. Extensive experiments on
TextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net
surpasses previous works with a noticeable margin and is more reasonable for
the TextVQA task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Fixed Sub-Center: A Better Way to Capture Data Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhemin Zhang, Xun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Treating class with a single center may hardly capture data distribution
complexities. Using multiple sub-centers is an alternative way to address this
problem. However, highly correlated sub-classes, the classifier's parameters
grow linearly with the number of classes, and lack of intra-class compactness
are three typical issues that need to be addressed in existing multi-subclass
methods. To this end, we propose to use Fixed Sub-Center (F-SC), which allows
the model to create more discrepant sub-centers while saving memory and cutting
computational costs considerably. The F-SC specifically, first samples a class
center Ui for each class from a uniform distribution, and then generates a
normal distribution for each class, where the mean is equal to Ui. Finally, the
sub-centers are sampled based on the normal distribution corresponding to each
class, and the sub-centers are fixed during the training process avoiding the
overhead of gradient calculation. Moreover, F-SC penalizes the Euclidean
distance between the samples and their corresponding sub-centers, it helps
remain intra-compactness. The experimental results show that F-SC significantly
improves the accuracy of both image classification and fine-grained recognition
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dense Correspondence from Synthetic Environments <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mithun Lal, Anthony Paproki, Nariman Habili, Lars Petersson, Olivier Salvado, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimation of human shape and pose from a single image is a challenging task.
It is an even more difficult problem to map the identified human shape onto a
3D human model. Existing methods map manually labelled human pixels in real 2D
images onto the 3D surface, which is prone to human error, and the sparsity of
available annotated data often leads to sub-optimal results. We propose to
solve the problem of data scarcity by training 2D-3D human mapping algorithms
using automatically generated synthetic data for which exact and dense 2D-3D
correspondence is known. Such a learning strategy using synthetic environments
has a high generalisation potential towards real-world data. Using different
camera parameter variations, background and lighting settings, we created
precise ground truth data that constitutes a wider distribution. We evaluate
the performance of models trained on synthetic using the COCO dataset and
validation framework. Results show that training 2D-3D mapping network models
on synthetic data is a viable alternative to using real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICIP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point
  Cloud Generation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzhi Tang, Yue Qian, Qijian Zhang, Yiming Zeng, Junhui Hou, Xuefei Zhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose WarpingGAN, an effective and efficient 3D point cloud generation
network. Unlike existing methods that generate point clouds by directly
learning the mapping functions between latent codes and 3D shapes, Warping-GAN
learns a unified local-warping function to warp multiple identical pre-defined
priors (i.e., sets of points uniformly distributed on regular 3D grids) into 3D
shapes driven by local structure-aware semantics. In addition, we also
ingeniously utilize the principle of the discriminator and tailor a stitching
loss to eliminate the gaps between different partitions of a generated shape
corresponding to different priors for boosting quality. Owing to the novel
generating mechanism, WarpingGAN, a single lightweight network after one-time
training, is capable of efficiently generating uniformly distributed 3D point
clouds with various resolutions. Extensive experimental results demonstrate the
superiority of our WarpingGAN over state-of-the-art methods in terms of
quantitative metrics, visual quality, and efficiency. The source code is
publicly available at https://github.com/yztang4/WarpingGAN.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofei Xie, Tianlin Li, Jian Wang, Lei Ma, Qing Guo, Felix Juefei-Xu, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has recently been widely applied to many applications across
different domains, e.g., image classification and audio recognition. However,
the quality of Deep Neural Networks (DNNs) still raises concerns in the
practical operational environment, which calls for systematic testing,
especially in safety-critical scenarios. Inspired by software testing, a number
of structural coverage criteria are designed and proposed to measure the test
adequacy of DNNs. However, due to the blackbox nature of DNN, the existing
structural coverage criteria are difficult to interpret, making it hard to
understand the underlying principles of these criteria. The relationship
between the structural coverage and the decision logic of DNNs is unknown.
Moreover, recent studies have further revealed the non-existence of correlation
between the structural coverage and DNN defect detection, which further posts
concerns on what a suitable DNN testing criterion should be.
  In this paper, we propose the interpretable coverage criteria through
constructing the decision structure of a DNN. Mirroring the control flow graph
of the traditional program, we first extract a decision graph from a DNN based
on its interpretation, where a path of the decision graph represents a decision
logic of the DNN. Based on the control flow and data flow of the decision
graph, we propose two variants of path coverage to measure the adequacy of the
test cases in exercising the decision logic. The higher the path coverage, the
more diverse decision logic the DNN is expected to be explored. Our large-scale
evaluation results demonstrate that: the path in the decision graph is
effective in characterizing the decision of the DNN, and the proposed coverage
criteria are also sensitive with errors including natural errors and
adversarial examples, and strongly correlated with the output impartiality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages. Accepted to ACM Transactions on Software Engineering and
  Methodology (TOSEM), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Reflectance for Shape Recovery with Shadow Handling <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxuan Li, Hongdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims at recovering the shape of a scene with unknown,
non-Lambertian, and possibly spatially-varying surface materials. When the
shape of the object is highly complex and that shadows cast on the surface, the
task becomes very challenging. To overcome these challenges, we propose a
coordinate-based deep MLP (multilayer perceptron) to parameterize both the
unknown 3D shape and the unknown reflectance at every surface point. This
network is able to leverage the observed photometric variance and shadows on
the surface, and recover both surface shape and general non-Lambertian
reflectance. We explicitly predict cast shadows, mitigating possible artifacts
on these shadowing regions, leading to higher estimation accuracy. Our
framework is entirely self-supervised, in the sense that it requires neither
ground truth shape nor BRDF. Tests on real-world images demonstrate that our
method outperform existing methods by a significant margin. Thanks to the small
size of the MLP-net, our method is an order of magnitude faster than previous
CNN-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022. Codes available in
  https://github.com/junxuan-li/Neural-Reflectance-PS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privileged Attribution Constrained Deep Networks for Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jules Bonnard, Arnaud Dapogny, Ferdinand Dhombres, Kévin Bailly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) is crucial in many research domains
because it enables machines to better understand human behaviours. FER methods
face the problems of relatively small datasets and noisy data that don't allow
classical networks to generalize well. To alleviate these issues, we guide the
model to concentrate on specific facial areas like the eyes, the mouth or the
eyebrows, which we argue are decisive to recognise facial expressions. We
propose the Privileged Attribution Loss (PAL), a method that directs the
attention of the model towards the most salient facial regions by encouraging
its attribution maps to correspond to a heatmap formed by facial landmarks.
Furthermore, we introduce several channel strategies that allow the model to
have more degrees of freedom. The proposed method is independent of the
backbone architecture and doesn't need additional semantic information at test
time. Finally, experimental results show that the proposed PAL method
outperforms current state-of-the-art methods on both RAF-DB and AffectNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expression Classification using Concatenation of Deep Neural Network for
  the 3rd ABAW3 Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Ngan Phan, Hong-Hai Nguyen, Van-Thong Huynh, Soo-Hyung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For computers to recognize human emotions, expression classification is an
equally important problem in the human-computer interaction area. In the 3rd
Affective Behavior Analysis In-The-Wild competition, the task of expression
classification includes 8 classes including 6 basic expressions of human faces
from videos. In this paper, we perform combination representation from RegNet,
Attention module, and Transformer Encoder for the expression classification
task. We achieve 35.87 \% for F1-score on the validation set of Aff-Wild2
dataset. This result shows the effectiveness of the proposed architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAMLP: A Frequency-Aware MLP-Like Architecture For Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kecheng Zheng, Yang Cao, Kai Zhu, Ruijing Zhao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MLP-like models built entirely upon multi-layer perceptrons have recently
been revisited, exhibiting the comparable performance with transformers. It is
one of most promising architectures due to the excellent trade-off between
network capability and efficiency in the large-scale recognition tasks.
However, its generalization performance to heterogeneous tasks is inferior to
other architectures (e.g., CNNs and transformers) due to the extensive
retention of domain information. To address this problem, we propose a novel
frequency-aware MLP architecture, in which the domain-specific features are
filtered out in the transformed frequency domain, augmenting the invariant
descriptor for label prediction. Specifically, we design an adaptive Fourier
filter layer, in which a learnable frequency filter is utilized to adjust the
amplitude distribution by optimizing both the real and imaginary parts. A
low-rank enhancement module is further proposed to rectify the filtered
features by adding the low-frequency components from SVD decomposition.
Finally, a momentum update strategy is utilized to stabilize the optimization
to fluctuation of model parameters and inputs by the output distillation with
weighted historical states. To our best knowledge, we are the first to propose
a MLP-like backbone for domain generalization. Extensive experiments on three
benchmarks demonstrate significant generalization performance, outperforming
the state-of-the-art methods by a margin of 3%, 4% and 9%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Heads or Tails: Towards Semantically Consistent Visual
  Counterfactuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Vandenhende, Dhruv Mahajan, Filip Radenovic, Deepti Ghadiyaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A visual counterfactual explanation replaces image regions in a query image
with regions from a distractor image such that the system's decision on the
transformed image changes to the distractor class. In this work, we present a
novel framework for computing visual counterfactual explanations based on two
key ideas. First, we enforce that the \textit{replaced} and \textit{replacer}
regions contain the same semantic part, resulting in more semantically
consistent explanations. Second, we use multiple distractor images in a
computationally efficient way and obtain more discriminative explanations with
fewer region replacements. Our approach is $\mathbf{27\%}$ more semantically
consistent and an order of magnitude faster than a competing method on three
fine-grained image recognition datasets. We highlight the utility of our
counterfactuals over existing works through machine teaching experiments where
we teach humans to classify different bird species. We also complement our
explanations with the vocabulary of parts and attributes that contributed the
most to the system's decision. In this task as well, we obtain state-of-the-art
results when using our counterfactual explanations relative to existing works,
reinforcing the importance of semantically consistent explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. We plan to make code available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Ensemble Approach for Facial Expression Analysis in Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Hai Nguyen, Van-Thong Huynh, Soo-Hyung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human emotions recognization contributes to the development of human-computer
interaction. The machines understanding human emotions in the real world will
significantly contribute to life in the future. This paper will introduce the
Affective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. The paper
focuses on solving the problem of the valence-arousal estimation and action
unit detection. For valence-arousal estimation, we conducted two stages:
creating new features from multimodel and temporal learning to predict
valence-arousal. First, we make new features; the Gated Recurrent Unit (GRU)
and Transformer are combined using a Regular Networks (RegNet) feature, which
is extracted from the image. The next step is the GRU combined with Local
Attention to predict valence-arousal. The Concordance Correlation Coefficient
(CCC) was used to evaluate the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-Supervised End-to-End CAD Retrieval to Scan Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Beyer, Angela Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CAD model retrieval to real-world scene observations has shown strong promise
as a basis for 3D perception of objects and a clean, lightweight mesh-based
scene representation; however, current approaches to retrieve CAD models to a
query scan rely on expensive manual annotations of 1:1 associations of CAD-scan
objects, which typically contain strong lower-level geometric differences. We
thus propose a new weakly-supervised approach to retrieve semantically and
structurally similar CAD models to a query 3D scanned scene without requiring
any CAD-scan associations, and only object detection information as oriented
bounding boxes. Our approach leverages a fully-differentiable top-$k$ retrieval
layer, enabling end-to-end training guided by geometric and perceptual
similarity of the top retrieved CAD models to the scan queries. We demonstrate
that our weakly-supervised approach can outperform fully-supervised retrieval
methods on challenging real-world ScanNet scans, and maintain robustness for
unseen class categories, achieving significantly improved performance over
fully-supervised state of the art in zero-shot CAD retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accompanying video at https://youtu.be/3bCUMxpscdQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Bias Identification on Medical Image <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhang, Lanjun Wang, Lian Ding, Senhua Zhu, Dandan Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning based medical image analysis highly depends on datasets.
Biases in the dataset can be learned by the model and degrade the
generalizability of the applications. There are studies on debiased models.
However, scientists and practitioners are difficult to identify implicit biases
in the datasets, which causes lack of reliable unbias test datasets to valid
models. To tackle this issue, we first define the data intrinsic bias
attribute, and then propose a novel bias identification framework for medical
image datasets. The framework contains two major components, KlotskiNet and
Bias Discriminant Direction Analysis(bdda), where KlostkiNet is to build the
mapping which makes backgrounds to distinguish positive and negative samples
and bdda provides a theoretical solution on determining bias attributes.
Experimental results on three datasets show the effectiveness of the bias
attributes discovered by the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust
  Correspondence Field Estimation and Pose Optimization <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Xu, Junyi Lin, Guofeng Zhang, Xiaogang Wang, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct estimating the 6-DoF object pose from a single color image is
challenging, and post-refinement is generally needed to achieve high-precision
estimation. In this paper, we propose a framework based on a recurrent neural
network (RNN) for object pose refinement, which is robust to erroneous initial
poses and occlusions. During the recurrent iterations, object pose refinement
is formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
for end-toend training. The correspondence field estimation and pose refinement
are conducted alternatively in each iteration to recover accurate object poses.
Furthermore, to improve the robustness to occlusions, we introduce a
consistencycheck mechanism based on the learned descriptors of the 3D model and
observed 2D image, which downweights the unreliable correspondences during pose
optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyRep: Bootstrapping Training with Dynamic Re-parameterization <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Shan You, Bohan Zhang, Yuxuan Du, Fei Wang, Chen Qian, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural re-parameterization (Rep) methods achieve noticeable improvements
on simple VGG-style networks. Despite the prevalence, current Rep methods
simply re-parameterize all operations into an augmented network, including
those that rarely contribute to the model's performance. As such, the price to
pay is an expensive computational overhead to manipulate these unnecessary
behaviors. To eliminate the above caveats, we aim to bootstrap the training
with minimal cost by devising a dynamic re-parameterization (DyRep) method,
which encodes Rep technique into the training process that dynamically evolves
the network structures. Concretely, our proposal adaptively finds the
operations which contribute most to the loss in the network, and applies Rep to
enhance their representational capacity. Besides, to suppress the noisy and
redundant operations introduced by Rep, we devise a de-parameterization
technique for a more compact re-parameterization. With this regard, DyRep is
more efficient than Rep since it smoothly evolves the given network instead of
constructing an over-parameterized network. Experimental results demonstrate
our effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\%$
on ImageNet and reduces $22\%$ runtime over the baseline. Code is available at:
https://github.com/hunto/DyRep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Compressed Sensing via Global Image Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Bran Lorenzana, Craig Engstrom, Shekhar S. Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNN) have demonstrated outstanding Compressed
Sensing (CS) performance compared to traditional, hand-crafted methods.
However, they are broadly limited in terms of generalisability, inductive bias
and difficulty to model long distance relationships. Transformer neural
networks (TNN) overcome such issues by implementing an attention mechanism
designed to capture dependencies between inputs. However, high-resolution tasks
typically require vision Transformers (ViT) to decompose an image into
patch-based tokens, limiting inputs to inherently local contexts. We propose a
novel image decomposition that naturally embeds images into low-resolution
inputs. These Kaleidoscope tokens (KD) provide a mechanism for global
attention, at the same computational cost as a patch-based approach. To
showcase this development, we replace CNN components in a well-known CS-MRI
neural network with TNN blocks and demonstrate the improvements afforded by KD.
We also propose an ensemble of image tokens, which enhance overall image
quality and reduces model size. Supplementary material is available:
https://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Pages, 4 Figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Fixation: Dynamic Window Visual <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Changlin Li, Guangrun Wang, Yun Xiao, Qing Du Xiaodan Liang Xiaojun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a surge of interest in visual transformers is to reduce the
computational cost by limiting the calculation of self-attention to a local
window. Most current work uses a fixed single-scale window for modeling by
default, ignoring the impact of window size on model performance. However, this
may limit the modeling potential of these window-based models for multi-scale
information. In this paper, we propose a novel method, named Dynamic Window
Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT
goes beyond the model that employs a fixed single window setting. To the best
of our knowledge, we are the first to use dynamic multi-scale windows to
explore the upper limit of the effect of window settings on model performance.
In DW-ViT, multi-scale information is obtained by assigning windows of
different sizes to different head groups of window multi-head self-attention.
Then, the information is dynamically fused by assigning different weights to
the multi-scale window branches. We conducted a detailed performance evaluation
on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related
state-of-the-art (SoTA) methods, DW-ViT obtains the best performance.
Specifically, compared with the current SoTA Swin Transformers
\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements
on all three datasets with similar parameters and computational costs. In
addition, DW-ViT exhibits good scalability and can be easily inserted into any
window-based visual transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct evaluation of progression or regression of disease burden in
  brain metastatic disease with Deep Neuroevolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Stember, Robert Young, Hrithwik Shalu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: A core component of advancing cancer treatment research is assessing
response to therapy. Doing so by hand, for example as per RECIST or RANO
criteria, is tedious, time-consuming, and can miss important tumor response
information; most notably, they exclude non-target lesions. We wish to assess
change in a holistic fashion that includes all lesions, obtaining simple,
informative, and automated assessments of tumor progression or regression. Due
to often low patient enrolments in clinical trials, we wish to make response
assessments with small training sets. Deep neuroevolution (DNE) can produce
radiology artificial intelligence (AI) that performs well on small training
sets. Here we use DNE for function approximation that predicts progression
versus regression of metastatic brain disease.
  Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training
set. Half of these pairs, separated in time, qualified as disease progression,
while the other 25 images constituted regression. We trained the parameters of
a relatively small CNN via mutations that consisted of random CNN weight
adjustments and mutation fitness. We then incorporated the best mutations into
the next generations CNN, repeating this process for approximately 50,000
generations. We applied the CNNs to our training set, as well as a separate
testing set with the same class balance of 25 progression and 25 regression
images.
  Results: DNE achieved monotonic convergence to 100% training set accuracy.
DNE also converged monotonically to 100% testing set accuracy.
  Conclusion: DNE can accurately classify brain-metastatic disease progression
versus regression. Future work will extend the input from 2D image slices to
full 3D volumes, and include the category of no change. We believe that an
approach such as our could ultimately provide a useful adjunct to RANO/RECIST
assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Image Manipulation with Background-guided Internal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongping Zhang, Huiwen He, Bryan A. Plummer, Zhenyu Liao, Huayan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image manipulation has attracted a lot of interest due to its wide range of
applications. Prior work modifies images either from low-level manipulation,
such as image inpainting or through manual edits via paintbrushes and
scribbles, or from high-level manipulation, employing deep generative networks
to output an image conditioned on high-level semantic input. In this study, we
propose Semantic Image Manipulation with Background-guided Internal Learning
(SIMBIL), which combines high-level and low-level manipulation. Specifically,
users can edit an image at the semantic level by applying changes on a scene
graph. Then our model manipulates the image at the pixel level according to the
modified scene graph. There are two major advantages of our approach. First,
high-level manipulation of scene graphs requires less manual effort from the
user compared to manipulating raw image pixels. Second, our low-level internal
learning approach is scalable to images of various sizes without reliance on
external visual datasets for training. We outperform the state-of-the-art in a
quantitative and qualitative evaluation on the CLEVR and Visual Genome
datasets. Experiments show 8 points improvement on FID scores (CLEVR) and 27%
improvement on user evaluation (Visual Genome), demonstrating the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keypoints Tracking via <span class="highlight-title">Transformer</span> Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksii Nasypanyi, Francois Rameau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this thesis, we propose a pioneering work on sparse keypoints tracking
across images using transformer networks. While deep learning-based keypoints
matching have been widely investigated using graph neural networks - and more
recently transformer networks, they remain relatively too slow to operate in
real-time and are particularly sensitive to the poor repeatability of the
keypoints detectors. In order to address these shortcomings, we propose to
study the particular case of real-time and robust keypoints tracking.
Specifically, we propose a novel architecture which ensures a fast and robust
estimation of the keypoints tracking between successive images of a video
sequence. Our method takes advantage of a recent breakthrough in computer
vision, namely, visual transformer networks. Our method consists of two
successive stages, a coarse matching followed by a fine localization of the
keypoints' correspondences prediction. Through various experiments, we
demonstrate that our approach achieves competitive results and demonstrates
high robustness against adverse conditions, such as illumination change,
occlusion and viewpoint differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Emotion Descriptors Estimation at the ABAW3 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Didan Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To describe complex emotional states, psychologists have proposed multiple
emotion descriptors: sparse descriptors like facial action units; continuous
descriptors like valence and arousal; and discrete class descriptors like
happiness and anger. According to Ekman and Friesen, 1969, facial action units
are sign vehicles that convey the emotion message, while discrete or continuous
emotion descriptors are the messages perceived and expressed by human.
  In this paper, we designed an architecture for multiple emotion descriptors
estimation in participating the ABAW3 Challenge. Based on the theory of Ekman
and Friesen, 1969, we designed distinct architectures to measure the sign
vehicles (i.e., facial action units) and the message (i.e., discrete emotions,
valence and arousal) given their different properties. The quantitative
experiments on the ABAW3 challenge dataset has shown the superior performance
of our approach over two baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The technical report for our multi-task approach in the ABAW3
  Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VL-Adapter: Parameter-Efficient Transfer Learning for
  <span class="highlight-title">Vision-and-Language</span> Tasks <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Sung, Jaemin Cho, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMesh: Differentiable Iso-Surface Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.11795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.11795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Guillard, Edoardo Remelli, Artem Lukoianov, Stephan R. Richter, Timur Bagautdinov, Pierre Baque, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric Deep Learning has recently made striking progress with the advent
of continuous deep implicit fields. They allow for detailed modeling of
watertight surfaces of arbitrary topology while not relying on a 3D Euclidean
grid, resulting in a learnable parameterization that is unlimited in
resolution.
  Unfortunately, these methods are often unsuitable for applications that
require an explicit mesh-based surface representation because converting an
implicit field to such a representation relies on the Marching Cubes algorithm,
which cannot be differentiated with respect to the underlying implicit field.
  In this work, we remove this limitation and introduce a differentiable way to
produce explicit surface mesh representations from Deep Implicit Fields. Our
key insight is that by reasoning on how implicit field perturbations impact
local surface geometry, one can ultimately differentiate the 3D location of
surface samples with respect to the underlying deep implicit field. We exploit
this to define DeepMesh - an end-to-end differentiable mesh representation that
can vary its topology.
  We validate our theoretical insight through several applications: Single view
3D Reconstruction via Differentiable Rendering, Physically-Driven Shape
Optimization, Full Scene 3D Reconstruction from Scans and End-to-End Training.
In all cases our end-to-end differentiable parameterization gives us an edge
over state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2006.03997</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Precision Neural Network Quantization via Learned Layer-wise
  Importance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Yaowei Wang, Wen Ji, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponentially large discrete search space in mixed-precision quantization
(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous
works usually resort to iterative search methods on the training set, which
consume hundreds or even thousands of GPU-hours. In this study, we reveal that
some unique learnable parameters in quantization, namely the scale factors in
the quantizer, can serve as importance indicators of a layer, reflecting the
contribution of that layer to the final accuracy at certain bit-widths. These
importance indicators naturally perceive the numerical transformation during
quantization-aware training, which can precisely and correctly provide
quantization sensitivity metrics of layers. However, a deep network always
contains hundreds of such indicators, and training them one by one would lead
to an excessive time cost. To overcome this issue, we propose a joint training
scheme that can obtain all indicators at once. It considerably speeds up the
indicators training process by parallelizing the original sequential training
processes. With these learned importance indicators, we formulate the MPQ
search problem as a one-time integer linear programming (ILP) problem. That
avoids the iterative search and significantly reduces search time without
limiting the bit-width search space. For example, MPQ search on ResNet18 with
our indicators takes only 0.06 seconds. Also, extensive experiments show our
approach can achieve SOTA accuracy on ImageNet for far-ranging models with
various constraints (e.g., BitOps, compress rate).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic Document Generator for Annotation-free Layout Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.06016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.06016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natraj Raman, Sameena Shah, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing the layout of a document to identify headers, sections, tables,
figures etc. is critical to understanding its content. Deep learning based
approaches for detecting the layout structure of document images have been
promising. However, these methods require a large number of annotated examples
during training, which are both expensive and time consuming to obtain. We
describe here a synthetic document generator that automatically produces
realistic documents with labels for spatial positions, extents and categories
of the layout elements. The proposed generative process treats every physical
component of a document as a random variable and models their intrinsic
dependencies using a Bayesian Network graph. Our hierarchical formulation using
stochastic templates allow parameter sharing between documents for retaining
broad themes and yet the distributional characteristics produces visually
unique samples, thereby capturing complex and diverse layouts. We empirically
illustrate that a deep layout detection model trained purely on the synthetic
documents can match the performance of a model that uses real documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Myers-Dean, Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized few-shot semantic segmentation was introduced to move beyond only
evaluating few-shot segmentation models on novel classes to include testing
their ability to remember base classes. While the current state-of-the-art
approach is based on meta-learning, it performs poorly and saturates in
learning after observing only a few shots. We propose the first fine-tuning
solution, and demonstrate that it addresses the saturation problem while
achieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We
also show that it outperforms existing methods, whether fine-tuning multiple
final layers or only the final layer. Finally, we present a triplet loss
regularization that shows how to redistribute the balance of performance
between novel and base categories so that there is a smaller gap between them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Includes supplementary materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Microfossil Identification via Deep Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09490v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09490v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tayfun Karaderi, Tilo Burghardt, Allison Y. Hsiang, Jacob Ramaer, Daniela N. Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply deep metric learning for the first time to the problem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualization options for
human experts and cannot be applied to open-set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualization of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning outperforms all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Keycode, network weights, and data splits are
published with this paper for full reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avalanche RL: a Continual Reinforcement Learning Library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Lucchesi, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Reinforcement Learning (CRL) is a challenging setting where an
agent learns to interact with an environment that is constantly changing over
time (the stream of experiences). In this paper, we describe Avalanche RL, a
library for Continual Reinforcement Learning which allows to easily train
agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and
supports any OpenAI Gym environment. Its design is based on Avalanche, one of
the more popular continual learning libraries, which allow us to reuse a large
number of continual learning strategies and improve the interaction between
reinforcement learning and continual learning researchers. Additionally, we
propose Continual Habitat-Lab, a novel benchmark and a high-level library which
enables the usage of the photorealistic simulator Habitat-Sim for CRL research.
Overall, Avalanche RL attempts to unify under a common framework continual
reinforcement learning applications, which we hope will foster the growth of
the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oriented RepPoints for Aerial Object Detection <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11111v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11111v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentong Li, Yijie Chen, Kaixuan Hu, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to the generic object, aerial targets are often non-axis aligned
with arbitrary orientations having the cluttered surroundings. Unlike the
mainstreamed approaches regressing the bounding box orientations, this paper
proposes an effective adaptive points learning approach to aerial object
detection by taking advantage of the adaptive points representation, which is
able to capture the geometric information of the arbitrary-oriented instances.
To this end, three oriented conversion functions are presented to facilitate
the classification and localization with accurate orientation. Moreover, we
propose an effective quality assessment and sample assignment scheme for
adaptive points learning toward choosing the representative oriented reppoints
samples during training, which is able to capture the non-axis aligned features
from adjacent objects or background noises. A spatial constraint is introduced
to penalize the outlier points for roust adaptive learning. Experimental
results on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD
and DIOR-R, demonstrate the efficacy of our proposed approach. The source code
is availabel at: https://github.com/LiWentomng/OrientedRepPoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D
  Medical Image Segmentation using HyperNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Peng, Andriy Myronenko, Ali Hatamizadeh, Vish Nath, Md Mahfuzur Rahman Siddiquee, Yufan He, Daguang Xu, Rama Chellappa, Dong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of 3D medical images is a challenging task due to the
high variability of the shape and pattern of objects (such as organs or
tumors). Given the recent success of deep learning in medical image
segmentation, Neural Architecture Search (NAS) has been introduced to find
high-performance 3D segmentation network architectures. However, because of the
massive computational requirements of 3D data and the discrete optimization
nature of architecture search, previous NAS methods require a long search time
or necessary continuous relaxation, and commonly lead to sub-optimal network
architectures. While one-shot NAS can potentially address these disadvantages,
its application in the segmentation domain has not been well studied in the
expansive multi-scale multi-path search space. To enable one-shot NAS for
medical image segmentation, our method, named HyperSegNAS, introduces a
HyperNet to assist super-net training by incorporating architecture topology
information. Such a HyperNet can be removed once the super-net is trained and
introduces no overhead during architecture search. We show that HyperSegNAS
yields better performing and more intuitive architectures compared to the
previous state-of-the-art (SOTA) segmentation networks; furthermore, it can
quickly and accurately find good architecture candidates under different
computing constraints. Our method is evaluated on public datasets from the
Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05151v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05151v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current adversarial attack research reveals the vulnerability of
learning-based classifiers against carefully crafted perturbations. However,
most existing attack methods have inherent limitations in cross-dataset
generalization as they rely on a classification layer with a closed set of
categories. Furthermore, the perturbations generated by these methods may
appear in regions easily perceptible to the human visual system (HVS). To
circumvent the former problem, we propose a novel algorithm that attacks
semantic similarity on feature representations. In this way, we are able to
fool classifiers without limiting attacks to a specific dataset. For
imperceptibility, we introduce the low-frequency constraint to limit
perturbations within high-frequency components, ensuring perceptual similarity
between adversarial examples and originals. Extensive experiments on three
datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online
platforms indicate that our attack can yield misleading and transferable
adversarial examples across architectures and datasets. Additionally,
visualization results and quantitative performance (in terms of four different
metrics) show that the proposed algorithm generates more imperceptible
perturbations than the state-of-the-art methods. Code is made available at.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 conference (accepted), 18 pages, 17 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nuclei instance segmentation and classification in histopathology images
  with StarDist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Weigert, Uwe Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance segmentation and classification of nuclei is an important task in
computational pathology. We show that StarDist, a deep learning based nuclei
segmentation method originally developed for fluorescence microscopy, can be
extended and successfully applied to histopathology images. This is
substantiated by conducting experiments on the Lizard dataset, and through
entering the Colon Nuclei Identification and Counting (CoNIC) challenge 2022.
At the end of the preliminary test phase of CoNIC, our approach ranked first on
the leaderboard for the segmentation and classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Answer-Driven Visual State Estimator for <span class="highlight-title">Goal-Oriented</span> Visual <span class="highlight-title">Dialogue</span> <span class="chip">ACM MM 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.00361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.00361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Xu, Fangxiang Feng, Xiaojie Wang, Yushu Yang, Huixing Jiang, Zhongyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A goal-oriented visual dialogue involves multi-turn interactions between two
agents, Questioner and Oracle. During which, the answer given by Oracle is of
great significance, as it provides golden response to what Questioner concerns.
Based on the answer, Questioner updates its belief on target visual content and
further raises another question. Notably, different answers drive into
different visual beliefs and future questions. However, existing methods always
indiscriminately encode answers after much longer questions, resulting in a
weak utilization of answers. In this paper, we propose an Answer-Driven Visual
State Estimator (ADVSE) to impose the effects of different answers on visual
states. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture
the answer-driven effect on visual attention by sharpening question-related
attention and adjusting it by answer-based logical operation at each turn. Then
based on the focusing attention, we get the visual state estimation by
Conditional Visual Information Fusion (CVIF), where overall information and
difference information are fused conditioning on the question-answer state. We
evaluate the proposed ADVSE to both question generator and guesser tasks on the
large-scale GuessWhat?! dataset and achieve the state-of-the-art performances
on both tasks. The qualitative results indicate that the ADVSE boosts the agent
to generate highly efficient questions and obtains reliable visual attentions
during the reasonable question generation and guess processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM International Conference on Multimedia (ACM MM 2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Predict, Prevent, and Evaluate: Disentangled Text-Driven Image
  Manipulation Empowered by <span class="highlight-title">Pre-Train</span>ed <span class="highlight-title">Vision-Language</span> Model <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Xu, Tianwei Lin, <span class="highlight-author">Hao Tan</span>g, Fu Li, Dongliang He, Nicu Sebe, Radu Timofte, Luc Van Gool, Errui Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve disentangled image manipulation, previous works depend heavily on
manual annotation. Meanwhile, the available manipulations are limited to a
pre-defined set the models were trained for. We propose a novel framework,
i.e., Predict, Prevent, and Evaluate (PPE), for disentangled text-driven image
manipulation that requires little manual annotation while being applicable to a
wide variety of manipulations. Our method approaches the targets by deeply
exploiting the power of the large-scale pre-trained vision-language model CLIP.
Concretely, we firstly Predict the possibly entangled attributes for a given
text command. Then, based on the predicted attributes, we introduce an
entanglement loss to Prevent entanglements during training. Finally, we propose
a new evaluation metric to Evaluate the disentangled image manipulation. We
verify the effectiveness of our method on the challenging face editing task.
Extensive experiments show that the proposed PPE framework achieves much better
quantitative and qualitative results than the up-to-date StyleCLIP baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04386v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04386v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends also on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct and Pserf. Experiments suggest that the proposed functions improve the
network performance significantly compared to the widely used activations like
ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and
5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in
CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet
V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small
  Object Detection <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehao Huang, Naiyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While general object detection with deep learning has achieved great success
in the past few years, the performance and efficiency of detecting small
objects are far from satisfactory. The most common and effective way to promote
small object detection is to use high-resolution images or feature maps.
However, both approaches induce costly computation since the computational cost
grows squarely as the size of images and features increases. To get the best of
two worlds, we propose QueryDet that uses a novel query mechanism to accelerate
the inference speed of feature-pyramid based object detectors. The pipeline
composes two steps: it first predicts the coarse locations of small objects on
low-resolution features and then computes the accurate detection results using
high-resolution features sparsely guided by those coarse positions. In this
way, we can not only harvest the benefit of high-resolution feature maps but
also avoid useless computation for the background area. On the popular COCO
dataset, the proposed method improves the detection mAP by 1.0 and mAP-small by
2.0, and the high-resolution inference speed is improved to 3.0x on average. On
VisDrone dataset, which contains more small objects, we create a new
state-of-the-art while gaining a 2.3x high-resolution acceleration on average.
Code is available at https://github.com/ChenhongyiYang/QueryDet-PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applications of Artificial Neural Networks in Microorganism Image
  Analysis: A Comprehensive <span class="highlight-title">Review</span> from Conventional Multilayer Perceptron to
  Popular Convolutional Neural Network and Potential Visual <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.00358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.00358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghua Zhang, Chen Li, Yimin Yin, Jiawei Zhang, Marcin Grzegorzek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microorganisms are widely distributed in the human daily living environment.
They play an essential role in environmental pollution control, disease
prevention and treatment, and food and drug production. The analysis of
microorganisms is essential for making full use of different microorganisms.
The conventional analysis methods are laborious and time-consuming. Therefore,
the automatic image analysis based on artificial neural networks is introduced
to optimize it. However, the automatic microorganism image analysis faces many
challenges, such as the requirement of a robust algorithm caused by various
application occasions, insignificant features and easy under-segmentation
caused by the image characteristic, and various analysis tasks. Therefore, we
conduct this review to comprehensively discuss the characteristics of
microorganism image analysis based on artificial neural networks. In this
review, the background and motivation are introduced first. Then, the
development of artificial neural networks and representative networks are
presented. After that, the papers related to microorganism image analysis based
on classical and deep neural networks are reviewed from the perspectives of
different tasks. In the end, the methodology analysis and potential direction
are discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Feature Learning and Relation Modeling for Tracking: A One-Stream
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current popular two-stream, two-stage tracking framework extracts the
template and the search region features separately and then performs relation
modeling, thus the extracted features lack the awareness of the target and have
limited target-background discriminability. To tackle the above issue, we
propose a novel one-stream tracking (OSTrack) framework that unifies feature
learning and relation modeling by bridging the template-search image pairs with
bidirectional information flows. In this way, discriminative target-oriented
features can be dynamically extracted by mutual guidance. Since no extra heavy
relation modeling module is needed and the implementation is highly
parallelized, the proposed tracker runs at a fast speed. To further improve the
inference efficiency, an in-network candidate early elimination module is
proposed based on the strong similarity prior calculated in the one-stream
framework. As a unified framework, OSTrack achieves state-of-the-art
performance on multiple benchmarks, in particular, it shows impressive results
on the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving
the existing best result (SwinTrack) by 4.3%. Besides, our method maintains a
good performance-speed trade-off and shows faster convergence. The code and
models will be available at https://github.com/botaoye/OSTrack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text to Image Generation with Semantic-Spatial Aware GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.00567v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.00567v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Hu, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image synthesis (T2I) aims to generate photo-realistic images which
are semantically consistent with the text descriptions. Existing methods are
usually built upon conditional generative adversarial networks (GANs) and
initialize an image from noise with sentence embedding, and then refine the
features with fine-grained word embedding iteratively. A close inspection of
their generated images reveals a major limitation: even though the generated
image holistically matches the description, individual image regions or parts
of somethings are often not recognizable or consistent with words in the
sentence, e.g. "a white crown". To address this problem, we propose a novel
framework Semantic-Spatial Aware GAN for synthesizing images from input text.
Concretely, we introduce a simple and effective Semantic-Spatial Aware block,
which (1) learns semantic-adaptive transformation conditioned on text to
effectively fuse text features and image features, and (2) learns a semantic
mask in a weakly-supervised way that depends on the current text-image fusion
process in order to guide the transformation spatially. Experiments on the
challenging COCO and CUB bird datasets demonstrate the advantage of our method
over the recent state-of-the-art approaches, regarding both visual fidelity and
alignment with input text description.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:1711.10485 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1804.03547v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1804.03547v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujiang Wang, Jie Shen, Stavros Petridis, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code implementation in Python is available at:
  https://github.com/ibug-group/face_reid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Generalization in Federated Learning by Seeking Flat Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Caldarola, Barbara Caputo, Marco Ciccone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Removed axessibility package for smaller output PDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Transformer</span>s for Unsupervised Object Discovery using
  Normalized Cut 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.11539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.11539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangtao Wang, Xi Shen, Shell Hu, Yuan Yuan, James Crowley, Dominique Vaufreydaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers trained with self-supervised learning using self-distillation
loss (DINO) have been shown to produce attention maps that highlight salient
foreground objects. In this paper, we demonstrate a graph-based approach that
uses the self-supervised transformer features to discover an object from an
image. Visual tokens are viewed as nodes in a weighted graph with edges
representing a connectivity score based on the similarity of tokens. Foreground
objects can then be segmented using a normalized graph-cut to group
self-similar regions. We solve the graph-cut problem using spectral clustering
with generalized eigen-decomposition and show that the second smallest
eigenvector provides a cutting solution since its absolute value indicates the
likelihood that a token belongs to a foreground object. Despite its simplicity,
this approach significantly boosts the performance of unsupervised object
discovery: we improve over the recent state of the art LOST by a margin of
6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The
performance can be further improved by adding a second stage class-agnostic
detector (CAD). Our proposed method can be easily extended to unsupervised
saliency detection and weakly supervised object detection. For unsupervised
saliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,
DUT-OMRON respectively compared to previous state of the art. For weakly
supervised object detection, we achieve competitive performance on CUB and
ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PatchFormer: An Efficient Point <span class="highlight-title">Transformer</span> with Patch Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.00207v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.00207v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Cheng, Haocheng Wan, Xinyi Shen, Zizhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The point cloud learning community witnesses a modeling shift from CNNs to
Transformers, where pure Transformer architectures have achieved top accuracy
on the major learning benchmarks. However, existing point Transformers are
computationally expensive since they need to generate a large attention map,
which has quadratic complexity (both in space and time) with respect to input
size. To solve this shortcoming, we introduce Patch ATtention (PAT) to
adaptively learn a much smaller set of bases upon which the attention maps are
computed. By a weighted summation upon these bases, PAT not only captures the
global shape context but also achieves linear complexity to input size. In
addition, we propose a lightweight Multi-Scale aTtention (MST) block to build
attentions among features of different scales, providing the model with
multi-scale features. Equipped with the PAT and MST, we construct our neural
architecture called PatchFormer that integrates both modules into a joint
framework for point cloud learning. Extensive experiments demonstrate that our
network achieves comparable accuracy on general point cloud learning tasks with
9.2x speed-up than previous point Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoRD: Rotation-Robust Descriptors and Orthographic Views for Local
  Feature Matching <span class="chip">IROS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.08573v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.08573v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Udit Singh Parihar, Aniket Gujarathi, Kinal Mehta, Satyajit Tourani, Sourav Garg, Michael Milford, K. Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of local detectors and descriptors in typical computer vision
pipelines work well until variations in viewpoint and appearance change become
extreme. Past research in this area has typically focused on one of two
approaches to this challenge: the use of projections into spaces more suitable
for feature matching under extreme viewpoint changes, and attempting to learn
features that are inherently more robust to viewpoint change. In this paper, we
present a novel framework that combines learning of invariant descriptors
through data augmentation and orthographic viewpoint projection. We propose
rotation-robust local descriptors, learnt through training data augmentation
based on rotation homographies, and a correspondence ensemble technique that
combines vanilla feature correspondences with those obtained through
rotation-robust features. Using a range of benchmark datasets as well as
contributing a new bespoke dataset for this research domain, we evaluate the
effectiveness of the proposed approach on key tasks including pose estimation
and visual place recognition. Our system outperforms a range of baseline and
state-of-the-art techniques, including enabling higher levels of place
recognition precision across opposing place viewpoints and achieves
practically-useful performance levels even under extreme viewpoint changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2021. Project Page:
  https://uditsinghparihar.github.io/RoRD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Graph-based Generative Face Anonymisation with Pose Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.05496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.05496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Dall'Asen, Yiming Wang, <span class="highlight-author">Hao Tan</span>g, Luca Zanella, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AnonyGAN, a GAN-based solution for face anonymisation which
replaces the visual information corresponding to a source identity with a
condition identity provided as any single image. With the goal to maintain the
geometric attributes of the source face, i.e., the facial pose and expression,
and to promote more natural face generation, we propose to exploit a Bipartite
Graph to explicitly model the relations between the facial landmarks of the
source identity and the ones of the condition identity through a deep model. We
further propose a landmark attention model to relax the manual selection of
facial landmarks, allowing the network to weight the landmarks for the best
visual naturalness and pose preservation. Finally, to facilitate the appearance
learning, we propose a hybrid training strategy to address the challenge caused
by the lack of direct pixel-level supervision. We evaluate our method and its
variants on two public datasets, CelebA and LFW, in terms of visual
naturalness, facial pose preservation and of its impacts on face detection and
re-identification. We prove that AnonyGAN significantly outperforms the
state-of-the-art methods in terms of visual naturalness, face detection and
pose preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21st International Conference on Image analysis and Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContrastMask: <span class="highlight-title">Contrastive Learning</span> to Segment Every Thing <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehui Wang, Kai Zhao, Ruixin Zhang, Shouhong Ding, Yan Wang, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially-supervised instance segmentation is a task which requests
segmenting objects from novel unseen categories via learning on limited seen
categories with annotated masks thus eliminating demands of heavy annotation
burden. The key to addressing this task is to build an effective class-agnostic
mask segmentation model. Unlike previous methods that learn such models only on
seen categories, in this paper, we propose a new method, named ContrastMask,
which learns a mask segmentation model on both seen and unseen categories under
a unified pixel-level contrastive learning framework. In this framework,
annotated masks of seen categories and pseudo masks of unseen categories serve
as a prior for contrastive learning, where features from the mask regions
(foreground) are pulled together, and are contrasted against those from the
background, and vice versa. Through this framework, feature discrimination
between foreground and background is largely improved, facilitating learning of
the class-agnostic mask segmentation model. Exhaustive experiments on the COCO
dataset demonstrate the superiority of our method, which outperforms previous
state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ gACSON software for automated segmentation and morphology analyses of
  myelinated axons in 3D electron microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Behanova, Ali Abdollahzadeh, Ilya Belevich, Eija Jokitalo, Alejandra Sierra, Jussi Tohka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and Objective: Advances in electron microscopy (EM) now allow
three-dimensional (3D) imaging of hundreds of micrometers of tissue with
nanometer-scale resolution, providing new opportunities to study the
ultrastructure of the brain. In this work, we introduce a freely available
Matlab-based gACSON software for visualization, segmentation, assessment, and
morphology analysis of myelinated axons in 3D-EM volumes of brain tissue
samples. Methods: The software is equipped with a graphical user interface
(GUI). It automatically segments the intra-axonal space of myelinated axons and
their corresponding myelin sheaths and allows manual segmentation,
proofreading, and interactive correction of the segmented components. gACSON
analyzes the morphology of myelinated axons, such as axonal diameter, axonal
eccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of
the software by segmenting and analyzing myelinated axons in six 3D-EM volumes
of rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).
Our results suggest that the equivalent diameter of myelinated axons in
somatosensory cortex was decreased in TBI animals five months after the injury.
Conclusions: Our results indicate that gACSON is a valuable tool for
visualization, segmentation, assessment, and morphology analysis of myelinated
axons in 3D-EM volumes. It is freely available at
https://github.com/AndreaBehan/g-ACSON under the MIT license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperdimensional Feature Fusion for Interpretable Out-Of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.05341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.05341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Wilson, Tobias Fischer, Niko Sünderhauf, Feras Dayoub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce powerful ideas from Hyperdimensional Computing into the
challenging field of Out-of-Distribution (OOD) detection. In contrast to most
existing work that performs OOD detection based on only a single layer of a
neural network, we use similarity-preserving semi-orthogonal projection
matrices to project the feature maps from multiple layers into a common vector
space. By repeatedly applying the bundling operation $\oplus$, we create
expressive class-specific descriptor vectors for all in-distribution classes.
At test time, a simple and efficient cosine similarity calculation between
descriptor vectors consistently identifies OOD samples with better performance
than the current state-of-the-art. We show that the hyperdimensional fusion of
multiple network layers is critical to achieve best general performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossPoint: <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Cross-Modal</span> <span class="highlight-title">Contrastive Learning</span> for 3D
  Point Cloud Understanding <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake, Amaya Dharmasiri, Kanchana Thilakarathna, Ranga Rodrigo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual annotation of large-scale point cloud dataset for varying tasks such
as 3D object classification, segmentation and detection is often laborious
owing to the irregular structure of point clouds. Self-supervised learning,
which operates without any human labeling, is a promising approach to address
this issue. We observe in the real world that humans are capable of mapping the
visual concepts learnt from 2D images to understand the 3D world. Encouraged by
this insight, we propose CrossPoint, a simple cross-modal contrastive learning
approach to learn transferable 3D point cloud representations. It enables a
3D-2D correspondence of objects by maximizing agreement between point clouds
and the corresponding rendered 2D image in the invariant space, while
encouraging invariance to transformations in the point cloud modality. Our
joint training objective combines the feature correspondences within and across
modalities, thus ensembles a rich learning signal from both 3D point cloud and
2D image modalities in a self-supervised fashion. Experimental results show
that our approach outperforms the previous unsupervised learning methods on a
diverse range of downstream tasks including 3D object classification and
segmentation. Further, the ablation studies validate the potency of our
approach for a better point cloud understanding. Code and pretrained models are
available at http://github.com/MohamedAfham/CrossPoint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class-Aware <span class="highlight-title">Contrastive</span> Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Kai Wu, Shuyi Zhang, Guannan Jiang, Yong Liu, Feng Zheng, Wei Zhang, Chengjie Wang, Long Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo-label-based semi-supervised learning (SSL) has achieved great success
on raw data utilization. However, its training procedure suffers from
confirmation bias due to the noise contained in self-generated artificial
labels. Moreover, the model's judgment becomes noisier in real-world
applications with extensive out-of-distribution data. To address this issue, we
propose a general method named Class-aware Contrastive Semi-Supervised Learning
(CCSSL), which is a drop-in helper to improve the pseudo-label quality and
enhance the model's robustness in the real-world setting. Rather than treating
real-world data as a union set, our method separately handles reliable
in-distribution data with class-wise clustering for blending into downstream
tasks and noisy out-of-distribution data with image-wise contrastive for better
generalization. Furthermore, by applying target re-weighting, we successfully
emphasize clean label learning and simultaneously reduce noisy label learning.
Despite its simplicity, our proposed CCSSL has significant performance
improvements over the state-of-the-art SSL methods on the standard datasets
CIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, we improve
FixMatch by 9.80% and CoMatch by 3.18%. Code is available
https://github.com/TencentYoutuResearch/Classification-SemiCLS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cvpr2022 accepted, half more page for adding rebuttal Infos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danila Rukhovich, Anna Vorontsova, Anton Konushin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, promising applications in robotics and augmented reality have
attracted considerable attention to 3D object detection from point clouds. In
this paper, we present FCAF3D - a first-in-class fully convolutional
anchor-free indoor 3D object detection method. It is a simple yet effective
method that uses a voxel representation of a point cloud and processes voxels
with sparse convolutions. FCAF3D can handle large-scale scenes with minimal
runtime through a single fully convolutional feed-forward pass. Existing 3D
object detection methods make prior assumptions on the geometry of objects, and
we argue that it limits their generalization ability. To get rid of any prior
assumptions, we propose a novel parametrization of oriented bounding boxes that
allows obtaining better results in a purely data-driven way. The proposed
method achieves state-of-the-art 3D object detection results in terms of
mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The
code and models are available at https://github.com/samsunglabs/fcaf3d.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AP-BSN: <span class="highlight-title">Self-Supervised</span> Denoising for Real-World Images via Asymmetric
  PD and Blind-Spot Network <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseok Lee, Sanghyun Son, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind-spot network (BSN) and its variants have made significant advances in
self-supervised denoising. Nevertheless, they are still bound to synthetic
noisy inputs due to less practical assumptions like pixel-wise independent
noise. Hence, it is challenging to deal with spatially correlated real-world
noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has
been proposed to remove the spatial correlation of real-world noise. However,
it is not trivial to integrate PD and BSN directly, which prevents the fully
self-supervised denoising model on real-world images. We propose an Asymmetric
PD (AP) to address this issue, which introduces different PD stride factors for
training and inference. We systematically demonstrate that the proposed AP can
resolve inherent trade-offs caused by specific PD stride factors and make BSN
applicable to practical scenarios. To this end, we develop AP-BSN, a
state-of-the-art self-supervised denoising method for real-world sRGB images.
We further propose random-replacing refinement, which significantly improves
the performance of our AP-BSN without any additional parameters. Extensive
studies demonstrate that our method outperforms the other self-supervised and
even unpaired denoising methods by a large margin, without using any additional
knowledge, e.g., noise level, regarding the underlying unknown noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossLoc: Scalable Aerial Localization Assisted by <span class="highlight-title">Multimodal</span> Synthetic
  Data <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09081v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09081v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Yan, Jianhao Zheng, Simon Reding, Shanci Li, Iordan Doytchinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a visual localization system that learns to estimate camera poses
in the real world with the help of synthetic data. Despite significant progress
in recent years, most learning-based approaches to visual localization target
at a single domain and require a dense database of geo-tagged images to
function well. To mitigate the data scarcity issue and improve the scalability
of the neural localization models, we introduce TOPO-DataGen, a versatile
synthetic data generation tool that traverses smoothly between the real and
virtual world, hinged on the geographic camera viewpoint. New large-scale
sim-to-real benchmark datasets are proposed to showcase and evaluate the
utility of the said synthetic data. Our experiments reveal that synthetic data
generically enhances the neural network performance on real data. Furthermore,
we introduce CrossLoc, a cross-modal visual representation learning approach to
pose estimation that makes full use of the scene coordinate ground truth via
self-supervision. Without any extra data, CrossLoc significantly outperforms
the state-of-the-art methods and achieves substantially higher real-data sample
efficiency. Our code and datasets are all available at
https://crossloc.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. Project page: https://crossloc.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unpaired Deep Image Deraining Using Dual <span class="highlight-title">Contrastive Learning</span> <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02973v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02973v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Jinshan Pan, Kui Jiang, Yufeng Li, Yufeng Huang, Caihua Kong, Longgang Dai, Zhentao Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning single image deraining (SID) networks from an unpaired set of clean
and rainy images is practical and valuable as acquiring paired real-world data
is almost infeasible. However, without the paired data as the supervision,
learning a SID network is challenging. Moreover, simply using existing unpaired
learning methods (e.g., unpaired adversarial learning and cycle-consistency
constraints) in the SID task is insufficient to learn the underlying
relationship from rainy inputs to clean outputs as there exists significant
domain gap between the rainy and clean images. In this paper, we develop an
effective unpaired SID adversarial framework which explores mutual properties
of the unpaired exemplars by a dual contrastive learning manner in a deep
feature space, named as DCD-GAN. The proposed method mainly consists of two
cooperative branches: Bidirectional Translation Branch (BTB) and Contrastive
Guidance Branch (CGB). Specifically, BTB exploits full advantage of the
circulatory architecture of adversarial consistency to generate abundant
exemplar pairs and excavates latent feature distributions between two domains
by equipping it with bidirectional mapping. Simultaneously, CGB implicitly
constrains the embeddings of different exemplars in the deep feature space by
encouraging the similar feature distributions closer while pushing the
dissimilar further away, in order to better facilitate rain removal and help
image restoration. Extensive experiments demonstrate that our method performs
favorably against existing unpaired deraining approaches on both synthetic and
real-world datasets, and generates comparable results against several
fully-supervised or semi-supervised models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Mesh-neural Representation for 3D Transparent Object
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Xu, Zihan Zhu, Hujun Bao, Weiwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method to reconstruct the 3D shapes of transparent objects
using hand-held captured images under natural light conditions. It combines the
advantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid
representation, to simplify the capture setting used in recent contributions.
After obtaining an initial shape through the multi-view silhouettes, we
introduce surface-based local MLPs to encode the vertex displacement field
(VDF) for the reconstruction of surface details. The design of local MLPs
allows to represent the VDF in a piece-wise manner using two layer MLP
networks, which is beneficial to the optimization algorithm. Defining local
MLPs on the surface instead of the volume also reduces the searching space.
Such a hybrid representation enables us to relax the ray-pixel correspondences
that represent the light path constraint to our designed ray-cell
correspondences, which significantly simplifies the implementation of
single-image based environment matting algorithm. We evaluate our
representation and reconstruction algorithm on several transparent objects with
ground truth models. Our experiments show that our method can produce
high-quality reconstruction results superior to state-of-the-art methods using
a simplified data acquisition setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imperceptible Transfer Attack and Defense on 3D Point Cloud
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daizong Liu, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although many efforts have been made into attack and defense on the 2D image
domain in recent years, few methods explore the vulnerability of 3D models.
Existing 3D attackers generally perform point-wise perturbation over point
clouds, resulting in deformed structures or outliers, which is easily
perceivable by humans. Moreover, their adversarial examples are generated under
the white-box setting, which frequently suffers from low success rates when
transferred to attack remote black-box models. In this paper, we study 3D point
cloud attacks from two new and challenging perspectives by proposing a novel
Imperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the
perturbation direction of each point along its normal vector of the
neighborhood surface, leading to generated examples with similar geometric
properties and thus enhancing the imperceptibility. 2) Transferability: we
develop an adversarial transformation model to generate the most harmful
distortions and enforce the adversarial examples to resist it, improving their
transferability to unknown black-box models. Further, we propose to train more
robust black-box 3D models to defend against such ITA attacks by learning more
discriminative point cloud representations. Extensive evaluations demonstrate
that our ITA attack is more imperceptible and transferable than
state-of-the-arts and validate the superiority of our defense strategy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Neural Bag of Whole-Words with Col<span class="highlight-title">BERT</span>er: Contextualized
  Late Interactions using Enhanced Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, Allan Hanbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in neural information retrieval has demonstrated large gains
in effectiveness, while often sacrificing the efficiency and interpretability
of the neural model compared to classical approaches. This paper proposes
ColBERTer, a neural retrieval model using contextualized late interaction
(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,
ColBERTer's reductions dramatically lower ColBERT's storage requirements while
simultaneously improving the interpretability of its token-matching scores. To
this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and
optional lexical matching components into one model. For its multi-vector
component, ColBERTer reduces the number of stored vectors per document by
learning unique whole-word representations for the terms in each document and
learning to identify and remove word representations that are not essential to
effective scoring. We employ an explicit multi-task, multi-stage training to
facilitate using very small vector dimensions. Results on the MS MARCO and
TREC-DL collection show that ColBERTer can reduce the storage footprint by up
to 2.5x, while maintaining effectiveness. With just one dimension per token in
its smallest setting, ColBERTer achieves index storage parity with the
plaintext size, with very strong effectiveness results. Finally, we demonstrate
ColBERTer's robustness on seven high-quality out-of-domain collections,
yielding statistically significant gains over traditional retrieval baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic system for searching of employees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Evtimova-Gardair, Tasho Tashev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many people have stress to leave their job and start a new one because of the
new environment and not enough knowledge about the culture and structure about
the new organization they are going to work in. New employees in company
normally need to integrate in their working place environment quicker to start
doing their job. That makes them ask a lot of questions to their colleagues and
sometimes their colleagues are too busy to answer those questions. In the
literature is defined that this problem could be solved when new employees use
digital system for information as the proposed system for searching of
information. Furthermore, the quality of the returned results from the
searching system is defined as a standard for the efficiency of the searching
systems. Because of this, it is proposed a semantic system for searching
information of employees in a company that will help to better orient new
employees in a company, to know the position and the function of each employee
in the company
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>132-136 pages, 9 figures,International Conference on Applied Physics,
  Simulation and Computing (APSAC)Croatia, Dubrovnik, Sept.28-29 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting the longevity of resources shared in scientific publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel E. Acuna, Jian Jian, Tong Zeng, Lizhen Liang, Han Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research has shown that most resources shared in articles (e.g., URLs to code
or data) are not kept up to date and mostly disappear from the web after some
years (Zeng et al., 2019). Little is known about the factors that differentiate
and predict the longevity of these resources. This article explores a range of
explanatory features related to the publication venue, authors, references, and
where the resource is shared. We analyze an extensive repository of
publications and, through web archival services, reconstruct how they looked at
different time points. We discover that the most important factors are related
to where and how the resource is shared, and surprisingly little is explained
by the author's reputation or prestige of the journal. By examining the places
where long-lasting resources are shared, we suggest that it is critical to
disseminate and create standards with modern technologies. Finally, we discuss
implications for reproducibility and recognizing scientific datasets as
first-class citizens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Reinforcement Learning for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.10665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.10665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanguo Lin, Yong Liu, Fan Lin, Lixin Zou, Pengcheng Wu, Wenhua Zeng, Huanhuan Chen, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been widely applied in different real-life scenarios
to help us find useful information. In particular, Reinforcement Learning (RL)
based recommender systems have become an emerging research topic in recent
years. Empirical results show that RL-based recommendation methods often
surpass most of supervised learning methods, owing to the interactive nature
and autonomous learning ability. Nevertheless, there are various challenges of
applying RL in recommender systems. To understand the challenges and relevant
solutions, there should be a reference for researchers and practitioners
working on RL-based recommender systems. To this end, we firstly provide a
thorough overview, comparisons, and summarization of RL approaches applied in
four typical recommendation scenarios, including interactive recommendation,
conversational recommendatin, sequential recommendation, and explainable
recommendation. We further systematically analyze the challenges and relevant
solutions on the basis of existing literature. Finally, under discussion for
open issues of RL and its limitations of recommender systems, we highlight some
potential research directions in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Validating Simulations of User Query Variants <span class="chip">ECIR22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Breuer, Norbert Fuhr, Philipp Schaer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System-oriented IR evaluations are limited to rather abstract understandings
of real user behavior. As a solution, simulating user interactions provides a
cost-efficient way to support system-oriented experiments with more realistic
directives when no interaction logs are available. While there are several user
models for simulated clicks or result list interactions, very few attempts have
been made towards query simulations, and it has not been investigated if these
can reproduce properties of real queries. In this work, we validate simulated
user query variants with the help of TREC test collections in reference to real
user queries that were made for the corresponding topics. Besides, we introduce
a simple yet effective method that gives better reproductions of real queries
than the established methods. Our evaluation framework validates the
simulations regarding the retrieval performance, reproducibility of topic score
distributions, shared task utility, effort and effect, and query term
similarity when compared with real user query variants. While the retrieval
effectiveness and statistical properties of the topic score distributions as
well as economic aspects are close to that of real queries, it is still
challenging to simulate exact term matches and later query reformulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECIR22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EILEEN: A recommendation system for scientific publications and grants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.09663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.09663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel E. Acuna, Kartik Nagre, Priya Matnani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding relevant scientific articles is crucial for advancing knowledge.
Recommendation systems are helpful for such purpose, although they have only
been applied to science recently. This article describes EILEEN (Exploratory
Innovator of LitEraturE Networks), a recommendation system for scientific
publications and grants with open source code and datasets. We describe
EILEEN's architecture for ingesting and processing documents and modeling the
recommendation system and keyphrase estimator. Using a unique dataset of log-in
user behavior, we validate our recommendation system against Latent Semantic
Analysis (LSA) and the standard ranking from Elasticsearch (Lucene scoring). We
find that a learning-to-rank with Random Forest achieves an AUC of 0.9,
significantly outperforming both baselines. Our results suggest that we can
substantially improve science recommendations and learn about scientists'
behavior through their search behavior. We make our system available through
eileen.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient
  Dexterous Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sridhar Pandian Arunachalam, Sneha Silwal, Ben Evans, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on StyleGAN show high performance on artistic portrait
generation by transfer learning with limited data. In this paper, we explore
more challenging exemplar-based high-resolution portrait style transfer by
introducing a novel DualStyleGAN with flexible control of dual styles of the
original face domain and the extended artistic portrait domain. Different from
StyleGAN, DualStyleGAN provides a natural way of style transfer by
characterizing the content and style of a portrait with an intrinsic style path
and a new extrinsic style path, respectively. The delicately designed extrinsic
style path enables our model to modulate both the color and complex structural
styles hierarchically to precisely pastiche the style example. Furthermore, a
novel progressive fine-tuning scheme is introduced to smoothly transform the
generative space of the model to the target domain, even with the above
modifications on the network architecture. Experiments demonstrate the
superiority of DualStyleGAN over state-of-the-art methods in high-quality
portrait style transfer and flexible style control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022. Code: https://github.com/williamyang1991/DualStyleGAN
  Project page: https://www.mmlab-ntu.com/project/dualstylegan/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Dropping for Efficient <span class="highlight-title">BERT</span> <span class="highlight-title">Pretrain</span>ing <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models generally allocate the same amount of computation
for each token in a given sequence. We develop a simple but effective "token
dropping" method to accelerate the pretraining of transformer models, such as
BERT, without degrading its performance on downstream tasks. In short, we drop
unimportant tokens starting from an intermediate layer in the model to make the
model focus on important tokens; the dropped tokens are later picked up by the
last layer of the model so that the model still produces full-length sequences.
We leverage the already built-in masked language modeling (MLM) loss to
identify unimportant tokens with practically no computational overhead. In our
experiments, this simple approach reduces the pretraining cost of BERT by 25%
while achieving similar overall fine-tuning performance on standard downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributionally Robust Optimization via Ball Oracle Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Carmon, Danielle Hausler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop and analyze algorithms for distributionally robust optimization
(DRO) of convex losses. In particular, we consider group-structured and bounded
$f$-divergence uncertainty sets. Our approach relies on an accelerated method
that queries a ball optimization oracle, i.e., a subroutine that minimizes the
objective within a small ball around the query point. Our main contribution is
efficient implementations of this oracle for DRO objectives. For DRO with $N$
non-smooth loss functions, the resulting algorithms find an $\epsilon$-accurate
solution with $\widetilde{O}\left(N\epsilon^{-2/3} + \epsilon^{-2}\right)$
first-order oracle queries to individual loss functions. Compared to existing
algorithms for this problem, we improve complexity by a factor of up to
$\epsilon^{-4/3}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Evolutionary Algorithms Safe Optimizers? <span class="chip">GECCO 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngmin Kim, Richard Allmendinger, Manuel López-Ibáñez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a type of constrained optimization problem, where the violation
of a constraint leads to an irrevocable loss, such as breakage of a valuable
experimental resource/platform or loss of human life. Such problems are
referred to as safe optimization problems (SafeOPs). While SafeOPs have
received attention in the machine learning community in recent years, there was
little interest in the evolutionary computation (EC) community despite some
early attempts between 2009 and 2011. Moreover, there is a lack of acceptable
guidelines on how to benchmark different algorithms for SafeOPs, an area where
the EC community has significant experience in. Driven by the need for more
efficient algorithms and benchmark guidelines for SafeOPs, the objective of
this paper is to reignite the interest of this problem class in the EC
community. To achieve this we (i) provide a formal definition of SafeOPs and
contrast it to other types of optimization problems that the EC community is
familiar with, (ii) investigate the impact of key SafeOP parameters on the
performance of selected safe optimization algorithms, (iii) benchmark EC
against state-of-the-art safe optimization algorithms from the machine learning
community, and (iv) provide an open-source Python framework to replicate and
extend our work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for GECCO 2022. 8 pages, excluding references, accompanied
  by supplementary material (4 pages, excluding references). 6 figures (and 6
  figures in supplementary material also)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Exemplar-Free Continual Learning in Vision <span class="highlight-title">Transformer</span>s: an
  Account of Attention, Functional and Weight Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Pelosin, Saurav Jha, Andrea Torsello, Bogdan Raducanu, Joost van de Weijer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Missing Sources with Adversarial Support-Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Kehrenberg, Myles Bartlett, Viktoriia Sharmanska, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When trained on diverse labeled data, machine learning models have proven
themselves to be a powerful tool in all facets of society. However, due to
budget limitations, deliberate or non-deliberate censorship, and other problems
during data collection and curation, the labeled training set might exhibit a
systematic shortage of data for certain groups. We investigate a scenario in
which the absence of certain data is linked to the second level of a two-level
hierarchy in the data. Inspired by the idea of protected groups from
algorithmic fairness, we refer to the partitions carved by this second level as
"subgroups"; we refer to combinations of subgroups and classes, or leaves of
the hierarchy, as "sources". To characterize the problem, we introduce the
concept of classes with incomplete subgroup support. The representational bias
in the training set can give rise to spurious correlations between the classes
and the subgroups which render standard classification models ungeneralizable
to unseen sources. To overcome this bias, we make use of an additional, diverse
but unlabeled dataset, called the "deployment set", to learn a representation
that is invariant to subgroup. This is done by adversarially matching the
support of the training and deployment sets in representation space. In order
to learn the desired invariance, it is paramount that the sets of samples
observed by the discriminator are balanced by class; this is easily achieved
for the training set, but requires using semi-supervised clustering for the
deployment set. We demonstrate the effectiveness of our method with experiments
on several datasets and variants of the problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-armed bandits for online optimization of language model
  <span class="highlight-title">pre-train</span>ing: the use case of dynamic masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iñigo Urteaga, Moulay-Zaïdane Draïdia, Tomer Lancewicki, Shahram Khadivi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models (TLMs) provide state-of-the-art performance
in many modern natural language processing applications. TLM training is
conducted in two phases. First, the model is pre-trained over large volumes of
text to minimize a generic objective function, such as the Masked Language
Model (MLM). Second, the model is fine-tuned in specific downstream tasks.
Pre-training requires large volumes of data and high computational resources,
while introducing many still unresolved design choices. For instance, selecting
hyperparameters for language model pre-training is often carried out based on
heuristics or grid-based searches. In this work, we propose a multi-armed
bandit-based online optimization framework for the sequential selection of
pre-training hyperparameters to optimize language model performance. We pose
the pre-training procedure as a sequential decision-making task, where at each
pre-training step, an agent must determine what hyperparameters to use towards
optimizing the pre-training objective. We propose a Thompson sampling bandit
algorithm, based on a surrogate Gaussian process reward model of the MLM
pre-training objective, for its sequential minimization. We empirically show
how the proposed Gaussian process based Thompson sampling pre-trains robust and
well-performing language models. Namely, by sequentially selecting masking
hyperparameters of the TLM, we achieve satisfactory performance in less epochs,
not only in terms of the pre-training MLM objective, but in diverse downstream
fine-tuning tasks. The proposed bandit-based technique provides an automated
hyperparameter selection method for pre-training TLMs of interest to
practitioners. In addition, our results indicate that, instead of MLM
pre-training with fixed masking probabilities, sequentially adapting the
masking hyperparameters improves both pre-training loss and downstream task
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, <span class="highlight-author">Devi Parikh</span>, Yaniv Taigman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image generation methods provide a simple yet exciting
conversion capability between text and image domains. While these methods have
incrementally improved the generated image fidelity and text relevancy, several
pivotal gaps remain unanswered, limiting applicability and quality. We propose
a novel text-to-image method that addresses these gaps by (i) enabling a simple
control mechanism complementary to text in the form of a scene, (ii)
introducing elements that substantially improve the tokenization process by
employing domain-specific knowledge over key image regions (faces and salient
objects), and (iii) adapting classifier-free guidance for the transformer use
case. Our model achieves state-of-the-art FID and human evaluation results,
unlocking the ability to generate high fidelity images in a resolution of
512x512 pixels, significantly improving visual quality. Through scene
controllability, we introduce several new capabilities: (i) Scene editing, (ii)
text editing with anchor scenes, (iii) overcoming out-of-distribution text
prompts, and (iv) story illustration generation, as demonstrated in the story
we wrote.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Tracking using Likelihood Modeling of Channel Features with
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Kram, Christopher Kraus, Tobias Feigl, Maximilian Stahlke, Jörg Robert, Christopher Mutschler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent localization frameworks exploit spatial information of complex channel
measurements (CMs) to estimate accurate positions even in multipath propagation
scenarios. State-of-the art CM fingerprinting(FP)-based methods employ
convolutional neural networks (CNN) to extract the spatial information.
However, they need spatially dense data sets (associated with high acquisition
and maintenance efforts) to work well -- which is rarely the case in practical
applications. If such data is not available (or its quality is low), we cannot
compensate the performance degradation of CNN-based FP as they do not provide
statistical position estimates, which prevents a fusion with other sources of
information on the observation level.
  We propose a novel localization framework that adapts well to sparse datasets
that only contain CMs of specific areas within the environment with strong
multipath propagation. Our framework compresses CMs into informative features
to unravel spatial information. It then regresses Gaussian processes (GPs) for
each of them, which imply statistical observation models based on
distance-dependent covariance kernels. Our framework combines the trained GPs
with line-of-sight ranges and a dynamics model in a particle filter. Our
measurements show that our approach outperforms state-of-the-art CNN
fingerprinting (0.52 m vs. 1.3 m MAE) on spatially sparse data collected in a
realistic industrial indoor environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Artificial Intelligence for Exhaust Gas Temperature of
  Turbofan Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Kefalas, Juan de Santiago Rojo Jr., Asteris Apostolidis, Dirk van den Herik, Bas van Stein, Thomas Bäck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven modeling is an imperative tool in various industrial
applications, including many applications in the sectors of aeronautics and
commercial aviation. These models are in charge of providing key insights, such
as which parameters are important on a specific measured outcome or which
parameter values we should expect to observe given a set of input parameters.
At the same time, however, these models rely heavily on assumptions (e.g.,
stationarity) or are "black box" (e.g., deep neural networks), meaning that
they lack interpretability of their internal working and can be viewed only in
terms of their inputs and outputs. An interpretable alternative to the "black
box" models and with considerably less assumptions is symbolic regression (SR).
SR searches for the optimal model structure while simultaneously optimizing the
model's parameters without relying on an a-priori model structure. In this
work, we apply SR on real-life exhaust gas temperature (EGT) data, collected at
high frequencies through the entire flight, in order to uncover meaningful
algebraic relationships between the EGT and other measurable engine parameters.
The experimental results exhibit promising model accuracy, as well as
explainability returning an absolute difference of 3{\deg}C compared to the
ground truth and demonstrating consistency from an engineering perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 20 pages, 4 figures. Supplemental material: 18 pages, 30
  figures. Published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Neural Bag of Whole-Words with Col<span class="highlight-title">BERT</span>er: Contextualized
  Late Interactions using Enhanced Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, Allan Hanbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in neural information retrieval has demonstrated large gains
in effectiveness, while often sacrificing the efficiency and interpretability
of the neural model compared to classical approaches. This paper proposes
ColBERTer, a neural retrieval model using contextualized late interaction
(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,
ColBERTer's reductions dramatically lower ColBERT's storage requirements while
simultaneously improving the interpretability of its token-matching scores. To
this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and
optional lexical matching components into one model. For its multi-vector
component, ColBERTer reduces the number of stored vectors per document by
learning unique whole-word representations for the terms in each document and
learning to identify and remove word representations that are not essential to
effective scoring. We employ an explicit multi-task, multi-stage training to
facilitate using very small vector dimensions. Results on the MS MARCO and
TREC-DL collection show that ColBERTer can reduce the storage footprint by up
to 2.5x, while maintaining effectiveness. With just one dimension per token in
its smallest setting, ColBERTer achieves index storage parity with the
plaintext size, with very strong effectiveness results. Finally, we demonstrate
ColBERTer's robustness on seven high-quality out-of-domain collections,
yielding statistically significant gains over traditional retrieval baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiFi++: a Unified Framework for Neural Vocoding, Bandwidth Extension and
  Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Andreev, Aibek Alanov, Oleg Ivanov, Dmitry Vetrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial networks have recently demonstrated outstanding
performance in neural vocoding outperforming best autoregressive and flow-based
models. In this paper, we show that this success can be extended to other tasks
of conditional audio generation. In particular, building upon HiFi vocoders, we
propose a novel HiFi++ general framework for neural vocoding, bandwidth
extension, and speech enhancement. We show that with the improved generator
architecture and simplified multi-discriminator training, HiFi++ performs on
par with the state-of-the-art in these tasks while spending significantly less
memory and computational resources. The effectiveness of our approach is
validated through a series of extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locally Asynchronous Stochastic Gradient Descent for Decentralised Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Avidor, Nadav Tal Israel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed training algorithms of deep neural networks show impressive
convergence speedup properties on very large problems. However, they inherently
suffer from communication related slowdowns and communication topology becomes
a crucial design choice. Common approaches supported by most machine learning
frameworks are: 1) Synchronous decentralized algorithms relying on a
peer-to-peer All Reduce topology that is sensitive to stragglers and
communication delays. 2) Asynchronous centralised algorithms with a server
based topology that is prone to communication bottleneck. Researchers also
suggested asynchronous decentralized algorithms designed to avoid the
bottleneck and speedup training, however, those commonly use inexact sparse
averaging that may lead to a degradation in accuracy. In this paper, we propose
Local Asynchronous SGD (LASGD), an asynchronous decentralized algorithm that
relies on All Reduce for model synchronization.
  We empirically validate LASGD's performance on image classification tasks on
the ImageNet dataset. Our experiments demonstrate that LASGD accelerates
training compared to SGD and state of the art gossip based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Dutch Draw: Constructing a Universal Baseline for Binary Prediction
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne van de Bijl, Jan Klein, Joris Pries, Sandjai Bhulai, Mark Hoogendoorn, Rob van der Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel prediction methods should always be compared to a baseline to know how
well they perform. Without this frame of reference, the performance score of a
model is basically meaningless. What does it mean when a model achieves an
$F_1$ of 0.8 on a test set? A proper baseline is needed to evaluate the
`goodness' of a performance score. Comparing with the latest state-of-the-art
model is usually insightful. However, being state-of-the-art can change rapidly
when newer models are developed. Contrary to an advanced model, a simple dummy
classifier could be used. However, the latter could be beaten too easily,
making the comparison less valuable. This paper presents a universal baseline
method for all binary classification models, named the Dutch Draw (DD). This
approach weighs simple classifiers and determines the best classifier to use as
a baseline. We theoretically derive the DD baseline for many commonly used
evaluation measures and show that in most situations it reduces to (almost)
always predicting either zero or one. Summarizing, the DD baseline is: (1)
general, as it is applicable to all binary classification problems; (2) simple,
as it is quickly determined without training or parameter-tuning; (3)
informative, as insightful conclusions can be drawn from the results. The DD
baseline serves two purposes. First, to enable comparisons across research
papers by this robust and universal baseline. Secondly, to provide a sanity
check during the development process of a prediction model. It is a major
warning sign when a model is outperformed by the DD baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwiftAgg+: Achieving Asymptotically Optimal Communication Load in Secure
  Aggregation for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tayyebeh Jahani-Nezhad, Mohammad Ali Maddah-Ali, Songze Li, Giuseppe Caire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SwiftAgg+, a novel secure aggregation protocol for federated
learning systems, where a central server aggregates local models of
$N\in\mathbb{N}$ distributed users, each of size $L \in \mathbb{N}$, trained on
their local data, in a privacy-preserving manner. SwiftAgg+ can significantly
reduce the communication overheads without any compromise on security, and
achieve the optimum communication load within a diminishing gap. Specifically,
in presence of at most $D$ dropout users, SwiftAgg+ achieves average per-user
communication load of $(1+\mathcal{O}(\frac{1}{N}))L$ and the server
communication load of $(1+\mathcal{O}(\frac{1}{N}))L$, with a worst-case
information-theoretic security guarantee, against any subset of up to $T$
semi-honest users who may also collude with the curious server. The proposed
SwiftAgg+ has also a flexibility to reduce the number of active communication
links at the cost of increasing the the communication load between the users
and the server. In particular, for any $K\in\mathbb{N}$, SwiftAgg+ can achieve
the uplink communication load of $(1+\frac{T}{K})L$, and per-user communication
load of up to $(1-\frac{1}{N})(1+\frac{T+D}{K})L$, where the number of
pair-wise active connections in the network is $\frac{N}{2}(K+T+D+1)$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2202.04169</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Prediction of Pulmonary Hypertension in Newborns using
  Echocardiograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Ragnarsdottir, Laura Manduchi, Holger Michel, Fabian Laumer, Sven Wellmann, Ece Ozkan, Julia Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary hypertension (PH) in newborns and infants is a complex condition
associated with several pulmonary, cardiac, and systemic diseases contributing
to morbidity and mortality. Therefore, accurate and early detection of PH is
crucial for successful management. Using echocardiography, the primary
diagnostic tool in pediatrics, human assessment is both time-consuming and
expertise-demanding, raising the need for an automated approach. In this work,
we present an interpretable multi-view video-based deep learning approach to
predict PH for a cohort of 194 newborns using echocardiograms. We use
spatio-temporal convolutional architectures for the prediction of PH from each
view, and aggregate the predictions of the different views using majority
voting. To the best of our knowledge, this is the first work for an automated
assessment of PH in newborns using echocardiograms. Our results show a mean
F1-score of 0.84 for severity prediction and 0.92 for binary detection using
10-fold cross-validation. We complement our predictions with saliency maps and
show that the learned model focuses on clinically relevant cardiac structures,
motivating its usage in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Orientation to Distinguish Overlapping Chromosomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Kluvanec, Thomas B. Phillips, Kenneth J. W. McCaffrey, Noura Al Moubayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A difficult step in the process of karyotyping is segmenting chromosomes that
touch or overlap. In an attempt to automate the process, previous studies
turned to Deep Learning methods, with some formulating the task as a semantic
segmentation problem. These models treat separate chromosome instances as
semantic classes, which we show to be problematic, since it is uncertain which
chromosome should be classed as #1 and #2. Assigning class labels based on
comparison rules, such as the shorter/longer chromosome alleviates, but does
not fully resolve the issue. Instead, we separate the chromosome instances in a
second stage, predicting the orientation of the chromosomes by the model and
use it as one of the key distinguishing factors of the chromosomes. We
demonstrate this method to be effective. Furthermore, we introduce a novel
Double-Angle representation that a neural network can use to predict the
orientation. The representation maps any direction and its reverse to the same
point. Lastly, we present a new expanded synthetic dataset, which is based on
Pommier's dataset, but addresses its issues with insufficient separation
between its training and testing sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference for Health, Inference, and Learning (CHIL) 2022 - Invited
  non-archival presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep-Discrete Learning Framework for Spherical Surface Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed A. Suliman, Logan Z. J. Williams, Abdulah Fawaz, Emma C. Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Explanations for Entity Resolution Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Teofili, Donatella Firmani, Nick Koudas, Vincenzo Martello, Paolo Merialdo, Divesh Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity resolution (ER) aims at matching records that refer to the same
real-world entity. Although widely studied for the last 50 years, ER still
represents a challenging data management problem, and several recent works have
started to investigate the opportunity of applying deep learning (DL)
techniques to solve this problem. In this paper, we study the fundamental
problem of explainability of the DL solution for ER. Understanding the matching
predictions of an ER solution is indeed crucial to assess the trustworthiness
of the DL model and to discover its biases. We treat the DL model as a black
box classifier and - while previous approaches to provide explanations for DL
predictions are agnostic to the classification task. we propose the CERTA
approach that is aware of the semantics of the ER problem. Our approach
produces both saliency explanations, which associate each attribute with a
saliency score, and counterfactual explanations, which provide examples of
values that can flip the prediction. CERTA builds on a probabilistic framework
that aims at computing the explanations evaluating the outcomes produced by
using perturbed copies of the input records. We experimentally evaluate CERTA's
explanations of state-of-the-art ER solutions based on DL models using publicly
available datasets, and demonstrate the effectiveness of CERTA over recently
proposed methods for this problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended critical regimes of deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Kevin Qu, Asem Wardak, Pulin Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have been successfully applied to many real-world
problems, but a complete understanding of their dynamical and computational
principles is still lacking. Conventional theoretical frameworks for analysing
DNNs often assume random networks with coupling weights obeying Gaussian
statistics. However, non-Gaussian, heavy-tailed coupling is a ubiquitous
phenomenon in DNNs. Here, by weaving together theories of heavy-tailed random
matrices and non-equilibrium statistical physics, we develop a new type of mean
field theory for DNNs which predicts that heavy-tailed weights enable the
emergence of an extended critical regime without fine-tuning parameters. In
this extended critical regime, DNNs exhibit rich and complex propagation
dynamics across layers. We further elucidate that the extended criticality
endows DNNs with profound computational advantages: balancing the contraction
as well as expansion of internal neural representations and speeding up
training processes, hence providing a theoretical guide for the design of
efficient neural architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Removal in Sampling-based Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaopeng Fu, Fengxiang He, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In International Conference on Learning Representations, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized incentives as feedback design in generalized Nash
  equilibrium problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Fabiani, Andrea Simonetto, Paul J. Goulart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate both stationary and time-varying, nonmonotone generalized Nash
equilibrium problems that exhibit symmetric interactions among the agents,
which are known to be potential. As may happen in practical cases, however, we
envision a scenario in which the formal expression of the underlying potential
function is not available, and we design a semi-decentralized Nash equilibrium
seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively
integrates the (possibly noisy and sporadic) agents' feedback to learn the
pseudo-gradients of the agents, and then design personalized incentives for
them. On their side, the agents receive those personalized incentives, compute
a solution to an extended game, and then return feedback measurements to the
coordinator. In the stationary setting, our algorithm returns a Nash
equilibrium in case the coordinator is endowed with standard learning policies,
while it returns a Nash equilibrium up to a constant, yet adjustable, error in
the time-varying case. As a motivating application, we consider the ridehailing
service provided by several companies with mobility as a service orchestration,
necessary to both handle competition among firms and avoid traffic congestion,
which is also adopted to run numerical experiments verifying our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2111.03854</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mc<span class="highlight-title">BERT</span>: Momentum <span class="highlight-title">Contrastive Learning</span> with <span class="highlight-title">BERT</span> for Zero-Shot Slot
  Filling <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seong-Hwan Heo, WonKee Lee, Jong-Hyeok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot slot filling has received considerable attention to cope with the
problem of limited available data for the target domain. One of the important
factors in zero-shot learning is to make the model learn generalized and
reliable representations. For this purpose, we present mcBERT, which stands for
momentum contrastive learning with BERT, to develop a robust zero-shot slot
filling model. mcBERT uses BERT to initialize the two encoders, the query
encoder and key encoder, and is trained by applying momentum contrastive
learning. Our experimental results on the SNIPS benchmark show that mcBERT
substantially outperforms the previous models, recording a new
state-of-the-art. Besides, we also show that each component composing mcBERT
contributes to the performance improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bioformers: Embedding <span class="highlight-title">Transformer</span>s for Ultra-Low Power sEMG-based
  Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Burrello, Francesco Bianco Morghet, Moritz Scherer, Simone Benatti, Luca Benini, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-machine interaction is gaining traction in rehabilitation tasks, such
as controlling prosthetic hands or robotic arms. Gesture recognition exploiting
surface electromyographic (sEMG) signals is one of the most promising
approaches, given that sEMG signal acquisition is non-invasive and is directly
related to muscle contraction. However, the analysis of these signals still
presents many challenges since similar gestures result in similar muscle
contractions. Thus the resulting signal shapes are almost identical, leading to
low classification accuracy. To tackle this challenge, complex neural networks
are employed, which require large memory footprints, consume relatively high
energy and limit the maximum battery life of devices used for classification.
This work addresses this problem with the introduction of the Bioformers. This
new family of ultra-small attention-based architectures approaches
state-of-the-art performance while reducing the number of parameters and
operations of 4.9X. Additionally, by introducing a new inter-subjects
pre-training, we improve the accuracy of our best Bioformer by 3.39%, matching
state-of-the-art accuracy without any additional inference cost. Deploying our
best performing Bioformer on a Parallel, Ultra-Low Power (PULP) microcontroller
unit (MCU), the GreenWaves GAP8, we achieve an inference latency and energy of
2.72 ms and 0.14 mJ, respectively, 8.0X lower than the previous
state-of-the-art neural network, while occupying just 94.2 kB of memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TCN Mapping Optimization for Ultra-Low Power Time-Series Edge Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Burrello, Alberto Dequino, Daniele Jahier Pagliari, Francesco Conti, Marcello Zanghieri, Enrico Macii, Luca Benini, Massimo Poncino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Convolutional Networks (TCNs) are emerging lightweight Deep Learning
models for Time Series analysis. We introduce an automated exploration approach
and a library of optimized kernels to map TCNs on Parallel Ultra-Low Power
(PULP) microcontrollers. Our approach minimizes latency and energy by
exploiting a layer tiling optimizer to jointly find the tiling dimensions and
select among alternative implementations of the causal and dilated
1D-convolution operations at the core of TCNs. We benchmark our approach on a
commercial PULP device, achieving up to 103X lower latency and 20.3X lower
energy than the Cube-AI toolkit executed on the STM32L4 and from 2.9X to 26.6X
lower energy compared to commercial closed-source and academic open-source
approaches on the same hardware target.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Horizon-Free Reinforcement Learning in Polynomial Time: the Power of
  Stationary Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhang, Xiangyang Ji, Simon S. Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper gives the first polynomial-time algorithm for tabular Markov
Decision Processes (MDP) that enjoys a regret bound \emph{independent on the
planning horizon}. Specifically, we consider tabular MDP with $S$ states, $A$
actions, a planning horizon $H$, total reward bounded by $1$, and the agent
plays for $K$ episodes. We design an algorithm that achieves an
$O\left(\mathrm{poly}(S,A,\log K)\sqrt{K}\right)$ regret in contrast to
existing bounds which either has an additional $\mathrm{polylog}(H)$
dependency~\citep{zhang2020reinforcement} or has an exponential dependency on
$S$~\citep{li2021settling}. Our result relies on a sequence of new structural
lemmas establishing the approximation power, stability, and concentration
property of stationary policies, which can have applications in other problems
related to Markov chains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rubik's Cube Operator: A Plug And Play Permutation Module for Better
  Arranging High Dimensional Industrial Data in Deep Convolutional Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoxiao Yang, Zhong Zheng, Zijun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convolutional neural network (CNN) has been widely applied to process the
industrial data based tensor input, which integrates data records of
distributed industrial systems from the spatial, temporal, and system dynamics
aspects. However, unlike images, information in the industrial data based
tensor is not necessarily spatially ordered. Thus, directly applying CNN is
ineffective. To tackle such issue, we propose a plug and play module, the
Rubik's Cube Operator (RCO), to adaptively permutate the data organization of
the industrial data based tensor to an optimal or suboptimal order of
attributes before being processed by CNNs, which can be updated with subsequent
CNNs together via the gradient-based optimizer. The proposed RCO maintains K
binary and right stochastic permutation matrices to permutate attributes of K
axes of the input industrial data based tensor. A novel learning process is
proposed to enable learning permutation matrices from data, where the
Gumbel-Softmax is employed to reparameterize elements of permutation matrices,
and the soft regularization loss is proposed and added to the task-specific
loss to ensure the feature diversity of the permuted data. We verify the
effectiveness of the proposed RCO via considering two representative learning
tasks processing industrial data via CNNs, the wind power prediction (WPP) and
the wind speed prediction (WSP) from the renewable energy domain. Computational
experiments are conducted based on four datasets collected from different wind
farms and the results demonstrate that the proposed RCO can improve the
performance of CNN based networks significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dense Correspondence from Synthetic Environments <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mithun Lal, Anthony Paproki, Nariman Habili, Lars Petersson, Olivier Salvado, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimation of human shape and pose from a single image is a challenging task.
It is an even more difficult problem to map the identified human shape onto a
3D human model. Existing methods map manually labelled human pixels in real 2D
images onto the 3D surface, which is prone to human error, and the sparsity of
available annotated data often leads to sub-optimal results. We propose to
solve the problem of data scarcity by training 2D-3D human mapping algorithms
using automatically generated synthetic data for which exact and dense 2D-3D
correspondence is known. Such a learning strategy using synthetic environments
has a high generalisation potential towards real-world data. Using different
camera parameter variations, background and lighting settings, we created
precise ground truth data that constitutes a wider distribution. We evaluate
the performance of models trained on synthetic using the COCO dataset and
validation framework. Results show that training 2D-3D mapping network models
on synthetic data is a viable alternative to using real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICIP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofei Xie, Tianlin Li, Jian Wang, Lei Ma, Qing Guo, Felix Juefei-Xu, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has recently been widely applied to many applications across
different domains, e.g., image classification and audio recognition. However,
the quality of Deep Neural Networks (DNNs) still raises concerns in the
practical operational environment, which calls for systematic testing,
especially in safety-critical scenarios. Inspired by software testing, a number
of structural coverage criteria are designed and proposed to measure the test
adequacy of DNNs. However, due to the blackbox nature of DNN, the existing
structural coverage criteria are difficult to interpret, making it hard to
understand the underlying principles of these criteria. The relationship
between the structural coverage and the decision logic of DNNs is unknown.
Moreover, recent studies have further revealed the non-existence of correlation
between the structural coverage and DNN defect detection, which further posts
concerns on what a suitable DNN testing criterion should be.
  In this paper, we propose the interpretable coverage criteria through
constructing the decision structure of a DNN. Mirroring the control flow graph
of the traditional program, we first extract a decision graph from a DNN based
on its interpretation, where a path of the decision graph represents a decision
logic of the DNN. Based on the control flow and data flow of the decision
graph, we propose two variants of path coverage to measure the adequacy of the
test cases in exercising the decision logic. The higher the path coverage, the
more diverse decision logic the DNN is expected to be explored. Our large-scale
evaluation results demonstrate that: the path in the decision graph is
effective in characterizing the decision of the DNN, and the proposed coverage
criteria are also sensitive with errors including natural errors and
adversarial examples, and strongly correlated with the output impartiality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages. Accepted to ACM Transactions on Software Engineering and
  Methodology (TOSEM), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mono vs Multilingual <span class="highlight-title">BERT</span>: A Case Study in Hindi and Marathi Named
  Entity Recognition <span class="chip">SC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onkar Litake, Maithili Sabane, Parth Patil, Aparna Ranade, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is the process of recognising and classifying
important information (entities) in text. Proper nouns, such as a person's
name, an organization's name, or a location's name, are examples of entities.
The NER is one of the important modules in applications like human resources,
customer support, search engines, content classification, and academia. In this
work, we consider NER for low-resource Indian languages like Hindi and Marathi.
The transformer-based models have been widely used for NER tasks. We consider
different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark
them on publicly available Hindi and Marathi NER datasets. We provide an
exhaustive comparison of different monolingual and multilingual
transformer-based models and establish simple baselines currently missing in
the literature. We show that the monolingual MahaRoBERTa model performs the
best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for
Hindi NER. We also perform cross-language evaluation and present mixed
observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICMISC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Unsupervised Knowledge Transfer from Social Discussions Help
  Argument Mining? <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Dutta, Jeevesh Juneja, Dipankar Das, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying argument components from unstructured texts and predicting the
relationships expressed among them are two primary steps of argument mining.
The intrinsic complexity of these tasks demands powerful learning models. While
pretrained Transformer-based Language Models (LM) have been shown to provide
state-of-the-art results over different NLP tasks, the scarcity of manually
annotated data and the highly domain-dependent nature of argumentation restrict
the capabilities of such models. In this work, we propose a novel transfer
learning strategy to overcome these challenges. We utilize argumentation-rich
social discussions from the ChangeMyView subreddit as a source of unsupervised,
argumentative discourse-aware knowledge by finetuning pretrained LMs on a
selectively masked language modeling task. Furthermore, we introduce a novel
prompt-based strategy for inter-component relation prediction that compliments
our proposed finetuning method while leveraging on the discourse context.
Exhaustive experiments show the generalization capability of our method on
these two tasks over within-domain as well as out-of-domain datasets,
outperforming several existing and employed strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyRep: Bootstrapping Training with Dynamic Re-parameterization <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Shan You, Bohan Zhang, Yuxuan Du, Fei Wang, Chen Qian, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural re-parameterization (Rep) methods achieve noticeable improvements
on simple VGG-style networks. Despite the prevalence, current Rep methods
simply re-parameterize all operations into an augmented network, including
those that rarely contribute to the model's performance. As such, the price to
pay is an expensive computational overhead to manipulate these unnecessary
behaviors. To eliminate the above caveats, we aim to bootstrap the training
with minimal cost by devising a dynamic re-parameterization (DyRep) method,
which encodes Rep technique into the training process that dynamically evolves
the network structures. Concretely, our proposal adaptively finds the
operations which contribute most to the loss in the network, and applies Rep to
enhance their representational capacity. Besides, to suppress the noisy and
redundant operations introduced by Rep, we devise a de-parameterization
technique for a more compact re-parameterization. With this regard, DyRep is
more efficient than Rep since it smoothly evolves the given network instead of
constructing an over-parameterized network. Experimental results demonstrate
our effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\%$
on ImageNet and reduces $22\%$ runtime over the baseline. Code is available at:
https://github.com/hunto/DyRep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual CheckList: Generation and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthikeyan K, Shaily Bhatt, Pankaj Singh, Somak Aditya, Sandipan Dandapat, Sunayana Sitaram, Monojit Choudhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation
of NLP systems has revealed high failure rates for basic capabilities for
multiple state-of-the-art and commercial models. However, the CheckList
creation process is manual which creates a bottleneck towards creation of
multilingual CheckLists catering 100s of languages. In this work, we explore
multiple approaches to generate and evaluate the quality of Multilingual
CheckList. We device an algorithm -- Automated Multilingual Checklist
Generation (AMCG) for automatically transferring a CheckList from a source to a
target language that relies on a reasonable machine translation system. We then
compare the CheckList generated by AMCG with CheckLists generated with
different levels of human intervention. Through in-depth crosslingual
experiments between English and Hindi, and broad multilingual experiments
spanning 11 languages, we show that the automatic approach can provide accurate
estimates of failure rates of a model across capabilities, as would a
human-verified CheckList, and better than CheckLists generated by humans from
scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kullback-Leibler control for discrete-time nonlinear systems on
  continuous spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaito Ito, Kenji Kashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kullback-Leibler (KL) control enables efficient numerical methods for
nonlinear optimal control problems. The crucial assumption of KL control is the
full controllability of the transition distribution. However, this assumption
is often violated when the dynamics evolves in a continuous space.
Consequently, applying KL control to problems with continuous spaces requires
some approximation, which leads to the lost of the optimality. To avoid such
approximation, in this paper, we reformulate the KL control problem for
continuous spaces so that it does not require unrealistic assumptions. The key
difference between the original and reformulated KL control is that the former
measures the control effort by KL divergence between controlled and
uncontrolled transition distributions while the latter replaces the
uncontrolled transition by a noise-driven transition. We show that the
reformulated KL control admits efficient numerical algorithms like the original
one without unreasonable assumptions. Specifically, the associated value
function can be computed by using a Monte Carlo method based on its path
integral representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Compressed Sensing via Global Image Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Bran Lorenzana, Craig Engstrom, Shekhar S. Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNN) have demonstrated outstanding Compressed
Sensing (CS) performance compared to traditional, hand-crafted methods.
However, they are broadly limited in terms of generalisability, inductive bias
and difficulty to model long distance relationships. Transformer neural
networks (TNN) overcome such issues by implementing an attention mechanism
designed to capture dependencies between inputs. However, high-resolution tasks
typically require vision Transformers (ViT) to decompose an image into
patch-based tokens, limiting inputs to inherently local contexts. We propose a
novel image decomposition that naturally embeds images into low-resolution
inputs. These Kaleidoscope tokens (KD) provide a mechanism for global
attention, at the same computational cost as a patch-based approach. To
showcase this development, we replace CNN components in a well-known CS-MRI
neural network with TNN blocks and demonstrate the improvements afforded by KD.
We also propose an ensemble of image tokens, which enhance overall image
quality and reduces model size. Supplementary material is available:
https://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Pages, 4 Figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Fixation: Dynamic Window Visual <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Changlin Li, Guangrun Wang, Yun Xiao, Qing Du Xiaodan Liang Xiaojun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a surge of interest in visual transformers is to reduce the
computational cost by limiting the calculation of self-attention to a local
window. Most current work uses a fixed single-scale window for modeling by
default, ignoring the impact of window size on model performance. However, this
may limit the modeling potential of these window-based models for multi-scale
information. In this paper, we propose a novel method, named Dynamic Window
Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT
goes beyond the model that employs a fixed single window setting. To the best
of our knowledge, we are the first to use dynamic multi-scale windows to
explore the upper limit of the effect of window settings on model performance.
In DW-ViT, multi-scale information is obtained by assigning windows of
different sizes to different head groups of window multi-head self-attention.
Then, the information is dynamically fused by assigning different weights to
the multi-scale window branches. We conducted a detailed performance evaluation
on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related
state-of-the-art (SoTA) methods, DW-ViT obtains the best performance.
Specifically, compared with the current SoTA Swin Transformers
\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements
on all three datasets with similar parameters and computational costs. In
addition, DW-ViT exhibits good scalability and can be easily inserted into any
window-based visual transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct evaluation of progression or regression of disease burden in
  brain metastatic disease with Deep Neuroevolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Stember, Robert Young, Hrithwik Shalu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: A core component of advancing cancer treatment research is assessing
response to therapy. Doing so by hand, for example as per RECIST or RANO
criteria, is tedious, time-consuming, and can miss important tumor response
information; most notably, they exclude non-target lesions. We wish to assess
change in a holistic fashion that includes all lesions, obtaining simple,
informative, and automated assessments of tumor progression or regression. Due
to often low patient enrolments in clinical trials, we wish to make response
assessments with small training sets. Deep neuroevolution (DNE) can produce
radiology artificial intelligence (AI) that performs well on small training
sets. Here we use DNE for function approximation that predicts progression
versus regression of metastatic brain disease.
  Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training
set. Half of these pairs, separated in time, qualified as disease progression,
while the other 25 images constituted regression. We trained the parameters of
a relatively small CNN via mutations that consisted of random CNN weight
adjustments and mutation fitness. We then incorporated the best mutations into
the next generations CNN, repeating this process for approximately 50,000
generations. We applied the CNNs to our training set, as well as a separate
testing set with the same class balance of 25 progression and 25 regression
images.
  Results: DNE achieved monotonic convergence to 100% training set accuracy.
DNE also converged monotonically to 100% testing set accuracy.
  Conclusion: DNE can accurately classify brain-metastatic disease progression
versus regression. Future work will extend the input from 2D image slices to
full 3D volumes, and include the category of no change. We believe that an
approach such as our could ultimately provide a useful adjunct to RANO/RECIST
assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk Consistent Multi-Class Learning from Label Proportions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Kobayashi, Yusuke Mukuta, Tatsuya Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses a multiclass learning from label proportions (MCLLP)
setting in which training instances are provided in bags and only the
proportion of each class within the bags is provided. Most existing MCLLP
methods impose bag-wise constraints on the prediction of instances or assign
them pseudo-labels; however, none of these methods have a theoretical
consistency. To solve this problem, a risk-consistent method is proposed for
instance classification using the empirical risk minimization framework, and
its estimation error bound is derived. An approximation method is proposed for
the proposed risk estimator, to apply it to large bags, by diverting the
constraints on bags in existing research. The proposed method can be applied to
any deep model or loss and is compatible with stochastic optimization.
Experiments are conducted on benchmarks to verify the effectiveness of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LHNN: Lattice Hypergraph Neural Network for VLSI Congestion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wang, Guibao Shen, Dong Li, Jianye Hao, Wulong Liu, Yu Huang, Hongzhong Wu, Yibo Lin, Guangyong Chen, Pheng Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise congestion prediction from a placement solution plays a crucial role
in circuit placement. This work proposes the lattice hypergraph (LH-graph), a
novel graph formulation for circuits, which preserves netlist data during the
whole learning process, and enables the congestion information propagated
geometrically and topologically. Based on the formulation, we further developed
a heterogeneous graph neural network architecture LHNN, jointing the routing
demand regression to support the congestion spot classification. LHNN
constantly achieves more than 35% improvements compared with U-nets and Pix2Pix
on the F1 score. We expect our work shall highlight essential procedures using
machine learning for congestion prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper in DAC 2022; 6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphCoCo: Graph Complementary <span class="highlight-title">Contrastive Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Sun, Junchi Yan, Chentao Wu, Yue Ding, Ruoxin Chen, Xiang Yu, Xinyu Lu, Jie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Contrastive Learning (GCL) has shown promising performance in graph
representation learning (GRL) without the supervision of manual annotations.
GCL can generate graph-level embeddings by maximizing the Mutual Information
(MI) between different augmented views of the same graph (positive pairs).
However, we identify an obstacle that the optimization of InfoNCE loss only
concentrates on a few embeddings dimensions, limiting the distinguishability of
embeddings in downstream graph classification tasks. This paper proposes an
effective graph complementary contrastive learning approach named GraphCoCo to
tackle the above issue. Specifically, we set the embedding of the first
augmented view as the anchor embedding to localize "highlighted" dimensions
(i.e., the dimensions contribute most in similarity measurement). Then remove
these dimensions in the embeddings of the second augmented view to discover
neglected complementary representations. Therefore, the combination of anchor
and complementary embeddings significantly improves the performance in
downstream tasks. Comprehensive experiments on various benchmark datasets are
conducted to demonstrate the effectiveness of GraphCoCo, and the results show
that our model outperforms the state-of-the-art methods. Source code will be
made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning
  Framework in Classification of Medical Images on Limited Data: A COVID-19
  Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Shikun Zhang, Naomi Fengqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  COVID-19 pandemic has spread rapidly and caused a shortage of global medical
resources. The efficiency of COVID-19 diagnosis has become highly significant.
As deep learning and convolutional neural network (CNN) has been widely
utilized and been verified in analyzing medical images, it has become a
powerful tool for computer-assisted diagnosis. However, there are two most
significant challenges in medical image classification with the help of deep
learning and neural networks, one of them is the difficulty of acquiring enough
samples, which may lead to model overfitting. Privacy concerns mainly bring the
other challenge since medical-related records are often deemed patients'
private information and protected by laws such as GDPR and HIPPA. Federated
learning can ensure the model training is decentralized on different devices
and no data is shared among them, which guarantees privacy. However, with data
located on different devices, the accessible data of each device could be
limited. Since transfer learning has been verified in dealing with limited data
with good performance, therefore, in this paper, We made a trial to implement
federated learning and transfer learning techniques using CNNs to classify
COVID-19 using lung CT scans. We also explored the impact of dataset
distribution at the client-side in federated learning and the number of
training epochs a model is trained. Finally, we obtained very high performance
with federated learning, demonstrating our success in leveraging accuracy and
privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPar2: Fast and Scalable PARAFAC2 Decomposition for Irregular Dense
  Tensors <span class="chip">ICDE '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Gi Jang, U Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an irregular dense tensor, how can we efficiently analyze it? An
irregular tensor is a collection of matrices whose columns have the same size
and rows have different sizes from each other. PARAFAC2 decomposition is a
fundamental tool to deal with an irregular tensor in applications including
phenotype discovery and trend analysis. Although several PARAFAC2 decomposition
methods exist, their efficiency is limited for irregular dense tensors due to
the expensive computations involved with the tensor. In this paper, we propose
DPar2, a fast and scalable PARAFAC2 decomposition method for irregular dense
tensors. DPar2 achieves high efficiency by effectively compressing each slice
matrix of a given irregular tensor, careful reordering of computations with the
compression results, and exploiting the irregularity of the tensor. Extensive
experiments show that DPar2 is up to 6.0x faster than competitors on real-world
irregular tensors while achieving comparable accuracy. In addition, DPar2 is
scalable with respect to the tensor size and target rank.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures. To appear at the 38th IEEE International
  Conference on Data Engineering (ICDE '22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bellman Residual Orthogonalization for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Zanette, Martin J. Wainwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new reinforcement learning principle that approximates the
Bellman equations by enforcing their validity only along an user-defined space
of test functions. Focusing on applications to model-free offline RL with
function approximation, we exploit this principle to derive confidence
intervals for off-policy evaluation, as well as to optimize over policies
within a prescribed policy class. We prove an oracle inequality on our policy
optimization procedure in terms of a trade-off between the value and
uncertainty of an arbitrary comparator policy. Different choices of test
function spaces allow us to tackle different problems within a common
framework. We characterize the loss of efficiency in moving from on-policy to
off-policy data using our procedures, and establish connections to
concentrability coefficients studied in past work. We examine in depth the
implementation of our methods with linear function approximation, and provide
theoretical guarantees with polynomial-time implementations even when Bellman
closure does not hold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Your Policy Regularizer is Secretly an Adversary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Brekelmans, Tim Genewein, Jordi Grau-Moya, Grégoire Delétang, Markus Kunesch, Shane Legg, Pedro Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL and
alpha-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VL-Adapter: Parameter-Efficient Transfer Learning for
  <span class="highlight-title">Vision-and-Language</span> Tasks <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Sung, Jaemin Cho, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Rates of (Locally) Differentially Private Heavy-tailed
  Multi-Armed Bandits <span class="chip">AISTATS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02575v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02575v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youming Tao, Yulian Wu, Peng Zhao, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate the problem of stochastic multi-armed bandits
(MAB) in the (local) differential privacy (DP/LDP) model. Unlike previous
results that assume bounded/sub-Gaussian reward distributions, we focus on the
setting where each arm's reward distribution only has $(1+v)$-th moment with
some $v\in (0, 1]$. In the first part, we study the problem in the central
$\epsilon$-DP model. We first provide a near-optimal result by developing a
private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the
result via a private and robust version of the Successive Elimination (SE)
algorithm. Finally, we establish the lower bound to show that the
instance-dependent regret of our improved algorithm is optimal. In the second
part, we study the problem in the $\epsilon$-LDP model. We propose an algorithm
that can be seen as locally private and robust version of SE algorithm, which
provably achieves (near) optimal rates for both instance-dependent and
instance-independent regret. Our results reveal differences between the problem
of private MAB with bounded/sub-Gaussian rewards and heavy-tailed rewards. To
achieve these (near) optimal rates, we develop several new hard instances and
private robust estimators as byproducts, which might be used to other related
problems. Finally, experiments also support our theoretical findings and show
the effectiveness of our algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at AISTATS 2022. A preliminary version
  of this paper was presented at the CCS 2021 workshop Privacy Preserving
  Machine Learning (PPML'21). In this version, we fixed some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate Shapley Values for explaining tree-based models <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salim I. Amoukou, Nicolas J-B. Brunel, Tangi Salaün
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Shapley Values (SV) are widely used in explainable AI, they can be
poorly understood and estimated, implying that their analysis may lead to
spurious inferences and explanations. As a starting point, we remind an
invariance principle for SV and derive the correct approach for computing the
SV of categorical variables that are particularly sensitive to the encoding
used. In the case of tree-based models, we introduce two estimators of Shapley
Values that exploit the tree structure efficiently and are more accurate than
state-of-the-art methods. Simulations and comparisons are performed with
state-of-the-art algorithms and show the practical gain of our approach.
Finally, we discuss the ability of SV to provide reliable local explanations.
We also provide a Python package that computes our estimators at
https://github.com/salimamoukou/acv00.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 25th International Conference on Artificial
  Intelligence and Statistics (AISTATS), 2022. V2: The section on Active
  Shapley Values has been removed in this updated version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Precision Neural Network Quantization via Learned Layer-wise
  Importance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Yaowei Wang, Wen Ji, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponentially large discrete search space in mixed-precision quantization
(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous
works usually resort to iterative search methods on the training set, which
consume hundreds or even thousands of GPU-hours. In this study, we reveal that
some unique learnable parameters in quantization, namely the scale factors in
the quantizer, can serve as importance indicators of a layer, reflecting the
contribution of that layer to the final accuracy at certain bit-widths. These
importance indicators naturally perceive the numerical transformation during
quantization-aware training, which can precisely and correctly provide
quantization sensitivity metrics of layers. However, a deep network always
contains hundreds of such indicators, and training them one by one would lead
to an excessive time cost. To overcome this issue, we propose a joint training
scheme that can obtain all indicators at once. It considerably speeds up the
indicators training process by parallelizing the original sequential training
processes. With these learned importance indicators, we formulate the MPQ
search problem as a one-time integer linear programming (ILP) problem. That
avoids the iterative search and significantly reduces search time without
limiting the bit-width search space. For example, MPQ search on ResNet18 with
our indicators takes only 0.06 seconds. Also, extensive experiments show our
approach can achieve SOTA accuracy on ImageNet for far-ranging models with
various constraints (e.g., BitOps, compress rate).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Implicit Bias of Gradient Descent for Temporal Extrapolation <span class="chip">AISTATS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edo Cohen-Karlik, Avichai Ben David, Nadav Cohen, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using recurrent neural networks (RNNs) it is common practice to apply
trained models to sequences longer than those seen in training. This
"extrapolating" usage deviates from the traditional statistical learning setup
where guarantees are provided under the assumption that train and test
distributions are identical. Here we set out to understand when RNNs can
extrapolate, focusing on a simple case where the data generating distribution
is memoryless. We first show that even with infinite training data, there exist
RNN models that interpolate perfectly (i.e., they fit the training data) yet
extrapolate poorly to longer sequences. We then show that if gradient descent
is used for training, learning will converge to perfect extrapolation under
certain assumptions on initialization. Our results complement recent studies on
the implicit bias of gradient descent, showing that it plays a key role in
extrapolation when learning temporal prediction models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures (plus appendix), AISTATS2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Myers-Dean, Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized few-shot semantic segmentation was introduced to move beyond only
evaluating few-shot segmentation models on novel classes to include testing
their ability to remember base classes. While the current state-of-the-art
approach is based on meta-learning, it performs poorly and saturates in
learning after observing only a few shots. We propose the first fine-tuning
solution, and demonstrate that it addresses the saturation problem while
achieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We
also show that it outperforms existing methods, whether fine-tuning multiple
final layers or only the final layer. Finally, we present a triplet loss
regularization that shows how to redistribute the balance of performance
between novel and base categories so that there is a smaller gap between them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Includes supplementary materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Microfossil Identification via Deep Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09490v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09490v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tayfun Karaderi, Tilo Burghardt, Allison Y. Hsiang, Jacob Ramaer, Daniela N. Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply deep metric learning for the first time to the problem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualization options for
human experts and cannot be applied to open-set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualization of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning outperforms all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Keycode, network weights, and data splits are
published with this paper for full reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shared Data and Algorithms for Deep Learning in Fundamental Physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.00656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.00656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Benato, Erik Buhmann, Martin Erdmann, Peter Fackeldey, Jonas Glombitza, Nikolai Hartmann, Gregor Kasieczka, William Korcari, Thomas Kuhr, Jan Steinheimer, Horst Stöcker, Tilman Plehn, Kai Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Python package that provides simply and unified access to a
collection of datasets from fundamental physics research - including particle
physics, astroparticle physics, and hadron- and nuclear physics - for
supervised machine learning studies. The datasets contain hadronic top quarks,
cosmic-ray induced air showers, phase transitions in hadronic matter, and
generator-level histories. While public datasets from multiple fundamental
physics disciplines already exist, the common interface and provided reference
models simplify future work on cross-disciplinary machine learning and transfer
learning in fundamental physics. We discuss the design and structure and line
out how additional datasets can be submitted for inclusion.
  As showcase application, we present a simple yet flexible graph-based neural
network architecture that can easily be applied to a wide range of supervised
learning tasks. We show that our approach reaches performance close to
dedicated methods on all datasets. To simplify adaptation for various problems,
we provide easy-to-follow instructions on how graph-based representations of
data structures, relevant for fundamental physics, can be constructed and
provide code implementations for several of them. Implementations are also
provided for our proposed method and all reference algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 5 tables - Version accepted by Computing and
  Software for Big Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibration of P-values for calibration and for deviation of a
  subpopulation from the full population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00100v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00100v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Tygert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The author's recent research papers, "Cumulative deviation of a subpopulation
from the full population" and "A graphical method of cumulative differences
between two subpopulations" (both published in volume 8 of Springer's
open-access "Journal of Big Data" during 2021), propose graphical methods and
summary statistics, without extensively calibrating formal significance tests.
The summary metrics and methods can measure the calibration of probabilistic
predictions and can assess differences in responses between a subpopulation and
the full population while controlling for a covariate or score via conditioning
on it. These recently published papers construct significance tests based on
the scalar summary statistics, but only sketch how to calibrate the attained
significance levels (also known as "P-values") for the tests. The present
article reviews and synthesizes work spanning many decades in order to detail
how to calibrate the P-values. The present paper presents computationally
efficient, easily implemented numerical methods for evaluating properly
calibrated P-values, together with rigorous mathematical proofs guaranteeing
their accuracy, and illustrates and validates the methods with open-source
software and numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCor: Correlation-Based Active Client Selection Strategy for
  Heterogeneous Federated Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.13822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.13822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minxue Tang, Xuefei Ning, Yitu Wang, Jingwei Sun, Yu Wang, Hai Li, Yiran Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Client-wise data heterogeneity is one of the major issues that hinder
effective training in federated learning (FL). Since the data distribution on
each client may vary dramatically, the client selection strategy can
significantly influence the convergence rate of the FL process. Active client
selection strategies are popularly proposed in recent studies. However, they
neglect the loss correlations between the clients and achieve only marginal
improvement compared to the uniform selection strategy. In this work, we
propose FedCor -- an FL framework built on a correlation-based client selection
strategy, to boost the convergence rate of FL. Specifically, we first model the
loss correlations between the clients with a Gaussian Process (GP). Based on
the GP model, we derive a client selection strategy with a significant
reduction of expected global loss in each round. Besides, we develop an
efficient GP training method with a low communication overhead in the FL
scenario by utilizing the covariance stationarity. Our experimental results
show that compared to the state-of-the-art method, FedCorr can improve the
convergence rates by $34\%\sim 99\%$ and $26\%\sim 51\%$ on FMNIST and
CIFAR-10, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Applicability of ML Fairness Notions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.16745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.16745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karima Makhlouf, Sami Zhioua, Catuscia Palamidessi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness emerged as an important requirement to guarantee that Machine
Learning (ML) predictive systems do not discriminate against specific
individuals or entire sub-populations, in particular, minorities. Given the
inherent subjectivity of viewing the concept of fairness, several notions of
fairness have been introduced in the literature. This paper is a survey that
illustrates the subtleties between fairness notions through a large number of
examples and scenarios. In addition, unlike other surveys in the literature, it
addresses the question of: which notion of fairness is most suited to a given
real-world scenario and why? Our attempt to answer this question consists in
(1) identifying the set of fairness-related characteristics of the real-world
scenario at hand, (2) analyzing the behavior of each fairness notion, and then
(3) fitting these two elements to recommend the most suitable fairness notion
in every specific setup. The results are summarized in a decision diagram that
can be used by practitioners and policymakers to navigate the relatively large
catalog of ML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avalanche RL: a Continual Reinforcement Learning Library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolò Lucchesi, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Reinforcement Learning (CRL) is a challenging setting where an
agent learns to interact with an environment that is constantly changing over
time (the stream of experiences). In this paper, we describe Avalanche RL, a
library for Continual Reinforcement Learning which allows to easily train
agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and
supports any OpenAI Gym environment. Its design is based on Avalanche, one of
the more popular continual learning libraries, which allow us to reuse a large
number of continual learning strategies and improve the interaction between
reinforcement learning and continual learning researchers. Additionally, we
propose Continual Habitat-Lab, a novel benchmark and a high-level library which
enables the usage of the photorealistic simulator Habitat-Sim for CRL research.
Overall, Avalanche RL attempts to unify under a common framework continual
reinforcement learning applications, which we hope will foster the growth of
the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Enhanced Semantic Hashing: Towards Effective and Efficient
  Retrieval for Streaming <span class="highlight-title">Multi-Modal</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao-Ming Wu, Xin Luo, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the vigorous development of multimedia equipment and applications,
efficient retrieval of large-scale multi-modal data has become a trendy
research topic. Thereinto, hashing has become a prevalent choice due to its
retrieval efficiency and low storage cost. Although multi-modal hashing has
drawn lots of attention in recent years, there still remain some problems. The
first point is that existing methods are mainly designed in batch mode and not
able to efficiently handle streaming multi-modal data. The second point is that
all existing online multi-modal hashing methods fail to effectively handle
unseen new classes which come continuously with streaming data chunks. In this
paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).
We design novel semantic-enhanced representation for data, which could help
handle the new coming classes, and thereby construct the enhanced semantic
objective function. An efficient and effective discrete online optimization
algorithm is further proposed for OASIS. Extensive experiments show that our
method can exceed the state-of-the-art models. For good reproducibility and
benefiting the community, our code and data are already available in
supplementary material and will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Local Convergence Theory for the Stochastic Gradient Descent Method in
  Non-Convex Optimization With Non-isolated Local Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehee Ko, Xiantao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-convex loss functions arise frequently in modern machine learning, and
for the theoretical analysis of stochastic optimization methods, the presence
of non-isolated minima presents a unique challenge that has remained
under-explored. In this paper, we study the local convergence of the stochastic
gradient descent method to non-isolated global minima. Under mild assumptions,
we estimate the probability for the iterations to stay near the minima by
adopting the notion of stochastic stability. After establishing such stability,
we present the lower bound complexity in terms of various error criteria for a
given error tolerance $\epsilon$ and a failure probability $\gamma$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Development of a Vertex Finding Algorithm using Recurrent Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.11906v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.11906v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiichi Goto, Taikan Suehara, Tamaki Yoshioka, Masakazu Kurata, Hajime Nagahara, Yuta Nakashima, Noriko Takemura, Masako Iwasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is a rapidly-evolving technology with possibility to
significantly improve physics reach of collider experiments. In this study we
developed a novel algorithm of vertex finding for future lepton colliders such
as the International Linear Collider. We deploy two networks; one is simple
fully-connected layers to look for vertex seeds from track pairs, and the other
is a customized Recurrent Neural Network with an attention mechanism and an
encoder-decoder structure to associate tracks to the vertex seeds. The
performance of the vertex finder is compared with the standard ILC
reconstruction algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04386v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04386v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends also on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct and Pserf. Experiments suggest that the proposed functions improve the
network performance significantly compared to the widely used activations like
ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and
5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in
CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet
V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Optimal Strategies for Temporal Tasks in Stochastic Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.04307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.04307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alper Kamil Bozkurt, Yu Wang, Michael M. Zavlanos, Miroslav Pajic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesis from linear temporal logic (LTL) specifications provides assured
controllers for autonomous systems operating in stochastic and potentially
adversarial environments. Automatic synthesis tools, however, require a model
of the environment to construct controllers. In this work, we introduce a
model-free reinforcement learning (RL) approach that derives controllers from
given LTL specifications even when the environment is completely unknown. We
model the problem of satisfying the LTL specifications as a stochastic game
(SG) between the controller and the adversarial environment; we then learn
optimal controller strategies that maximize the probability of satisfying the
LTL specifications against the worst-case environment behavior. We first
construct a product game using the deterministic parity automaton (DPA)
translated from the given LTL specification. By deriving distinct rewards and
discount factors from the acceptance condition of the DPA, we reduce the
maximization of the worst-case probability of satisfying the LTL specification
into the maximization of a discounted reward objective in the product game;
this allows for the use of model-free RL algorithms to learn an optimal
controller strategy. To deal with the common scalability problems when the
number of colors defining the acceptance condition of the DPA is large, we
propose a lazy color generation method where distinct rewards and discount
factors are utilized only when needed, and an approximate method where the
controller eventually focuses on only one color. In several case studies, we
show that our approach is scalable to a wide range of LTL formulas,
significantly outperforming existing methods for learning controllers from LTL
specifications in SGs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Waveform Learning for Next-Generation Wireless Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.00998v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.00998v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fayçal Ait Aoudia, Jakob Hoydis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a learning-based method for the joint design of a transmit and
receive filter, the constellation geometry and associated bit labeling, as well
as a neural network (NN)-based detector. The method maximizes an achievable
information rate, while simultaneously satisfying constraints on the adjacent
channel leakage ratio (ACLR) and peak-to-average power ratio (PAPR). This
allows control of the tradeoff between spectral containment, peak power, and
communication rate. Evaluation on an additive white Gaussian noise (AWGN)
channel shows significant reduction of ACLR and PAPR compared to a conventional
baseline relying on quadrature amplitude modulation (QAM) and
root-raised-cosine (RRC), without significant loss of information rate. When
considering a 3rd Generation Partnership Project (3GPP) multipath channel, the
learned waveform and neural receiver enable competitive or higher rates than an
orthogonal frequency division multiplexing (OFDM) baseline, while reducing the
ACLR by 10 dB and the PAPR by 2 dB. The proposed method incurs no additional
complexity on the transmitter side and might be an attractive tool for waveform
design of beyond-5G systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text to Image Generation with Semantic-Spatial Aware GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.00567v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.00567v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Hu, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image synthesis (T2I) aims to generate photo-realistic images which
are semantically consistent with the text descriptions. Existing methods are
usually built upon conditional generative adversarial networks (GANs) and
initialize an image from noise with sentence embedding, and then refine the
features with fine-grained word embedding iteratively. A close inspection of
their generated images reveals a major limitation: even though the generated
image holistically matches the description, individual image regions or parts
of somethings are often not recognizable or consistent with words in the
sentence, e.g. "a white crown". To address this problem, we propose a novel
framework Semantic-Spatial Aware GAN for synthesizing images from input text.
Concretely, we introduce a simple and effective Semantic-Spatial Aware block,
which (1) learns semantic-adaptive transformation conditioned on text to
effectively fuse text features and image features, and (2) learns a semantic
mask in a weakly-supervised way that depends on the current text-image fusion
process in order to guide the transformation spatially. Experiments on the
challenging COCO and CUB bird datasets demonstrate the advantage of our method
over the recent state-of-the-art approaches, regarding both visual fidelity and
alignment with input text description.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:1711.10485 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of high order closure terms from fully kinetic
  simulations using machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.09916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.09916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brecht Laperre, Jorge Amaya, Sara Jamal, Giovanni Lapenta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulations of large-scale plasma systems are typically based on a fluid
approximation approach. These models construct a moment-based system of
equations that approximate the particle-based physics as a fluid, but as a
result lack the small-scale physical processes available to fully kinetic
models. Traditionally, empirical closure relations are used to close the
moment-based system of equations, which typically approximate the pressure
tensor or heat flux. The more accurate the closure relation, the stronger the
simulation approaches kinetic-based results. In this paper, new closure terms
are constructed using machine learning techniques. Two different machine
learning models, a multi-layer perceptron and a gradient boosting regressor,
synthesize a local closure relation for the pressure tensor and heat flux
vector from fully kinetic simulations of a 2D magnetic reconnection problem.
The models are compared to an existing closure relation for the pressure
tensor, and the applicability of the models is discussed. The initial results
show that the models can capture the diagonal components of the pressure tensor
accurately, and show promising results for the heat flux, opening the way for
new experiments in multi-scale modeling. We find that the sampling of the
points used to train both models play a capital role in their accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 12 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Generalization in Federated Learning by Seeking Flat Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Caldarola, Barbara Caputo, Marco Ciccone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Removed axessibility package for smaller output PDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training of speech enhancement systems often does not incorporate knowledge
of human perception and thus can lead to unnatural sounding results.
Incorporating psychoacoustically motivated speech perception metrics as part of
model training via a predictor network has recently gained interest. However,
the performance of such predictors is limited by the distribution of metric
scores that appear in the training data. In this work, we propose MetricGAN+/-
(an extension of MetricGAN+, one such metric-motivated system) which introduces
an additional network - a "de-generator" which attempts to improve the
robustness of the prediction network (and by extension of the generator) by
ensuring observation of a wider range of metric scores in training.
Experimental results on the VoiceBank-DEMAND dataset show relative improvement
in PESQ score of 3.8% (3.05 vs 3.22 PESQ score), as well as better
generalisation to unseen noise and speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Smells in Public <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arumoy Shome, Luis Cruz, Arie van Deursen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of Artificial Intelligence (AI) in high-stakes domains such as
healthcare, wildlife preservation, autonomous driving and criminal justice
system calls for a data-centric approach to AI. Data scientists spend the
majority of their time studying and wrangling the data, yet tools to aid them
with data analysis are lacking. This study identifies the recurrent data
quality issues in public datasets. Analogous to code smells, we introduce a
novel catalogue of data smells that can be used to indicate early signs of
problems or technical debt in machine learning systems. To understand the
prevalence of data quality issues in datasets, we analyse 25 public datasets
and identify 14 data smells.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamically-Scaled Deep Canonical Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Friedlander, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical Correlation Analysis (CCA) is a method for feature extraction of
two views by finding maximally correlated linear projections of them. Several
variants of CCA have been introduced in the literature, in particular, variants
based on deep neural networks for learning highly correlated nonlinear
transformations of two views. As these models are parameterized conventionally,
their learnable parameters remain independent of the inputs after the training
process, which may limit their capacity for learning highly correlated
representations. We introduce a novel dynamic scaling method for training an
input-dependent canonical correlation model. In our deep-CCA models, the
parameters of the last layer are scaled by a second neural network that is
conditioned on the model's input, resulting in a parameterization that is
dependent on the input samples. We evaluate our model on multiple datasets and
demonstrate that the learned representations are more correlated in comparison
to the conventionally-parameterized CCA-based models and also obtain preferable
retrieval results. Our code is available at
https://github.com/tomerfr/DynamicallyScaledDeepCCA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High pressure hydrogen by machine learning and quantum Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Tirelli, Giacomo Tenti, Kousuke Nakano, Sandro Sorella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have developed a technique combining the accuracy of quantum Monte Carlo
in describing the electron correlation with the efficiency of a Machine
Learning Potential (MLP). We use kernel regression in combination with SOAP
(Smooth Overlap of Atomic Position) features, implemented here in a very
efficient way. The key ingredients are: i) a sparsification technique, based on
farthest point sampling, ensuring generality and transferability of our MLPs
and ii) the so called $\Delta$-learning, allowing a small training data set, a
fundamental property for highly accurate but computationally demanding
calculations, such as the ones based on quantum Monte Carlo. As the first
application we present a benchmark study of the liquid-liquid transition of
high-pressure hydrogen and show the quality of our MLP, by emphasizing the
importance of high accuracy for this very debated subject, where experiments
are difficult in the lab, and theory is still far from being conclusive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>revised exposition, performed more validation tests. Comments
  welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ gACSON software for automated segmentation and morphology analyses of
  myelinated axons in 3D electron microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Behanova, Ali Abdollahzadeh, Ilya Belevich, Eija Jokitalo, Alejandra Sierra, Jussi Tohka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and Objective: Advances in electron microscopy (EM) now allow
three-dimensional (3D) imaging of hundreds of micrometers of tissue with
nanometer-scale resolution, providing new opportunities to study the
ultrastructure of the brain. In this work, we introduce a freely available
Matlab-based gACSON software for visualization, segmentation, assessment, and
morphology analysis of myelinated axons in 3D-EM volumes of brain tissue
samples. Methods: The software is equipped with a graphical user interface
(GUI). It automatically segments the intra-axonal space of myelinated axons and
their corresponding myelin sheaths and allows manual segmentation,
proofreading, and interactive correction of the segmented components. gACSON
analyzes the morphology of myelinated axons, such as axonal diameter, axonal
eccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of
the software by segmenting and analyzing myelinated axons in six 3D-EM volumes
of rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).
Our results suggest that the equivalent diameter of myelinated axons in
somatosensory cortex was decreased in TBI animals five months after the injury.
Conclusions: Our results indicate that gACSON is a valuable tool for
visualization, segmentation, assessment, and morphology analysis of myelinated
axons in 3D-EM volumes. It is freely available at
https://github.com/AndreaBehan/g-ACSON under the MIT license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Implicit Bias Towards Minimal Depth of Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Galanti, Liane Galanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the implicit bias of gradient based training methods to favor
low-depth solutions when training deep neural networks. Recent results in the
literature suggest that penultimate layer representations learned by a
classifier over multiple classes exhibit a clustering property, called neural
collapse. We demonstrate empirically that the neural collapse property extends
beyond the penultimate layer and tends to emerge in intermediate layers as
well. In this regards, we hypothesize that gradient based methods are
implicitly biased towards selecting neural networks of minimal depth for
achieving this clustering property.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAFITE: Towards Language-Free Training for Text-to-Image Generation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13792v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13792v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022, https://github.com/drboog/Lafite</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithm Fairness in AI for Medicine and Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.00603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.00603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard J. Chen, Tiffany Y. Chen, Jana Lipkova, Judy J. Wang, Drew F. K. Williamson, Ming Y. Lu, Sharifa Sahai, Faisal Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current development and deployment of many artificial intelligence
(AI) systems in healthcare, algorithm fairness is a challenging problem in
delivering equitable care. Recent evaluation of AI models stratified across
race sub-populations have revealed inequalities in how patients are diagnosed,
given treatments, and billed for healthcare costs. In this perspective article,
we summarize the intersectional field of fairness in machine learning through
the context of current issues in healthcare, outline how algorithmic biases
(e.g. - image acquisition, genetic variation, intra-observer labeling
variability) arise in current clinical workflows and their resulting healthcare
disparities. Lastly, we also review emerging technology for mitigating bias via
federated learning, disentanglement, and model explainability, and their role
in AI-SaMD development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-distribution Generalization with Causal Invariant Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11528v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11528v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Wang, Mingyang Yi, Zhitang Chen, Shengyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, it is important and desirable to learn a model
that performs well on out-of-distribution (OOD) data. Recently, causality has
become a powerful tool to tackle the OOD generalization problem, with the idea
resting on the causal mechanism that is invariant across domains of interest.
To leverage the generally unknown causal mechanism, existing works assume a
linear form of causal feature or require sufficiently many and diverse training
domains, which are usually restrictive in practice. In this work, we obviate
these assumptions and tackle the OOD problem without explicitly recovering the
causal feature. Our approach is based on transformations that modify the
non-causal feature but leave the causal part unchanged, which can be either
obtained from prior knowledge or learned from the training data in the
multi-domain scenario. Under the setting of invariant causal mechanism, we
theoretically show that if all such transformations are available, then we can
learn a minimax optimal model across the domains using only single domain data.
Noticing that knowing a complete set of these causal invariant transformations
may be impractical, we further show that it suffices to know only a subset of
these transformations. Based on the theoretical findings, a regularized
training procedure is proposed to improve the OOD generalization capability.
Extensive experimental results on both synthetic and real datasets verify the
effectiveness of the proposed algorithm, even with only a few causal invariant
transformations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by cvpr2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning for Demand Driven Services in Logistics and
  Transportation Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.04462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.04462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefang Zong, Tao Feng, Tong Xia, Depeng Jin, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent technology development brings the booming of numerous new
Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand
delivery, express systems and warehousing. In DDS, a service loop is an
elemental structure, including its service worker, the service providers and
corresponding service targets. The service workers should transport either
humans or parcels from the providers to the target locations. Various planning
tasks within DDS can thus be classified into two individual stages: 1)
Dispatching, which is to form service loops from demand/supply distributions,
and 2)Routing, which is to decide specific serving orders within the
constructed loops. Generating high-quality strategies in both stages is
important to develop DDS but faces several challenging. Meanwhile, deep
reinforcement learning (DRL) has been developed rapidly in recent years. It is
a powerful tool to solve these problems since DRL can learn a parametric model
without relying on too many problem-based assumptions and optimize long-term
effect by learning sequential decisions. In this survey, we first define DDS,
then highlight common applications and important decision/control problems
within. For each problem, we comprehensively introduce the existing DRL
solutions. We also introduce open simulation environments for development and
evaluation of DDS applications. Finally, we analyze remaining challenges and
discuss further research opportunities in DRL solutions for DDS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. survey preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Portrait Delighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Weir, Junhong Zhao, Andrew Chalmers, Taehyun Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Search for Feedback in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.09478v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.09478v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Wang, Karthikeya S. Parunandi, Aayushman Sharma, Raman Goyal, Suman Chakravorty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical
system is equivalent to the search for an optimal feedback law utilizing the
simulations/ rollouts of the dynamical system. Most RL techniques search over a
complex global nonlinear feedback parametrization making them suffer from high
training times as well as variance. Instead, we advocate searching over a local
feedback representation consisting of an open-loop sequence, and an associated
optimal linear feedback law completely determined by the open-loop. We show
that this alternate approach results in highly efficient training, the answers
obtained are repeatable and hence reliable, and the resulting closed
performance is superior to global state-of-the-art RL techniques. Finally, if
we replan, whenever required, which is feasible due to the fast and reliable
local solution, it allows us to recover global optimality of the resulting
feedback law.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:1904.08361</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Representation Learning for Reliable Robotic Monitoring
  of Fruit Anomalies <span class="chip">ICRA2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.10135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.10135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyeong Choi, Owen Would, Adrian Salazar-Gomez, Grzegorz Cielniak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation can be a simple yet powerful tool for autonomous robots to
fully utilise available data for selfsupervised identification of atypical
scenes or objects. State-of-the-art augmentation methods arbitrarily embed
"structural" peculiarity on typical images so that classifying these artefacts
can provide guidance for learning representations for the detection of
anomalous visual signals. In this paper, however, we argue that learning such
structure-sensitive representations can be a suboptimal approach to some
classes of anomaly (e.g., unhealthy fruits) which could be better recognised by
a different type of visual element such as "colour". We thus propose Channel
Randomisation as a novel data augmentation method for restricting neural
networks to learn encoding of "colour irregularity" whilst predicting
channel-randomised images to ultimately build reliable fruit-monitoring robots
identifying atypical fruit qualities. Our experiments show that (1) this
colour-based alternative can better learn representations for consistently
accurate identification of fruit anomalies in various fruit species, and also,
(2) unlike other methods, the validation accuracy can be utilised as a
criterion for early stopping of training in practice due to positive
correlation between the performance in the self-supervised
colour-differentiation task and the subsequent detection rate of actual
anomalous fruits. Also, the proposed approach is evaluated on a new
agricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images
gathered by a mobile robot, which we share online to encourage active
agri-robotics research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA2022; Codes and data are all available online</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FormNet: Structural Encoding beyond Sequential Modeling in Form Document
  Information Extraction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence modeling has demonstrated state-of-the-art performance on natural
language and document understanding tasks. However, it is challenging to
correctly serialize tokens in form-like documents in practice due to their
variety of layout patterns. We propose FormNet, a structure-aware sequence
model to mitigate the suboptimal serialization of forms. First, we design Rich
Attention that leverages the spatial relationship between tokens in a form for
more precise attention score calculation. Second, we construct Super-Tokens for
each word by embedding representations from their neighboring tokens through
graph convolutions. FormNet therefore explicitly recovers local syntactic
information that may have been lost during serialization. In experiments,
FormNet outperforms existing methods with a more compact model size and less
pre-training data, establishing new state-of-the-art performance on CORD, FUNSD
and Payment benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Emotion Recognition using Visual-audio-linguistic
  information: A Technical Report for ABAW3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Su Zhang, Ruyi An, Yi Ding, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a cross-modal co-attention model for continuous emotion
recognition using visual-audio-linguistic information. The model consists of
four blocks. The visual, audio, and linguistic blocks are used to learn the
spatial-temporal features of the multimodal input. A co-attention block is
designed to fuse the learned enbeddings with the multihead co-attention
mechanism. The visual encoding from the visual block is concatenated with the
attention feature to emphasize the visual information. To make full use of the
data and alleviate over-fitting, the cross-validation is carried out on the
training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. The achieved CCC on
validation set is 0.450 for valence and 0.651 for arousal, which significantly
outperforms the baseline method with the corresponding CCC of 0.310 and 0.170,
respectively. The code is available at https://github.com/sucv/ABAW3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:2107.01175</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score difficulty analysis for piano performance education based on
  fingering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Ramoneda, Nazif Can Tamer, Vsevolod Eremenko, Xavier Serra, Marius Miron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce score difficulty classification as a sub-task of
music information retrieval (MIR), which may be used in music education
technologies, for personalised curriculum generation, and score retrieval. We
introduce a novel dataset for our task, Mikrokosmos-difficulty, containing 147
piano pieces in symbolic representation and the corresponding difficulty labels
derived by its composer B\'ela Bart\'ok and the publishers. As part of our
methodology, we propose piano technique feature representations based on
different piano fingering algorithms. We use these features as input for two
classifiers: a Gated Recurrent Unit neural network (GRU) with attention
mechanism and gradient-boosted trees trained on score segments. We show that
for our dataset fingering based features perform better than a simple baseline
considering solely the notes in the score. Furthermore, the GRU with attention
mechanism classifier surpasses the gradient-boosted trees. Our proposed models
are interpretable and are capable of generating difficulty feedback both
locally, on short term segments, and globally, for whole pieces. Code,
datasets, models, and an online demo are made available for reproducibility
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Escaping from Language Bias and OCR Error: Semantics-Centered
  Text Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Fang, Gangyan Zeng, Yu Zhou, Daiqing Wu, Can Ma, Dayong Hu, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texts in scene images convey critical information for scene understanding and
reasoning. The abilities of reading and reasoning matter for the model in the
text-based visual question answering (TextVQA) process. However, current
TextVQA models do not center on the text and suffer from several limitations.
The model is easily dominated by language biases and optical character
recognition (OCR) errors due to the absence of semantic guidance in the answer
prediction process. In this paper, we propose a novel Semantics-Centered
Network (SC-Net) that consists of an instance-level contrastive semantic
prediction module (ICSP) and a semantics-centered transformer module (SCT).
Equipped with the two modules, the semantics-centered model can resist the
language biases and the accumulated errors from OCR. Extensive experiments on
TextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net
surpasses previous works with a noticeable margin and is more reasonable for
the TextVQA task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steganalysis of Image with Adaptively Parametric Activation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Su, Meiyin Han, Junle Liang, Songsen Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steganalysis as a method to detect whether image contains se-cret message, is
a crucial study avoiding the imperils from abus-ing steganography. The point of
steganalysis is to detect the weak embedding signals which is hardly learned by
convolution-al layer and easily suppressed. In this paper, to enhance
embed-ding signals, we study the insufficiencies of activation function,
filters and loss function from the aspects of reduce embedding signal loss and
enhance embedding signal capture ability. Adap-tive Parametric Activation
Module is designed to reserve nega-tive embedding signal. For embedding signal
capture ability enhancement, we add constraints on the high-pass filters to
im-prove residual diversity which enables the filters extracts rich embedding
signals. Besides, a loss function based on contrastive learning is applied to
overcome the limitations of cross-entropy loss by maximum inter-class distance.
It helps the network make a distinction between embedding signals and semantic
edges. We use images from BOSSbase 1.01 and make stegos by WOW and S-UNIWARD
for experiments. Compared to state-of-the-art methods, our method has a
competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIMusicGuru: Music Assisted Human Pose Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Snehesh Shrestha, Cornelia Fermüller, Tianyu Huang, Pyone Thant Win, Adam Zukerman, Chethan M. Parameshwara, Yiannis Aloimonos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose Estimation techniques rely on visual cues available through observations
represented in the form of pixels. But the performance is bounded by the frame
rate of the video and struggles from motion blur, occlusions, and temporal
coherence. This issue is magnified when people are interacting with objects and
instruments, for example playing the violin. Standard approaches for
postprocessing use interpolation and smoothing functions to filter noise and
fill gaps, but they cannot model highly non-linear motion. We present a method
that leverages our understanding of the high degree of a causal relationship
between the sound produced and the motion that produces them. We use the audio
signature to refine and predict accurate human body pose motion models. We
propose MAPnet (Music Assisted Pose network) for generating a fine grain motion
model from sparse input pose sequences but continuous audio. To accelerate
further research in this domain, we also open-source MAPdat, a new multi-modal
dataset of 3D violin playing motion with music. We perform a comparison of
different standard machine learning models and perform analysis on input
modalities, sampling techniques, and audio and motion features. Experiments on
MAPdat suggest multi-modal approaches like ours as a promising direction for
tasks previously approached with visual methods only. Our results show both
qualitatively and quantitatively how audio can be combined with visual
observation to help improve any pose estimation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Enhanced Semantic Hashing: Towards Effective and Efficient
  Retrieval for Streaming <span class="highlight-title">Multi-Modal</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao-Ming Wu, Xin Luo, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the vigorous development of multimedia equipment and applications,
efficient retrieval of large-scale multi-modal data has become a trendy
research topic. Thereinto, hashing has become a prevalent choice due to its
retrieval efficiency and low storage cost. Although multi-modal hashing has
drawn lots of attention in recent years, there still remain some problems. The
first point is that existing methods are mainly designed in batch mode and not
able to efficiently handle streaming multi-modal data. The second point is that
all existing online multi-modal hashing methods fail to effectively handle
unseen new classes which come continuously with streaming data chunks. In this
paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).
We design novel semantic-enhanced representation for data, which could help
handle the new coming classes, and thereby construct the enhanced semantic
objective function. An efficient and effective discrete online optimization
algorithm is further proposed for OASIS. Extensive experiments show that our
method can exceed the state-of-the-art models. For good reproducibility and
benefiting the community, our code and data are already available in
supplementary material and will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Speech Enhancement using <span class="highlight-title">Multimodal</span> Deep Convolutional
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1709.00944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1709.00944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus on addressing audio information only. In this work, inspired
by multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model. In the proposed AVDCNN SE model, audio
and visual data are first processed using individual CNNs, and then, fused into
a joint network to generate enhanced speech at the output layer. The AVDCNN
model is trained in an end-to-end manner, and parameters are jointly learned
through back-propagation. We evaluate enhanced speech using five objective
criteria. Results show that the AVDCNN yields notably better performance,
compared with an audio-only CNN-based SE model and two conventional SE
approaches, confirming the effectiveness of integrating visual information into
the SE process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is the same as arXiv:1703.10893v2. Apologies for the
  inconvenience</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-23T00:00:00Z">2022-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Gender Bias in Distilled Language Models via Counterfactual
  Role Reversal <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models excel at generating coherent text, and model compression
techniques such as knowledge distillation have enabled their use in
resource-constrained settings. However, these models can be biased in multiple
ways, including the unfounded association of male and female genders with
gender-neutral professions. Therefore, knowledge distillation without any
fairness constraints may preserve or exaggerate the teacher model's biases onto
the distilled model. To this end, we present a novel approach to mitigate
gender disparity in text generation by learning a fair model during knowledge
distillation. We propose two modifications to the base knowledge distillation
based on counterfactual role reversal$\unicode{x2014}$modifying teacher
probabilities and augmenting the training set. We evaluate gender polarity
across professions in open-ended text generated from the resulting distilled
and finetuned GPT$\unicode{x2012}$2 models and demonstrate a substantial
reduction in gender disparity with only a minor compromise in utility. Finally,
we observe that language models that reduce gender polarity in language
generation do not improve embedding fairness or downstream classification
fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamically Refined Regularization for Improving Cross-corpora Hate
  Speech Detection <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tulika Bose, Nikolaos Aletras, Irina Illina, Dominique Fohr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech classifiers exhibit substantial performance degradation when
evaluated on datasets different from the source. This is due to learning
spurious correlations between words that are not necessarily relevant to
hateful language, and hate speech labels from the training corpus. Previous
work has attempted to mitigate this problem by regularizing specific terms from
pre-defined static dictionaries. While this has been demonstrated to improve
the generalizability of classifiers, the coverage of such methods is limited
and the dictionaries require regular manual updates from human experts. In this
paper, we propose to automatically identify and reduce spurious correlations
using attribution methods with dynamic refinement of the list of terms that
need to be regularized during training. Our approach is flexible and improves
the cross-corpora performance over previous work independently and in
combination with pre-defined dictionaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2022 preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational historical linguistics and language diversity in South
  Asia <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryaman Arora, Adam Farris, Samopriya Basu, Suresh Kolichala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages; accepted to ACL 2022 Theme Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Cross-Lingual Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual summarization is the task of generating a summary in one
language (e.g., English) for the given document(s) in a different language
(e.g., Chinese). Under the globalization background, this task has attracted
increasing attention of the computational linguistics community. Nevertheless,
there still remains a lack of comprehensive review for this task. Therefore, we
present the first systematic critical review on the datasets, approaches and
challenges in this field. Specifically, we carefully organize existing datasets
and approaches according to different construction methods and solution
paradigms, respectively. For each type of datasets or approaches, we thoroughly
introduce and summarize previous efforts and further compare them with each
other to provide deeper analyses. In the end, we also discuss promising
directions and offer our thoughts to facilitate future research. This survey is
for both beginners and experts in cross-lingual summarization, and we hope it
will serve as a starting point as well as a source of new ideas for researchers
and engineers interested in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Context-Aware Feature Fusion Framework for Punctuation Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangjun Wu, Kebin Fang, Yao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accomplish the punctuation restoration task, most existing approaches
focused on leveraging extra information (e.g., part-of-speech tags) or
addressing the class imbalance problem. Recent works have widely applied the
transformer-based language models and significantly improved their
effectiveness. To the best of our knowledge, an inherent issue has remained
neglected: the attention of individual heads in the transformer will be diluted
or powerless while feeding the long non-punctuation utterances. Since those
previous contexts, not the followings, are comparatively more valuable to the
current position, it's hard to achieve a good balance by independent attention.
In this paper, we propose a novel Feature Fusion framework based on two-type
Attentions (FFA) to alleviate the shortage. It introduces a two-stream
architecture. One module involves interaction between attention heads to
encourage the communication, and another masked attention module captures the
dependent feature representation. Then, it aggregates two feature embeddings to
fuse information and enhances context-awareness. The experiments on the popular
benchmark dataset IWSLT demonstrate that our approach is effective. Without
additional data, it obtains comparable performance to the current
state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-based <span class="highlight-title">Pre-train</span>ed Model for Personality and Interpersonal
  Reactivity Prediction <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Li, Yixuan Weng, Qiya Song, Fuyan Ma, Bin Sun, Shutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the LingJing team's method to the Workshop on
Computational Approaches to Subjectivity, Sentiment & Social Media Analysis
(WASSA) 2022 shared task on Personality Prediction (PER) and Reactivity Index
Prediction (IRI). In this paper, we adopt the prompt-based method with the
pre-trained language model to accomplish these tasks. Specifically, the prompt
is designed to provide the extra knowledge for enhancing the pre-trained model.
Data augmentation and model ensemble are adopted for obtaining better results.
Extensive experiments are performed, which shows the effectiveness of the
proposed method. On the final submission, our system achieves a Pearson
Correlation Coefficient of 0.2301 and 0.2546 on Track 3 and Track 4
respectively. We ranked Top-1 on both sub-tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The shared task paper described the contributions of the Workshop on
  Computational Approaches to Subjectivity, Sentiment & Social Media Analysis
  (WASSA) @ ACL-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The VoicePrivacy 2022 Challenge Evaluation Plan 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalia Tomashenko, Xin Wang, Xiaoxiao Miao, Hubert Nourtel, Pierre Champion, Massimiliano Todisco, Emmanuel Vincent, Nicholas Evans, Junichi Yamagishi, Jean François Bonastre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For new participants - Executive summary: (1) The task is to develop a voice
anonymization system for speech data which conceals the speaker's voice
identity while protecting linguistic content, paralinguistic attributes,
intelligibility and naturalness. (2) Training, development and evaluation
datasets are provided in addition to 3 different baseline anonymization
systems, evaluation scripts, and metrics. Participants apply their developed
anonymization systems, run evaluation scripts and submit objective evaluation
results and anonymized speech data to the organizers. (3) Results will be
presented at a workshop held in conjunction with INTERSPEECH 2022 to which all
participants are invited to present their challenge systems and to submit
additional workshop papers.
  For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:
(1) A stronger, semi-informed attack model in the form of an automatic speaker
verification (ASV) system trained on anonymized (per-utterance) speech data.
(2) Complementary metrics comprising the equal error rate (EER) as a privacy
metric, the word error rate (WER) as a primary utility metric, and the pitch
correlation and gain of voice distinctiveness as secondary utility metrics. (3)
A new ranking policy based upon a set of minimum target privacy requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Fast Polarity Labelling of Massive Data Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huilin Wu, Mian Lu, Zhao Zheng, Shuhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many of the existing sentiment analysis techniques are based on supervised
learning, and they demand the availability of valuable training datasets to
train their models. When dataset freshness is critical, the annotating of high
speed unlabelled data streams becomes critical but remains an open problem. In
this paper, we propose PLStream, a novel Apache Flink-based framework for fast
polarity labelling of massive data streams, like Twitter tweets or online
product reviews. We address the associated implementation challenges and
propose a list of techniques including both algorithmic improvements and system
optimizations. A thorough empirical validation with two real-world workloads
demonstrates that PLStream is able to generate high quality labels (almost 80%
accuracy) in the presence of high-speed continuous unlabelled data streams
(almost 16,000 tuples/sec) without any manual efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Input-specific Attention Subnetworks for Adversarial Detection <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emil Biju, Anirudh Sriram, Pratyush Kumar, Mitesh M Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention heads are characteristic of Transformer models and have been
well studied for interpretability and pruning. In this work, we demonstrate an
altogether different utility of attention heads, namely for adversarial
detection. Specifically, we propose a method to construct input-specific
attention subnetworks (IAS) from which we extract three features to
discriminate between authentic and adversarial inputs. The resultant detector
significantly improves (by over 7.5%) the state-of-the-art adversarial
detection accuracy for the BERT encoder on 10 NLU datasets with 11 different
adversarial attack types. We also demonstrate that our method (a) is more
accurate for larger models which are likely to have more spurious correlations
and thus vulnerable to adversarial attack, and (b) performs well even with
modest training sets of adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of ACL 2022, 14 pages, 6 Tables and 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Structure Generation for Universal Information Extraction <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information extraction suffers from its varying targets, heterogeneous
structures, and demand-specific schemas. In this paper, we propose a unified
text-to-structure generation framework, namely UIE, which can universally model
different IE tasks, adaptively generate targeted structures, and
collaboratively learn general IE abilities from different knowledge sources.
Specifically, UIE uniformly encodes different extraction structures via a
structured extraction language, adaptively generates target extractions via a
schema-based prompt mechanism - structural schema instructor, and captures the
common IE abilities via a large-scale pre-trained text-to-structure model.
Experiments show that UIE achieved the state-of-the-art performance on 4 IE
tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings
for a wide range of entity, relation, event and sentiment extraction tasks and
their unification. These results verified the effectiveness, universality, and
transferability of UIE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of ACL2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ERNIE-SPARSE: Learning Hierarchical Efficient <span class="highlight-title">Transformer</span> Through
  Regularized Self-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Yang Liu</span>, Jiaxiang Liu, Li Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu Sun, Hao Tian, Hua Wu, <span class="highlight-author">Haifeng Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Transformer has recently attracted a lot of attention since the
ability for reducing the quadratic dependency on the sequence length. We argue
that two factors, information bottleneck sensitivity and inconsistency between
different attention topologies, could affect the performance of the Sparse
Transformer. This paper proposes a well-designed model named ERNIE-Sparse. It
consists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to
sequentially unify local and global information. (ii) Self-Attention
Regularization (SAR) method, a novel regularization designed to minimize the
distance for transformers with different attention topologies. To evaluate the
effectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we
perform experiments on a multi-modal long sequence modeling task benchmark,
Long Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse
significantly outperforms a variety of strong baseline methods including the
dense attention and other efficient sparse attention methods and achieves
improvements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the
effectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text
classification and 2 QA downstream tasks, achieve improvements on
classification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%
(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ing to Match for Unified Low-shot Relation Extraction <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangchao Liu, Hongyu Lin, Xianpei Han, Boxi Cao, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of ACL2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECO v1: Towards Event-Centric Opinion Mining <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoxi Xu, Hongyu Lin, Meng Liao, Xianpei Han, Jin Xu, Wei Tan, Yingfei Sun, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Events are considered as the fundamental building blocks of the world. Mining
event-centric opinions can benefit decision making, people communication, and
social good. Unfortunately, there is little literature addressing event-centric
opinion mining, although which significantly diverges from the well-studied
entity-centric opinion mining in connotation, structure, and expression. In
this paper, we propose and formulate the task of event-centric opinion mining
based on event-argument structure and expression categorizing theory. We also
benchmark this task by constructing a pioneer corpus and designing a two-step
benchmark framework. Experiment results show that event-centric opinion mining
is feasible and challenging, and the proposed task, dataset, and baselines are
beneficial for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Prompt</span> Probe <span class="highlight-title">Pretrain</span>ed Language Models? Understanding the Invisible
  Risks from a Causal View <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Cao, Hongyu Lin, Xianpei Han, Fangchao Liu, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based probing has been widely used in evaluating the abilities of
pretrained language models (PLMs). Unfortunately, recent studies have
discovered such an evaluation may be inaccurate, inconsistent and unreliable.
Furthermore, the lack of understanding its inner workings, combined with its
wide applicability, has the potential to lead to unforeseen risks for
evaluating and applying PLMs in real-world applications. To discover,
understand and quantify the risks, this paper investigates the prompt-based
probing from a causal view, highlights three critical biases which could induce
biased results and conclusions, and proposes to conduct debiasing via causal
intervention. This paper provides valuable insights for the design of unbiased
datasets, better probing frameworks and more reliable evaluations of pretrained
language models. Furthermore, our conclusions also echo that we need to rethink
the criteria for identifying better pretrained language models. We openly
released the source code and data at https://github.com/c-box/causalEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of ACL2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IAM: A Comprehensive and Large-Scale <span class="highlight-title">Dataset</span> for Integrated Argument
  Mining Tasks <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liying Cheng, Lidong Bing, Ruidan He, Qian Yu, Yan Zhang, Luo Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, a debate usually requires a manual preparation process,
including reading plenty of articles, selecting the claims, identifying the
stances of the claims, seeking the evidence for the claims, etc. As the AI
debate attracts more attention these years, it is worth exploring the methods
to automate the tedious process involved in the debating system. In this work,
we introduce a comprehensive and large dataset named IAM, which can be applied
to a series of argument mining tasks, including claim extraction, stance
classification, evidence extraction, etc. Our dataset is collected from over 1k
articles related to 123 topics. Near 70k sentences in the dataset are fully
annotated based on their argument properties (e.g., claims, stances, evidence,
etc.). We further propose two new integrated argument mining tasks associated
with the debate preparation process: (1) claim extraction with stance
classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a
pipeline approach and an end-to-end method for each integrated task separately.
Promising experimental results are reported to show the values and challenges
of our proposed tasks, and motivate future research on argument mining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, accepted by ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat-Capsule: A Hierarchical Capsule for <span class="highlight-title">Dialog</span>-level Emotion Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yequan Wang, Xuying Meng, Yiyi Liu, Aixin Sun, Yao Wang, Yinhe Zheng, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many studies on dialog emotion analysis focus on utterance-level emotion
only. These models hence are not optimized for dialog-level emotion detection,
i.e. to predict the emotion category of a dialog as a whole. More importantly,
these models cannot benefit from the context provided by the whole dialog. In
real-world applications, annotations to dialog could fine-grained, including
both utterance-level tags (e.g. speaker type, intent category, and emotion
category), and dialog-level tags (e.g. user satisfaction, and emotion curve
category). In this paper, we propose a Context-based Hierarchical Attention
Capsule~(Chat-Capsule) model, which models both utterance-level and
dialog-level emotions and their interrelations. On a dialog dataset collected
from customer support of an e-commerce platform, our model is also able to
predict user satisfaction and emotion curve category. Emotion curve refers to
the change of emotions along the development of a conversation. Experiments
show that the proposed Chat-Capsule outperform state-of-the-art baselines on
both benchmark dataset and proprietary dataset. Source code will be released
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Named Entity Recognition with Self-describing Networks <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Chen, Qing Liu, Hongyu Lin, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot NER needs to effectively capture information from limited instances
and transfer useful knowledge from external resources. In this paper, we
propose a self-describing mechanism for few-shot NER, which can effectively
leverage illustrative instances and precisely transfer knowledge from external
resources by describing both entity types and mentions using a universal
concept set. Specifically, we design Self-describing Networks (SDNet), a
Seq2Seq generation model which can universally describe mentions using
concepts, automatically map novel entity types to concepts, and adaptively
recognize entities on-demand. We pre-train SDNet with large-scale corpus, and
conduct experiments on 8 benchmarks from different domains. Experiments show
that SDNet achieves competitive performances on all benchmarks and achieves the
new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of ACL2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kogkalidis, Michael Moortgat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The syntactic categories of categorial grammar formalisms are structured
units made of smaller, indivisible primitives, bound together by the underlying
grammar's category formation rules. In the trending approach of constructive
supertagging, neural models are increasingly made aware of the internal
category structure, which in turn enables them to more reliably predict rare
and out-of-vocabulary categories, with significant implications for grammars
previously deemed too complex to find practical use. In this work, we revisit
constructive supertagging from a graph-theoretic perspective, and propose a
framework based on heterogeneous dynamic graph convolutions aimed at exploiting
the distinctive structure of a supertagger's output space. We test our approach
on a number of categorial grammar datasets spanning different languages and
grammar formalisms, achieving substantial improvements over previous state of
the art scores. Code will be made available at
https://github.com/konstantinosKokos/dynamic-graph-supertagging
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages plus references, unpublished preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Integrating Vectorized Lexical Constraints for Neural Machine
  Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Wang, Zhixing Tan, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexically constrained neural machine translation (NMT), which controls the
generation of NMT models with pre-specified constraints, is important in many
practical scenarios. Due to the representation gap between discrete constraints
and continuous vectors in NMT models, most existing works choose to construct
synthetic data or modify the decoding algorithm to impose lexical constraints,
treating the NMT model as a black box. In this work, we propose to open this
black box by directly integrating the constraints into NMT models.
Specifically, we vectorize source and target constraints into continuous keys
and values, which can be utilized by the attention modules of NMT models. The
proposed integration method is based on the assumption that the correspondence
between keys and values in attention modules is naturally suitable for modeling
constraint pairs. Experimental results show that our method consistently
outperforms several representative baselines on four language pairs,
demonstrating the superiority of integrating vectorized lexical constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2022 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Expressive Speaking Style Modelling with Hierarchical Context
  Information for Mandarin Speech Synthesis <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Shiyin Kang, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous works on expressive speech synthesis mainly focus on current
sentence. The context in adjacent sentences is neglected, resulting in
inflexible speaking style for the same text, which lacks speech variations. In
this paper, we propose a hierarchical framework to model speaking style from
context. A hierarchical context encoder is proposed to explore a wider range of
contextual information considering structural relationship in context,
including inter-phrase and inter-sentence relations. Moreover, to encourage
this encoder to learn style representation better, we introduce a novel
training strategy with knowledge distillation, which provides the target for
encoder training. Both objective and subjective evaluations on a Mandarin
lecture dataset demonstrate that the proposed method can significantly improve
the naturalness and expressiveness of the synthesized speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Converse -- A Tree-Based Modular <span class="highlight-title">Task-Oriented</span> <span class="highlight-title">Dialogue</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Xie, Xinyi Yang, Angela S. Lin, Feihong Wu, Kazuma Hashimoto, Jin Qu, Young Mo Kang, Wenpeng Yin, Huan Wang, Semih Yavuz, Gang Wu, Michael Jones, <span class="highlight-author">Richard Socher</span>, Yingbo Zhou, Wenhao Liu, <span class="highlight-author">Caiming Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a system that can have meaningful conversations with humans to help
accomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).
It has defined the meaning of AI since the beginning. A lot has been
accomplished in this area recently, with voice assistant products entering our
daily lives and chat bot systems becoming commonplace in customer service. At
first glance there seems to be no shortage of options for dialogue systems.
However, the frequently deployed dialogue systems today seem to all struggle
with a critical weakness - they are hard to build and harder to maintain. At
the core of the struggle is the need to script every single turn of
interactions between the bot and the human user. This makes the dialogue
systems more difficult to maintain as the tasks become more complex and more
tasks are added to the system. In this paper, we propose Converse, a flexible
tree-based modular task-oriented dialogue system. Converse uses an and-or tree
structure to represent tasks and offers powerful multi-task dialogue
management. Converse supports task dependency and task switching, which are
unique features compared to other open-source dialogue frameworks. At the same
time, Converse aims to make the bot building process easy and simple, for both
professional and non-professional software developers. The code is available at
https://github.com/salesforce/Converse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AbductionRules: Training <span class="highlight-title">Transformer</span>s to Explain Unexpected Inputs <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have recently been shown to be capable of reliably performing
logical reasoning over facts and rules expressed in natural language, but
abductive reasoning - inference to the best explanation of an unexpected
observation - has been underexplored despite significant applications to
scientific discovery, common-sense reasoning, and model interpretability.
  We present AbductionRules, a group of natural language datasets designed to
train and test generalisable abduction over natural-language knowledge bases.
We use these datasets to finetune pretrained Transformers and discuss their
performance, finding that our models learned generalisable abductive techniques
but also learned to exploit the structure of our data. Finally, we discuss the
viability of this approach to abductive reasoning and ways in which it may be
improved in future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theoretically Grounded Benchmark for Evaluating Machine Commonsense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrique Santos, Ke Shen, Alice M. Mulvehill, Yasaman Razeghi, Deborah L. McGuinness, Mayank Kejriwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming machines with commonsense reasoning (CSR) abilities is a
longstanding challenge in the Artificial Intelligence community. Current CSR
benchmarks use multiple-choice (and in relatively fewer cases, generative)
question-answering instances to evaluate machine commonsense. Recent progress
in transformer-based language representation models suggest that considerable
progress has been made on existing benchmarks. However, although tens of CSR
benchmarks currently exist, and are growing, it is not evident that the full
suite of commonsense capabilities have been systematically evaluated.
Furthermore, there are doubts about whether language models are 'fitting' to a
benchmark dataset's training partition by picking up on subtle, but normatively
irrelevant (at least for CSR), statistical features to achieve good performance
on the testing partition. To address these challenges, we propose a benchmark
called Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based
on discriminative question answering, but with questions designed to evaluate
diverse aspects of commonsense, such as space, time, and world states. TG-CSR
is based on a subset of commonsense categories first proposed as a viable
theory of commonsense by Gordon and Hobbs. The benchmark is also designed to be
few-shot (and in the future, zero-shot), with only a few training and
validation examples provided. This report discusses the structure and
construction of the benchmark. Preliminary results suggest that the benchmark
is challenging even for advanced language representation models designed for
discriminative CSR question answering tasks.
  Benchmark access and leaderboard:
https://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:
https://usc-isi-i2.github.io/TGCSR/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> An Empirical Study of Memorization in NLP <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, <span class="highlight-author">Jing Jiang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent study by Feldman (2020) proposed a long-tail theory to explain the
memorization behavior of deep learning models. However, memorization has not
been empirically verified in the context of NLP, a gap addressed by this work.
In this paper, we use three different NLP tasks to check if the long-tail
theory holds. Our experiments demonstrate that top-ranked memorized training
instances are likely atypical, and removing the top-memorized training
instances leads to a more serious drop in test accuracy compared with removing
training instances randomly. Furthermore, we develop an attribution method to
better understand why a training instance is memorized. We empirically show
that our memorization attribution method is faithful, and share our interesting
finding that the top-memorized parts of a training instance tend to be features
negatively correlated with the class label.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022. Code & data available at
  https://github.com/xszheng2020/memorization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALT: um software para análise de legibilidade de textos em Língua
  Portuguesa 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleice Carvalho de Lima Moreno, Marco P. M. de Souza, Nelson Hein, Adriana Kroenke Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the initial stage of human life, communication, seen as a process of
social interaction, was always the best way to reach consensus between the
parties. Understanding and credibility in this process are essential for the
mutual agreement to be validated. But, how to do it so that this communication
reaches the great mass? This is the main challenge when what is sought is the
dissemination of information and its approval. In this context, this study
presents the ALT software, developed from original readability metrics adapted
to the Portuguese language, available on the web, to reduce communication
difficulties. The development of the software was motivated by the theory of
communicative action of Habermas, which uses a multidisciplinary style to
measure the credibility of the discourse in the communication channels used to
build and maintain a safe and healthy relationship with the public.
  --
  No est\'agio inicial da vida humana a comunica\c{c}\~ao, vista como um
processo de intera\c{c}\~ao social, foi sempre o melhor caminho para o consenso
entre as partes. O entendimento e a credibilidade nesse processo s\~ao
fundamentais para que o acordo m\'utuo seja validado. Mas, como faz\^e-lo de
forma que essa comunica\c{c}\~ao alcance a grande massa? Esse \'e o principal
desafio quando o que se busca \'e a difus\~ao da informa\c{c}\~ao e a sua
aprova\c{c}\~ao. Nesse contexto, este estudo apresenta o software ALT,
desenvolvido a partir de m\'etricas de legibilidade originais adaptadas para a
L\'ingua Portuguesa, dispon\'ivel na web, para reduzir as dificuldades na
comunica\c{c}\~ao. O desenvolvimento do software foi motivado pela teoria do
agir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para
medir a credibilidade do discurso nos canais de comunica\c{c}\~ao utilizados
para construir e manter uma rela\c{c}\~ao segura e saud\'avel com o p\'ublico.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures, in Portuguese, see software in
  https://legibilidade.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study on Learning and Improving the Search Objective for
  Unsupervised Paraphrasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Steven Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in unsupervised text generation has been gaining attention over the
years. One recent approach is local search towards a heuristically defined
objective, which specifies language fluency, semantic meanings, and other
task-specific attributes. Search in the sentence space is realized by
word-level edit operations including insertion, replacement, and deletion.
However, such objective function is manually designed with multiple components.
Although previous work has shown maximizing this objective yields good
performance in terms of true measure of success (i.e. BLEU and iBLEU), the
objective landscape is considered to be non-smooth with significant noises,
posing challenges for optimization. In this dissertation, we address the
research problem of smoothing the noise in the heuristic search objective by
learning to model the search dynamics. Then, the learned model is combined with
the original objective function to guide the search in a bootstrapping fashion.
Experimental results show that the learned models combined with the original
search objective can indeed provide a smoothing effect, improving the search
performance by a small margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ThingTalk: An Extensible, Executable Representation Language for
  <span class="highlight-title">Task-Oriented</span> <span class="highlight-title">Dialogue</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monica S. Lam, Giovanni Campagna, Mehrad Moradshahi, Sina J. Semnani, Silei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented conversational agents rely on semantic parsers to translate
natural language to formal representations. In this paper, we propose the
design and rationale of the ThingTalk formal representation, and how the design
improves the development of transactional task-oriented agents.
  ThingTalk is built on four core principles: (1) representing user requests
directly as executable statements, covering all the functionality of the agent,
(2) representing dialogues formally and succinctly to support accurate
contextual semantic parsing, (3) standardizing types and interfaces to maximize
reuse between agents, and (4) allowing multiple, independently-developed agents
to be composed in a single virtual assistant. ThingTalk is developed as part of
the Genie Framework that allows developers to quickly build transactional
agents given a database and APIs.
  We compare ThingTalk to existing representations: SMCalFlow, SGD, TreeDST.
Compared to the others, the ThingTalk design is both more general and more
cost-effective. Evaluated on the MultiWOZ benchmark, using ThingTalk and
associated tools yields a new state of the art accuracy of 79% turn-by-turn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Training for Improving Model Robustness? Look at Both
  Prediction and Interpretation <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjie Chen, Yangfeng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural language models show vulnerability to adversarial examples which are
semantically similar to their original counterparts with a few words replaced
by their synonyms. A common way to improve model robustness is adversarial
training which follows two steps-collecting adversarial examples by attacking a
target model, and fine-tuning the model on the augmented dataset with these
adversarial examples. The objective of traditional adversarial training is to
make a model produce the same correct predictions on an original/adversarial
example pair. However, the consistency between model decision-makings on two
similar texts is ignored. We argue that a robust model should behave
consistently on original/adversarial example pairs, that is making the same
predictions (what) based on the same reasons (how) which can be reflected by
consistent interpretations. In this work, we propose a novel feature-level
adversarial training method named FLAT. FLAT aims at improving model robustness
in terms of both predictions and interpretations. FLAT incorporates variational
word masks in neural networks to learn global word importance and play as a
bottleneck teaching the model to make predictions based on important words.
FLAT explicitly shoots at the vulnerability problem caused by the mismatch
between model understandings on the replaced words and their synonyms in
original/adversarial example pairs by regularizing the corresponding global
word importance scores. Experiments show the effectiveness of FLAT in improving
the robustness with respect to both predictions and interpretations of four
neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks
on four text classification tasks. The models trained via FLAT also show better
robustness than baseline models on unforeseen adversarial examples across
different attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Linearizing <span class="highlight-title">Transformer</span> with Key-Value Memory Bank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, <span class="highlight-author">Deng Cai</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has brought great success to a wide range of natural language
processing tasks. Nevertheless, the computational overhead of the vanilla
transformer scales quadratically with sequence length. Many efforts have been
made to develop more efficient transformer variants. A line of work (e.g.,
Linformer) projects the input sequence into a low-rank space, achieving linear
time complexity. However, Linformer does not suit well for text generation
tasks as the sequence length must be pre-specified. We propose MemSizer, an
approach also projects the source sequence into lower dimension representation
but can take input with dynamic length, with a different perspective of the
attention mechanism. MemSizer not only achieves the same linear time complexity
but also enjoys efficient recurrent-style autoregressive generation, which
yields constant memory complexity and reduced computation at inference. We
demonstrate that MemSizer provides an improved tradeoff between efficiency and
accuracy over the vanilla transformer and other linear variants in language
modeling and machine translation tasks, revealing a viable direction towards
further inference efficiency improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Model Supervised by Understanding Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06043v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06043v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> UniPELT: A Unified Framework for Parameter-Efficient Language Model
  Tuning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, <span class="highlight-author">Jiawei Han</span>, Wen-tau Yih, Madian Khabsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent parameter-efficient language model tuning (PELT) methods manage to
match the performance of fine-tuning with much fewer trainable parameters and
perform especially well when training data is limited. However, different PELT
methods may perform rather differently on the same task, making it nontrivial
to select the most appropriate method for a specific task, especially
considering the fast-growing number of new PELT methods and tasks. In light of
model diversity and the difficulty of model selection, we propose a unified
framework, UniPELT, which incorporates different PELT methods as submodules and
learns to activate the ones that best suit the current data or task setup via
gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%
gains compared to the best individual PELT method that it incorporates and even
outperforms fine-tuning under different setups. Moreover, UniPELT generally
surpasses the upper bound that takes the best performance of all its submodules
used individually on each task, indicating that a mixture of multiple PELT
methods may be inherently more effective than single methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Visual-<span class="highlight-title">Prompt</span> Temporal Answering Grounding in Medical
  Instructional Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06667v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06667v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Li, Yixuan Weng, Bin Sun, Shutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms other
state-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,
which demonstrates the effectiveness of visual prompt and the text span
predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Repair in the Presence of Spelling Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.07878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.07878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Bast, Matthias Hertel, Mostafa M. Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the following tokenization repair problem: Given a natural
language text with any combination of missing or spurious spaces, correct
these. Spelling errors can be present, but it's not part of the problem to
correct them. For example, given: "Tispa per isabout token izaionrep air",
compute "Tis paper is about tokenizaion repair". We identify three key
ingredients of high-quality tokenization repair, all missing from previous
work: deep language models with a bidirectional component, training the models
on text with spelling errors, and making use of the space information already
present. Our methods also improve existing spell checkers by fixing not only
more tokenization errors but also more spelling errors: once it is clear which
characters form a word, it is much easier for them to figure out the correct
word. We provide six benchmarks that cover three use cases (OCR errors, text
extraction from PDF, human errors) and the cases of partially correct space
information and all spaces missing. We evaluate our methods against the best
existing methods and a non-trivial baseline. We provide full reproducibility
under https://ad.cs.uni-freiburg.de/publications .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Position Bias in Simultaneous Machine Translation with
  Length-Aware Framework <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2022 main conference. 14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Simple but Effective Pluggable Entity Lookup Table for <span class="highlight-title">Pre-train</span>ed
  Language Models <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deming Ye, <span class="highlight-author">Yankai Lin</span>, Peng Li, Maosong Sun, <span class="highlight-author">Zhiyuan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) cannot well recall rich factual knowledge
of entities exhibited in large-scale corpora, especially those rare entities.
In this paper, we propose to build a simple but effective Pluggable Entity
Lookup Table (PELT) on demand by aggregating the entity's output
representations of multiple occurrences in the corpora. PELT can be compatibly
plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared
to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation
with capability of acquiring knowledge from out-of-domain corpora for domain
adaptation scenario. The experiments on knowledge-related tasks demonstrate
that our method, PELT, can flexibly and effectively transfer entity knowledge
from related corpora into PLMs with different architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022. The code and models are available at
  https://github.com/thunlp/PELT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Packed Levitated Marker for Entity and Relation Extraction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06067v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06067v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deming Ye, <span class="highlight-author">Yankai Lin</span>, Peng Li, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent entity and relation extraction works focus on investigating how to
obtain a better span representation from the pre-trained encoder. However, a
major limitation of existing works is that they ignore the interrelation
between spans (pairs). In this work, we propose a novel span representation
approach, named Packed Levitated Markers (PL-Marker), to consider the
interrelation between the spans (pairs) by strategically packing the markers in
the encoder. In particular, we propose a neighborhood-oriented packing
strategy, which considers the neighbor spans integrally to better model the
entity boundary information. Furthermore, for those more complicated span pair
classification tasks, we design a subject-oriented packing strategy, which
packs each subject and all its objects to model the interrelation between the
same-subject span pairs. The experimental results show that, with the enhanced
marker feature, our model advances baselines on six NER benchmarks, and obtains
a 4.1%-4.3% strict relation F1 improvement with higher speed over previous
state-of-the-art models on ACE04 and ACE05.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022. The code and models are available at
  https://github.com/thunlp/PL-Marker</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoisyTune: A Little Noise Can Help You Finetune <span class="highlight-title">Pretrain</span>ed Language
  Models Better <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively finetuning pretrained language models (PLMs) is critical for
their success in downstream tasks. However, PLMs may have risks in overfitting
the pretraining tasks and data, which usually have gap with the target
downstream tasks. Such gap may be difficult for existing PLM finetuning methods
to overcome and lead to suboptimal performance. In this paper, we propose a
very simple yet effective method named NoisyTune to help better finetune PLMs
on downstream tasks by adding some noise to the parameters of PLMs before
fine-tuning. More specifically, we propose a matrix-wise perturbing method
which adds different uniform noises to different parameter matrices based on
their standard deviations. In this way, the varied characteristics of different
types of parameters in PLMs can be considered. Extensive experiments on both
GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can
consistently empower the finetuning of different PLMs on different downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.04736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.04736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Langedijk, Verna Dankers, Phillip Lippe, Sander Bos, Bryan Cardenas Guevara, Helen Yannakoudakis, Ekaterina Shutova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning, or learning to learn, is a technique that can help to overcome
resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to
new tasks. We apply model-agnostic meta-learning (MAML) to the task of
cross-lingual dependency parsing. We train our model on a diverse set of
languages to learn a parameter initialization that can adapt quickly to new
languages. We find that meta-learning with pre-training can significantly
improve upon the performance of language transfer and standard supervised
learning baselines for a variety of unseen, typologically diverse, and
low-resource languages, in a few-shot learning setup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>- Add additional results (Appendix D) - Cosmetic updates for
  camera-ready version ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Data Gap between Training and Inference for Unsupervised
  Neural Machine Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08394v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08394v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei He, Xing Wang, Rui Wang, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Back-translation is a critical component of Unsupervised Neural Machine
Translation (UNMT), which generates pseudo parallel data from target
monolingual data. A UNMT model is trained on the pseudo parallel data with
translated source, and translates natural source sentences in inference. The
source discrepancy between training and inference hinders the translation
performance of UNMT models. By carefully designing experiments, we identify two
representative characteristics of the data gap in source: (1) style gap (i.e.,
translated vs. natural text style) that leads to poor generalization
capability; (2) content gap that induces the model to produce hallucination
content biased towards the target language. To narrow the data gap, we propose
an online self-training approach, which simultaneously uses the pseudo parallel
data {natural source, translated target} to mimic the inference scenario.
Experimental results on several widely-used language pairs show that our
approach outperforms two strong baselines (XLM and MASS) by remedying the style
and content gaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overlap-based Vocabulary Generation Improves Cross-lingual Transfer
  Among Related Languages <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaidehi Patil, Partha Talukdar, Sunita Sarawagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained multilingual language models such as mBERT and XLM-R have
demonstrated great potential for zero-shot cross-lingual transfer to low
web-resource languages (LRL). However, due to limited model capacity, the large
difference in the sizes of available monolingual corpora between high
web-resource languages (HRL) and LRLs does not provide enough scope of
co-embedding the LRL with the HRL, thereby affecting downstream task
performance of LRLs. In this paper, we argue that relatedness among languages
in a language family along the dimension of lexical overlap may be leveraged to
overcome some of the corpora limitations of LRLs. We propose Overlap BPE
(OBPE), a simple yet effective modification to the BPE vocabulary generation
algorithm which enhances overlap across related languages. Through extensive
experiments on multiple NLP tasks and datasets, we observe that OBPE generates
a vocabulary that increases the representation of LRLs via tokens shared with
HRLs. This results in improved zero-shot transfer from related HRLs to LRLs
without reducing HRL representation and accuracy. Unlike previous studies that
dismissed the importance of token-overlap, we show that in the low-resource
related language setting, token overlap matters. Synthetically reducing the
overlap to zero can cause as much as a four-fold drop in zero-shot transfer
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear at the ACL 2022 Main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triples-to-Text Generation with Reinforcement Learning Based
  Graph-augmented Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanning Gao, Lingfei Wu, Hongyun Zhang, Zhihua Wei, Po Hu, Fangli Xu, Bo Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating Hierarchy into Text Encoder: a <span class="highlight-title">Contrastive Learning</span>
  Approach for Hierarchical Text Classification <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Peiyi Wang, Lianzhe Huang, Xin Sun, Houfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical text classification is a challenging subtask of multi-label
classification due to its complex label hierarchy. Existing methods encode text
and label hierarchy separately and mix their representations for
classification, where the hierarchy remains unchanged for all input text.
Instead of modeling them separately, in this work, we propose Hierarchy-guided
Contrastive Learning (HGCLR) to directly embed the hierarchy into a text
encoder. During training, HGCLR constructs positive samples for input text
under the guidance of the label hierarchy. By pulling together the input text
and its positive sample, the text encoder can learn to generate the
hierarchy-aware text representation independently. Therefore, after training,
the HGCLR enhanced text encoder can dispense with the redundant hierarchy.
Extensive experiments on three benchmark datasets verify the effectiveness of
HGCLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical <span class="highlight-title">Survey</span> of the Effectiveness of Debiasing Techniques for
  <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Meade, Elinor Poole-Dayan, Siva Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown pre-trained language models capture social biases from
the large amounts of text they are trained on. This has attracted attention to
developing techniques that mitigate such biases. In this work, we perform an
empirical survey of five recently proposed bias mitigation techniques:
Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace
Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of
each technique using three intrinsic bias benchmarks while also measuring the
impact of these techniques on a model's language modeling ability, as well as
its performance on downstream NLU tasks. We experimentally find that: (1)
Self-Debias is the strongest debiasing technique, obtaining improved scores on
all bias benchmarks; (2) Current debiasing techniques perform less consistently
when mitigating non-gender biases; And (3) improvements on bias benchmarks such
as StereoSet and CrowS-Pairs by using debiasing strategies are often
accompanied by a decrease in language modeling ability, making it difficult to
determine whether the bias mitigation was effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Bounded Context-Free-Grammar via LSTM and the
  <span class="highlight-title">Transformer</span>:Difference and Explanations <span class="chip">AAAI22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shi, Sicun Gao, Yuandong Tian, Xinyun Chen, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long Short-Term Memory (LSTM) and Transformers are two popular neural
architectures used for natural language processing tasks. Theoretical results
show that both are Turing-complete and can represent any context-free language
(CFL).In practice, it is often observed that Transformer models have better
representation power than LSTM. But the reason is barely understood. We study
such practical differences between LSTM and Transformer and propose an
explanation based on their latent space decomposition patterns. To achieve this
goal, we introduce an oracle training paradigm, which forces the decomposition
of the latent representation of LSTM and the Transformer and supervises with
the transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With
the forced decomposition, we show that the performance upper bounds of LSTM and
Transformer in learning CFL are close: both of them can simulate a stack and
perform stack operation along with state transitions. However, the absence of
forced decomposition leads to the failure of LSTM models to capture the stack
and stack operations, while having a marginal impact on the Transformer model.
Lastly, we connect the experiment on the prototypical PDA to a real-world
parsing task to re-verify the conclusions
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted By AAAI22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-Verse: Bidirectional Generation Between Image and Text <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11133v10">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11133v10.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hassid, Michelle Tadmor Ramanovich, Brendan Shillingford, Miaosen Wang, Ye Jia, Tal Remez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.
Motivated by dubbing, VDTTS takes advantage of video frames as an additional
input alongside text, and generates speech that matches the video signal. We
demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech
that not only has prosodic variations like natural pauses and pitch, but is
also synchronized to the input video. Experimentally, we show our model
produces well-synchronized outputs, approaching the video-speech
synchronization quality of the ground-truth, on several challenging benchmarks
including "in-the-wild" content from VoxCeleb2. Supplementary demo videos
demonstrating video-speech synchronization, robustness to speaker ID swapping,
and prosody, presented at the project page.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CLIPScore: A Reference-free Evaluation Metric for Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.08718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.08718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, <span class="highlight-author">Yejin Choi</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning has conventionally relied on reference-based automatic
evaluations, where machine captions are compared against captions written by
humans. This is in contrast to the reference-free manner in which humans assess
caption quality.
  In this paper, we report the surprising empirical finding that CLIP (Radford
et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from
the web, can be used for robust automatic evaluation of image captioning
without the need for references. Experiments spanning several corpora
demonstrate that our new reference-free metric, CLIPScore, achieves the highest
correlation with human judgements, outperforming existing reference-based
metrics like CIDEr and SPICE. Information gain experiments demonstrate that
CLIPScore, with its tight focus on image-text compatibility, is complementary
to existing reference-based metrics that emphasize text-text similarities.
Thus, we also present a reference-augmented version, RefCLIPScore, which
achieves even higher correlation. Beyond literal description tasks, several
case studies reveal domains where CLIPScore performs well (clip-art images,
alt-text rating), but also where it is relatively weaker in comparison to
reference-based metrics, e.g., news captions that require richer contextual
knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S<span class="highlight-title">GPT</span>: <span class="highlight-title">GPT</span> Sentence Embeddings for Semantic Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08904v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08904v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT transformers are the largest language models available, yet semantic
search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for
applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric
search.
  SGPT-BE produces semantically meaningful sentence embeddings by contrastive
fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion
parameter SGPT-BE outperforms the best available sentence embeddings by 6%
setting a new state-of-the-art on BEIR. It outperforms the concurrently
proposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes
250,000 times more parameters.
  SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1
billion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It
beats the supervised state-of-the-art on 7 datasets, but significantly loses on
other datasets. We show how this can be alleviated by adapting the prompt.
  SGPT-BE and SGPT-CE performance scales with model size. Yet, increased
latency, storage and compute costs should be considered. Code, models and
result files are freely available at https://github.com/Muennighoff/sgpt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, 12 tables. v2 corrects a misreported nDCG@10
  number for the SGPT-BE-5.8B model. v3 updates SGPT-BE-5.8B scores based on
  retrained models with larger batch sizes v4 removes a superfluous table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Salient Object Detection with Spectral Cluster Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyungin Shin, Samuel Albanie, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the challenging task of unsupervised salient object
detection (SOD) by leveraging spectral clustering on self-supervised features.
We make the following contributions: (i) We revisit spectral clustering and
demonstrate its potential to group the pixels of salient objects; (ii) Given
mask proposals from multiple applications of spectral clustering on image
features computed from various self-supervised models, e.g., MoCov2, SwAV,
DINO, we propose a simple but effective winner-takes-all voting mechanism for
selecting the salient masks, leveraging object priors based on framing and
distinctiveness; (iii) Using the selected object segmentation as pseudo
groundtruth masks, we train a salient object detector, dubbed SelfMask, which
outperforms prior approaches on three unsupervised SOD benchmarks. Code is
publicly available at https://github.com/NoelShin/selfmask.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Mesh-neural Representation for 3D Transparent Object
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Xu, Zihan Zhu, Hujun Bao, Wewei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method to reconstruct the 3D shapes of transparent objects
using hand-held captured images under natural light conditions. It combines the
advantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid
representation, to simplify the capture setting used in recent contributions.
After obtaining an initial shape through the multi-view silhouettes, we
introduce surface-based local MLPs to encode the vertex displacement field
(VDF) for the reconstruction of surface details. The design of local MLPs
allows to represent the VDF in a piece-wise manner using two layer MLP
networks, which is beneficial to the optimization algorithm. Defining local
MLPs on the surface instead of the volume also reduces the searching space.
Such a hybrid representation enables us to relax the ray-pixel correspondences
that represent the light path constraint to our designed ray-cell
correspondences, which significantly simplifies the implementation of
single-image based environment matting algorithm. We evaluate our
representation and reconstruction algorithm on several transparent objects with
ground truth models. Our experiments show that our method can produce
high-quality reconstruction results superior to state-of-the-art methods using
a simplified data acquisition setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StructToken : Rethinking Semantic Segmentation with Structural Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangjian Lin, Zhanhao Liang, Junjun He, Miao Zheng, Shengwei Tian, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present structure token (StructToken), a new paradigm for
semantic segmentation. From a perspective on semantic segmentation as per-pixel
classification, the previous deep learning-based methods learn the per-pixel
representation first through an encoder and a decoder head and then classify
each pixel representation to a specific category to obtain the semantic masks.
Differently, we propose a structure-aware algorithm that takes structural
information as prior to predict semantic masks directly without per-pixel
classification. Specifically, given an input image, the learnable structure
token interacts with the image representations to reason the final semantic
masks. Three interaction approaches are explored and the results not only
outperform the state-of-the-art methods but also contain more structural
information. Experiments are conducted on three widely used datasets including
ADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could
serve as an alternative for semantic segmentation and inspire future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Fairness of Chest X-ray Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Natalie Dullerud, Karsten Roth, Lauren Oakden-Rayner, Stephen Robert Pfohl, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have reached or surpassed human-level performance in the
field of medical imaging, especially in disease diagnosis using chest x-rays.
However, prior work has found that such classifiers can exhibit biases in the
form of gaps in predictive performance across protected groups. In this paper,
we question whether striving to achieve zero disparities in predictive
performance (i.e. group fairness) is the appropriate fairness definition in the
clinical setting, over minimax fairness, which focuses on maximizing the
performance of the worst-case group. We benchmark the performance of nine
methods in improving classifier fairness across these two definitions. We find,
consistent with prior work on non-clinical data, that methods which strive to
achieve better worst-group performance do not outperform simple data balancing.
We also find that methods which achieve group fairness do so by worsening
performance for all groups. In light of these results, we discuss the utility
of fairness definitions in the clinical setting, advocating for an
investigation of the bias-inducing mechanisms in the underlying data generating
process whenever possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in CHIL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoMAE: Masked Autoencoders are Data-Efficient Learners for
  <span class="highlight-title">Self-Supervised</span> Video <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Tong, Yibing Song, Jue Wang, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training video transformers on extra large-scale datasets is generally
required to achieve premier performance on relatively small datasets. In this
paper, we show that video masked autoencoders (VideoMAE) are data-efficient
learners for self-supervised video pre-training (SSVP). We are inspired by the
recent ImageMAE and propose customized video tube masking and reconstruction.
These simple designs turn out to be effective for overcoming information
leakage caused by the temporal correlation during video reconstruction. We
obtain three important findings on SSVP: (1) An extremely high proportion of
masking ratio (i.e., 90% to 95%) still yields favorable performance of
VideoMAE. The temporally redundant video content enables higher masking ratio
than that of images. (2) VideoMAE achieves impressive results on very small
datasets (i.e., around 3k-4k videos) without using any extra data. This is
partially ascribed to the challenging task of video reconstruction to enforce
high-level structure learning. (3) VideoMAE shows that data quality is more
important than data quantity for SSVP. Domain shift between pre-training and
target datasets are important issues in SSVP. Notably, our VideoMAE with the
vanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on
Something-Something V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any
extra data. Code will be released at https://github.com/MCG-NJU/VideoMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R3M: A Universal Visual Representation for Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how visual representations pre-trained on diverse human video data
can enable data-efficient learning of downstream robotic manipulation tasks.
Concretely, we pre-train a visual representation using the Ego4D human video
dataset using a combination of time-contrastive learning, video-language
alignment, and an L1 penalty to encourage sparse and compact representations.
The resulting representation, R3M, can be used as a frozen perception module
for downstream policy learning. Across a suite of 12 simulated robot
manipulation tasks, we find that R3M improves task success by over 20% compared
to training from scratch and by over 10% compared to state-of-the-art visual
representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika
Panda arm to learn a range of manipulation tasks in a real, cluttered apartment
given just 20 demonstrations. Code and pre-trained models are available at
https://tinyurl.com/robotr3m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuMan: Neural Human Radiance Field from a Single Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, Anurag Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your "Attention" Deserves Attention: A Self-Diversified Multi-Channel
  Attention for Facial Action Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Li, Zhihua Li, Huiyuan Yang, Geran Zhao, Lijun Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual attention has been extensively studied for learning fine-grained
features in both facial expression recognition (FER) and Action Unit (AU)
detection. A broad range of previous research has explored how to use attention
modules to localize detailed facial parts (e,g. facial action units), learn
discriminative features, and learn inter-class correlation. However, few
related works pay attention to the robustness of the attention module itself.
Through experiments, we found neural attention maps initialized with different
feature maps yield diverse representations when learning to attend the
identical Region of Interest (ROI). In other words, similar to general feature
learning, the representational quality of attention maps also greatly affects
the performance of a model, which means unconstrained attention learning has
lots of randomnesses. This uncertainty lets conventional attention learning
fall into sub-optimal. In this paper, we propose a compact model to enhance the
representational and focusing power of neural attention maps and learn the
"inter-attention" correlation for refined attention maps, which we term the
"Self-Diversified Multi-Channel Attention Network (SMA-Net)". The proposed
method is evaluated on two benchmark databases (BP4D and DISFA) for AU
detection and four databases (CK+, MMI, BU-3DFE, and BP4D+) for facial
expression recognition. It achieves superior performance compared to the
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynamicEarthNet: Daily Multi-Spectral Satellite <span class="highlight-title">Dataset</span> for Semantic
  Change Segmentation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aysim Toker, Lukas Kondmann, Mark Weber, Marvin Eisenberger, Andrés Camero, Jingliang Hu, Ariadna Pregel Hoderlein, Çağlar Şenaras, Timothy Davis, Daniel Cremers, Giovanni Marchisio, Xiao Xiang Zhu, Laura Leal-Taixé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth observation is a fundamental tool for monitoring the evolution of land
use in specific areas of interest. Observing and precisely defining change, in
this context, requires both time-series data and pixel-wise segmentations. To
that end, we propose the DynamicEarthNet dataset that consists of daily,
multi-spectral satellite observations of 75 selected areas of interest
distributed over the globe with imagery from Planet Labs. These observations
are paired with pixel-wise monthly semantic segmentation labels of 7 land use
and land cover (LULC) classes. DynamicEarthNet is the first dataset that
provides this unique combination of daily measurements and high-quality labels.
In our experiments, we compare several established baselines that either
utilize the daily observations as additional training data (semi-supervised
learning) or multiple observations at once (spatio-temporal learning) as a
point of reference for future research. Finally, we propose a new evaluation
metric SCS that addresses the specific challenges associated with time-series
semantic change segmentation. The data is available at:
https://mediatum.ub.tum.de/1650201.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022, evaluation webpage:
  https://codalab.lisn.upsaclay.fr/competitions/2882</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GriTS: Grid table similarity metric for table structure recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Smock, Rohith Pesala, Robin Abraham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new class of evaluation metric for table
structure recognition, grid table similarity (GriTS). Unlike prior metrics,
GriTS evaluates the correctness of a predicted table directly in its natural
form as a matrix. To create a similarity measure between matrices, we
generalize the two-dimensional largest common substructure (2D-LCS) problem,
which is NP-hard, to the 2D most similar substructures (2D-MSS) problem and
propose a polynomial-time heuristic for solving it. We validate empirically
using the PubTables-1M dataset that comparison between matrices exhibits more
desirable behavior than alternatives for table structure recognition
evaluation. GriTS also unifies all three subtasks of cell topology recognition,
cell location recognition, and cell content recognition within the same
framework, which simplifies the evaluation and enables more meaningful
comparisons across different types of structure recognition approaches. Code
will be released at https://github.com/microsoft/table-transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-label <span class="highlight-title">Transformer</span> for Action Unit Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauthier Tallec, Edouard Yvinec, Arnaud Dapogny, Kevin Bailly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiaised
computational face representations and has therefore aroused great interest in
the past few years. One of main obstacles toward building efficient deep
learning based AU detection system facial images database annotated by AU
experts. In that extent the ABAW challenge paves the way toward better AU
detection as it involves a ~2M frames AU annotated dataset. In this paper, we
present our submission to the ABAW3 challenge. In a nutshell, we applied a
multi-label detection transformer that leverage multi-head attention to learn
which part of the face image is the most relevant to predict each AU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refine-Net: Normal Refinement Neural Network for Noisy Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhou, Honghua Chen, Yingkui Zhang, Mingqiang Wei, Haoran Xie, Jun Wang, Tong Lu, Jing Qin, Xiao-Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point normal, as an intrinsic geometric property of 3D objects, not only
serves conventional geometric tasks such as surface consolidation and
reconstruction, but also facilitates cutting-edge learning-based techniques for
shape analysis and generation. In this paper, we propose a normal refinement
network, called Refine-Net, to predict accurate normals for noisy point clouds.
Traditional normal estimation wisdom heavily depends on priors such as surface
shapes or noise distributions, while learning-based solutions settle for single
types of hand-crafted features. Differently, our network is designed to refine
the initial normal of each point by extracting additional information from
multiple feature representations. To this end, several feature modules are
developed and incorporated into Refine-Net by a novel connection module.
Besides the overall network architecture of Refine-Net, we propose a new
multi-scale fitting patch selection scheme for the initial normal estimation,
by absorbing geometry domain knowledge. Also, Refine-Net is a generic normal
estimation framework: 1) point normals obtained from other methods can be
further refined, and 2) any feature module related to the surface geometric
structures can be potentially integrated into the framework. Qualitative and
quantitative evaluations demonstrate the clear superiority of Refine-Net over
the state-of-the-arts on both synthetic and real-scanned datasets. Our code is
available at https://github.com/hrzhou2/refinenet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CroMo: <span class="highlight-title">Cross-Modal</span> Learning for Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannick Verdié, Jifei Song, Barnabé Mas, Benjamin Busam, Aleš Leonardis, Steven McDonagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based depth estimation has witnessed recent progress in multiple
directions; from self-supervision using monocular video to supervised methods
offering highest accuracy. Complementary to supervision, further boosts to
performance and robustness are gained by combining information from multiple
signals. In this paper we systematically investigate key trade-offs associated
with sensor and modality design choices as well as related model training
strategies. Our study leads us to a new method, capable of connecting
modality-specific advantages from polarisation, Time-of-Flight and
structured-light inputs. We propose a novel pipeline capable of estimating
depth from monocular polarisation for which we evaluate various training
signals. The inversion of differentiable analytic models thereby connects scene
geometry with polarisation and ToF signals and enables self-supervised and
cross-modal learning. In the absence of existing multimodal datasets, we
examine our approach with a custom-made multi-modal camera rig and collect
CroMo; the first dataset to consist of synchronized stereo polarisation,
indirect ToF and structured-light depth, captured at video rates. Extensive
experiments on challenging video scenes confirm both qualitative and
quantitative pipeline advantages where we are able to outperform competitive
monocular depth estimation method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Framework to Reconstruct Face under Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gourango Modak, Shuvra Smaran Das, Md. Ajharul Islam Miraj, Md. Kishor Morol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning-based image reconstruction methods have shown significant
success in removing objects from pictures, they have yet to achieve acceptable
results for attributing consistency to gender, ethnicity, expression, and other
characteristics like the topological structure of the face. The purpose of this
work is to extract the mask region from a masked image and rebuild the area
that has been detected. This problem is complex because (i) it is difficult to
determine the gender of an image hidden behind a mask, which causes the network
to become confused and reconstruct the male face as a female or vice versa;
(ii) we may receive images from multiple angles, making it extremely difficult
to maintain the actual shape, topological structure of the face and a natural
image; and (iii) there are problems with various mask forms because, in some
cases, the area of the mask cannot be anticipated precisely; certain parts of
the mask remain on the face after completion. To solve this complex task, we
split the problem into three phases: landmark detection, object detection for
the targeted mask area, and inpainting the addressed mask region. To begin, to
solve the first problem, we have used gender classification, which detects the
actual gender behind a mask, then we detect the landmark of the masked facial
image. Second, we identified the non-face item, i.e., the mask, and used the
Mask R-CNN network to create the binary mask of the observed mask area.
Thirdly, we developed an inpainting network that uses anticipated landmarks to
create realistic images. To segment the mask, this article uses a mask R-CNN
and offers a binary segmentation map for identifying the mask area.
Additionally, we generated the image utilizing landmarks as structural guidance
through a GAN-based network. The studies presented in this paper use the FFHQ
and CelebA datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 9 figures, 2022 7th Conference on Data Science and Machine
  Learning Applications (CDMA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptively Re-weighting Multi-Loss Untrained <span class="highlight-title">Transformer</span> for Sparse-View
  Cone-Beam CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Wu, Yangdi Xu, Yingying Xu, Guangwei Wu, Qingqing Chen, Hongxiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cone-Beam Computed Tomography (CBCT) has been proven useful in diagnosis, but
how to shorten scanning time with lower radiation dosage and how to efficiently
reconstruct 3D image remain as the main issues for clinical practice. The
recent development of tomographic image reconstruction on sparse-view
measurements employs deep neural networks in a supervised way to tackle such
issues, whereas the success of model training requires quantity and quality of
the given paired measurements/images. We propose a novel untrained Transformer
to fit the CBCT inverse solver without training data. It is mainly comprised of
an untrained 3D Transformer of billions of network weights and a multi-level
loss function with variable weights. Unlike conventional deep neural networks
(DNNs), there is no requirement of training steps in our approach. Upon
observing the hardship of optimising Transformer, the variable weights within
the loss function are designed to automatically update together with the
iteration process, ultimately stabilising its optimisation. We evaluate the
proposed approach on two publicly available datasets: SPARE and Walnut. The
results show a significant performance improvement on image quality metrics
with streak artefact reduction in the visualisation. We also provide a clinical
report by an experienced radiologist to assess our reconstructed images in a
diagnosis point of view. The source code and the optimised models are available
from the corresponding author on request at the moment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Adapted Random Forest Vision (3DARFV) for Untangling
  Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency
  at the Utmost Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Alfarisi, Zeyar Aung, Qingfeng Huang, Ashraf Al-Khateeb, Hamed Alhashmi, Mohamed Abdelsalam, Salem Alzaabi, Haifa Alyazeedi, Anthony Tzes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planetary exploration depends heavily on 3D image data to characterize the
static and dynamic properties of the rock and environment. Analyzing 3D images
requires many computations, causing efficiency to suffer lengthy processing
time alongside large energy consumption. High-Performance Computing (HPC)
provides apparent efficiency at the expense of energy consumption. However, for
remote explorations, the conveyed surveillance and the robotized sensing need
faster data analysis with ultimate accuracy to make real-time decisions. In
such environments, access to HPC and energy is limited. Therefore, we realize
that reducing the number of computations to optimal and maintaining the desired
accuracy leads to higher efficiency. This paper demonstrates the semantic
segmentation capability of a probabilistic decision tree algorithm, 3D Adapted
Random Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at
the utmost accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation-Based Sampling for Pixel- to Image-Level Aggregation in
  Weakly-Supervised Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvi Jonnarth, Michael Felsberg, Yushan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification networks can be used to localize and segment objects in images
by means of class activation maps (CAMs). However, without pixel-level
annotations, they are known to (1) mainly focus on discriminative regions, and
(2) to produce diffuse CAMs without well-defined prediction contours. In this
work, we approach both problems with two contributions for improving CAM
learning. First, we incorporate importance sampling based on the class-wise
probability mass function induced by the CAMs to produce stochastic image-level
class predictions. This results in CAMs which activate over a larger extent of
the objects. Second, we formulate a feature similarity loss term which aims to
match the prediction contours with edges in the image. As a third contribution,
we conduct experiments on the PASCAL VOC and MS-COCO benchmark datasets to
demonstrate that these modifications significantly increase the performance in
terms of contour accuracy, while being comparable to current state-of-the-art
methods in terms of region similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MT-UDA: Towards Unsupervised <span class="highlight-title">Cross-modal</span>ity Medical Image Segmentation
  with Limited Source Labels <span class="chip">MICCAI 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Zhao, Kaixin Xu, Shumeng Li, Zeng Zeng, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep convolutional neural networks (DCNNs) benefits from high
volumes of annotated data. However, annotating medical images is laborious,
expensive, and requires human expertise, which induces the label scarcity
problem. Especially when encountering the domain shift, the problem becomes
more serious. Although deep unsupervised domain adaptation (UDA) can leverage
well-established source domain annotations and abundant target domain data to
facilitate cross-modality image segmentation and also mitigate the label
paucity problem on the target domain, the conventional UDA methods suffer from
severe performance degradation when source domain annotations are scarce. In
this paper, we explore a challenging UDA setting - limited source domain
annotations. We aim to investigate how to efficiently leverage unlabeled data
from the source and target domains with limited source annotations for
cross-modality image segmentation. To achieve this, we propose a new
label-efficient UDA framework, termed MT-UDA, in which the student model
trained with limited source labels learns from unlabeled data of both domains
by two teacher models respectively in a semi-supervised manner. More
specifically, the student model not only distills the intra-domain semantic
knowledge by encouraging prediction consistency but also exploits the
inter-domain anatomical information by enforcing structural consistency.
Consequently, the student model can effectively integrate the underlying
knowledge beneath available data resources to mitigate the impact of source
label scarcity and yield improved cross-modality segmentation performance. We
evaluate our method on MM-WHS 2017 dataset and demonstrate that our approach
outperforms the state-of-the-art methods by a large margin under the
source-label scarcity scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by MICCAI 2021, code at:
  https://github.com/jacobzhaoziyuan/MT-UDA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMEMO: Social Memory for Trajectory Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective modeling of human interactions is of utmost importance when
forecasting behaviors such as future trajectories. Each individual, with its
motion, influences surrounding agents since everyone obeys to social
non-written rules such as collision avoidance or group following. In this paper
we model such interactions, which constantly evolve through time, by looking at
the problem from an algorithmic point of view, i.e. as a data manipulation
task. We present a neural network based on an end-to-end trainable working
memory, which acts as an external storage where information about each agent
can be continuously written, updated and recalled. We show that our method is
capable of learning explainable cause-effect relationships between motions of
different agents, obtaining state-of-the-art results on multiple trajectory
forecasting datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Attention-based Method for Action Unit Detection at the 3rd ABAW
  Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy Le Hoai, Eunchae Lim, Eunbin Choi, Sieun Kim, Sudarshan Pant, Guee-Sang Lee, Soo-Huyng Kim, Hyung-Jeong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Action Coding System is an approach for modeling the complexity of
human emotional expression. Automatic action unit (AU) detection is a crucial
research area in human-computer interaction. This paper describes our
submission to the third Affective Behavior Analysis in-the-wild (ABAW)
competition 2022. We proposed a method for detecting facial action units in the
video. At the first stage, a lightweight CNN-based feature extractor is
employed to extract the feature map from each video frame. Then, an attention
module is applied to refine the attention map. The attention encoded vector is
derived using a weighted sum of the feature map and the attention scores later.
Finally, the sigmoid function is used at the output layer to make the
prediction suitable for multi-label AUs detection. We achieved a macro F1 score
of 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Caner Yüzügüler, Nikolaos Dimitriadis, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing resource utilization in target platforms is key to achieving high
performance during DNN inference. While optimizations have been proposed for
inference latency, memory footprint, and energy consumption, prior
hardware-aware neural architecture search (NAS) methods have omitted resource
utilization, preventing DNNs to take full advantage of the target inference
platforms. Modeling resource utilization efficiently and accurately is
challenging, especially for widely-used array-based inference accelerators such
as Google TPU. In this work, we propose a novel hardware-aware NAS framework
that does not only optimize for task accuracy and inference latency, but also
for resource utilization. We also propose and validate a new computational
model for resource utilization in inference accelerators. By using the proposed
NAS framework and the proposed resource utilization model, we achieve 2.8 - 4x
speedup for DNN inference compared to prior hardware-aware NAS methods while
attaining similar or improved accuracy in image classification on CIFAR-10 and
Imagenet-100 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the (Limited) Generalization of MasterFace Attacks and Its Relation
  to the Capacity of Face Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Terhörst, Florian Bierbaum, Marco Huber, Naser Damer, Florian Kirchbuchner, Kiran Raja, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A MasterFace is a face image that can successfully match against a large
portion of the population. Since their generation does not require access to
the information of the enrolled subjects, MasterFace attacks represent a
potential security risk for widely-used face recognition systems. Previous
works proposed methods for generating such images and demonstrated that these
attacks can strongly compromise face recognition. However, previous works
followed evaluation settings consisting of older recognition models, limited
cross-dataset and cross-model evaluations, and the use of low-scale testing
data. This makes it hard to state the generalizability of these attacks. In
this work, we comprehensively analyse the generalizability of MasterFace
attacks in empirical and theoretical investigations. The empirical
investigations include the use of six state-of-the-art FR models, cross-dataset
and cross-model evaluation protocols, and utilizing testing datasets of
significantly higher size and variance. The results indicate a low
generalizability when MasterFaces are training on a different face recognition
model than the one used for testing. In these cases, the attack performance is
similar to zero-effort imposter attacks. In the theoretical investigations, we
define and estimate the face capacity and the maximum MasterFace coverage under
the assumption that identities in the face space are well separated. The
current trend of increasing the fairness and generalizability in face
recognition indicates that the vulnerability of future systems might further
decrease. We conclude that MasterFaces should not be seen as a threat to face
recognition systems but, on the contrary, seen as a tool to understand and
enhance the robustness of face recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based <span class="highlight-title">Multimodal</span> Information Fusion for Facial Expression
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhang, Zhimeng Zhang, Feng Qiu, Suzhen Wang, Bowen Ma, Hao Zeng, Rudong An, Yu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression analysis has been a crucial research problem in the
computer vision area. With the recent development of deep learning techniques
and large-scale in-the-wild annotated datasets, facial expression analysis is
now aimed at challenges in real world settings. In this paper, we introduce our
submission to CVPR2022 Competition on Affective Behavior Analysis in-the-wild
(ABAW) that defines four competition tasks, including expression
classification, action unit detection, valence-arousal estimation, and a
multi-task-learning. The available multimodal information consist of spoken
words, speech prosody, and visual expression in videos. Our work proposes four
unified transformer-based network frameworks to create the fusion of the above
multimodal information. The preliminary results on the official Aff-Wild2
dataset are reported and demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper-Spectral Imaging for Overlapping Plastic Flakes Segmentation <span class="chip">ICIP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillem Martinez, Maya Aghaei, Martin Dijkstra, Bhalaji Nagarajan, Femke Jaarsma, Jaap van de Loosdrecht, Petia Radeva, Klaas Dijkstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the hyper-spectral imaging unique potentials in grasping the polymer
characteristics of different materials, it is commonly used in sorting
procedures. In a practical plastic sorting scenario, multiple plastic flakes
may overlap which depending on their characteristics, the overlap can be
reflected in their spectral signature. In this work, we use hyper-spectral
imaging for the segmentation of three types of plastic flakes and their
possible overlapping combinations. We propose an intuitive and simple
multi-label encoding approach, bitfield encoding, to account for the
overlapping regions. With our experiments, we show that the bitfield encoding
improves over the baseline single-label approach and we further demonstrate its
potential in predicting multiple labels for overlapping classes even when the
model is only trained with non-overlapping classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICIP2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Text Line Detection in Historical Documents: Learning and
  Evaluation Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélodie Boillet, Christopher Kermorvant, Thierry Paquet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text line segmentation is one of the key steps in historical document
understanding. It is challenging due to the variety of fonts, contents, writing
styles and the quality of documents that have degraded through the years.
  In this paper, we address the limitations that currently prevent people from
building line segmentation models with a high generalization capacity. We
present a study conducted using three state-of-the-art systems Doc-UFCN,
dhSegment and ARU-Net and show that it is possible to build generic models
trained on a wide variety of historical document datasets that can correctly
segment diverse unseen pages. This paper also highlights the importance of the
annotations used during training: each existing dataset is annotated
differently. We present a unification of the annotations and show its positive
impact on the final text recognition results. In this end, we present a
complete evaluation strategy using standard pixel-level metrics, object-level
ones and introducing goal-oriented metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazel Doughty, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to understand how actions are performed and identify subtle
differences, such as 'fold firmly' vs. 'fold gently'. To this end, we propose a
method which recognizes adverbs across different actions. However, such
fine-grained annotations are difficult to obtain and their long-tailed nature
makes it challenging to recognize adverbs in rare action-adverb compositions.
Our approach therefore uses semi-supervised learning with multiple adverb
pseudo-labels to leverage videos with only action labels. Combined with
adaptive thresholding of these pseudo-adverbs we are able to make efficient use
of the available data while tackling the long-tailed distribution.
Additionally, we gather adverb annotations for three existing video retrieval
datasets, which allows us to introduce the new tasks of recognizing adverbs in
unseen action-adverb compositions and unseen domains. Experiments demonstrate
the effectiveness of our method, which outperforms prior work in recognizing
adverbs and semi-supervised works adapted for adverb recognition. We also show
how adverbs can relate fine-grained actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Semi-Supervised Deep Facial Expression Recognition with An
  Adaptive Confidence Margin <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Li, Nannan Wang, Xi Yang, Xiaoyu Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Only parts of unlabeled data are selected to train models for most
semi-supervised learning methods, whose confidence scores are usually higher
than the pre-defined threshold (i.e., the confidence margin). We argue that the
recognition performance should be further improved by making full use of all
unlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM)
to fully leverage all unlabeled data for semi-supervised deep facial expression
recognition. All unlabeled samples are partitioned into two subsets by
comparing their confidence scores with the adaptively learned confidence margin
at each training epoch: (1) subset I including samples whose confidence scores
are no lower than the margin; (2) subset II including samples whose confidence
scores are lower than the margin. For samples in subset I, we constrain their
predictions to match pseudo labels. Meanwhile, samples in subset II participate
in the feature-level contrastive objective to learn effective facial expression
features. We extensively evaluate Ada-CM on four challenging datasets, showing
that our method achieves state-of-the-art performance, especially surpassing
fully-supervised baselines in a semi-supervised manner. Ablation study further
proves the effectiveness of our method. The source code is available at
https://github.com/hangyu94/Ada-CM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Object Detection for Streaming Perception <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinrong Yang, Songtao Liu, Zeming Li, Xiaoping Li, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving requires the model to perceive the environment and (re)act
within a low latency for safety. While past works ignore the inevitable changes
in the environment after processing, streaming perception is proposed to
jointly evaluate the latency and accuracy into a single metric for video online
perception. In this paper, instead of searching trade-offs between accuracy and
speed like previous works, we point out that endowing real-time models with the
ability to predict the future is the key to dealing with this problem. We build
a simple and effective framework for streaming perception. It equips a novel
DualFlow Perception module (DFP), which includes dynamic and static flows to
capture the moving trend and basic detection feature for streaming prediction.
Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to
generate adaptive weights for objects with different moving speeds. Our simple
method achieves competitive performance on Argoverse-HD dataset and improves
the AP by 4.9% compared to the strong baseline, validating its effectiveness.
Our code will be made available at https://github.com/yancie-yjr/StreamYOLO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Binary Morphological Neural Network <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Aouad, Hugues Talbot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last ten years, Convolutional Neural Networks (CNNs) have formed the
basis of deep-learning architectures for most computer vision tasks. However,
they are not necessarily optimal. For example, mathematical morphology is known
to be better suited to deal with binary images. In this work, we create a
morphological neural network that handles binary inputs and outputs. We propose
their construction inspired by CNNs to formulate layers adapted to such images
by replacing convolutions with erosions and dilations. We give explainable
theoretical results on whether or not the resulting learned networks are indeed
morphological operators. We present promising experimental results designed to
learn basic binary operators, and we have made our code publicly available
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a submission to ICIP 2022. 7 pages. 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DR.VIC: Decomposition and Reasoning for Video Individual Counting <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Han, Lei Bai, Junyu Gao, Qi Wang, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian counting is a fundamental tool for understanding pedestrian
patterns and crowd flow analysis. Existing works (e.g., image-level pedestrian
counting, crossline crowd counting et al.) either only focus on the image-level
counting or are constrained to the manual annotation of lines. In this work, we
propose to conduct the pedestrian counting from a new perspective - Video
Individual Counting (VIC), which counts the total number of individual
pedestrians in the given video (a person is only counted once). Instead of
relying on the Multiple Object Tracking (MOT) techniques, we propose to solve
the problem by decomposing all pedestrians into the initial pedestrians who
existed in the first frame and the new pedestrians with separate identities in
each following frame. Then, an end-to-end Decomposition and Reasoning Network
(DRNet) is designed to predict the initial pedestrian count with the density
estimation method and reason the new pedestrian's count of each frame with the
differentiable optimal transport. Extensive experiments are conducted on two
datasets with congested pedestrians and diverse scenes, demonstrating the
effectiveness of our method over baselines with great superiority in counting
the individual pedestrians. Code: https://github.com/taohan10200/DRNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022. [camera ready with supplement]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autofocus for Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lin, Yinqiang Zhang, Lei Yu, Bin Zhou, Xiaowei Luo, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focus control (FC) is crucial for cameras to capture sharp images in
challenging real-world scenarios. The autofocus (AF) facilitates the FC by
automatically adjusting the focus settings. However, due to the lack of
effective AF methods for the recently introduced event cameras, their FC still
relies on naive AF like manual focus adjustments, leading to poor adaptation in
challenging real-world conditions. In particular, the inherent differences
between event and frame data in terms of sensing modality, noise, temporal
resolutions, etc., bring many challenges in designing an effective AF method
for event cameras. To address these challenges, we develop a novel event-based
autofocus framework consisting of an event-specific focus measure called event
rate (ER) and a robust search strategy called event-based golden search (EGS).
To verify the performance of our method, we have collected an event-based
autofocus dataset (EAD) containing well-synchronized frames, events, and focal
positions in a wide variety of challenging scenes with severe lighting and
motion conditions. The experiments on this dataset and additional real-world
scenarios demonstrated the superiority of our method over state-of-the-art
approaches in terms of efficiency and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> HDR Imaging from Motion and Exposure Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Nazarczuk, Sibi Catley-Chandar, Ales Leonardis, Eduardo Pérez Pellitero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent High Dynamic Range (HDR) techniques extend the capabilities of current
cameras where scenes with a wide range of illumination can not be accurately
captured with a single low-dynamic-range (LDR) image. This is generally
accomplished by capturing several LDR images with varying exposure values whose
information is then incorporated into a merged HDR image. While such approaches
work well for static scenes, dynamic scenes pose several challenges, mostly
related to the difficulty of finding reliable pixel correspondences.
Data-driven approaches tackle the problem by learning an end-to-end mapping
with paired LDR-HDR training data, but in practice generating such HDR
ground-truth labels for dynamic scenes is time-consuming and requires complex
procedures that assume control of certain dynamic elements of the scene (e.g.
actor pose) and repeatable lighting conditions (stop-motion capturing). In this
work, we propose a novel self-supervised approach for learnable HDR estimation
that alleviates the need for HDR ground-truth labels. We propose to leverage
the internal statistics of LDR images to create HDR pseudo-labels. We
separately exploit static and well-exposed parts of the input images, which in
conjunction with synthetic illumination clipping and motion augmentation
provide high quality training examples. Experimental results show that the HDR
models trained using our proposed self-supervision approach achieve performance
competitive with those trained under full supervision, and are to a large
extent superior to previous methods that equally do not require any
supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Generalized Textured Surface Anomaly Detection <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Fu Chen, Yu-Min Liu, Chia-Ching Lin, Trista Pei-Chun Chen, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection aims to identify abnormal data that deviates from the
normal ones, while typically requiring a sufficient amount of normal data to
train the model for performing this task. Despite the success of recent anomaly
detection methods, performing anomaly detection in an unseen domain remain a
challenging task. In this paper, we address the task of domain-generalized
textured surface anomaly detection. By observing normal and abnormal surface
data across multiple source domains, our model is expected to be generalized to
an unseen textured surface of interest, in which only a small number of normal
data can be observed during testing. Although with only image-level labels
observed in the training data, our patch-based meta-learning model exhibits
promising generalization ability: not only can it generalize to unseen image
domains, but it can also localize abnormal regions in the query image. Our
experiments verify that our model performs favorably against state-of-the-art
anomaly detection and domain generalization approaches in various settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE International Conference on Multimedia and Expo
  (ICME) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lane detection with Position Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Xie, Jiacheng Han, Dezhen Qi, Feng Chen, Kaer Huang, Jianwei Shuai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, lane detection has made great progress in autonomous driving. RESA
(REcurrent Feature-Shift Aggregator) is based on image segmentation. It
presents a novel module to enrich lane feature after preliminary feature
extraction with an ordinary CNN. For Tusimple dataset, there is not too
complicated scene and lane has more prominent spatial features. On the basis of
RESA, we introduce the method of position embedding to enhance the spatial
features. The experimental results show that this method has achieved the best
accuracy 96.93% on Tusimple dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cell segmentation from telecentric bright-field transmitted light
  microscopic images using a Residual Attention U-Net: a case study on HeLa
  line 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ghaznavi, Renata Rychtarikova, Mohammadmehdi Saberioon, Dalibor Stys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Living cell segmentation from bright-field light microscopic images is
challenging due to the image complexity and temporal changes in the living
cells. Recently developed deep learning (DL)-based methods became popular in
medical and microscopic image segmentation tasks due to their success and
promising outcomes. The main objective of this paper is to develop a deep
learning, UNet-based method to segment the living cells of the HeLa line in
bright-field transmitted light microscopy. To find the most suitable
architecture for our datasets, we have proposed a residual attention U-Net and
compared it with an attention and a simple U-Net architecture. The attention
mechanism highlights the remarkable features and suppresses activations in the
irrelevant image regions. The residual mechanism overcomes with vanishing
gradient problem. The Mean-IoU score for our datasets reaches 0.9505, 0.9524,
and 0.9530 for the simple, attention, and residual attention U-Net,
respectively. We achieved the most accurate semantic segmentation results in
the Mean-IoU and Dice metrics by applying the residual and attention mechanisms
together. The watershed method applied to this best - Residual Attention -
semantic segmentation result gave the segmentation with the specific
information for each cell.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAN: a Segmentation-free Document Attention Network for Handwritten
  Document Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Coquenet, Clément Chatelain, Thierry Paquet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unconstrained handwritten document recognition is a challenging computer
vision task. It is traditionally handled by a two-step approach combining line
segmentation followed by text line recognition. For the first time, we propose
an end-to-end segmentation-free architecture for the task of handwritten
document recognition: the Document Attention Network. In addition to the text
recognition, the model is trained to label text parts using begin and end tags
in an XML-like fashion. This model is made up of an FCN encoder for feature
extraction and a stack of transformer decoder layers for a recurrent
token-by-token prediction process. It takes whole text documents as input and
sequentially outputs characters, as well as logical layout tokens. Contrary to
the existing segmentation-based approaches, the model is trained without using
any segmentation label. We achieve competitive results on the READ dataset at
page level, as well as double-page level with a CER of 3.53% and 3.69%,
respectively. We also provide results for the RIMES dataset at page level,
reaching 4.54% of CER.
  We provide all source code and pre-trained model weights at
https://github.com/FactoDeepLearning/DAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-Based Dense Reconstruction Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Xiao, Guohui Wang, Yi Chen, Jinghong Nan, Yongfeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are a new type of sensors that are different from traditional
cameras. Each pixel is triggered asynchronously by event. The trigger event is
the change of the brightness irradiated on the pixel. If the increment or
decrement of brightness is higher than a certain threshold, an event is output.
Compared with traditional cameras, event cameras have the advantages of high
dynamic range and no motion blur. Since events are caused by the apparent
motion of intensity edges, the majority of 3D reconstructed maps consist only
of scene edges, i.e., semi-dense maps, which is not enough for some
applications. In this paper, we propose a pipeline to realize event-based dense
reconstruction. First, deep learning is used to reconstruct intensity images
from events. And then, structure from motion (SfM) is used to estimate camera
intrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)
is used to complete dense reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ev-TTA: <span class="highlight-title">Test</span>-Time Adaptation for Event-Based Object Recognition <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Kim, Inwoo Hwang, Young Min Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Ev-TTA, a simple, effective test-time adaptation algorithm for
event-based object recognition. While event cameras are proposed to provide
measurements of scenes with fast motions or drastic illumination changes, many
existing event-based recognition algorithms suffer from performance
deterioration under extreme conditions due to significant domain shifts. Ev-TTA
mitigates the severe domain gaps by fine-tuning the pre-trained classifiers
during the test phase using loss functions inspired by the spatio-temporal
characteristics of events. Since the event data is a temporal stream of
measurements, our loss function enforces similar predictions for adjacent
events to quickly adapt to the changed environment online. Also, we utilize the
spatial correlations between two polarities of events to handle noise under
extreme illumination, where different polarities of events exhibit distinctive
noise distributions. Ev-TTA demonstrates a large amount of performance gain on
a wide range of event-based object recognition tasks without extensive
additional training. Our formulation can be successfully applied regardless of
input representations and further extended into regression tasks. We expect
Ev-TTA to provide the key technique to deploy event-based vision algorithms in
challenging real-world applications where significant domain shift is
inevitable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Scale-Equivalent <span class="highlight-title">Distillation</span> for Semi-Supervised Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushan Guo, Yao Mu, Jianyu Chen, Tianqi Wang, Yi<span class="highlight-author">zhou Yu</span>, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on
self-training, i.e., generating hard pseudo-labels by a teacher model on
unlabeled data as supervisory signals. Although they achieved certain success,
the limited labeled data in semi-supervised learning scales up the challenges
of object detection. We analyze the challenges these methods meet with the
empirical experiment results. We find that the massive False Negative samples
and inferior localization precision lack consideration. Besides, the large
variance of object sizes and class imbalance (i.e., the extreme ratio between
background and object) hinder the performance of prior arts. Further, we
overcome these challenges by introducing a novel approach, Scale-Equivalent
Distillation (SED), which is a simple yet effective end-to-end knowledge
distillation framework robust to large object size variance and class
imbalance. SED has several appealing benefits compared to the previous works.
(1) SED imposes a consistency regularization to handle the large scale variance
problem. (2) SED alleviates the noise problem from the False Negative samples
and inferior localization precision. (3) A re-weighting strategy can implicitly
screen the potential foreground regions of the unlabeled data to reduce the
effect of class imbalance. Extensive experiments show that SED consistently
outperforms the recent state-of-the-art methods on different datasets with
significant margins. For example, it surpasses the supervised counterpart by
more than 10 mAP when using 5% and 10% labeled data on MS-COCO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Method of Data Augmentation to Train a Small Area Fingerprint
  Recognition Deep Neural Network with a Normal Fingerprint Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JuSong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fingerprints are popular among the biometric based systems due to ease of
acquisition, uniqueness and availability. Nowadays it is used in smart phone
security, digital payment and digital locker. The traditional fingerprint
matching methods based on minutiae are mainly applicable for large-area
fingerprint and the accuracy rate would reduce significantly when dealing with
small-area fingerprint from smart phone. There are many attempts to using deep
learning for small-area fingerprint recognition, and there are many successes.
But training deep neural network needs a lot of datasets for training. There is
no well-known dataset for small-area, so we have to make datasets ourselves. In
this paper, we propose a method of data augmentation to train a small-area
fingerprint recognition deep neural network with a normal fingerprint database
(such as FVC2002) and verify it via tests. The experimental results showed the
efficiency of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Characteristic Learning Method with Micro-Doppler Signatures for
  Pedestrian Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xiang, Yu Huang, Haodong Xu, Guangbo Zhang, Wenyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of pedestrians using radar micro-Doppler signatures has
become a hot topic in recent years. In this paper, we propose a
multi-characteristic learning (MCL) model with clusters to jointly learn
discrepant pedestrian micro-Doppler signatures and fuse the knowledge learned
from each cluster into final decisions. Time-Doppler spectrogram (TDS) and
signal statistical features extracted from FMCW radar, as two categories of
micro-Doppler signatures, are used in MCL to learn the micro-motion information
inside pedestrians' free walking patterns. The experimental results show that
our model achieves a higher accuracy rate and is more stable for pedestrian
identification than other studies, which make our model more practical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Selection by Clustering for <span class="highlight-title">Contrastive Learning</span> in Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqiang Wang, Tao Zhu, Liming Chen, Huansheng Ning, Yaping Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has been applied to Human Activity Recognition (HAR)
based on sensor data owing to its ability to achieve performance comparable to
supervised learning with a large amount of unlabeled data and a small amount of
labeled data. The pre-training task for contrastive learning is generally
instance discrimination, which specifies that each instance belongs to a single
class, but this will consider the same class of samples as negative examples.
Such a pre-training task is not conducive to human activity recognition tasks,
which are mainly classification tasks. To address this problem, we follow
SimCLR to propose a new contrastive learning framework that negative selection
by clustering in HAR, which is called ClusterCLHAR. Compared with SimCLR, it
redefines the negative pairs in the contrastive loss function by using
unsupervised clustering methods to generate soft labels that mask other samples
of the same cluster to avoid regarding them as negative samples. We evaluate
ClusterCLHAR on three benchmark datasets, USC-HAD, MotionSense, and UCI-HAR,
using mean F1-score as the evaluation metric. The experiment results show that
it outperforms all the state-of-the-art methods applied to HAR in
self-supervised learning and semi-supervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Object Detection via Knowledge Inheritance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Yang, Chi Zhang, Ruibo Li, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection (FSOD), which aims at learning a generic detector
that can adapt to unseen tasks with scarce training samples, has witnessed
consistent improvement recently. However, most existing methods ignore the
efficiency issues, e.g., high computational complexity and slow adaptation
speed. Notably, efficiency has become an increasingly important evaluation
metric for few-shot techniques due to an emerging trend toward embedded AI. To
this end, we present an efficient pretrain-transfer framework (PTF) baseline
with no computational increment, which achieves comparable results with
previous state-of-the-art (SOTA) methods. Upon this baseline, we devise an
initializer named knowledge inheritance (KI) to reliably initialize the novel
weights for the box classifier, which effectively facilitates the knowledge
transfer process and boosts the adaptation speed. Within the KI initializer, we
propose an adaptive length re-scaling (ALR) strategy to alleviate the vector
length inconsistency between the predicted novel weights and the pretrained
base weights. Finally, our approach not only achieves the SOTA results across
three public benchmarks, i.e., PASCAL VOC, COCO and LVIS, but also exhibits
high efficiency with 1.8-9.0x faster adaptation speed against the other methods
on COCO/LVIS benchmark during few-shot transfer. To our best knowledge, this is
the first work to consider the efficiency problem in FSOD. We hope to motivate
a trend toward powerful yet efficient few-shot technique development. The codes
are publicly available at https://github.com/Ze-Yang/Efficient-FSOD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-free <span class="highlight-title">Transformer</span> Architecture Search <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Xing Sun, Yonghong Tian, Jie Chen, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Vision Transformer (ViT) has achieved remarkable success in several
computer vision tasks. The progresses are highly relevant to the architecture
design, then it is worthwhile to propose Transformer Architecture Search (TAS)
to search for better ViTs automatically. However, current TAS methods are
time-consuming and existing zero-cost proxies in CNN do not generalize well to
the ViT search space according to our experimental observations. In this paper,
for the first time, we investigate how to conduct TAS in a training-free manner
and devise an effective training-free TAS (TF-TAS) scheme. Firstly, we observe
that the properties of multi-head self-attention (MSA) and multi-layer
perceptron (MLP) in ViTs are quite different and that the synaptic diversity of
MSA affects the performance notably. Secondly, based on the observation, we
devise a modular strategy in TF-TAS that evaluates and ranks ViT architectures
from two theoretical perspectives: synaptic diversity and synaptic saliency,
termed as DSS-indicator. With DSS-indicator, evaluation results are strongly
correlated with the test accuracies of ViT models. Experimental results
demonstrate that our TF-TAS achieves a competitive performance against the
state-of-the-art manually or automatically design ViT architectures, and it
promotes the searching efficiency in ViT search space greatly: from about $24$
GPU days to less than $0.5$ GPU days. Moreover, the proposed DSS-indicator
outperforms the existing cutting-edge zero-cost approaches (e.g., TE-score and
NASWOT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Driven Deep Learning for Computational Magnetic Resonance
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerstin Hammernik, Thomas Küstner, Burhaneddin Yaman, Zhengnan Huang, Daniel Rueckert, Florian Knoll, Mehmet Akçakaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-driven deep learning methods have emerged as a powerful tool for
computational magnetic resonance imaging (MRI) problems, pushing reconstruction
performance to new limits. This article provides an overview of the recent
developments in incorporating physics information into learning-based MRI
reconstruction. We consider inverse problems with both linear and non-linear
forward models for computational MRI, and review the classical approaches for
solving these. We then focus on physics-driven deep learning approaches,
covering physics-driven loss functions, plug-and-play methods, generative
models, and unrolled networks. We highlight domain-specific challenges such as
real- and complex-valued building blocks of neural networks, and translational
applications in MRI with linear and non-linear forward models. Finally, we
discuss common issues and open challenges, and draw connections to the
importance of physics-driven learning when combined with other downstream tasks
in the medical imaging pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Learning of Adversarial Example: Towards Good
  Generalizations for Deepfake Detection <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, Jue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies in deepfake detection have yielded promising results when the
training and testing face forgeries are from the same dataset. However, the
problem remains challenging when one tries to generalize the detector to
forgeries created by unseen methods in the training dataset. This work
addresses the generalizable deepfake detection from a simple principle: a
generalizable representation should be sensitive to diverse types of forgeries.
Following this principle, we propose to enrich the "diversity" of forgeries by
synthesizing augmented forgeries with a pool of forgery configurations and
strengthen the "sensitivity" to the forgeries by enforcing the model to predict
the forgery configurations. To effectively explore the large forgery
augmentation space, we further propose to use the adversarial training strategy
to dynamically synthesize the most challenging forgeries to the current model.
Through extensive experiments, we show that the proposed strategies are
surprisingly effective (see Figure 1), and they could achieve superior
performance than the current state-of-the-art methods. Code is available at
\url{https://github.com/liangchen527/SLADD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Prediction of Lung Squamous Cell Carcinoma Recurrence With
  <span class="highlight-title">Self-supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicheng Zhu, Carlos Fernandez-Granda, Narges Razavian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung squamous cell carcinoma (LSCC) has a high recurrence and metastasis
rate. Factors influencing recurrence and metastasis are currently unknown and
there are no distinct histopathological or morphological features indicating
the risks of recurrence and metastasis in LSCC. Our study focuses on the
recurrence prediction of LSCC based on H&E-stained histopathological
whole-slide images (WSI). Due to the small size of LSCC cohorts in terms of
patients with available recurrence information, standard end-to-end learning
with various convolutional neural networks for this task tends to overfit.
Also, the predictions made by these models are hard to interpret.
Histopathology WSIs are typically very large and are therefore processed as a
set of smaller tiles. In this work, we propose a novel conditional
self-supervised learning (SSL) method to learn representations of WSI at the
tile level first, and leverage clustering algorithms to identify the tiles with
similar histopathological representations. The resulting representations and
clusters from self-supervision are used as features of a survival model for
recurrence prediction at the patient level. Using two publicly available
datasets from TCGA and CPTAC, we show that our LSCC recurrence prediction
survival model outperforms both LSCC pathological stage-based approach and
machine learning baselines such as multiple instance learning. The proposed
method also enables us to explain the recurrence histopathological risk factors
via the derived clusters. This can help pathologists derive new hypotheses
regarding morphological features associated with LSCC recurrence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Frequency Filtering for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Lin, Zhizheng Zhang, Zhipeng Huang, Yan Lu, Cuiling Lan, Peng Chu, Quanzeng You, Jiang Wang, Zicheng Liu, Amey Parulkar, Viraj Navkal, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the generalization capability of Deep Neural Networks (DNNs) is
critical for their practical uses, which has been a longstanding challenge.
Some theoretical studies have revealed that DNNs have preferences to different
frequency components in the learning process and indicated that this may affect
the robustness of learned features. In this paper, we propose Deep Frequency
Filtering (DFF) for learning domain-generalizable features, which is the first
endeavour to explicitly modulate frequency components of different transfer
difficulties across domains during training. To achieve this, we perform Fast
Fourier Transform (FFT) on feature maps at different layers, then adopt a
light-weight module to learn the attention masks from frequency representations
after FFT to enhance transferable frequency components while suppressing the
components not conductive to generalization. Further, we empirically compare
different types of attention for implementing our conceptualized DFF. Extensive
experiments demonstrate the effectiveness of the proposed DFF and show that
applying DFF on a plain baseline outperforms the state-of-the-art methods on
different domain generalization tasks, including close-set classification and
open-set retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biceph-Net: A robust and lightweight framework for the diagnosis of
  Alzheimer's disease using 2D-MRI scans and deep similarity learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. H. Rashid, A. Gupta, J. Gupta, M. Tanveer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the
significant causes of death in the elderly population. Many deep learning
techniques have been proposed to diagnose AD using Magnetic Resonance Imaging
(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is
challenging as the inter-slice information gets lost. To this end, we propose a
novel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D
MRI scans that model both the intra-slice and inter-slice information.
Biceph-Net has been experimentally shown to perform similar to other
Spatio-temporal neural networks while being computationally more efficient.
Biceph-Net is also superior in performance compared to vanilla 2D convolutional
neural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has
an inbuilt neighbourhood-based model interpretation feature that can be
exploited to understand the classification decision taken by the network.
Biceph-Net experimentally achieves a test accuracy of 100% in the
classification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive
Impairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Robust Scene Flow Estimation via the Alignment of
  Probability Density Functions <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan He, Patrick Emami, Sanjay Ranka, Anand Rangarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new self-supervised scene flow estimation
approach for a pair of consecutive point clouds. The key idea of our approach
is to represent discrete point clouds as continuous probability density
functions using Gaussian mixture models. Scene flow estimation is therefore
converted into the problem of recovering motion from the alignment of
probability density functions, which we achieve using a closed-form expression
of the classic Cauchy-Schwarz divergence. Unlike existing
nearest-neighbor-based approaches that use hard pairwise correspondences, our
proposed approach establishes soft and implicit point correspondences between
point clouds and generates more robust and accurate scene flow in the presence
of missing correspondences and outliers. Comprehensive experiments show that
our method makes noticeable gains over the Chamfer Distance and the Earth
Mover's Distance in real-world environments and achieves state-of-the-art
performance among self-supervised learning methods on FlyingThings3D and KITTI,
even outperforming some supervised methods with ground truth annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI Conference on Artificial Intelligence (2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Censor by Noisy Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Chopra, Abhinav Java, Abhishek Singh, Vivek Sharma, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point clouds are an increasingly ubiquitous input modality and the raw signal
can be efficiently processed with recent progress in deep learning. This signal
may, often inadvertently, capture sensitive information that can leak semantic
and geometric properties of the scene which the data owner does not want to
share. The goal of this work is to protect sensitive information when learning
from point clouds; by censoring the sensitive information before the point
cloud is released for downstream tasks. Specifically, we focus on preserving
utility for perception tasks while mitigating attribute leakage attacks. The
key motivating insight is to leverage the localized saliency of perception
tasks on point clouds to provide good privacy-utility trade-offs. We realize
this through a mechanism called Censoring by Noisy Sampling (CBNS), which is
composed of two modules: i) Invariant Sampler: a differentiable point-cloud
sampler which learns to remove points invariant to utility and ii) Noisy
Distorter: which learns to distort sampled points to decouple the sensitive
information from utility, and mitigate privacy leakage. We validate the
effectiveness of CBNS through extensive comparisons with state-of-the-art
baselines and sensitivity analyses of key design choices. Results show that
CBNS achieves superior privacy-utility trade-offs on multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Motion Deblurring and Frame Interpolation with Events <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Slow shutter speed and long exposure time of frame-based cameras often cause
visual blur and loss of inter-frame information, degenerating the overall
quality of captured videos. To this end, we present a unified framework of
event-based motion deblurring and frame interpolation for blurry video
enhancement, where the extremely low latency of events is leveraged to
alleviate motion blur and facilitate intermediate frame prediction.
Specifically, the mapping relation between blurry frames and sharp latent
images is first predicted by a learnable double integral network, and a fusion
network is then proposed to refine the coarse results via utilizing the
information from consecutive blurry inputs and the concurrent events. By
exploring the mutual constraints among blurry frames, latent images, and event
streams, we further propose a self-supervised learning framework to enable
network training with real-world blurry videos and events. Extensive
experiments demonstrate that our method compares favorably against the
state-of-the-art approaches and achieves remarkable performance on both
synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Transformer</span>s for Robust Few-shot Cross-domain Face
  Anti-spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsin-Ping Huang, Deqing Sun, Yaojie Liu, Wen-Sheng Chu, Taihong Xiao, Jinwei Yuan, Hartwig Adam, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent face anti-spoofing methods perform well under the intra-domain
setups, an effective approach needs to account for much larger appearance
variations of images acquired in complex scenes with different sensors for
robust performance. In this paper, we present adaptive vision transformers
(ViT) for robust cross-domain face anti-spoofing. Specifically, we adopt ViT as
a backbone to exploit its strength to account for long-range dependencies among
pixels. We further introduce the ensemble adapters module and feature-wise
transformation layers in the ViT to adapt to different domains for robust
performance with a few samples. Experiments on several benchmark datasets show
that the proposed models achieve both robust and competitive performance
against the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image
  Segmentation using Partially Labeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruining Deng, Quan Liu, Can Cui, Zuhayr Asad, Haichun Yang, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-assisted quantitative analysis on Giga-pixel pathology images has
provided a new avenue in histology examination. The innovations have been
largely focused on cancer pathology (i.e., tumor segmentation and
characterization). In non-cancer pathology, the learning algorithms can be
asked to examine more comprehensive tissue types simultaneously, as a
multi-label setting. The prior arts typically needed to train multiple
segmentation networks in order to match the domain-specific knowledge for
heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal
tubular, distal tubular, peritubular capillaries, and arteries). In this paper,
we propose a dynamic single segmentation network (Omni-Seg) that learns to
segment multiple tissue types using partially labeled images (i.e., only one
tissue type is labeled for each training image) for renal pathology. By
learning from ~150,000 patch-wise pathological images from six tissue types,
the proposed Omni-Seg network achieved superior segmentation accuracy and less
resource consumption when compared to the previous the multiple-network and
multi-head design. In the testing stage, the proposed method obtains
"completely labeled" tissue segmentation results using only "partially labeled"
training images. The source code is available at
https://github.com/ddrrnn123/Omni-Seg
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Constrained Least Squares for Blind Image Super-Resolution <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Luo, Haibin Huang, Lei Yu, Youwei Li, Haoqiang Fan, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of blind image super-resolution(SR) with
a reformulated degradation model and two novel modules. Following the common
practices of blind SR, our method proposes to improve both the kernel
estimation as well as the kernel-based high-resolution image restoration. To be
more specific, we first reformulate the degradation model such that the
deblurring kernel estimation can be transferred into the low-resolution space.
On top of this, we introduce a dynamic deep linear filter module. Instead of
learning a fixed kernel for all images, it can adaptively generate deblurring
kernel weights conditional on the input and yield a more robust kernel
estimation. Subsequently, a deep constrained least square filtering module is
applied to generate clean features based on the reformulation and estimated
kernel. The deblurred feature and the low input image feature are then fed into
a dual-path structured SR network and restore the final high-resolution result.
To evaluate our method, we further conduct evaluations on several benchmarks,
including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed
method achieves better accuracy and visual improvements against
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImpliCity: City Modeling from Satellite Images with Deep Implicit
  Occupancy Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Corinne Stucker, Bingxin Ke, Yuanwen Yue, Shengyu Huang, Iro Armeni, Konrad Schindler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution optical satellite sensors, combined with dense stereo
algorithms, have made it possible to reconstruct 3D city models from space.
However, these models are, in practice, rather noisy and tend to miss small
geometric features that are clearly visible in the images. We argue that one
reason for the limited quality may be a too early, heuristic reduction of the
triangulated 3D point cloud to an explicit height field or surface mesh. To
make full use of the point cloud and the underlying images, we introduce
ImpliCity, a neural representation of the 3D scene as an implicit, continuous
occupancy field, driven by learned embeddings of the point cloud and a stereo
pair of ortho-photos. We show that this representation enables the extraction
of high-quality DSMs: with image resolution 0.5$\,$m, ImpliCity reaches a
median height error of $\approx\,$0.7$\,$m and outperforms competing methods,
especially w.r.t. building reconstruction, featuring intricate roof details,
smooth surfaces, and straight, regular outlines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the International Annals of the
  Photogrammetry, Remote Sensing and Spatial Information Sciences (camera-ready
  version + supplementary material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Behaviour of Vision <span class="highlight-title">Transformer</span>s with Token-consistent
  Stochastic Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Popovic, Danda Pani Paudel, Thomas Probst, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce token-consistent stochastic layers in vision transformers,
without causing any severe drop in performance. The added stochasticity
improves network calibration, robustness and strengthens privacy. We use linear
layers with token-consistent stochastic parameters inside the multilayer
perceptron blocks, without altering the architecture of the transformer. The
stochastic parameters are sampled from the uniform distribution, both during
training and inference. The applied linear operations preserve the topological
structure, formed by the set of tokens passing through the shared multilayer
perceptron. This operation encourages the learning of the recognition task to
rely on the topological structures of the tokens, instead of their values,
which in turn offers the desired robustness and privacy of the visual features.
The effectiveness of the token-consistent stochasticity is demonstrated on
three different applications, namely, network calibration, adversarial
robustness, and feature privacy, by boosting the performance of the respective
established baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static
  Models by Fitting Feature-level Space-time Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR 2022 (Source Code:
  https://github.com/jx-zhong-for-academic-purpose/Kinet )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring Restaurant Styles by Mining Crowd Sourced Photos from
  User-<span class="highlight-title">Review</span> Websites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1611.06301v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1611.06301v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofu Liao, Yuncheng Li, Tianran Hu, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When looking for a restaurant online, user uploaded photos often give people
an immediate and tangible impression about a restaurant. Due to their
informativeness, such user contributed photos are leveraged by restaurant
review websites to provide their users an intuitive and effective search
experience. In this paper, we present a novel approach to inferring restaurant
types or styles (ambiance, dish styles, suitability for different occasions)
from user uploaded photos on user-review websites. To that end, we first
collect a novel restaurant photo dataset associating the user contributed
photos with the restaurant styles from TripAdvior. We then propose a deep
multi-instance multi-label learning (MIML) framework to deal with the unique
problem setting of the restaurant style classification task. We employ a
two-step bootstrap strategy to train a multi-label convolutional neural network
(CNN). The multi-label CNN is then used to compute the confidence scores of
restaurant styles for all the images associated with a restaurant. The computed
confidence scores are further used to train a final binary classifier for each
restaurant style tag. Upon training, the styles of a restaurant can be profiled
by analyzing restaurant photos with the trained multi-label CNN and SVM models.
Experimental evaluation has demonstrated that our crowd sourcing-based approach
can effectively infer the restaurant style when there are a sufficient number
of user uploaded photos for a given restaurant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted by IEEE BigData 2016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Face Completion with Semantic Knowledge and Collaborative Adversarial
  Learning <span class="chip">ACCV2018</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1812.03252v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1812.03252v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofu Liao, Gareth Funka-Lea, Yefeng Zheng, Jiebo Luo, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike a conventional background inpainting approach that infers a missing
area from image patches similar to the background, face completion requires
semantic knowledge about the target object for realistic outputs. Current image
inpainting approaches utilize generative adversarial networks (GANs) to achieve
such semantic understanding. However, in adversarial learning, the semantic
knowledge is learned implicitly and hence good semantic understanding is not
always guaranteed. In this work, we propose a collaborative adversarial
learning approach to face completion to explicitly induce the training process.
Our method is formulated under a novel generative framework called
collaborative GAN (collaGAN), which allows better semantic understanding of a
target object through collaborative learning of multiple tasks including face
completion, landmark detection, and semantic segmentation. Together with the
collaGAN, we also introduce an inpainting concentrated scheme such that the
model emphasizes more on inpainting instead of autoencoding. Extensive
experiments show that the proposed designs are indeed effective and
collaborative adversarial learning provides better feature representations of
the faces. In comparison with other generative image inpainting models and
single task learning methods, our solution produces superior performances on
all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appear in ACCV2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Knowledge is Better: <span class="highlight-title">Cross-Modal</span>ity Volume Completion and 3D+2D
  Segmentation for Intracardiac Echocardiography Contouring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1812.03507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1812.03507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofu Liao, Yucheng Tang, Gareth Funka-Lea, Jiebo Luo, Shaohua Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using catheter ablation to treat atrial fibrillation increasingly relies on
intracardiac echocardiography (ICE) for an anatomical delineation of the left
atrium and the pulmonary veins that enter the atrium. However, it is a
challenge to build an automatic contouring algorithm because ICE is noisy and
provides only a limited 2D view of the 3D anatomy. This work provides the first
automatic solution to segment the left atrium and the pulmonary veins from ICE.
In this solution, we demonstrate the benefit of building a cross-modality
framework that can leverage a database of diagnostic images to supplement the
less available interventional images. To this end, we develop a novel deep
neural network approach that uses the (i) 3D geometrical information provided
by a position sensor embedded in the ICE catheter and the (ii) 3D image
appearance information from a set of computed tomography cardiac volumes. We
evaluate the proposed approach over 11,000 ICE images collected from 150
clinical patients. Experimental results show that our model is significantly
better than a direct 2D image-to-image deep neural network segmentation,
especially for less-observed structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse
  Surgical Instrument Usage for Context-aware Assistance <span class="chip">MICCAI 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.00548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.00548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Rivoir, Sebastian Bodenstedt, Isabel Funke, Felix von Bechtolsheim, Marius Distler, Jürgen Weitz, Stefanie Speidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intra-operative anticipation of instrument usage is a necessary component for
context-aware assistance in surgery, e.g. for instrument preparation or
semi-automation of robotic tasks. However, the sparsity of instrument
occurrences in long videos poses a challenge. Current approaches are limited as
they assume knowledge on the timing of future actions or require dense temporal
segmentations during training and inference. We propose a novel learning task
for anticipation of instrument usage in laparoscopic videos that overcomes
these limitations. During training, only sparse instrument annotations are
required and inference is done solely on image data. We train a probabilistic
model to address the uncertainty associated with future events. Our approach
outperforms several baselines and is competitive to a variant using richer
annotations. We demonstrate the model's ability to quantify task-relevant
uncertainties. To the best of our knowledge, we are the first to propose a
method for anticipating instruments in surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skin Disease Classification versus Skin Lesion Characterization:
  Achieving Robust Diagnosis using Multi-label Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1812.03520v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1812.03520v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofu Liao, Yuncheng Li, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate what a practically useful approach is in order
to achieve robust skin disease diagnosis. A direct approach is to target the
ground truth diagnosis labels, while an alternative approach instead focuses on
determining skin lesion characteristics that are more visually consistent and
discernible. We argue that, for computer-aided skin disease diagnosis, it is
both more realistic and more useful that lesion type tags should be considered
as the target of an automated diagnosis system such that the system can first
achieve a high accuracy in describing skin lesions, and in turn facilitate
disease diagnosis using lesion characteristics in conjunction with other
evidence. To further meet such an objective, we employ convolutional neural
networks (CNNs) for both the disease-targeted and lesion-targeted
classifications. We have collected a large-scale and diverse dataset of 75,665
skin disease images from six publicly available dermatology atlantes. Then we
train and compare both disease-targeted and lesion-targeted classifiers,
respectively. For disease-targeted classification, only 27.6% top-1 accuracy
and 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of
0.42. In contrast, for lesion-targeted classification, we can achieve a much
higher mAP of 0.70.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Multi-task Learning Approach to Skin Lesion Classification <span class="chip">AAAI 2017</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1812.03527v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1812.03527v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofu Liao, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin lesion identification is a key step toward dermatological diagnosis.
When describing a skin lesion, it is very important to note its body site
distribution as many skin diseases commonly affect particular parts of the
body. To exploit the correlation between skin lesions and their body site
distributions, in this study, we investigate the possibility of improving skin
lesion classification using the additional context information provided by body
location. Specifically, we build a deep multi-task learning (MTL) framework to
jointly optimize skin lesion classification and body location classification
(the latter is used as an inductive bias). Our MTL framework uses the
state-of-the-art ImageNet pretrained model with specialized loss functions for
the two related tasks. Our experiments show that the proposed MTL based method
performs more robustly than its standalone (single-task) counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2017 Joint Workshop on Health Intelligence W3PHIAI 2017 (W3PHI &
  HIAI), San Francisco, CA, 2017</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Mask Pyramid Network for CT/CBCT Metal Artifact Reduction
  with Joint Projection-Sinogram Correction <span class="chip">MICCAI 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1907.00294v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1907.00294v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofu Liao, Wei-An Lin, Zhimin Huo, Levon Vogelsang, William J. Sehnert, S. Kevin Zhou, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A conventional approach to computed tomography (CT) or cone beam CT (CBCT)
metal artifact reduction is to replace the X-ray projection data within the
metal trace with synthesized data. However, existing projection or sinogram
completion methods cannot always produce anatomically consistent information to
fill the metal trace, and thus, when the metallic implant is large, significant
secondary artifacts are often introduced. In this work, we propose to replace
metal artifact affected regions with anatomically consistent content through
joint projection-sinogram correction as well as adversarial learning. To handle
the metallic implants of diverse shapes and large sizes, we also propose a
novel mask pyramid network that enforces the mask information across the
network's encoding layers and a mask fusion loss that reduces early saturation
of adversarial training. Our experimental results show that the proposed
projection-sinogram correction designs are effective and our method recovers
information from the metal traces better than the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to MICCAI 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainability-Aware One Point Attack for Point Cloud Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04158v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04158v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Tan, Helena Kotthaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proposition of neural networks for point clouds, deep learning has
started to shine in the field of 3D object recognition while researchers have
shown an increased interest to investigate the reliability of point cloud
networks by adversarial attacks. However, most of the existing studies aim to
deceive humans or defense algorithms, while the few that address the operation
principles of the models themselves remain flawed in terms of critical point
selection. In this work, we propose two adversarial methods: One Point Attack
(OPA) and Critical Traversal Attack (CTA), which incorporate the explainability
technologies and aim to explore the intrinsic operating principle of point
cloud networks and their sensitivity against critical points perturbations. Our
results show that popular point cloud networks can be deceived with almost
$100\%$ success rate by shifting only one point from the input instance. In
addition, we show the interesting impact of different point attribution
distributions on the adversarial robustness of point cloud networks. Finally,
we discuss how our approaches facilitate the explainability study for point
cloud networks. To the best of our knowledge, this is the first
point-cloud-based adversarial approach concerning explainability. Our code is
available at https://github.com/Explain3D/Exp-One-Point-Atk-PC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Visual-<span class="highlight-title">Prompt</span> Temporal Answering Grounding in Medical
  Instructional Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06667v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06667v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Li, Yixuan Weng, Bin Sun, Shutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms other
state-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,
which demonstrates the effectiveness of visual prompt and the text span
predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Fine-tuning for Backdoor Defense: Connecting Backdoor
  Attacks to Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Rong Jin, Gang Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are known to be vulnerable to both backdoor
attacks as well as adversarial attacks. In the literature, these two types of
attacks are commonly treated as distinct problems and solved separately, since
they belong to training-time and inference-time attacks respectively. However,
in this paper we find an intriguing connection between them: for a model
planted with backdoors, we observe that its adversarial examples have similar
behaviors as its triggered samples, i.e., both activate the same subset of DNN
neurons. It indicates that planting a backdoor into a model will significantly
affect the model's adversarial examples. Based on this observations, we design
a new Adversarial Fine-Tuning (AFT) algorithm to defend against backdoor
attacks. We empirically show that, against 5 state-of-the-art backdoor attacks,
our AFT can effectively erase the backdoor triggers without obvious performance
degradation on clean samples and significantly outperforms existing defense
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Arbitrary-Oriented Object Detection: Classification based
  Approaches Revisited <span class="chip">ECCV2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.05597v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.05597v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Yang, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arbitrary-oriented object detection has been a building block for rotation
sensitive tasks. We first show that the boundary problem suffered in existing
dominant regression-based rotation detectors, is caused by angular periodicity
or corner ordering, according to the parameterization protocol. We also show
that the root cause is that the ideal predictions can be out of the defined
range. Accordingly, we transform the angular prediction task from a regression
problem to a classification one. For the resulting circularly distributed angle
classification problem, we first devise a Circular Smooth Label technique to
handle the periodicity of angle and increase the error tolerance to adjacent
angles. To reduce the excessive model parameters by Circular Smooth Label, we
further design a Densely Coded Labels, which greatly reduces the length of the
encoding. Finally, we further develop an object heading detection module, which
can be useful when the exact heading orientation information is needed e.g. for
ship and plane heading detection. We release our OHD-SJTU dataset and OHDet
detector for heading detection. Extensive experimental results on three
large-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU,
and face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show
the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 16 figures, 18 tables, journal version of CSL (ECCV2020)
  and DCL (CVPR2021), accepted by IJCV2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting abnormal events in video is commonly framed as a one-class
classification task, where training videos contain only normal events, while
test videos encompass both normal and abnormal events. In this scenario,
anomaly detection is an open-set problem. However, some studies assimilate
anomaly detection to action recognition. This is a closed-set scenario that
fails to test the capability of systems at detecting new anomaly types. To this
end, we propose UBnormal, a new supervised open-set benchmark composed of
multiple virtual scenes for video anomaly detection. Unlike existing data sets,
we introduce abnormal events annotated at the pixel level at training time, for
the first time enabling the use of fully-supervised learning methods for
abnormal event detection. To preserve the typical open-set formulation, we make
sure to include disjoint sets of anomaly types in our training and test
collections of videos. To our knowledge, UBnormal is the first video anomaly
detection benchmark to allow a fair head-to-head comparison between one-class
open-set models and supervised closed-set models, as shown in our experiments.
Moreover, we provide empirical evidence showing that UBnormal can enhance the
performance of a state-of-the-art anomaly detection framework on two prominent
data sets, Avenue and ShanghaiTech. Our benchmark is freely available at
https://github.com/lilygeorgescu/UBnormal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022. Paper + supplementary (15 pages, 9 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransVPR: <span class="highlight-title">Transformer</span>-based place recognition with multi-level attention
  aggregation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruotong Wang, Yanqing Shen, Weiliang Zuo, Sanping Zhou, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition is a challenging task for applications such as
autonomous driving navigation and mobile robot localization. Distracting
elements presenting in complex scenes often lead to deviations in the
perception of visual place. To address this problem, it is crucial to integrate
information from only task-relevant regions into image representations. In this
paper, we introduce a novel holistic place recognition model, TransVPR, based
on vision Transformers. It benefits from the desirable property of the
self-attention operation in Transformers which can naturally aggregate
task-relevant features. Attentions from multiple levels of the Transformer,
which focus on different regions of interest, are further combined to generate
a global image representation. In addition, the output tokens from Transformer
layers filtered by the fused attention mask are considered as key-patch
descriptors, which are used to perform spatial matching to re-rank the
candidates retrieved by the global image features. The whole model allows
end-to-end training with a single objective and image-level supervision.
TransVPR achieves state-of-the-art performance on several real-world benchmarks
while maintaining low computational time and storage requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Neural Fields: Learning Functions on Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah Lähner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes & different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QMagFace: Simple and Accurate Quality-Aware Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13475v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13475v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Terhörst, Malte Ihlefeld, Marco Huber, Naser Damer, Florian Kirchbuchner, Kiran Raja, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition systems have to deal with large variabilities (such as
different poses, illuminations, and expressions) that might lead to incorrect
matching decisions. These variabilities can be measured in terms of face image
quality which is defined over the utility of a sample for recognition. Previous
works on face recognition either do not employ this valuable information or
make use of non-inherently fit quality estimates. In this work, we propose a
simple and effective face recognition solution (QMagFace) that combines a
quality-aware comparison score with a recognition model based on a
magnitude-aware angular margin loss. The proposed approach includes
model-specific face image qualities in the comparison process to enhance the
recognition performance under unconstrained circumstances. Exploiting the
linearity between the qualities and their comparison scores induced by the
utilized loss, our quality-aware comparison function is simple and highly
generalizable. The experiments conducted on several face recognition databases
and benchmarks demonstrate that the introduced quality-awareness leads to
consistent improvements in the recognition performance. Moreover, the proposed
QMagFace approach performs especially well under challenging circumstances,
such as cross-pose, cross-age, or cross-quality. Consequently, it leads to
state-of-the-art performances on several face recognition benchmarks, such as
98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace
is publicly available
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring and Evaluating Image Restoration Potential in Dynamic Scenes <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Zhang, Shaolin Su, Yu Zhu, Qingsen Yan, Jinqiu Sun, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dynamic scenes, images often suffer from dynamic blur due to superposition
of motions or low signal-noise ratio resulted from quick shutter speed when
avoiding motions. Recovering sharp and clean results from the captured images
heavily depends on the ability of restoration methods and the quality of the
input. Although existing research on image restoration focuses on developing
models for obtaining better restored results, fewer have studied to evaluate
how and which input image leads to superior restored quality. In this paper, to
better study an image's potential value that can be explored for restoration,
we propose a novel concept, referring to image restoration potential (IRP).
Specifically, We first establish a dynamic scene imaging dataset containing
composite distortions and applied image restoration processes to validate the
rationality of the existence to IRP. Based on this dataset, we investigate
several properties of IRP and propose a novel deep model to accurately predict
IRP values. By gradually distilling and selective fusing the degradation
features, the proposed model shows its superiority in IRP prediction. Thanks to
the proposed model, we are then able to validate how various image restoration
related applications are benefited from IRP prediction. We show the potential
usages of IRP as a filtering principle to select valuable frames, an auxiliary
guidance to improve restoration models, and even an indicator to optimize
camera settings for capturing better images under dynamic scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi-Modal</span> Masked <span class="highlight-title">Pre-Train</span>ing for Monocular Panoramic Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Yan, Xiang Li, Kun Wang, Zhenyu Zhang, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we formulate a potentially valuable panoramic depth completion
(PDC) task as panoramic 3D cameras often produce 360{\deg} depth with missing
data in complex scenes. Its goal is to recover dense panoramic depths from raw
sparse ones and panoramic RGB images. To deal with the PDC task, we train a
deep network that takes both depth and image as inputs for the dense panoramic
depth recovery. However, it needs to face a challenging optimization problem of
the network parameters due to its non-convex objective function. To address
this problem, we propose a simple yet effective approach termed M{^3}PT:
multi-modal masked pre-training. Specifically, during pre-training, we
simultaneously cover up patches of the panoramic RGB image and sparse depth by
shared random mask, then reconstruct the sparse depth in the masked regions. To
our best knowledge, it is the first time that we show the effectiveness of
masked pre-training in a multi-modal vision task, instead of the single-modal
task resolved by masked autoencoders (MAE). Different from MAE where
fine-tuning completely discards the decoder part of pre-training, there is no
architectural difference between the pre-training and fine-tuning stages in our
M$^{3}$PT as they only differ in the prediction density, which potentially
makes the transfer learning more convenient and effective. Extensive
experiments verify the effectiveness of M{^3}PT on three panoramic datasets.
Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,
51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.
Codes and pre-trained models are available at
https://github.com/anonymoustbd/MMMPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Triangular 3D Models, Materials, and Lighting From Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12503v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12503v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Müller, Sanja Fidler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an efficient method for joint optimization of topology, materials
and lighting from multi-view image observations. Unlike recent multi-view
reconstruction approaches, which typically produce entangled 3D representations
encoded in neural networks, we output triangle meshes with spatially-varying
materials and environment lighting that can be deployed in any traditional
graphics engine unmodified. We leverage recent work in differentiable
rendering, coordinate-based networks to compactly represent volumetric
texturing, alongside differentiable marching tetrahedrons to enable
gradient-based optimization directly on the surface mesh. Finally, we introduce
a differentiable formulation of the split sum approximation of environment
lighting to efficiently recover all-frequency lighting. Experiments show our
extracted models used in advanced scene editing, material decomposition, and
high quality view interpolation, all running at interactive rates in
triangle-based renderers (rasterizers and path tracers). Project website:
https://nvlabs.github.io/nvdiffrec/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://nvlabs.github.io/nvdiffrec/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Gyel Sun, Hyunjae Ahn, HyunGyu Lee, Injung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new adapter network for adapting a pre-trained
deep neural network to a target domain with minimal computation. The proposed
model, unidirectional thin adapter (UDTA), helps the classifier adapt to new
data by providing auxiliary features that complement the backbone network. UDTA
takes outputs from multiple layers of the backbone as input features but does
not transmit any feature to the backbone. As a result, UDTA can learn without
computing the gradient of the backbone, which saves computation for training
significantly. In addition, since UDTA learns the target task without modifying
the backbone, a single backbone can adapt to multiple tasks by learning only
UDTAs separately. In experiments on five fine-grained classification datasets
consisting of a small number of samples, UDTA significantly reduced computation
and training time required for backpropagation while showing comparable or even
improved accuracy compared with conventional adapter models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages and 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-shape <span class="highlight-title">Transformer</span> for Underwater Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11843v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11843v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lintao Peng, Chunli Zhu, Liheng Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The light absorption and scattering of underwater impurities lead to poor
underwater imaging quality. The existing data-driven based underwater image
enhancement (UIE) techniques suffer from the lack of a large-scale dataset
containing various underwater scenes and high-fidelity reference images.
Besides, the inconsistent attenuation in different color channels and space
areas is not fully considered for boosted enhancement. In this work, we
constructed a large-scale underwater image (LSUI) dataset including 5004 image
pairs, and reported an U-shape Transformer network where the transformer model
is for the first time introduced to the UIE task. The U-shape Transformer is
integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)
module and a spatial-wise global feature modeling transformer (SGFMT) module,
which reinforce the network's attention to the color channels and space areas
with more serious attenuation. Meanwhile, in order to further improve the
contrast and saturation, a novel loss function combining RGB, LAB and LCH color
spaces is designed following the human vision principle. The extensive
experiments on available datasets validate the state-of-the-art performance of
the reported technique with more than 2dB superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Channel <span class="highlight-title">Self-Supervision</span> for Online Knowledge <span class="highlight-title">Distillation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiao Fan, Xuan Cheng, Xiaomin Wang, Chun Yang, Pan Deng, Minghui Liu, Jiali Deng, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, researchers have shown an increased interest in the online
knowledge distillation. Adopting an one-stage and end-to-end training fashion,
online knowledge distillation uses aggregated intermediated predictions of
multiple peer models for training. However, the absence of a powerful teacher
model may result in the homogeneity problem between group peers, affecting the
effectiveness of group distillation adversely. In this paper, we propose a
novel online knowledge distillation method, \textbf{C}hannel
\textbf{S}elf-\textbf{S}upervision for Online Knowledge Distillation (CSS),
which structures diversity in terms of input, target, and network to alleviate
the homogenization problem. Specifically, we construct a dual-network
multi-branch structure and enhance inter-branch diversity through
self-supervised learning, adopting the feature-level transformation and
augmenting the corresponding labels. Meanwhile, the dual network structure has
a larger space of independent parameters to resist the homogenization problem
during distillation. Extensive quantitative experiments on CIFAR-100 illustrate
that our method provides greater diversity than OKDDip and we also give pretty
performance improvement, even over the state-of-the-art such as PCL. The
results on three fine-grained datasets (StanfordDogs, StanfordCars,
CUB-200-211) also show the significant generalization capability of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Autonomous Crop-Agnostic Visual Navigation in Arable Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.11936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.11936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Ahmadi, Michael Halstead, Chris McCool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation of a robot in agricultural fields is essential for
every task from crop monitoring to weed management and fertilizer application.
Many current approaches rely on accurate GPS, however, such technology is
expensive and also prone to failure (e.g. through lack of coverage). As such,
autonomous navigation through sensors that can interpret their environment
(such as cameras) is important to achieve the goal of autonomy in agriculture.
In this paper, we introduce a purely vision-based navigation scheme that is
able to reliably guide the robot through row-crop fields without manual
intervention. Independent of any global localization or mapping, this approach
is able to accurately follow the crop-rows and switch between the rows, only
using onboard cameras. With the help of a novel crop-row detection and a novel
crop-row switching technique, our navigation scheme can be deployed in a wide
range of fields with different canopy types in various growth stages with
limited parameter tuning, creating a crop agnostic navigation approach. We have
extensively evaluated our approach in three different fields under various
illumination conditions using our agricultural robotic platform (BonnBot-I).
For navigation, our approach is evaluated on five crop types and achieves an
average navigation accuracy of 3.82cm relative to manual teleoperation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watermarking Images in <span class="highlight-title">Self-Supervised</span> Latent Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Hervé Jégou, Matthijs Douze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit watermarking techniques based on pre-trained deep networks, in the
light of self-supervised approaches. We present a way to embed both marks and
binary messages into their latent spaces, leveraging data augmentation at
marking time. Our method can operate at any resolution and creates watermarks
robust to a broad range of transformations (rotations, crops, JPEG, contrast,
etc). It significantly outperforms the previous zero-bit methods, and its
performance on multi-bit watermarking is on par with state-of-the-art
encoder-decoder architectures trained end-to-end for watermarking. The code is
available at github.com/facebookresearch/ssl_watermarking
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Decomposition for Image Manipulation and Beyond <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.00788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.00788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Fu Chen, Jia-Wei Yan, Ya-Fan Su, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation disentanglement aims at learning interpretable features, so
that the output can be recovered or manipulated accordingly. While existing
works like infoGAN and AC-GAN exist, they choose to derive disjoint attribute
code for feature disentanglement, which is not applicable for existing/trained
generative models. In this paper, we propose a decomposition-GAN (dec-GAN),
which is able to achieve the decomposition of an existing latent representation
into content and attribute features. Guided by the classifier pre-trained on
the attributes of interest, our dec-GAN decomposes the attributes of interest
from the latent representation, while data recovery and feature consistency
objectives enforce the learning of our proposed method. Our experiments on
multiple image datasets confirm the effectiveness and robustness of our dec-GAN
over recent representation disentanglement models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IEEE International Conference in Image Processing (ICIP)
  2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Memory Learning for Fine-Grained Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youming Deng, Yansheng Li, Yongjun Zhang, Xiang Xiang, Jian Wang, Jingdong Chen, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the
dataset due to the crowd-sourced labeling, and the long-tail problem is also
pronounced. Given this tricky situation, many existing SGG methods treat the
predicates equally and learn the model under the supervision of
mixed-granularity predicates in one stage, leading to relatively coarse
predictions. In order to alleviate the negative impact of the suboptimum
mixed-granularity annotation and long-tail effect problems, this paper proposes
a novel Hierarchical Memory Learning (HML) framework to learn the model from
simple to complex, which is similar to the human beings' hierarchical memory
learning process. After the autonomous partition of coarse and fine predicates,
the model is first trained on the coarse predicates and then learns the fine
predicates. In order to realize this hierarchical learning pattern, this paper,
for the first time, formulates the HML framework using the new Concept
Reconstruction (CR) and Model Reconstruction (MR) constraints. It is worth
noticing that the HML framework can be taken as one general optimization
strategy to improve various SGG models, and significant improvement can be
achieved on the SGG benchmark (i.e., Visual Genome).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Few-Shot Object Detection for Robotics <span class="chip">ICRA 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.02641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.02641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiting Li, Haiyue Zhu, Sichao Tian, Fan Feng, Jun Ma, Chek Sing Teo, Cheng Xiang, Prahlad Vadakkepat, Tong Heng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental few-shot learning is highly expected for practical robotics
applications. On one hand, robot is desired to learn new tasks quickly and
flexibly using only few annotated training samples; on the other hand, such new
additional tasks should be learned in a continuous and incremental manner
without forgetting the previous learned knowledge dramatically. In this work,
we propose a novel Class-Incremental Few-Shot Object Detection (CI-FSOD)
framework that enables deep object detection network to perform effective
continual learning from just few-shot samples without re-accessing the previous
training data. We achieve this by equipping the widely-used Faster-RCNN
detector with three elegant components. Firstly, to best preserve performance
on the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES)
architecture which decouples the representation learning of base and novel
categories into different spaces. Secondly, to mitigate the catastrophic
forgetting on the accumulated novel classes, we propose a Sequential Model
Fusion (SMF) method, which is able to achieve long-term memory without
additional storage cost. Thirdly, to promote inter-task class separation in
feature space, we propose a novel regularization technique that extends the
classification boundary further away from the previous classes to avoid
misclassification. Overall, our framework is simple yet effective and
outperforms the previous SOTA with a significant margin of 2.4 points in AP
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning <span class="highlight-title">Transformer</span> Features for Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zeng, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective image quality evaluation is a challenging task, which aims to
measure the quality of a given image automatically. According to the
availability of the reference images, there are Full-Reference and No-Reference
IQA tasks, respectively. Most deep learning approaches use regression from deep
features extracted by Convolutional Neural Networks. For the FR task, another
option is conducting a statistical comparison on deep features. For all these
methods, non-local information is usually neglected. In addition, the
relationship between FR and NR tasks is less explored. Motivated by the recent
success of transformers in modeling contextual information, we propose a
unified IQA framework that utilizes CNN backbone and transformer encoder to
extract features. The proposed framework is compatible with both FR and NR
modes and allows for a joint training scheme. Evaluation experiments on three
standard IQA datasets, i.e., LIVE, CSIQ and TID2013, and KONIQ-10K, show that
our proposed model can achieve state-of-the-art FR performance. In addition,
comparable NR performance is achieved in extensive experiments, and the results
show that the NR performance can be leveraged by the joint training scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping ViTs: Towards Liberating Vision <span class="highlight-title">Transformer</span>s from
  <span class="highlight-title">Pre-train</span>ing <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofei Zhang, Jiarui Duan, Mengqi Xue, Jie Song, Li Sun, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, vision Transformers (ViTs) are developing rapidly and starting to
challenge the domination of convolutional neural networks (CNNs) in the realm
of computer vision (CV). With the general-purpose Transformer architecture
replacing the hard-coded inductive biases of convolution, ViTs have surpassed
CNNs, especially in data-sufficient circumstances. However, ViTs are prone to
over-fit on small datasets and thus rely on large-scale pre-training, which
expends enormous time. In this paper, we strive to liberate ViTs from
pre-training by introducing CNNs' inductive biases back to ViTs while
preserving their network architectures for higher upper bound and setting up
more suitable optimization objectives. To begin with, an agent CNN is designed
based on the given ViT with inductive biases. Then a bootstrapping training
algorithm is proposed to jointly optimize the agent and ViT with weight
sharing, during which the ViT learns inductive biases from the intermediate
features of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k
with limited training data have shown encouraging results that the inductive
biases help ViTs converge significantly faster and outperform conventional CNNs
with even fewer parameters. Our code is publicly available at
https://github.com/zhfeing/Bootstrapping-ViTs-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Robust Convolutional Neural Networks with Relevant Feature
  Focusing via Explanations <span class="chip">ICME 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Adachi, Shin'ya Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing image recognition techniques based on convolutional neural networks
(CNNs) basically assume that the training and test datasets are sampled from
i.i.d distributions. However, this assumption is easily broken in the real
world because of the distribution shift that occurs when the co-occurrence
relations between objects and backgrounds in input images change. Under this
type of distribution shift, CNNs learn to focus on features that are not
task-relevant, such as backgrounds from the training data, and degrade their
accuracy on the test data. To tackle this problem, we propose relevant feature
focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via
explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc
explanation modules, it can be easily applied to off-the-shelf CNNs.
Furthermore, ReFF requires no additional inference cost at test time because it
is only used for regularization while training. We demonstrate that CNNs
trained with ReFF focus on features relevant to the target task and that ReFF
improves the test-time accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sensor Data Augmentation by Resampling for <span class="highlight-title">Contrastive Learning</span> in Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqiang Wang, Tao Zhu, Jingyuan Gan, Liming Chen, Huansheng Ning, Yaping Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has contributed to the advancement of sensor-based Human
Activity Recognition (HAR), it is usually a costly and challenging supervised
task with the needs of a large amount of labeled data. To alleviate this issue,
contrastive learning has been applied for sensor-based HAR. Data augmentation
is an essential part of contrastive learning and has a significant impact on
the performance of downstream tasks. However, current popular augmentation
methods do not achieve competitive performance in contrastive learning for
sensor-based HAR. Motivated by this issue, we propose a new sensor data
augmentation method by resampling, which simulates more realistic activity data
by varying the sampling frequency to maximize the coverage of the sampling
space. In addition, we extend MoCo, a popular contrastive learning framework,
to MoCoHAR for HAR. The resampling augmentation method will be evaluated on two
contrastive learning frameworks, SimCLRHAR and MoCoHAR, using UCI-HAR,
MotionSensor, and USC-HAD datasets. The experiment results show that the
resampling augmentation method outperforms all state-of-the-art methods under a
small amount of labeled data, on SimCLRHAR and MoCoHAR, with mean F1-score as
the evaluation metric. The results also demonstrate that not all data
augmentation methods have positive effects in the contrastive learning
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and
  Extracting Them from 2D Renderings <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.13450v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.13450v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, Peyman Milanfar, Feng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PartImageNet: A Large, High-Quality <span class="highlight-title">Dataset</span> of Parts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xiaoding Yuan, Jie-Neng Chen, Shuai Liu, Cheng Yang, Qihang Yu, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is natural to represent objects in terms of their parts. This has the
potential to improve the performance of algorithms for object recognition and
segmentation but can also help for downstream tasks like activity recognition.
Research on part-based models, however, is hindered by the lack of datasets
with per-pixel part annotations. This is partly due to the difficulty and high
cost of annotating object parts so it has rarely been done except for humans
(where there exists a big literature on part-based models). To help address
this problem, we propose PartImageNet, a large, high-quality dataset with part
segmentation annotations. It consists of $158$ classes from ImageNet with
approximately $24,000$ images. PartImageNet is unique because it offers
part-level annotations on a general set of classes including non-rigid,
articulated objects, while having an order of magnitude larger size compared to
existing part datasets (excluding datasets of humans). It can be utilized for
many vision tasks including Object Segmentation, Semantic Part Segmentation,
Few-shot Learning and Part Discovery. We conduct comprehensive experiments
which study these tasks and set up a set of baselines. The dataset and scripts
are released at https://github.com/TACJu/PartImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on
  Crowd Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.11849v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.11849v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kha Gia Quach, Ngan Le, Chi Nhan Duong, Ibsa Jalata, Kaushik Roy, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group-level emotion recognition (ER) is a growing research area as the
demands for assessing crowds of all sizes are becoming an interest in both the
security arena as well as social media. This work extends the earlier ER
investigations, which focused on either group-level ER on single images or
within a video, by fully investigating group-level expression recognition on
crowd videos. In this paper, we propose an effective deep feature level fusion
mechanism to model the spatial-temporal information in the crowd videos. In our
approach, the fusing process is performed on the deep feature domain by a
generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that
models spatial information relationships. Furthermore, we extend our proposed
spatial NVPF approach to the spatial-temporal NVPF approach to learn the
temporal information between frames. To demonstrate the robustness and
effectiveness of each component in the proposed approach, three experiments
were conducted: (i) evaluation on AffectNet database to benchmark the proposed
EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to
benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)
examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos
(GECV) dataset composed of 627 videos collected from publicly available
sources. GECV dataset is a collection of videos containing crowds of people.
Each video is labeled with emotion categories at three levels: individual
faces, group of people, and the entire video frame.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In press at Patter Recognition Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-similarity based Hyperrelation Network for few-shot segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangwen Shi, Zhe Cui, Shaobing Zhang, Miao Cheng, Lian He, Xianghong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot semantic segmentation aims at recognizing the object regions of
unseen categories with only a few annotated examples as supervision. The key to
few-shot segmentation is to establish a robust semantic relationship between
the support and query images and to prevent overfitting. In this paper, we
propose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle
the few-shot semantic segmentation problem. In MSHNet, we propose a new
Generative Prototype Similarity (GPS), which together with cosine similarity
can establish a strong semantic relation between the support and query images.
The locally generated prototype similarity based on global feature is logically
complementary to the global cosine similarity based on local feature, and the
relationship between the query image and the supported image can be expressed
more comprehensively by using the two similarities simultaneously. In addition,
we propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge
multi-layer, multi-shot and multi-similarity hyperrelational features. MSHNet
is built on the basis of similarity rather than specific category features,
which can achieve more general unity and effectively reduce overfitting. On two
benchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet
achieves new state-of-the-art performances on 1-shot and 5-shot semantic
segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Adaptation of GAN in Just One CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gihyun Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Image compressed version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Review</span> of Computer Vision in Sports: Open Issues, Future
  Trends and Research Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Banoth Thulasya Naik, Mohammad Farukh Hashmi, Neeraj Dhanraj Bokde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in video analysis of sports and computer vision
techniques have achieved significant improvements to enable a variety of
critical operations. To provide enhanced information, such as detailed complex
analysis in sports like soccer, basketball, cricket, badminton, etc., studies
have focused mainly on computer vision techniques employed to carry out
different tasks. This paper presents a comprehensive review of sports video
analysis for various applications high-level analysis such as detection and
classification of players, tracking player or ball in sports and predicting the
trajectories of player or ball, recognizing the teams strategies, classifying
various events in sports. The paper further discusses published works in a
variety of application-specific tasks related to sports and the present
researchers views regarding them. Since there is a wide research scope in
sports for deploying computer vision techniques in various sports, some of the
publicly available datasets related to a particular sport have been provided.
This work reviews a detailed discussion on some of the artificial
intelligence(AI)applications in sports vision, GPU-based work stations, and
embedded platforms. Finally, this review identifies the research directions,
probable challenges, and future trends in the area of visual recognition in
sports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking Distance Calibration for Cross-Domain Few-Shot Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Li, Shaogang Gong, Chengjie Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in few-shot learning promotes a more realistic cross-domain
setting, where the source and target datasets are from different domains. Due
to the domain gap and disjoint label spaces between source and target datasets,
their shared knowledge is extremely limited. This encourages us to explore more
information in the target domain rather than to overly elaborate training
strategies on the source domain as in many existing methods. Hence, we start
from a generic representation pre-trained by a cross-entropy loss and a
conventional distance-based classifier, along with an image retrieval view, to
employ a re-ranking process for calibrating a target distance matrix by
discovering the reciprocal k-nearest neighbours within the task. Assuming the
pre-trained representation is biased towards the source, we construct a
non-linear subspace to minimise task-irrelevant features therewithin while keep
more transferrable discriminative information by a hyperbolic tangent
transformation. The calibrated distance in this target-aware non-linear
subspace is complementary to that in the pre-trained representation. To impose
such distance calibration information onto the pre-trained representation, a
Kullback-Leibler divergence loss is employed to gradually guide the model
towards the calibrated distance-based distribution. Extensive evaluations on
eight target domains show that this target ranking calibration process can
improve conventional distance-based classifiers in few-shot learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.04891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.04891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyong Hu, Bo Yang, Guangchi Fang, Yulan Guo, Ales Leonardis, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labelling point clouds fully is highly time-consuming and costly. As larger
point cloud datasets with billions of points become more common, we ask whether
the full annotation is even necessary, demonstrating that existing baselines
designed under a fully annotated assumption only degrade slightly even when
faced with 1% random point annotations. However, beyond this point, e.g., at
0.1% annotations, segmentation accuracy is unacceptably low. We observe that,
as point clouds are samples of the 3D world, the distribution of points in a
local neighborhood is relatively homogeneous, exhibiting strong semantic
similarity. Motivated by this, we propose a new weak supervision method to
implicitly augment highly sparse supervision signals. Extensive experiments
demonstrate the proposed Semantic Query Network (SQN) achieves promising
performance on seven large-scale open datasets under weak supervision schemes,
while requiring only 0.1% randomly annotated points for training, greatly
reducing annotation cost and effort. The code is available at
https://github.com/QingyongHu/SQN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Texture Estimator for Implicit Representation Function <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewon Lee, Kyong Hwan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works with an implicit neural function shed light on representing
images in arbitrary resolution. However, a standalone multi-layer perceptron
shows limited performance in learning high-frequency components. In this paper,
we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for
natural images, enabling an implicit function to capture fine details while
reconstructing images in a continuous manner. When jointly trained with a deep
super-resolution (SR) architecture, LTE is capable of characterizing image
textures in 2D Fourier space. We show that an LTE-based neural function
achieves favorable performance against existing deep SR methods within an
arbitrary-scale factor. Furthermore, we demonstrate that our implementation
takes the shortest running time compared to previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ditto: Building Digital Twins of Articulated Objects from Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Jiang, Cheng-Chun Hsu, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures; Code and additional results are available at
  https://ut-austin-rpl.github.io/Ditto</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomaly Detection via Reverse <span class="highlight-title">Distillation</span> from One-Class Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqiu Deng, Xingyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective "reverse distillation" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Aware Single-Stage Models for Multi-Person 3D Pose
  Estimation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07697v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07697v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Wang, Xuecheng Nie, Xiaochao Qu, Yunpeng Chen, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel Distribution-Aware Single-stage (DAS) model
for tackling the challenging multi-person 3D pose estimation problem. Different
from existing top-down and bottom-up methods, the proposed DAS model
simultaneously localizes person positions and their corresponding body joints
in the 3D camera space in a one-pass manner. This leads to a simplified
pipeline with enhanced efficiency. In addition, DAS learns the true
distribution of body joints for the regression of their positions, rather than
making a simple Laplacian or Gaussian assumption as previous works. This
provides valuable priors for model prediction and thus boosts the
regression-based scheme to achieve competitive performance with volumetric-base
ones. Moreover, DAS exploits a recursive update strategy for progressively
approaching to regression target, alleviating the optimization difficulty and
further lifting the regression performance. DAS is implemented with a fully
Convolutional Neural Network and end-to-end learnable. Comprehensive
experiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior
efficiency of the proposed DAS model, specifically 1.5x speedup over previous
best model, and its stat-of-the-art accuracy for multi-person 3D pose
estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2022. Code will be released</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent Searching System for Medical Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Evtimova-Gardair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the paper is proposed a model of multi-agent security system for searching
a medical information in Internet. The advantages when using mobile agent are
described, so that to perform searching in Internet. Nowadays, multi-agent
systems found their application into distribution of decisions. For modeling
the proposed multi-agent medical system is used JADE. Finally, the results when
using mobile agent are generated that could reflect performance when working
with BIG DATA. The proposed system is having also relatively high precision
96%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Volume 16, 2019, pp.140-145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Multi-View Learning for Tire Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Ranvier, Kilian Bourhis, Khalid Benabdeslem, Bruno Canitia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are constantly using recommender systems, often without even noticing.
They build a profile of our person in order to recommend the content we will
most likely be interested in. The data representing the users, their
interactions with the system or the products may come from different sources
and be of a various nature. Our goal is to use a multi-view learning approach
to improve our recommender system and improve its capacity to manage multi-view
data. We propose a comparative study between several state-of-the-art
multi-view models applied to our industrial data. Our study demonstrates the
relevance of using multi-view learning within recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> PEAR: Personalized Re-ranking with Contextualized <span class="highlight-title">Transformer</span> for
  Recommendation <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Jieming Zhu, Weiwen Liu, Liangcai Su, Guohao Cai, <span class="highlight-author">Qi Zhang</span>, Ruiming Tang, Xi Xiao, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of recommender systems is to provide ordered item lists to users
that best match their interests. As a critical task in the recommendation
pipeline, re-ranking has received increasing attention in recent years. In
contrast to conventional ranking models that score each item individually,
re-ranking aims to explicitly model the mutual influences among items to
further refine the ordering of items given an initial ranking list. In this
paper, we present a personalized re-ranking model (dubbed PEAR) based on
contextualized transformer. PEAR makes several major improvements over the
existing methods. Specifically, PEAR not only captures feature-level and
item-level interactions, but also models item contexts from both the initial
ranking list and the historical clicked item list. In addition to item-level
ranking score prediction, we also augment the training of PEAR with a
list-level classification task to assess users' satisfaction on the whole
ranking list. Experimental results on both public and production datasets have
shown the superior effectiveness of PEAR compared to the previous re-ranking
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorpusVis: Visual Analysis of Digital Sheet Music Collections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Miller, Julius Rauscher, Daniel A. Keim, Mennatallah El-Assady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manually investigating sheet music collections is challenging for music
analysts due to the magnitude and complexity of underlying features,
structures, and contextual information. However, applying sophisticated
algorithmic methods would require advanced technical expertise that analysts do
not necessarily have. Bridging this gap, we contribute CorpusVis, an
interactive visual workspace, enabling scalable and multi-faceted analysis. Our
proposed visual analytics dashboard provides access to computational methods,
generating varying perspectives on the same data. The proposed application uses
metadata including composers, type, epoch, and low-level features, such as
pitch, melody, and rhythm. To evaluate our approach, we conducted a pair
analytics study with nine participants. The qualitative results show that
CorpusVis supports users in performing exploratory and confirmatory analysis,
leading them to new insights and findings. In addition, based on three
exemplary workflows, we demonstrate how to apply our approach to different
tasks, such as exploring musical features or comparing composers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, Computer Graphics Forum 2022, EuroVis Conference
  (Full Papers)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Birds with One Stone: Unified Model Learning for Both Recall and
  Ranking in News Recommendation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.07404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.07404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recall and ranking are two critical steps in personalized news
recommendation. Most existing news recommender systems conduct personalized
news recall and ranking separately with different models. However, maintaining
multiple models leads to high computational cost and poses great challenge to
meeting the online latency requirement of news recommender systems. In order to
handle this problem, in this paper we propose UniRec, a unified method for
recall and ranking in news recommendation. In our method, we first infer user
embedding for ranking from the historical news click behaviors of a user using
a user encoder model. Then we derive the user embedding for recall from the
obtained user embedding for ranking by using it as the attention query to
select a set of basis user embeddings which encode different general user
interests and synthesize them into a user embedding for recall. The extensive
experiments on benchmark dataset demonstrate that our method can improve both
efficiency and effectiveness for recall and ranking in news recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Rec: <span class="highlight-title">Multimodal</span> News Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.07407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.07407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate news representation is critical for news recommendation. Most of
existing news representation methods learn news representations only from news
texts while ignore the visual information in news like images. In fact, users
may click news not only because of the interest in news titles but also due to
the attraction of news images. Thus, images are useful for representing news
and predicting user behaviors. In this paper, we propose a multimodal news
recommendation method, which can incorporate both textual and visual
information of news to learn multimodal news representations. We first extract
region-of-interests (ROIs) from news images via object detection. Then we use a
pre-trained visiolinguistic model to encode both news texts and news image ROIs
and model their inherent relatedness using co-attentional Transformers. In
addition, we propose a crossmodal candidate-aware attention network to select
relevant historical clicked news for accurate user modeling by measuring the
crossmodal relatedness between clicked news and candidate news. Experiments
validate that incorporating multimodal news information can effectively improve
news recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S<span class="highlight-title">GPT</span>: <span class="highlight-title">GPT</span> Sentence Embeddings for Semantic Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08904v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08904v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT transformers are the largest language models available, yet semantic
search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for
applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric
search.
  SGPT-BE produces semantically meaningful sentence embeddings by contrastive
fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion
parameter SGPT-BE outperforms the best available sentence embeddings by 6%
setting a new state-of-the-art on BEIR. It outperforms the concurrently
proposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes
250,000 times more parameters.
  SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1
billion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It
beats the supervised state-of-the-art on 7 datasets, but significantly loses on
other datasets. We show how this can be alleviated by adapting the prompt.
  SGPT-BE and SGPT-CE performance scales with model size. Yet, increased
latency, storage and compute costs should be considered. Code, models and
result files are freely available at https://github.com/Muennighoff/sgpt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, 12 tables. v2 corrects a misreported nDCG@10
  number for the SGPT-BE-5.8B model. v3 updates SGPT-BE-5.8B scores based on
  retrained models with larger batch sizes v4 removes a superfluous table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised <span class="highlight-title">Pre-Train</span>ing on Patient Population Graphs for Patient-Level
  Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Anees Kazi, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training has shown success in different areas of machine learning, such
as Computer Vision (CV), Natural Language Processing (NLP) and medical imaging.
However, it has not been fully explored for clinical data analysis. Even though
an immense amount of Electronic Health Record (EHR) data is recorded, data and
labels can be scarce if the data is collected in small hospitals or deals with
rare diseases. In such scenarios, pre-training on a larger set of EHR data
could improve the model performance. In this paper, we apply unsupervised
pre-training to heterogeneous, multi-modal EHR data for patient outcome
prediction. To model this data, we leverage graph deep learning over population
graphs. We first design a network architecture based on graph transformer
designed to handle various input feature types occurring in EHR data, like
continuous, discrete, and time-series features, allowing better multi-modal
data fusion. Further, we design pre-training methods based on masked imputation
to pre-train our network before fine-tuning on different end tasks.
Pre-training is done in a fully unsupervised fashion, which lays the groundwork
for pre-training on large public datasets with different tasks and similar
modalities in the future. We test our method on two medical datasets of patient
records, TADPOLE and MIMIC-III, including imaging and non-imaging features and
different prediction tasks. We find that our proposed graph based pre-training
method helps in modeling the data at a population level and further improves
performance on the fine tuning tasks in terms of AUC on average by 4.15% for
MIMIC and 7.64% for TADPOLE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Poincaré 2.0: Machine Learning Conservation Laws from
  Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Varun Madhavan, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a machine learning algorithm that discovers conservation laws from
differential equations, both numerically (parametrized as neural networks) and
symbolically, ensuring their functional independence (a non-linear
generalization of linear independence). Our independence module can be viewed
as a nonlinear generalization of singular value decomposition. Our method can
readily handle inductive biases for conservation laws. We validate it with
examples including the 3-body problem, the KdV equation and nonlinear
Schr\"odinger equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Fairness of Chest X-ray Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Natalie Dullerud, Karsten Roth, Lauren Oakden-Rayner, Stephen Robert Pfohl, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have reached or surpassed human-level performance in the
field of medical imaging, especially in disease diagnosis using chest x-rays.
However, prior work has found that such classifiers can exhibit biases in the
form of gaps in predictive performance across protected groups. In this paper,
we question whether striving to achieve zero disparities in predictive
performance (i.e. group fairness) is the appropriate fairness definition in the
clinical setting, over minimax fairness, which focuses on maximizing the
performance of the worst-case group. We benchmark the performance of nine
methods in improving classifier fairness across these two definitions. We find,
consistent with prior work on non-clinical data, that methods which strive to
achieve better worst-group performance do not outperform simple data balancing.
We also find that methods which achieve group fairness do so by worsening
performance for all groups. In light of these results, we discuss the utility
of fairness definitions in the clinical setting, advocating for an
investigation of the bias-inducing mechanisms in the underlying data generating
process whenever possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in CHIL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R3M: A Universal Visual Representation for Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how visual representations pre-trained on diverse human video data
can enable data-efficient learning of downstream robotic manipulation tasks.
Concretely, we pre-train a visual representation using the Ego4D human video
dataset using a combination of time-contrastive learning, video-language
alignment, and an L1 penalty to encourage sparse and compact representations.
The resulting representation, R3M, can be used as a frozen perception module
for downstream policy learning. Across a suite of 12 simulated robot
manipulation tasks, we find that R3M improves task success by over 20% compared
to training from scratch and by over 10% compared to state-of-the-art visual
representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika
Panda arm to learn a range of manipulation tasks in a real, cluttered apartment
given just 20 demonstrations. Code and pre-trained models are available at
https://tinyurl.com/robotr3m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Policy Regularizer is Secretly an Adversary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rob Brekelmans, Tim Genewein, Jordi Grau-Moya, Grégoire Delétang, Markus Kunesch, Shane Legg, Pedro Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL- and
{\alpha}-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Regret for Cascading Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Vial, Sujay Sanghavi, Sanjay Shakkottai, R. Srikant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cascading bandits model the task of learning to rank $K$ out of $L$ items
over $n$ rounds of partial feedback. For this model, the minimax (i.e.,
gap-free) regret is poorly understood; in particular, the best known lower and
upper bounds are $\Omega(\sqrt{nL/K})$ and $\tilde{O}(\sqrt{nLK})$,
respectively. We improve the lower bound to $\Omega(\sqrt{nL})$ and show
CascadeKL-UCB (which ranks items by their KL-UCB indices) attains it up to log
terms. Surprisingly, we also show CascadeUCB1 (which ranks via UCB1) can suffer
suboptimal $\Omega(\sqrt{nLK})$ regret. This sharply contrasts with standard
$L$-armed bandits, where the corresponding algorithms both achieve the minimax
regret $\sqrt{nL}$ (up to log terms), and the main advantage of KL-UCB is only
to improve constants in the gap-dependent bounds. In essence, this contrast
occurs because Pinsker's inequality is tight for hard problems in the $L$-armed
case but loose (by a factor of $K$) in the cascading case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Gender Bias in Distilled Language Models via Counterfactual
  Role Reversal <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models excel at generating coherent text, and model compression
techniques such as knowledge distillation have enabled their use in
resource-constrained settings. However, these models can be biased in multiple
ways, including the unfounded association of male and female genders with
gender-neutral professions. Therefore, knowledge distillation without any
fairness constraints may preserve or exaggerate the teacher model's biases onto
the distilled model. To this end, we present a novel approach to mitigate
gender disparity in text generation by learning a fair model during knowledge
distillation. We propose two modifications to the base knowledge distillation
based on counterfactual role reversal$\unicode{x2014}$modifying teacher
probabilities and augmenting the training set. We evaluate gender polarity
across professions in open-ended text generated from the resulting distilled
and finetuned GPT$\unicode{x2012}$2 models and demonstrate a substantial
reduction in gender disparity with only a minor compromise in utility. Finally,
we observe that language models that reduce gender polarity in language
generation do not improve embedding fairness or downstream classification
fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Top-down Supervised Learning Approach to Hierarchical Multi-label
  Classification in Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Romero, Jorge Finke, Camilo Rocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Node classification is the task of inferring or predicting missing node
attributes from information available for other nodes in a network. This paper
presents a general prediction model to hierarchical multi-label classification
(HMC), where the attributes to be inferred can be specified as a strict poset.
It is based on a top-down classification approach that addresses hierarchical
multi-label classification with supervised learning by building a local
classifier per class. The proposed model is showcased with a case study on the
prediction of gene functions for Oryza sativa Japonica, a variety of rice. It
is compared to the Hierarchical Binomial-Neighborhood, a probabilistic model,
by evaluating both approaches in terms of prediction performance and
computational cost. The results in this work support the working hypothesis
that the proposed model can achieve good levels of prediction efficiency, while
scaling up in relation to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GriTS: Grid table similarity metric for table structure recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Smock, Rohith Pesala, Robin Abraham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new class of evaluation metric for table
structure recognition, grid table similarity (GriTS). Unlike prior metrics,
GriTS evaluates the correctness of a predicted table directly in its natural
form as a matrix. To create a similarity measure between matrices, we
generalize the two-dimensional largest common substructure (2D-LCS) problem,
which is NP-hard, to the 2D most similar substructures (2D-MSS) problem and
propose a polynomial-time heuristic for solving it. We validate empirically
using the PubTables-1M dataset that comparison between matrices exhibits more
desirable behavior than alternatives for table structure recognition
evaluation. GriTS also unifies all three subtasks of cell topology recognition,
cell location recognition, and cell content recognition within the same
framework, which simplifies the evaluation and enables more meaningful
comparisons across different types of structure recognition approaches. Code
will be released at https://github.com/microsoft/table-transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Clustering and Multiple Kernel Learning without Pairwise
  Constraint Relaxation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Boecking, Vincent Jeanselme, Artur Dubrawski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering under pairwise constraints is an important knowledge discovery
tool that enables the learning of appropriate kernels or distance metrics to
improve clustering performance. These pairwise constraints, which come in the
form of must-link and cannot-link pairs, arise naturally in many applications
and are intuitive for users to provide. However, the common practice of
relaxing discrete constraints to a continuous domain to ease optimization when
learning kernels or metrics can harm generalization, as information which only
encodes linkage is transformed to informing distances. We introduce a new
constrained clustering algorithm that jointly clusters data and learns a kernel
in accordance with the available pairwise constraints. To generalize well, our
method is designed to maximize constraint satisfaction without relaxing
pairwise constraints to a continuous domain where they inform distances. We
show that the proposed method outperforms existing approaches on a large number
of diverse publicly available datasets, and we discuss how our method can scale
to handling large data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pathways: Asynchronous Distributed Dataflow for ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi, Laurent El Shafey, Chandramohan A. Thekkath, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the design of a new large scale orchestration layer for
accelerators. Our system, Pathways, is explicitly designed to enable
exploration of new systems and ML research ideas, while retaining state of the
art performance for current models. Pathways uses a sharded dataflow graph of
asynchronous operators that consume and produce futures, and efficiently
gang-schedules heterogeneous parallel computations on thousands of accelerators
while coordinating data transfers over their dedicated interconnects. Pathways
makes use of a novel asynchronous distributed dataflow design that lets the
control plane execute in parallel despite dependencies in the data plane. This
design, with careful engineering, allows Pathways to adopt a single-controller
model that makes it easier to express complex new parallelism patterns. We
demonstrate that Pathways can achieve performance parity (~100% accelerator
utilization) with state-of-the-art systems when running SPMD computations over
2048 TPUs, while also delivering throughput comparable to the SPMD case for
Transformer models that are pipelined across 16 stages, or sharded across two
islands of accelerators connected over a data center network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Approach to Probabilistic Forecasting of Weather 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Rittler, Carlo Graziani, Jiali Wang, Rao Kotamarthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss an approach to probabilistic forecasting based on two chained
machine-learning steps: a dimensional reduction step that learns a reduction
map of predictor information to a low-dimensional space in a manner designed to
preserve information about forecast quantities; and a density estimation step
that uses the probabilistic machine learning technique of normalizing flows to
compute the joint probability density of reduced predictors and forecast
quantities. This joint density is then renormalized to produce the conditional
forecast distribution. In this method, probabilistic calibration testing plays
the role of a regularization procedure, preventing overfitting in the second
step, while effective dimensional reduction from the first step is the source
of forecast sharpness. We verify the method using a 22-year 1-hour cadence time
series of Weather Research and Forecasting (WRF) simulation data of surface
wind on a grid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures. Submitted to Artificial Intelligence for Earth
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Graph Learning Meets Dimensionality Reduction <span class="chip">IJCNN 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Morehead, Watchanan Chantapakul, Jianlin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has recently received increased attention from
machine learning researchers. By enabling effective propagation of known labels
in graph-based deep learning (GDL) algorithms, SSL is poised to become an
increasingly used technique in GDL in the coming years. However, there are
currently few explorations in the graph-based SSL literature on exploiting
classical dimensionality reduction techniques for improved label propagation.
In this work, we investigate the use of dimensionality reduction techniques
such as PCA, t-SNE, and UMAP to see their effect on the performance of graph
neural networks (GNNs) designed for semi-supervised propagation of node labels.
Our study makes use of benchmark semi-supervised GDL datasets such as the Cora
and Citeseer datasets to allow meaningful comparisons of the representations
learned by each algorithm when paired with a dimensionality reduction
technique. Our comprehensive benchmarks and clustering visualizations
quantitatively and qualitatively demonstrate that, under certain conditions,
employing a priori and a posteriori dimensionality reduction to GNN inputs and
outputs, respectively, can simultaneously improve the effectiveness of
semi-supervised node label propagation and node clustering. Our source code is
freely available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, and 5 tables. Submitted to the 2022 International
  Joint Conference on Neural Networks (IJCNN 2022). Source code is available at
  https://github.com/amorehead/SSL-With-DR-And-GNNs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling Theorems for Unsupervised Learning in Linear Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julián Tachella, Dongdong Chen, Mike Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving a linear inverse problem requires knowledge about the underlying
signal model. In many applications, this model is a priori unknown and has to
be learned from data. However, it is impossible to learn the model using
observations obtained via a single incomplete measurement operator, as there is
no information outside the range of the inverse operator, resulting in a
chicken-and-egg problem: to learn the model we need reconstructed signals, but
to reconstruct the signals we need to know the model. Two ways to overcome this
limitation are using multiple measurement operators or assuming that the signal
model is invariant to a certain group action. In this paper, we present
necessary and sufficient sampling conditions for learning the signal model from
partial measurements which only depend on the dimension of the model, and the
number of operators or properties of the group action that the model is
invariant to. As our results are agnostic of the learning algorithm, they shed
light into the fundamental limitations of learning from incomplete data and
have implications in a wide range set of practical algorithms, such as
dictionary learning, matrix completion and deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2201.12151</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Spatial-Temporal Attention Multi-Graph Convolution Network for
  Ride-Hailing Demand Prediction Based on Periodicity with Offset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Xing, Chenguang Zhao, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ride-hailing service is becoming a leading part in urban transportation. To
improve the efficiency of ride-hailing service, accurate prediction of
transportation demand is a fundamental challenge. In this paper, we tackle this
problem from both aspects of network structure and data-set formulation. For
network design, we propose a spatial-temporal attention multi-graph convolution
network (STA-MGCN). A spatial-temporal layer in STA-MGCN is developed to
capture the temporal correlations by temporal attention mechanism and temporal
gate convolution, and the spatial correlations by multigraph convolution. A
feature cluster layer is introduced to learn latent regional functions and to
reduce the computation burden. For the data-set formulation, we develop a novel
approach which considers the transportation feature of periodicity with offset.
Instead of only using history data during the same time period, the history
order demand in forward and backward neighboring time periods from yesterday
and last week are also included. Extensive experiments on the three real-world
datasets of New-York, Chicago and Chengdu show that the proposed algorithm
achieves the state-of-the-art performance for ride-hailing demand prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum-enhanced Markov chain Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Layden, Guglielmo Mazzola, Ryan V. Mishmash, Mario Motta, Pawel Wocjan, Jin-Sung Kim, Sarah Sheldon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from complicated probability distributions is a hard computational
problem arising in many fields, including statistical physics, optimization,
and machine learning. Quantum computers have recently been used to sample from
complicated distributions that are hard to sample from classically, but which
seldom arise in applications. Here we introduce a quantum algorithm to sample
from distributions that pose a bottleneck in several applications, which we
implement on a superconducting quantum processor. The algorithm performs Markov
chain Monte Carlo (MCMC), a popular iterative sampling technique, to sample
from the Boltzmann distribution of classical Ising models. In each step, the
quantum processor explores the model in superposition to propose a random move,
which is then accepted or rejected by a classical computer and returned to the
quantum processor, ensuring convergence to the desired Boltzmann distribution.
We find that this quantum algorithm converges in fewer iterations than common
classical MCMC alternatives on relevant problem instances, both in simulations
and experiments. It therefore opens a new path for quantum computers to solve
useful--not merely difficult--problems in the near term.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Framework to Reconstruct Face under Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gourango Modak, Shuvra Smaran Das, Md. Ajharul Islam Miraj, Md. Kishor Morol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning-based image reconstruction methods have shown significant
success in removing objects from pictures, they have yet to achieve acceptable
results for attributing consistency to gender, ethnicity, expression, and other
characteristics like the topological structure of the face. The purpose of this
work is to extract the mask region from a masked image and rebuild the area
that has been detected. This problem is complex because (i) it is difficult to
determine the gender of an image hidden behind a mask, which causes the network
to become confused and reconstruct the male face as a female or vice versa;
(ii) we may receive images from multiple angles, making it extremely difficult
to maintain the actual shape, topological structure of the face and a natural
image; and (iii) there are problems with various mask forms because, in some
cases, the area of the mask cannot be anticipated precisely; certain parts of
the mask remain on the face after completion. To solve this complex task, we
split the problem into three phases: landmark detection, object detection for
the targeted mask area, and inpainting the addressed mask region. To begin, to
solve the first problem, we have used gender classification, which detects the
actual gender behind a mask, then we detect the landmark of the masked facial
image. Second, we identified the non-face item, i.e., the mask, and used the
Mask R-CNN network to create the binary mask of the observed mask area.
Thirdly, we developed an inpainting network that uses anticipated landmarks to
create realistic images. To segment the mask, this article uses a mask R-CNN
and offers a binary segmentation map for identifying the mask area.
Additionally, we generated the image utilizing landmarks as structural guidance
through a GAN-based network. The studies presented in this paper use the FFHQ
and CelebA datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 9 figures, 2022 7th Conference on Data Science and Machine
  Learning Applications (CDMA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Adapted Random Forest Vision (3DARFV) for Untangling
  Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency
  at the Utmost Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Alfarisi, Zeyar Aung, Qingfeng Huang, Ashraf Al-Khateeb, Hamed Alhashmi, Mohamed Abdelsalam, Salem Alzaabi, Haifa Alyazeedi, Anthony Tzes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planetary exploration depends heavily on 3D image data to characterize the
static and dynamic properties of the rock and environment. Analyzing 3D images
requires many computations, causing efficiency to suffer lengthy processing
time alongside large energy consumption. High-Performance Computing (HPC)
provides apparent efficiency at the expense of energy consumption. However, for
remote explorations, the conveyed surveillance and the robotized sensing need
faster data analysis with ultimate accuracy to make real-time decisions. In
such environments, access to HPC and energy is limited. Therefore, we realize
that reducing the number of computations to optimal and maintaining the desired
accuracy leads to higher efficiency. This paper demonstrates the semantic
segmentation capability of a probabilistic decision tree algorithm, 3D Adapted
Random Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at
the utmost accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation-Based Sampling for Pixel- to Image-Level Aggregation in
  Weakly-Supervised Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvi Jonnarth, Michael Felsberg, Yushan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification networks can be used to localize and segment objects in images
by means of class activation maps (CAMs). However, without pixel-level
annotations, they are known to (1) mainly focus on discriminative regions, and
(2) to produce diffuse CAMs without well-defined prediction contours. In this
work, we approach both problems with two contributions for improving CAM
learning. First, we incorporate importance sampling based on the class-wise
probability mass function induced by the CAMs to produce stochastic image-level
class predictions. This results in CAMs which activate over a larger extent of
the objects. Second, we formulate a feature similarity loss term which aims to
match the prediction contours with edges in the image. As a third contribution,
we conduct experiments on the PASCAL VOC and MS-COCO benchmark datasets to
demonstrate that these modifications significantly increase the performance in
terms of contour accuracy, while being comparable to current state-of-the-art
methods in terms of region similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Multi-View Learning for Tire Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Ranvier, Kilian Bourhis, Khalid Benabdeslem, Bruno Canitia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are constantly using recommender systems, often without even noticing.
They build a profile of our person in order to recommend the content we will
most likely be interested in. The data representing the users, their
interactions with the system or the products may come from different sources
and be of a various nature. Our goal is to use a multi-view learning approach
to improve our recommender system and improve its capacity to manage multi-view
data. We propose a comparative study between several state-of-the-art
multi-view models applied to our industrial data. Our study demonstrates the
relevance of using multi-view learning within recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Caner Yüzügüler, Nikolaos Dimitriadis, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing resource utilization in target platforms is key to achieving high
performance during DNN inference. While optimizations have been proposed for
inference latency, memory footprint, and energy consumption, prior
hardware-aware neural architecture search (NAS) methods have omitted resource
utilization, preventing DNNs to take full advantage of the target inference
platforms. Modeling resource utilization efficiently and accurately is
challenging, especially for widely-used array-based inference accelerators such
as Google TPU. In this work, we propose a novel hardware-aware NAS framework
that does not only optimize for task accuracy and inference latency, but also
for resource utilization. We also propose and validate a new computational
model for resource utilization in inference accelerators. By using the proposed
NAS framework and the proposed resource utilization model, we achieve 2.8 - 4x
speedup for DNN inference compared to prior hardware-aware NAS methods while
attaining similar or improved accuracy in image classification on CIFAR-10 and
Imagenet-100 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verification of safety critical control policies using kernel methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaus Vertovec, Sina Ober-Blöbaum, Kostas Margellos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton-Jacobi reachability methods for safety-critical control have been
well studied, but the safety guarantees derived rely on the accuracy of the
numerical computation. Thus, it is crucial to understand and account for any
inaccuracies that occur due to uncertainty in the underlying dynamics and
environment as well as the induced numerical errors. To this end, we propose a
framework for modeling the error of the value function inherent in
Hamilton-Jacobi reachability using a Gaussian process. The derived safety
controller can be used in conjuncture with arbitrary controllers to provide a
safe hybrid control law. The marginal likelihood of the Gaussian process then
provides a confidence metric used to determine switches between a least
restrictive controller and a safety controller. We test both the prediction as
well as the correction capabilities of the presented method in a classical
pursuit-evasion example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper published in 2022 European Control Conference (ECC), 6 pages, 4
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamically-Scaled Deep Canonical Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Friedlander, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical Correlation Analysis (CCA) is a method for feature extraction of
two views by finding maximally correlated linear projections of them. Several
variants of CCA have been introduced in the literature, in particular, variants
based on deep neural networks for learning highly correlated nonlinear
transformations of two views. As these models are parameterized conventionally,
their learnable parameters remain independent of the inputs after the training
process, which may limit their capacity for learning highly correlated
representations. We introduce a novel dynamic scaling method for training an
input-dependent canonical correlation model. In our deep-CCA models, the
parameters of the last layer are scaled by a second neural network that is
conditioned on the model's input, resulting in a parameterization that is
dependent on the input samples. We evaluate our model on multiple datasets and
demonstrate that the learned representations are more correlated in comparison
to the conventionally-parameterized CCA-based models and also obtain preferable
retrieval results. Our code is available at
https://github.com/tomerfr/DynamicallyScaledDeepCCA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training of speech enhancement systems often does not incorporate knowledge
of human perception and thus can lead to unnatural sounding results.
Incorporating psychoacoustically motivated speech perception metrics as part of
model training via a predictor network has recently gained interest. However,
the performance of such predictors is limited by the distribution of metric
scores that appear in the training data.In this work, we propose MetricGAN+/-
(an extension of MetricGAN+, one such metric-motivated system) which introduces
an additional network - a "de-generator" which attempts to improve the
robustness of the prediction network (and by extension of the generator) by
ensuring observation of a wider range of metric scores in training.
Experimental results on the VoiceBank-DEMAND dataset show relative improvement
in PESQ score of $3.8\%$ ($3.05$ vs $3.22$ PESQ score), as well as better
generalisation to unseen noise and speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethereum Fraud Detection with Heterogeneous Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Kanezashi, Toyotaro Suzumura, Xin Liu, Takahiro Hirofuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While transactions with cryptocurrencies such as Ethereum are becoming more
prevalent, fraud and other criminal transactions are not uncommon. Graph
analysis algorithms and machine learning techniques detect suspicious
transactions that lead to phishing in large transaction networks. Many graph
neural network (GNN) models have been proposed to apply deep learning
techniques to graph structures. Although there is research on phishing
detection using GNN models in the Ethereum transaction network, models that
address the scale of the number of vertices and edges and the imbalance of
labels have not yet been studied. In this paper, we compared the model
performance of GNN models on the actual Ethereum transaction network dataset
and phishing reported label data to exhaustively compare and verify which GNN
models and hyperparameters produce the best accuracy. Specifically, we
evaluated the model performance of representative homogeneous GNN models which
consider single-type nodes and edges and heterogeneous GNN models which support
different types of nodes and edges. We showed that heterogeneous models had
better model performance than homogeneous models. In particular, the RGCN model
achieved the best performance in the overall metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Binary Morphological Neural Network <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Aouad, Hugues Talbot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last ten years, Convolutional Neural Networks (CNNs) have formed the
basis of deep-learning architectures for most computer vision tasks. However,
they are not necessarily optimal. For example, mathematical morphology is known
to be better suited to deal with binary images. In this work, we create a
morphological neural network that handles binary inputs and outputs. We propose
their construction inspired by CNNs to formulate layers adapted to such images
by replacing convolutions with erosions and dilations. We give explainable
theoretical results on whether or not the resulting learned networks are indeed
morphological operators. We present promising experimental results designed to
learn basic binary operators, and we have made our code publicly available
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a submission to ICIP 2022. 7 pages. 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards explaining the generalization gap in neural networks using
  topological data analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubén Ballester, Xavier Arnal Clemente, Carles Casacuberta, Meysam Madadi, Ciprian A. Corneanu, Sergio Escalera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how neural networks generalize on unseen data is crucial for
designing more robust and reliable models. In this paper, we study the
generalization gap of neural networks using methods from topological data
analysis. For this purpose, we compute homological persistence diagrams of
weighted graphs constructed from neuron activation correlations after a
training phase, aiming to capture patterns that are linked to the
generalization capacity of the network. We compare the usefulness of different
numerical summaries from persistence diagrams and show that a combination of
some of them can accurately predict and partially explain the generalization
gap without the need of a test set. Evaluation on two computer vision
recognition tasks (CIFAR10 and SVHN) shows competitive generalization gap
prediction when compared against state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The BP Dependency Function: a Generic Measure of Dependence between
  Random Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guus Berkelmans, Joris Pries, Sandjai Bhulai, Rob van der Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring and quantifying dependencies between random variables (RV's) can
give critical insights into a data-set. Typical questions are: `Do underlying
relationships exist?', `Are some variables redundant?', and `Is some target
variable $Y$ highly or weakly dependent on variable $X$?' Interestingly,
despite the evident need for a general-purpose measure of dependency between
RV's, common practice of data analysis is that most data analysts use the
Pearson correlation coefficient (PCC) to quantify dependence between RV's,
while it is well-recognized that the PCC is essentially a measure for linear
dependency only. Although many attempts have been made to define more generic
dependency measures, there is yet no consensus on a standard, general-purpose
dependency function. In fact, several ideal properties of a dependency function
have been proposed, but without much argumentation. Motivated by this, in this
paper we will discuss and revise the list of desired properties and propose a
new dependency function that meets all these requirements. This general-purpose
dependency function provides data analysts a powerful means to quantify the
level of dependence between variables. To this end, we also provide Python code
to determine the dependency function for use in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wider or Deeper Neural Network Architecture for Acoustic Scene
  Classification with Mismatched Recording Devices <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lam Pham, Khoa Dinh, Dat Ngo, Hieu Tang, Alexander Schindler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a robust and low complexity system for Acoustic
Scene Classification (ASC), the task of identifying the scene of an audio
recording. We first construct an ASC baseline system in which a novel
inception-residual-based network architecture is proposed to deal with the
mismatched recording device issue. To further improve the performance but still
satisfy the low complexity model, we apply two techniques: ensemble of multiple
spectrograms and channel reduction on the ASC baseline system. By conducting
extensive experiments on the benchmark DCASE 2020 Task 1A Development dataset,
we achieve the best model performing an accuracy of 69.9% and a low complexity
of 2.4M trainable parameters, which is competitive to the state-of-the-art ASC
systems and potential for real-life applications on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavDreams: Towards Camera-Only RL Navigation Among Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Dugas, Olov Andersson, Roland Siegwart, Jen Jen Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomously navigating a robot in everyday crowded spaces requires solving
complex perception and planning challenges. When using only monocular image
sensor data as input, classical two-dimensional planning approaches cannot be
used. While images present a significant challenge when it comes to perception
and planning, they also allow capturing potentially important details, such as
complex geometry, body movement, and other visual cues. In order to
successfully solve the navigation task from only images, algorithms must be
able to model the scene and its dynamics using only this channel of
information. We investigate whether the world model concept, which has shown
state-of-the-art results for modeling and learning policies in Atari games as
well as promising results in 2D LiDAR-based crowd navigation, can also be
applied to the camera-based navigation problem. To this end, we create
simulated environments where a robot must navigate past static and moving
humans without colliding in order to reach its goal. We find that
state-of-the-art methods are able to achieve success in solving the navigation
problem, and can generate dream-like predictions of future image-sequences
which show consistent geometry and moving persons. We are also able to show
that policy performance in our high-fidelity sim2real simulation scenario
transfers to the real world by testing the policy on a real robot. We make our
simulator, models and experiments available at
https://github.com/danieldugas/NavDreams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Input-specific Attention Subnetworks for Adversarial Detection <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emil Biju, Anirudh Sriram, Pratyush Kumar, Mitesh M Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention heads are characteristic of Transformer models and have been
well studied for interpretability and pruning. In this work, we demonstrate an
altogether different utility of attention heads, namely for adversarial
detection. Specifically, we propose a method to construct input-specific
attention subnetworks (IAS) from which we extract three features to
discriminate between authentic and adversarial inputs. The resultant detector
significantly improves (by over 7.5%) the state-of-the-art adversarial
detection accuracy for the BERT encoder on 10 NLU datasets with 11 different
adversarial attack types. We also demonstrate that our method (a) is more
accurate for larger models which are likely to have more spurious correlations
and thus vulnerable to adversarial attack, and (b) performs well even with
modest training sets of adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of ACL 2022, 14 pages, 6 Tables and 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing the accuracy and resolution of precipitation forecasts using
  deep generative models <span class="chip">AISTATS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilan Price, Stephan Rasp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately forecasting extreme rainfall is notoriously difficult, but is also
ever more crucial for society as climate change increases the frequency of such
extremes. Global numerical weather prediction models often fail to capture
extremes, and are produced at too low a resolution to be actionable, while
regional, high-resolution models are hugely expensive both in computation and
labour. In this paper we explore the use of deep generative models to
simultaneously correct and downscale (super-resolve) global ensemble forecasts
over the Continental US. Specifically, using fine-grained radar observations as
our ground truth, we train a conditional Generative Adversarial Network --
coined CorrectorGAN -- via a custom training procedure and augmented loss
function, to produce ensembles of high-resolution, bias-corrected forecasts
based on coarse, global precipitation forecasts in addition to other relevant
meteorological fields. Our model outperforms an interpolation baseline, as well
as super-resolution-only and CNN-based univariate methods, and approaches the
performance of an operational regional high-resolution model across an array of
established probabilistic metrics. Crucially, CorrectorGAN, once trained,
produces predictions in seconds on a single machine. These results raise
exciting questions about the necessity of regional models, and whether
data-driven downscaling and correction methods can be transferred to data-poor
regions that so far have had no access to high-resolution forecasts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AISTATS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Fully Distributed Federated Learning with Adaptive Local Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Georgatos, Christos Mavrokefalidis, Kostas Berberidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, data-driven, machine and deep learning approaches have provided
unprecedented performance in various complex tasks, including image
classification and object detection, and in a variety of application areas,
like autonomous vehicles, medical imaging and wireless communications.
Traditionally, such approaches have been deployed, along with the involved
datasets, on standalone devices. Recently, a shift has been observed towards
the so-called Edge Machine Learning, in which centralized architectures are
adopted that allow multiple devices with local computational and storage
resources to collaborate with the assistance of a centralized server. The
well-known federated learning approach is able to utilize such architectures by
allowing the exchange of only parameters with the server, while keeping the
datasets private to each contributing device. In this work, we propose a fully
distributed, diffusion-based learning algorithm that does not require a central
server and propose an adaptive combination rule for the cooperation of the
devices. By adopting a classification task on the MNIST dataset, the efficacy
of the proposed algorithm over corresponding counterparts is demonstrated via
the reduction of the number of collaboration rounds required to achieve an
acceptable accuracy level in non- IID dataset scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> PEAR: Personalized Re-ranking with Contextualized <span class="highlight-title">Transformer</span> for
  Recommendation <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Jieming Zhu, Weiwen Liu, Liangcai Su, Guohao Cai, <span class="highlight-author">Qi Zhang</span>, Ruiming Tang, Xi Xiao, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of recommender systems is to provide ordered item lists to users
that best match their interests. As a critical task in the recommendation
pipeline, re-ranking has received increasing attention in recent years. In
contrast to conventional ranking models that score each item individually,
re-ranking aims to explicitly model the mutual influences among items to
further refine the ordering of items given an initial ranking list. In this
paper, we present a personalized re-ranking model (dubbed PEAR) based on
contextualized transformer. PEAR makes several major improvements over the
existing methods. Specifically, PEAR not only captures feature-level and
item-level interactions, but also models item contexts from both the initial
ranking list and the historical clicked item list. In addition to item-level
ranking score prediction, we also augment the training of PEAR with a
list-level classification task to assess users' satisfaction on the whole
ranking list. Experimental results on both public and production datasets have
shown the superior effectiveness of PEAR compared to the previous re-ranking
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Node Representation Learning in Graph via Node-to-Neighbourhood Mutual
  Information Maximization <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dong, Junsheng Wu, Yi Luo, Zongyuan Ge, Peng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key towards learning informative node representations in graphs lies in
how to gain contextual information from the neighbourhood. In this work, we
present a simple-yet-effective self-supervised node representation learning
strategy via directly maximizing the mutual information between the hidden
representations of nodes and their neighbourhood, which can be theoretically
justified by its link to graph smoothing. Following InfoNCE, our framework is
optimized via a surrogate contrastive loss, where the positive selection
underpins the quality and efficiency of representation learning. To this end,
we propose a topology-aware positive sampling strategy, which samples positives
from the neighbourhood by considering the structural dependencies between nodes
and thus enables positive selection upfront. In the extreme case when only one
positive is sampled, we fully avoid expensive neighbourhood aggregation. Our
methods achieve promising performance on various node classification datasets.
It is also worth mentioning by applying our loss function to MLP based node
encoders, our methods can be orders of faster than existing solutions. Our
codes and supplementary materials are available at
https://github.com/dongwei156/n2n.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper is accepted to CVPR 2022. Codes and supplementary materials are
  available at https://github.com/dongwei156/n2n</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Distinguishers for Negation-Limited Weak Pseudorandom Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihuai Chen, Siyao Guo, Qian Li, Chengyu Lin, Xiaoming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how to distinguish circuits with $\log k$ negations (a.k.a
$k$-monotone functions) from uniformly random functions in
$\exp\left(\tilde{O}\left(n^{1/3}k^{2/3}\right)\right)$ time using random
samples. The previous best distinguisher, due to the learning algorithm by
Blais, Cannone, Oliveira, Servedio, and Tan (RANDOM'15), requires
$\exp\big(\tilde{O}(n^{1/2} k)\big)$ time.
  Our distinguishers are based on Fourier analysis on \emph{slices of the
Boolean cube}. We show that some "middle" slices of negation-limited circuits
have strong low-degree Fourier concentration and then we apply a variation of
the classic Linial, Mansour, and Nisan "Low-Degree algorithm" (JACM'93) on
slices. Our techniques also lead to a slightly improved weak learner for
negation limited circuits under the uniform distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Characteristic Learning Method with Micro-Doppler Signatures for
  Pedestrian Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xiang, Yu Huang, Haodong Xu, Guangbo Zhang, Wenyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of pedestrians using radar micro-Doppler signatures has
become a hot topic in recent years. In this paper, we propose a
multi-characteristic learning (MCL) model with clusters to jointly learn
discrepant pedestrian micro-Doppler signatures and fuse the knowledge learned
from each cluster into final decisions. Time-Doppler spectrogram (TDS) and
signal statistical features extracted from FMCW radar, as two categories of
micro-Doppler signatures, are used in MCL to learn the micro-motion information
inside pedestrians' free walking patterns. The experimental results show that
our model achieves a higher accuracy rate and is more stable for pedestrian
identification than other studies, which make our model more practical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kogkalidis, Michael Moortgat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The syntactic categories of categorial grammar formalisms are structured
units made of smaller, indivisible primitives, bound together by the underlying
grammar's category formation rules. In the trending approach of constructive
supertagging, neural models are increasingly made aware of the internal
category structure, which in turn enables them to more reliably predict rare
and out-of-vocabulary categories, with significant implications for grammars
previously deemed too complex to find practical use. In this work, we revisit
constructive supertagging from a graph-theoretic perspective, and propose a
framework based on heterogeneous dynamic graph convolutions aimed at exploiting
the distinctive structure of a supertagger's output space. We test our approach
on a number of categorial grammar datasets spanning different languages and
grammar formalisms, achieving substantial improvements over previous state of
the art scores. Code will be made available at
https://github.com/konstantinosKokos/dynamic-graph-supertagging
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages plus references, unpublished preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Competition: What Makes Joint Training of <span class="highlight-title">Multi-modal</span> Network
  Fail in Deep Learning? (Provably) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of deep multi-modal learning in practice, it
has not been well-explained in theory. Recently, it has been observed that the
best uni-modal network outperforms the jointly trained multi-modal network,
which is counter-intuitive since multiple signals generally bring more
information. This work provides a theoretical explanation for the emergence of
such performance gap in neural networks for the prevalent joint training
framework. Based on a simplified data distribution that captures the realistic
property of multi-modal data, we prove that for the multi-modal late-fusion
network with (smoothed) ReLU activation trained jointly by gradient descent,
different modalities will compete with each other. The encoder networks will
learn only a subset of modalities. We refer to this phenomenon as modality
competition. The losing modalities, which fail to be discovered, are the
origins where the sub-optimality of joint training comes from. Experimentally,
we illustrate that modality competition matches the intrinsic behavior of
late-fusion joint training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Driven Deep Learning for Computational Magnetic Resonance
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerstin Hammernik, Thomas Küstner, Burhaneddin Yaman, Zhengnan Huang, Daniel Rueckert, Florian Knoll, Mehmet Akçakaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-driven deep learning methods have emerged as a powerful tool for
computational magnetic resonance imaging (MRI) problems, pushing reconstruction
performance to new limits. This article provides an overview of the recent
developments in incorporating physics information into learning-based MRI
reconstruction. We consider inverse problems with both linear and non-linear
forward models for computational MRI, and review the classical approaches for
solving these. We then focus on physics-driven deep learning approaches,
covering physics-driven loss functions, plug-and-play methods, generative
models, and unrolled networks. We highlight domain-specific challenges such as
real- and complex-valued building blocks of neural networks, and translational
applications in MRI with linear and non-linear forward models. Finally, we
discuss common issues and open challenges, and draw connections to the
importance of physics-driven learning when combined with other downstream tasks
in the medical imaging pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biceph-Net: A robust and lightweight framework for the diagnosis of
  Alzheimer's disease using 2D-MRI scans and deep similarity learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. H. Rashid, A. Gupta, J. Gupta, M. Tanveer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the
significant causes of death in the elderly population. Many deep learning
techniques have been proposed to diagnose AD using Magnetic Resonance Imaging
(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is
challenging as the inter-slice information gets lost. To this end, we propose a
novel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D
MRI scans that model both the intra-slice and inter-slice information.
Biceph-Net has been experimentally shown to perform similar to other
Spatio-temporal neural networks while being computationally more efficient.
Biceph-Net is also superior in performance compared to vanilla 2D convolutional
neural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has
an inbuilt neighbourhood-based model interpretation feature that can be
exploited to understand the classification decision taken by the network.
Biceph-Net experimentally achieves a test accuracy of 100% in the
classification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive
Impairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Censor by Noisy Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Chopra, Abhinav Java, Abhishek Singh, Vivek Sharma, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point clouds are an increasingly ubiquitous input modality and the raw signal
can be efficiently processed with recent progress in deep learning. This signal
may, often inadvertently, capture sensitive information that can leak semantic
and geometric properties of the scene which the data owner does not want to
share. The goal of this work is to protect sensitive information when learning
from point clouds; by censoring the sensitive information before the point
cloud is released for downstream tasks. Specifically, we focus on preserving
utility for perception tasks while mitigating attribute leakage attacks. The
key motivating insight is to leverage the localized saliency of perception
tasks on point clouds to provide good privacy-utility trade-offs. We realize
this through a mechanism called Censoring by Noisy Sampling (CBNS), which is
composed of two modules: i) Invariant Sampler: a differentiable point-cloud
sampler which learns to remove points invariant to utility and ii) Noisy
Distorter: which learns to distort sampled points to decouple the sensitive
information from utility, and mitigate privacy leakage. We validate the
effectiveness of CBNS through extensive comparisons with state-of-the-art
baselines and sensitivity analyses of key design choices. Results show that
CBNS achieves superior privacy-utility trade-offs on multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Adaptive Gradient Method with Energy and Momentum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailiang Liu, Xuping Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel algorithm for gradient-based optimization of stochastic
objective functions. The method may be seen as a variant of SGD with momentum
equipped with an adaptive learning rate automatically adjusted by an 'energy'
variable. The method is simple to implement, computationally efficient, and
well suited for large-scale machine learning problems. The method exhibits
unconditional energy stability for any size of the base learning rate. We
provide a regret bound on the convergence rate under the online convex
optimization framework. We also establish the energy-dependent convergence rate
of the algorithm to a stationary point in the stochastic non-convex setting. In
addition, a sufficient condition is provided to guarantee a positive lower
threshold for the energy variable. Our experiments demonstrate that the
algorithm converges fast while generalizing better than or as well as SGD with
momentum in training deep neural networks, and compares also favorably to Adam.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Emulation Framework for Fire Front Spread <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Bolt, Joel Janek Dabrowski, Carolyn Huston, Petra Kuhnert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting bushfire spread is an important element in fire prevention and
response efforts. Empirical observations of bushfire spread can be used to
estimate fire response under certain conditions. These observations form
rate-of-spread models, which can be used to generate simulations. We use
machine learning to drive the emulation approach for bushfires and show that
emulation has the capacity to closely reproduce simulated fire-front data. We
present a preliminary emulator approach with the capacity for fast emulation of
complex simulations. Large numbers of predictions can then be generated as part
of ensemble estimation techniques, which provide more robust and reliable
forecasts of stochastic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Machine Learning and the Physical Sciences Workshop, NeurIPS, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-EDM: Early Detection Model for 3D-Printer Faults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harim Jeong, Joo Hun Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of 3D printers in different price ranges and sizes, they are
no longer just for professionals. However, it is still challenging to use a 3D
printer perfectly. Especially, in the case of the Fused Deposition Method, it
is very difficult to perform with accurate calibration. Previous studies have
suggested that these problems can be detected using sensor data and image data
with machine learning methods. However, there are difficulties to apply the
proposed method due to extra installation of additional sensors. Considering
actual use in the future, we focus on generating the lightweight early
detection model with easily collectable data. Proposed early detection model
through Convolutional Neural Network shows significant fault classification
accuracy with 96.72% for the binary classification task, and 93.38% for
multi-classification task respectively. By this research, we hope that general
users of 3D printers can use the printer accurately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KSII The 13th International Conference on
  Internet(ICONI)2021. Copyright 2021 KSII</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out of Distribution Detection, Generalization, and Robustness Triangle
  with Maximum Probability Theorem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Emad Marvasti, Ehsan Emad Marvasti, Ulas Bagci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maximum Probability Framework, powered by Maximum Probability Theorem, is a
recent theoretical development, aiming to formally define probabilistic models,
guiding development of objective functions, and regularization of probabilistic
models. MPT uses the probability distribution that the models assume on random
variables to provide an upper bound on probability of the model. We apply MPT
to challenging out-of-distribution (OOD) detection problems in computer vision
by incorporating MPT as a regularization scheme in training of CNNs and their
energy based variants. We demonstrate the effectiveness of the proposed method
on 1080 trained models, with varying hyperparameters, and conclude that MPT
based regularization strategy both stabilizes and improves the generalization
and robustness of base models in addition to improved OOD performance on
CIFAR10, CIFAR100 and MNIST datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein Distributionally Robust Optimization via Wasserstein
  Barycenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Tsz-Kit Lau, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications in statistics and machine learning, the availability of
data samples from multiple sources has become increasingly prevalent. On the
other hand, in distributionally robust optimization, we seek data-driven
decisions which perform well under the most adverse distribution from a nominal
distribution constructed from data samples within a certain distance of
probability distributions. However, it remains unclear how to achieve such
distributional robustness when data samples from multiple sources are
available. In this paper, we propose constructing the nominal distribution in
Wasserstein distributionally robust optimization problems through the notion of
Wasserstein barycenter as an aggregation of data samples from multiple sources.
Under specific choices of the loss function, the proposed formulation admits a
tractable reformulation as a finite convex program, with powerful finite-sample
and asymptotic guarantees. We illustrate our proposed method through concrete
examples with nominal distributions of location-scatter families and
distributionally robust maximum likelihood estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Should Machine Learning Models Report to Us When They Are Clueless? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roozbeh Yousefzadeh, Xuenan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The right to AI explainability has consolidated as a consensus in the
research community and policy-making. However, a key component of
explainability has been missing: extrapolation, which describes the extent to
which AI models can be clueless when they encounter unfamiliar samples (i.e.,
samples outside a convex hull of their training sets, as we will explain down
below). We report that AI models extrapolate outside their range of familiar
data, frequently and without notifying the users and stakeholders. Knowing
whether a model has extrapolated or not is a fundamental insight that should be
included in explaining AI models in favor of transparency and accountability.
Instead of dwelling on the negatives, we offer ways to clear the roadblocks in
promoting AI transparency. Our analysis commentary accompanying practical
clauses useful to include in AI regulations such as the National AI Initiative
Act in the US and the AI Act by the European Commission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel VQ-VAEs for Improved Pixel Art Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Saravanan, Matthew Guzdial
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has had a great deal of success in image processing.
However, the focus of this work has largely been on realistic images, ignoring
more niche art styles such as pixel art. Additionally, many traditional machine
learning models that focus on groups of pixels do not work well with pixel art,
where individual pixels are important. We propose the Pixel VQ-VAE, a
specialized VQ-VAE model that learns representations of pixel art. We show that
it outperforms other models in both the quality of embeddings as well as
performance on downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix Completion with Heterogonous Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilqar Ramazanli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix completion problem has been studied extensively under the condition
that each entry has uniform observation cost. And the problem has been explored
withing adaptive or nonadaptive, exact or estimation categories. In this paper,
we propose a method that approaches to problem in a different category that,
price of checking different entries varies accross the matrix. We study under
two type of cost model, first one is each column has different cost, but
withing a column, every entry has different cost of observation. Also, another
cost model is each entry within the matrix are different no matter if they are
in the same column or row.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2002.02431</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Balloch, Zhiyu Lin, Mustafa Hussain, Aarun Srinivas, Robert Wright, Xiangyu Peng, Julia Kim, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robust body of reinforcement learning techniques have been developed to
solve complex sequential decision making problems. However, these methods
assume that train and evaluation tasks come from similarly or identically
distributed environments. This assumption does not hold in real life where
small novel changes to the environment can make a previously learned policy
fail or introduce simpler solutions that might never be found. To that end we
explore the concept of {\em novelty}, defined in this work as the sudden change
to the mechanics or properties of environment. We provide an ontology of for
novelties most relevant to sequential decision making, which distinguishes
between novelties that affect objects versus actions, unary properties versus
non-unary relations, and the distribution of solutions to a task. We introduce
NovGrid, a novelty generation framework built on MiniGrid, acting as a toolkit
for rapidly developing and evaluating novelty-adaptation-enabled reinforcement
learning techniques. Along with the core NovGrid we provide exemplar novelties
aligned with our ontology and instantiate them as novelty templates that can be
applied to many MiniGrid-compliant environments. Finally, we present a set of
metrics built into our framework for the evaluation of
novelty-adaptation-enabled machine-learning techniques, and show
characteristics of a baseline RL model using these metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, AAAI Spring Symposium 2022 on Designing
  Artificial Intelligence for Open Worlds (Long Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Optical Controlling Environment and Reinforcement Learning Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abulikemu Abuduweili, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has the potential to address various scientific
problems. In this paper, we implement an optics simulation environment for
reinforcement learning based controllers. The environment incorporates
nonconvex and nonlinear optical phenomena as well as more realistic
time-dependent noise. Then we provide the benchmark results of several
state-of-the-art reinforcement learning algorithms on the proposed simulation
environment. In the end, we discuss the difficulty of controlling the
real-world optical environment with reinforcement learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study on Learning and Improving the Search Objective for
  Unsupervised Paraphrasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Steven Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in unsupervised text generation has been gaining attention over the
years. One recent approach is local search towards a heuristically defined
objective, which specifies language fluency, semantic meanings, and other
task-specific attributes. Search in the sentence space is realized by
word-level edit operations including insertion, replacement, and deletion.
However, such objective function is manually designed with multiple components.
Although previous work has shown maximizing this objective yields good
performance in terms of true measure of success (i.e. BLEU and iBLEU), the
objective landscape is considered to be non-smooth with significant noises,
posing challenges for optimization. In this dissertation, we address the
research problem of smoothing the noise in the heuristic search objective by
learning to model the search dynamics. Then, the learned model is combined with
the original objective function to guide the search in a bootstrapping fashion.
Experimental results show that the learned models combined with the original
search objective can indeed provide a smoothing effect, improving the search
performance by a small margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast on-line signature recognition based on VQ with time modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan-Manuel Pascual-Gaspar, Marcos Faundez-Zanuy, Carlos Vivaracho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a multi-section vector quantization approach for on-line
signature recognition. We have used the MCYT database, which consists of 330
users and 25 skilled forgeries per person performed by 5 different impostors.
This database is larger than those typically used in the literature.
Nevertheless, we also provide results from the SVC database.
  Our proposed system outperforms the winner of SVC with a reduced
computational requirement, which is around 47 times lower than DTW. In
addition, our system improves the database storage requirements due to vector
compression, and is more privacy-friendly as it is not possible to recover the
original signature using the codebooks. Experimental results with MCYT provide
a 99.76% identification rate and 2.46% EER (skilled forgeries and individual
threshold). Experimental results with SVC are 100% of identification rate and
0% (individual threshold) and 0.31% (general threshold) when using a
two-section VQ approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, published in Engineering Applications of Artificial
  Intelligence, Volume 24, Issue 2, 2011, Pages 368-377, ISSN 0952-1976</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Efficient Exploration through Human Seeded Rapidly-exploring
  Random Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Zuo, Logan Schick, Matthew Gombolay, Nakul Gopalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern day computer games have extremely large state and action spaces. To
detect bugs in these games' models, human testers play the games repeatedly to
explore the game and find errors in the games. Such game play is exhaustive and
time consuming. Moreover, since robotics simulators depend on similar methods
of model specification and debugging, the problem of finding errors in the
model is of interest for the robotics community to ensure robot behaviors and
interactions are consistent in simulators. Previous methods have used
reinforcement learning and search based methods including Rapidly-exploring
Random Trees (RRT) to explore a game's state-action space to find bugs.
However, such search and exploration based methods are not efficient at
exploring the state-action space without a pre-defined heuristic. In this work
we attempt to combine a human-tester's expertise in solving games, and the
exhaustiveness of RRT to search a game's state space efficiently with high
coverage. This paper introduces human-seeded RRT (HS-RRT) and
behavior-cloning-assisted RRT (CA-RRT) in testing the number of game states
searched and the time taken to explore those game states. We compare our
methods to an existing weighted RRT baseline for game exploration testing
studied. We find HS-RRT and CA-RRT both explore more game states in fewer tree
expansions/iterations when compared to the existing baseline. In each test,
CA-RRT reached more states on average in the same number of iterations as RRT.
In our tested environments, CA-RRT was able to reach the same number of states
as RRT by more than 5000 fewer iterations on average, almost a 50% reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HRI 2022 Workshop - MLHRC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mokey: Enabling Narrow Fixed-Point Inference for Out-of-the-Box
  Floating-Point <span class="highlight-title">Transformer</span> Models <span class="chip">ISCA '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hadi Zadeh, Mostafa Mahmoud, Ameer Abdelhadi, Andreas Moshovos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasingly larger and better Transformer models keep advancing
state-of-the-art accuracy and capability for Natural Language Processing
applications. These models demand more computational power, storage, and
energy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit
floating-point transformer models by quantizing all values to 4-bit indexes
into dictionaries of representative 16-bit fixed-point centroids. Mokey does
not need fine-tuning, an essential feature as often the training resources or
datasets are not available to many. Exploiting the range of values that
naturally occur in transformer models, Mokey selects centroid values to also
fit an exponential curve. This unique feature enables Mokey to replace the bulk
of the original multiply-accumulate operations with narrow 3b fixed-point
additions resulting in an area- and energy-efficient hardware accelerator
design. Over a set of state-of-the-art transformer models, the Mokey
accelerator delivers an order of magnitude improvements in energy efficiency
over a Tensor Cores-based accelerator while improving performance by at least
$4\times$ and as much as $15\times$ depending on the model and on-chip
buffering capacity. Optionally, Mokey can be used as a memory compression
assist for any other accelerator, transparently stashing wide floating-point or
fixed-point activations or weights into narrow 4-bit indexes. Mokey proves
superior to prior state-of-the-art quantization methods for Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 49th IEEE/ACM International Symposium on Computer
  Architecture (ISCA '22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in
  Deep Metric Learning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Dullerud, Karsten Roth, Kimia Hamidieh, Nicolas Papernot, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep metric learning (DML) enables learning with less supervision through its
emphasis on the similarity structure of representations. There has been much
work on improving generalization of DML in settings like zero-shot retrieval,
but little is known about its implications for fairness. In this paper, we are
the first to evaluate state-of-the-art DML methods trained on imbalanced data,
and to show the negative impact these representations have on minority subgroup
performance when used for downstream tasks. In this work, we first define
fairness in DML through an analysis of three properties of the representation
space -- inter-class alignment, intra-class alignment, and uniformity -- and
propose finDML, the fairness in non-balanced DML benchmark to characterize
representation fairness. Utilizing finDML, we find bias in DML representations
to propagate to common downstream classification tasks. Surprisingly, this bias
is propagated even when training data in the downstream task is re-balanced. To
address this problem, we present Partial Attribute De-correlation (PARADE) to
de-correlate feature representations from sensitive attributes and reduce
performance gaps between subgroups in both embedding space and downstream
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Bayesian Optimization for Biological Sequence Design with
  Denoising Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, Andrew Gordon Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a gold standard for query-efficient continuous
optimization. However, its adoption for drug and antibody sequence design has
been hindered by the discrete, high-dimensional nature of the decision
variables. We develop a new approach (LaMBO) which jointly trains a denoising
autoencoder with a discriminative multi-task Gaussian process head, enabling
gradient-based optimization of multi-objective acquisition functions in the
latent space of the autoencoder. These acquisition functions allow LaMBO to
balance the explore-exploit trade-off over multiple design rounds, and to
balance objective tradeoffs by optimizing sequences at many different points on
the Pareto frontier. We evaluate LaMBO on a small-molecule task based on the
ZINC dataset and introduce a new large-molecule task targeting fluorescent
proteins. In our experiments, LaMBO outperforms genetic optimizers and does not
require a large pretraining corpus, demonstrating that Bayesian optimization is
practical and effective for biological sequence design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Model Aggregation for Fast and Robust Federated Learning in
  Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung T. Nguyen, H. Vincent Poor, Mung Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a prime candidate for distributed machine learning at
the network edge due to the low communication complexity and privacy protection
among other attractive properties. However, existing algorithms face issues
with slow convergence and/or robustness of performance due to the considerable
heterogeneity of data distribution, computation and communication capability at
the edge. In this work, we tackle both of these issues by focusing on the key
component of model aggregation in federated learning systems and studying
optimal algorithms to perform this task. Particularly, we propose a contextual
aggregation scheme that achieves the optimal context-dependent bound on loss
reduction in each round of optimization. The aforementioned context-dependent
bound is derived from the particular participating devices in that round and an
assumption on smoothness of the overall loss function. We show that this
aggregation leads to a definite reduction of loss function at every round.
Furthermore, we can integrate our aggregation with many existing algorithms to
obtain the contextual versions. Our experimental results demonstrate
significant improvements in convergence speed and robustness of the contextual
versions compared to the original algorithms. We also consider different
variants of the contextual aggregation and show robust performance even in the
most extreme settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards All-Purpose Domain Adaptation Under Confounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin McCarter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current domain adaptation methods address the problems of covariate shift or
label shift, but are not applicable to the setting where they occur
simultaneously and interact with each other. In this paper, we propose an
assumption, confounded shift, to begin to address this problem. We also propose
a framework for this task, based on minimizing the expected divergence between
the source and target conditional distributions. Within this framework, we
propose using the reverse KL divergence, demonstrating the use of both
parametric linear Gaussian and nonparametric nonlinear Gaussian Process
estimators of the conditional distribution. We also propose using the Maximum
Mean Discrepancy (MMD) within our framework. To make confounded domain
adaptation with the MMD effective, we propose an intelligent dynamic strategy
for choosing the kernel bandwidth, which may be of independent interest even
outside of the confounded shift context. Finally, we show that our approach is
advantageous on a variety of synthetic and real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Multi-Antenna Frequency-Selective Channels via Meta-Learned
  Linear Filters based on Long-Short Term Channel Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwoo Park, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An efficient data-driven prediction strategy for multi-antenna
frequency-selective channels must operate based on a small number of pilot
symbols. This paper proposes novel channel prediction algorithms that address
this goal by integrating transfer and meta-learning with a reduced-rank
parametrization of the channel. The proposed methods optimize linear predictors
by utilizing data from previous frames, which are generally characterized by
distinct propagation characteristics, in order to enable fast training on the
time slots of the current frame. The proposed predictors rely on a novel
long-short-term decomposition (LSTD) of the linear prediction model that
leverages the disaggregation of the channel into long-term space-time
signatures and fading amplitudes. We first develop predictors for
single-antenna frequency-flat channels based on transfer/meta-learned quadratic
regularization. Then, we introduce transfer and meta-learning algorithms for
LSTD-based prediction models that build on equilibrium propagation (EP) and
alternating least squares (ALS). Numerical results under the 3GPP 5G standard
channel model demonstrate the impact of transfer and meta-learning on reducing
the number of pilots for channel prediction, as well as the merits of the
proposed LSTD parametrization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for journal publication, subsumes (arXiv:2110.00414)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Challenges of Continuous <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senthil Purushwalkam, Pedro Morgado, Abhinav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Understanding the Influence of Controllable Factors with a Feature
  Attribution Algorithm: a Medical Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veera Raghava Reddy Kovvuri, Siyuan Liu, Monika Seisenberger, Berndt Müller, Xiuyi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature attribution XAI algorithms enable their users to gain insight into
the underlying patterns of large datasets through their feature importance
calculation. Existing feature attribution algorithms treat all features in a
dataset homogeneously, which may lead to misinterpretation of consequences of
changing feature values. In this work, we consider partitioning features into
controllable and uncontrollable parts and propose the Controllable fActor
Feature Attribution (CAFA) approach to compute the relative importance of
controllable features. We carried out experiments applying CAFA to two existing
datasets and our own COVID-19 non-pharmaceutical control measures dataset.
Experimental results show that with CAFA, we are able to exclude influences
from uncontrollable features in our explanation while keeping the full dataset
for prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Classifier Conservativeness and Robustness by Polynomiality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Marco Loog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We illustrate the detrimental effect, such as overconfident decisions, that
exponential behavior can have in methods like classical LDA and logistic
regression. We then show how polynomiality can remedy the situation. This,
among others, leads purposefully to random-level performance in the tails, away
from the bulk of the training data. A directly related, simple, yet important
technical novelty we subsequently present is softRmax: a reasoned alternative
to the standard softmax function employed in contemporary (deep) neural
networks. It is derived through linking the standard softmax to Gaussian
class-conditional models, as employed in LDA, and replacing those by a
polynomial alternative. We show that two aspects of softRmax, conservativeness
and inherent gradient regularization, lead to robustness against adversarial
attacks without gradient obfuscation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Possibility Before Utility: Learning And Using Hierarchical Affordances <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robby Costales, Shariq Iqbal, Fei Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning algorithms struggle on tasks with complex hierarchical
dependency structures. Humans and other intelligent agents do not waste time
assessing the utility of every high-level action in existence, but instead only
consider ones they deem possible in the first place. By focusing only on what
is feasible, or "afforded", at the present moment, an agent can spend more time
both evaluating the utility of and acting on what matters. To this end, we
present Hierarchical Affordance Learning (HAL), a method that learns a model of
hierarchical affordances in order to prune impossible subtasks for more
effective learning. Existing works in hierarchical reinforcement learning
provide agents with structural representations of subtasks but are not
affordance-aware, and by grounding our definition of hierarchical affordances
in the present state, our approach is more flexible than the multitude of
approaches that ground their subtask dependencies in a symbolic history. While
these logic-based methods often require complete knowledge of the subtask
hierarchy, our approach is able to utilize incomplete and varying symbolic
specifications. Furthermore, we demonstrate that relative to
non-affordance-aware methods, HAL agents are better able to efficiently learn
complex tasks, navigate environment stochasticity, and acquire diverse skills
in the absence of extrinsic supervision -- all of which are hallmarks of human
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-efficient Iterative Lower Bound Optimization of Deep Reactive
  Policies for Planning in Continuous MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siow Meng Low, Akshat Kumar, Scott Sanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have enabled optimization of deep reactive
policies (DRPs) for continuous MDP planning by encoding a parametric policy as
a deep neural network and exploiting automatic differentiation in an end-to-end
model-based gradient descent framework. This approach has proven effective for
optimizing DRPs in nonlinear continuous MDPs, but it requires a large number of
sampled trajectories to learn effectively and can suffer from high variance in
solution quality. In this work, we revisit the overall model-based DRP
objective and instead take a minorization-maximization perspective to
iteratively optimize the DRP w.r.t. a locally tight lower-bounded objective.
This novel formulation of DRP learning as iterative lower bound optimization
(ILBO) is particularly appealing because (i) each step is structurally easier
to optimize than the overall objective, (ii) it guarantees a monotonically
improving objective under certain theoretical conditions, and (iii) it reuses
samples between iterations thus lowering sample complexity. Empirical
evaluation confirms that ILBO is significantly more sample-efficient than the
state-of-the-art DRP planner and consistently produces better solution quality
with lower variance. We additionally demonstrate that ILBO generalizes well to
new problem instances (i.e., different initial states) without requiring
retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> UniPELT: A Unified Framework for Parameter-Efficient Language Model
  Tuning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, <span class="highlight-author">Jiawei Han</span>, Wen-tau Yih, Madian Khabsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent parameter-efficient language model tuning (PELT) methods manage to
match the performance of fine-tuning with much fewer trainable parameters and
perform especially well when training data is limited. However, different PELT
methods may perform rather differently on the same task, making it nontrivial
to select the most appropriate method for a specific task, especially
considering the fast-growing number of new PELT methods and tasks. In light of
model diversity and the difficulty of model selection, we propose a unified
framework, UniPELT, which incorporates different PELT methods as submodules and
learns to activate the ones that best suit the current data or task setup via
gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%
gains compared to the best individual PELT method that it incorporates and even
outperforms fine-tuning under different setups. Moreover, UniPELT generally
surpasses the upper bound that takes the best performance of all its submodules
used individually on each task, indicating that a mixture of multiple PELT
methods may be inherently more effective than single methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards an Understanding of Default Policies in Multitask Policy
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02994v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02994v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Moskovitz, Michael Arbel, Jack Parker-Holder, Aldo Pacchiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of the recent success of deep reinforcement learning has been driven by
regularized policy optimization (RPO) algorithms with strong performance across
multiple domains. In this family of methods, agents are trained to maximize
cumulative reward while penalizing deviation in behavior from some reference,
or default policy. In addition to empirical success, there is a strong
theoretical foundation for understanding RPO methods applied to single tasks,
with connections to natural gradient, trust region, and variational approaches.
However, there is limited formal understanding of desirable properties for
default policies in the multitask setting, an increasingly important domain as
the field shifts towards training more generally capable agents. Here, we take
a first step towards filling this gap by formally linking the quality of the
default policy to its effect on optimization. Using these results, we then
derive a principled RPO algorithm for multitask learning with strong
performance guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Top $K$ Ranking for Multi-Armed Bandit with Noisy Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evrard Garcelon, Vashist Avadhanula, Alessandro Lazaric, Matteo Pirotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a multi-armed bandit setting where, at the beginning of each
round, the learner receives noisy independent, and possibly biased,
\emph{evaluations} of the true reward of each arm and it selects $K$ arms with
the objective of accumulating as much reward as possible over $T$ rounds. Under
the assumption that at each round the true reward of each arm is drawn from a
fixed distribution, we derive different algorithmic approaches and theoretical
guarantees depending on how the evaluations are generated. First, we show a
$\widetilde{O}(T^{2/3})$ regret in the general case when the observation
functions are a genearalized linear function of the true rewards. On the other
hand, we show that an improved $\widetilde{O}(\sqrt{T})$ regret can be derived
when the observation functions are noisy linear functions of the true rewards.
Finally, we report an empirical validation that confirms our theoretical
findings, provides a thorough comparison to alternative approaches, and further
supports the interest of this setting in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static
  Models by Fitting Feature-level Space-time Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR 2022 (Source Code:
  https://github.com/jx-zhong-for-academic-purpose/Kinet )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Models for information propagation on graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver R. A. Dunbar, Charles M. Elliott, Lisa Maria Kreusser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and unify classes of different models for information propagation
over graphs. In a first class, propagation is modeled as a wave which emanates
from a set of known nodes at an initial time, to all other unknown nodes at
later times with an ordering determined by the arrival time of the information
wave front. A second class of models is based on the notion of a travel time
along paths between nodes. The time of information propagation from an initial
known set of nodes to a node is defined as the minimum of a generalized travel
time over subsets of all admissible paths. A final class is given by imposing a
local equation of an eikonal form at each unknown node, with boundary
conditions at the known nodes. The solution value of the local equation at a
node is coupled to those of neighbouring nodes with lower values. We provide
precise formulations of the model classes and prove equivalences between them.
Motivated by the connection between first arrival time model and the eikonal
equation in the continuum setting, we derive mean field limits for graphs based
on uniform grids in Euclidean space under grid refinement. For a specific
parameter setting, we demonstrate that the solution on the grid approximates
the Euclidean distance, and illustrate the use of front propagation on graphs
to semi-supervised learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-less Neural Networks: Teaching Old MLPs New Tricks via
  <span class="highlight-title">Distillation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichang Zhang, Yozen Liu, Yizhou Sun, Neil Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are popular for graph machine learning and have
shown great results on wide node classification tasks. Yet, they are less
popular for practical deployments in the industry owing to their scalability
challenges incurred by data dependency. Namely, GNN inference depends on
neighbor nodes multiple hops away from the target, and fetching them burdens
latency-constrained applications. Existing inference acceleration methods like
pruning and quantization can speed up GNNs by reducing
Multiplication-and-ACcumulation (MAC) operations, but the improvements are
limited given the data dependency is not resolved. Conversely, multi-layer
perceptrons (MLPs) have no graph dependency and infer much faster than GNNs,
even though they are less accurate than GNNs for node classification in
general. Motivated by these complementary strengths and weaknesses, we bring
GNNs and MLPs together via knowledge distillation (KD). Our work shows that the
performance of MLPs can be improved by large margins with GNN KD. We call the
distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference
graph dependency. We show that GLNNs with competitive accuracy infer faster
than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X.
Under a production setting involving both transductive and inductive
predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by
12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows
when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN
as a handy choice for latency-constrained applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Encrypted Linear Contextual Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.09927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.09927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evrard Garcelon, Vianney Perchet, Matteo Pirotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual bandit is a general framework for online learning in sequential
decision-making problems that has found application in a wide range of domains,
including recommendation systems, online advertising, and clinical trials.
  A critical aspect of bandit methods is that they require to observe the
contexts --i.e., individual or group-level data-- and rewards in order to solve
the sequential problem. The large deployment in industrial applications has
increased interest in methods that preserve the users' privacy. In this paper,
we introduce a privacy-preserving bandit framework based on homomorphic
encryption{\color{violet} which allows computations using encrypted data}. The
algorithm \textit{only} observes encrypted information (contexts and rewards)
and has no ability to decrypt it. Leveraging the properties of homomorphic
encryption, we show that despite the complexity of the setting, it is possible
to solve linear contextual bandits over encrypted data with a
$\widetilde{O}(d\sqrt{T})$ regret bound in any linear contextual bandit
problem, while keeping data encrypted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Neural Production Systems: Learning Rule-Governed Visual Dynamics <span class="chip">NeurIPS'21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.01937v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.01937v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Goyal, Aniket Didolkar, Nan Rosemary Ke, Charles Blundell, Philippe Beaudoin, Nicolas Heess, Michael Mozer, <span class="highlight-author">Yoshua Bengio</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual environments are structured, consisting of distinct objects or
entities. These entities have properties -- both visible and latent -- that
determine the manner in which they interact with one another. To partition
images into entities, deep-learning researchers have proposed structural
inductive biases such as slot-based architectures. To model interactions among
entities, equivariant graph neural nets (GNNs) are used, but these are not
particularly well suited to the task for two reasons. First, GNNs do not
predispose interactions to be sparse, as relationships among independent
entities are likely to be. Second, GNNs do not factorize knowledge about
interactions in an entity-conditional manner. As an alternative, we take
inspiration from cognitive science and resurrect a classic approach, production
systems, which consist of a set of rule templates that are applied by binding
placeholder variables in the rules to specific entities. Rules are scored on
their match to entities, and the best fitting rules are applied to update
entity properties. In a series of experiments, we demonstrate that this
architecture achieves a flexible, dynamic flow of control and serves to
factorize entity-specific and rule-based information. This disentangling of
knowledge achieves robust future-state prediction in rich visual environments,
outperforming state-of-the-art methods using GNNs, and allows for the
extrapolation from simple (few object) environments to more complex
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for Charged Particle Tracking on FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02048v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02048v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Elabd, Vesal Razavimaleki, Shi-Yu Huang, Javier Duarte, Markus Atkinson, Gage DeZoort, Peter Elmer, Scott Hauck, Jin-Xuan Hu, Shih-Chieh Hsu, Bo-Cheng Lai, Mark Neubauer, Isobel Ojalvo, Savannah Thais, Matthew Trahms
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The determination of charged particle trajectories in collisions at the CERN
Large Hadron Collider (LHC) is an important but challenging problem, especially
in the high interaction density conditions expected during the future
high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a
type of geometric deep learning algorithm that has successfully been applied to
this task by embedding tracker data as a graph -- nodes represent hits, while
edges represent possible track segments -- and classifying the edges as true or
fake track segments. However, their study in hardware- or software-based
trigger applications has been limited due to their large computational cost. In
this paper, we introduce an automated translation workflow, integrated into a
broader tool called $\texttt{hls4ml}$, for converting GNNs into firmware for
field-programmable gate arrays (FPGAs). We use this translation tool to
implement GNNs for charged particle tracking, trained using the TrackML
challenge dataset, on FPGAs with designs targeting different graph sizes, task
complexites, and latency/throughput requirements. This work could enable the
inclusion of charged particle tracking GNNs at the trigger level for HL-LHC
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 17 figures, 1 table, published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coupling streaming AI and HPC ensembles to achieve 100-1000x faster
  biomolecular simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.04797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.04797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Brace, Igor Yakushin, Heng Ma, Anda Trifan, Todd Munson, Ian Foster, Arvind Ramanathan, Hyungro Lee, Matteo Turilli, Shantenu Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of ML methods to dynamically steer ensemble-based simulations
promises significant improvements in the performance of scientific
applications. We present DeepDriveMD, a tool for a range of prototypical
ML-driven HPC simulation scenarios, and use it to quantify improvements in the
scientific performance of ML-driven ensemble-based applications. We discuss its
design and characterize its performance. Motivated by the potential for further
scientific improvements and applicability to more sophisticated physical
systems, we extend the design of DeepDriveMD to support stream-based
communication between simulations and learning methods. It demonstrates a 100x
speedup to fold proteins, and performs 1.6x more simulations per unit time,
improving resource utilization compared to the sequential framework.
Experiments are performed on leadership-class platforms, at scales of up to
O(1000) nodes, and for production workloads. We establish DeepDriveMD as a
high-performance framework for ML-driven HPC simulation scenarios, that
supports diverse simulation and ML back-ends, and which enables new scientific
insights by improving length- and time-scale accessed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">overview</span> of active learning methods for insurance with fairness
  appreciation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romuald Elie, Caroline Hillairet, François Hu, Marc Juillard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses and solves some challenges in the adoption of machine
learning in insurance with the democratization of model deployment. The first
challenge is reducing the labelling effort (hence focusing on the data quality)
with the help of active learning, a feedback loop between the model inference
and an oracle: as in insurance the unlabeled data is usually abundant, active
learning can become a significant asset in reducing the labelling cost. For
that purpose, this paper sketches out various classical active learning
methodologies before studying their empirical impact on both synthetic and real
datasets. Another key challenge in insurance is the fairness issue in model
inferences. We will introduce and integrate a post-processing fairness for
multi-class tasks in this active learning framework to solve these two issues.
Finally numerical experiments on unfair datasets highlight that the proposed
setup presents a good compromise between model precision and fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning in Observable POMDPs in Quasipolynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Golowich, Ankur Moitra, Dhruv Rohatgi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially Observable Markov Decision Processes (POMDPs) are a natural and
general model in reinforcement learning that take into account the agent's
uncertainty about its current state. In the literature on POMDPs, it is
customary to assume access to a planning oracle that computes an optimal policy
when the parameters are known, even though the problem is known to be
computationally hard. Almost all existing planning algorithms either run in
exponential time, lack provable performance guarantees, or require placing
strong assumptions on the transition dynamics under every possible policy. In
this work, we revisit the planning problem and ask: are there natural and
well-motivated assumptions that make planning easy?
  Our main result is a quasipolynomial-time algorithm for planning in
(one-step) observable POMDPs. Specifically, we assume that well-separated
distributions on states lead to well-separated distributions on observations,
and thus the observations are at least somewhat informative in each step.
Crucially, this assumption places no restrictions on the transition dynamics of
the POMDP; nevertheless, it implies that near-optimal policies admit
quasi-succinct descriptions, which is not true in general (under standard
hardness assumptions). Our analysis is based on new quantitative bounds for
filter stability -- i.e. the rate at which an optimal filter for the latent
state forgets its initialization. Furthermore, we prove matching hardness for
planning in observable POMDPs under the Exponential Time Hypothesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and
  Beyond <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chulhee Yun, Shashank Rajput, Suvrit Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In distributed learning, local SGD (also known as federated averaging) and
its simple baseline minibatch SGD are widely studied optimization methods. Most
existing analyses of these methods assume independent and unbiased gradient
estimates obtained via with-replacement sampling. In contrast, we study
shuffling-based variants: minibatch and local Random Reshuffling, which draw
stochastic gradients without replacement and are thus closer to practice. For
smooth functions satisfying the Polyak-{\L}ojasiewicz condition, we obtain
convergence bounds (in the large epoch regime) which show that these
shuffling-based variants converge faster than their with-replacement
counterparts. Moreover, we prove matching lower bounds showing that our
convergence analysis is tight. Finally, we propose an algorithmic modification
called synchronized shuffling that leads to convergence rates faster than our
lower bounds in near-homogeneous settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 camera-ready (selected for an oral presentation); 76 pages,
  3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LGGNet: Learning from Local-Global-Graph Representations for
  Brain-Computer Interface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.02786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.02786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Neethu Robinson, Qiuhao Zeng, Cuntai Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuropsychological studies suggest that co-operative activities among
different brain functional areas drive high-level cognitive processes. To learn
the brain activities within and among different functional areas of the brain,
we propose LGGNet, a novel neurologically inspired graph neural network, to
learn local-global-graph representations of electroencephalography (EEG) for
Brain-Computer Interface (BCI). The input layer of LGGNet comprises a series of
temporal convolutions with multi-scale 1D convolutional kernels and
kernel-level attentive fusion. It captures temporal dynamics of EEG which then
serves as input to the proposed local and global graph-filtering layers. Using
a defined neurophysiologically meaningful set of local and global graphs,
LGGNet models the complex relations within and among functional areas of the
brain. Under the robust nested cross-validation settings, the proposed method
is evaluated on three publicly available datasets for four types of cognitive
classification tasks, namely, the attention, fatigue, emotion, and preference
classification tasks. LGGNet is compared with state-of-the-art methods, such as
DeepConvNet, EEGNet, R2G-STNN, TSception, and HRNN. The results show that
LGGNet outperforms these methods, and the improvements are statistically
significant (p<0.05) in most cases. The results show that bringing neuroscience
prior knowledge into neural network design yields an improvement of
classification performance. The source code can be found at
https://github.com/yi-ding-cs/LGG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting abnormal events in video is commonly framed as a one-class
classification task, where training videos contain only normal events, while
test videos encompass both normal and abnormal events. In this scenario,
anomaly detection is an open-set problem. However, some studies assimilate
anomaly detection to action recognition. This is a closed-set scenario that
fails to test the capability of systems at detecting new anomaly types. To this
end, we propose UBnormal, a new supervised open-set benchmark composed of
multiple virtual scenes for video anomaly detection. Unlike existing data sets,
we introduce abnormal events annotated at the pixel level at training time, for
the first time enabling the use of fully-supervised learning methods for
abnormal event detection. To preserve the typical open-set formulation, we make
sure to include disjoint sets of anomaly types in our training and test
collections of videos. To our knowledge, UBnormal is the first video anomaly
detection benchmark to allow a fair head-to-head comparison between one-class
open-set models and supervised closed-set models, as shown in our experiments.
Moreover, we provide empirical evidence showing that UBnormal can enhance the
performance of a state-of-the-art anomaly detection framework on two prominent
data sets, Avenue and ShanghaiTech. Our benchmark is freely available at
https://github.com/lilygeorgescu/UBnormal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022. Paper + supplementary (15 pages, 9 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedIPR: Ownership Verification for Federated Deep Neural Network Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.13236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.13236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Li, Lixin Fan, Hanlin Gu, Jie Li, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning models are collaboratively developed upon valuable
training data owned by multiple parties. During the development and deployment
of federated models, they are exposed to risks including illegal copying,
re-distribution, misuse and/or free-riding. To address these risks, the
ownership verification of federated learning models is a prerequisite that
protects federated learning model intellectual property rights (IPR) i.e.,
FedIPR. We propose a novel federated deep neural network (FedDNN) ownership
verification scheme that allows private watermarks to be embedded and verified
to claim legitimate IPR of FedDNN models. In the proposed scheme, each client
independently verifies the existence of the model watermarks and claims
respective ownership of the federated model without disclosing neither private
training data nor private watermark information. The effectiveness of embedded
watermarks is theoretically justified by the rigorous analysis of conditions
under which watermarks can be privately embedded and detected by multiple
clients. Moreover, extensive experimental results on computer vision and
natural language processing tasks demonstrate that varying bit-length
watermarks can be embedded and reliably detected without compromising original
model performances. Our watermarking scheme is also resilient to various
federated training settings and robust against removal attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adjacency constraint for efficient hierarchical reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.00213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.00213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is large. Searching in a large goal space poses
difficulty for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that in a deterministic Markov Decision
Process (MDP), the proposed adjacency constraint preserves the optimal
hierarchical policy, while in a stochastic MDP the adjacency constraint induces
a bounded state-value suboptimality determined by the MDP's transition
structure. We further show that this constraint can be practically implemented
by training an adjacency network that can discriminate between adjacent and
non-adjacent subgoals. Experimental results on discrete and continuous control
tasks including challenging simulated robot locomotion and manipulation tasks
show that incorporating the adjacency constraint significantly boosts the
performance of state-of-the-art goal-conditioned HRL approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2006.11485</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Decentralized Controllers with Graph Neural Networks and
  Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.14906v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.14906v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Gama, Qingbiao Li, Ekaterina Tolstaya, Amanda Prorok, Alejandro Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems consisting of a set of autonomous agents face the challenge
of having to accomplish a global task, relying only on local information. While
centralized controllers are readily available, they face limitations in terms
of scalability and implementation, as they do not respect the distributed
information structure imposed by the network system of agents. Given the
difficulties in finding optimal decentralized controllers, we propose a novel
framework using graph neural networks (GNNs) to \emph{learn} these controllers.
GNNs are well-suited for the task since they are naturally distributed
architectures and exhibit good scalability and transferability properties. We
show that GNNs learn appropriate decentralized controllers by means of
imitation learning, leverage their permutation invariance properties to
successfully scale to larger teams and transfer to unseen scenarios at
deployment time. The problems of flocking and multi-agent path planning are
explored to illustrate the potential of GNNs in learning decentralized
controllers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast quantum state reconstruction via accelerated non-convex programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.07006v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.07006v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyung Lyle Kim, George Kollias, Amir Kalev, Ken X. Wei, Anastasios Kyrillidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new quantum state reconstruction method that combines ideas from
compressed sensing, non-convex optimization, and acceleration methods. The
algorithm, called Momentum-Inspired Factored Gradient Descent (\texttt{MiFGD}),
extends the applicability of quantum tomography for larger systems. Despite
being a non-convex method, \texttt{MiFGD} converges \emph{provably} close to
the true density matrix at an accelerated linear rate, in the absence of
experimental and statistical noise, and under common assumptions. With this
manuscript, we present the method, prove its convergence property and provide
Frobenius norm bound guarantees with respect to the true density matrix. From a
practical point of view, we benchmark the algorithm performance with respect to
other existing methods, in both synthetic and real experiments performed on an
IBM's quantum processing unit. We find that the proposed algorithm performs
orders of magnitude faster than state of the art approaches, with the same or
better accuracy. In both synthetic and real experiments, we observed accurate
and robust reconstruction, despite experimental and statistical noise in the
tomographic data. Finally, we provide a ready-to-use code for state tomography
of multi-qubit systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Neural Fields: Learning Functions on Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah Lähner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes & different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PatrickStar: Parallel Training of <span class="highlight-title">Pre-train</span>ed Models via Chunk-based
  Memory Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.05818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.05818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Fang, Yang Yu, Zilin Zhu, Shenggui Li, Yang You, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained model (PTM) is revolutionizing Artificial intelligence (AI)
technology. It can learn general language features on massive data and then be
fine-tuned on task-specific data. Unfortunately, the computing hardware
requirement of PTM training is prohibitively expensive, which makes it a game
for a small proportion of people in the AI community. Therefore, we proposed a
system called PatrickStar to lower the hardware requirements of PTMs and make
them accessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory
space to store the model data. Different from existing works, we first manage
the model data in a fine-grained manner by organizing them in memory chunks and
dynamically distributing them in the heterogeneous memory space. Guided by the
runtime memory statistics collected in a warm-up iteration, chunks are
orchestrated efficiently in heterogeneous memory and generate lower CPU-GPU
data transmission volume. Symbiosis with the Zero Redundancy Optimizer,
PatrickStar scales to multiple GPUs using data parallelism, with lower
communication bandwidth requirements and more efficient bandwidth utilization.
The system can train tasks on bigger models and larger batch sizes, which
existing works cannot complete. Experimental results show that PatrickStar
trains a 12 billion parameters GPT model, 1.5x as large as the model scale
limit of the SOTA works, on an 8xV100 and 240GB CPU memory node, and also
achieves significantly higher computing efficiency than SOTA. Even on a $700
personal computer, it can train a 0.7 billion parameter GPT model. Our code is
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Adversarial Examples to Quantify Membership Information
  Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Del Grosso, Hamid Jalalzai, Georg Pichler, Catuscia Palamidessi, Pablo Piantanida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of personal data for training machine learning systems comes with a
privacy threat and measuring the level of privacy of a model is one of the
major challenges in machine learning today. Identifying training data based on
a trained model is a standard way of measuring the privacy risks induced by the
model. We develop a novel approach to address the problem of membership
inference in pattern recognition models, relying on information provided by
adversarial examples. The strategy we propose consists of measuring the
magnitude of a perturbation necessary to build an adversarial example. Indeed,
we argue that this quantity reflects the likelihood of belonging to the
training data. Extensive numerical experiments on multivariate data and an
array of state-of-the-art target models show that our method performs
comparable or even outperforms state-of-the-art strategies, but without
requiring any additional training samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stein Variational Gradient Descent with Multiple Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.09338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.09338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingzhong Ai, Shiyu Liu, Lirong He, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stein variational gradient descent (SVGD) and its variants have shown
promising successes in approximate inference for complex distributions. In
practice, we notice that the kernel used in SVGD-based methods has a decisive
effect on the empirical performance. Radial basis function (RBF) kernel with
median heuristics is a common choice in previous approaches, but unfortunately
this has proven to be sub-optimal. Inspired by the paradigm of Multiple Kernel
Learning (MKL), our solution to this flaw is using a combination of multiple
kernels to approximate the optimal kernel, rather than a single one which may
limit the performance and flexibility. Specifically, we first extend Kernelized
Stein Discrepancy (KSD) to its multiple kernels view called Multiple Kernelized
Stein Discrepancy (MKSD) and then leverage MKSD to construct a general
algorithm Multiple Kernel SVGD (MK-SVGD). Further, MKSVGD can automatically
assign a weight to each kernel without any other parameters, which means that
our method not only gets rid of optimal kernel dependence but also maintains
computational efficiency. Experiments on various tasks and models demonstrate
that our proposed method consistently matches or outperforms the competing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watermarking Images in <span class="highlight-title">Self-Supervised</span> Latent Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Hervé Jégou, Matthijs Douze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit watermarking techniques based on pre-trained deep networks, in the
light of self-supervised approaches. We present a way to embed both marks and
binary messages into their latent spaces, leveraging data augmentation at
marking time. Our method can operate at any resolution and creates watermarks
robust to a broad range of transformations (rotations, crops, JPEG, contrast,
etc). It significantly outperforms the previous zero-bit methods, and its
performance on multi-bit watermarking is on par with state-of-the-art
encoder-decoder architectures trained end-to-end for watermarking. The code is
available at github.com/facebookresearch/ssl_watermarking
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Approximate Optimal Transport Distances using Quantization <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaspard Beugnot, Aude Genevay, Kristjan Greenewald, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport (OT) is a popular tool in machine learning to compare
probability measures geometrically, but it comes with substantial computational
burden. Linear programming algorithms for computing OT distances scale
cubically in the size of the input, making OT impractical in the large-sample
regime. We introduce a practical algorithm, which relies on a quantization
step, to estimate OT distances between measures given cheap sample access. We
also provide a variant of our algorithm to improve the performance of
approximate solvers, focusing on those for entropy-regularized transport. We
give theoretical guarantees on the benefits of this quantization step and
display experiments showing that it behaves well in practice, providing a
practical approximation algorithm that can be used as a drop-in replacement for
existing OT estimators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of the Conference on Uncertainty in
  Artificial Intelligence 2021 (UAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic self-similar blow up profile for 3-D Euler via
  physics-informed neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongji Wang, Ching-Yao Lai, Javier Gómez-Serrano, Tristan Buckmaster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a new numerical framework, employing physics-informed neural
networks, to find a smooth self-similar solution for the Boussinesq equations.
The solution in addition corresponds to an asymptotic self-similar profile for
the 3-dimensional Euler equations in the presence of a cylindrical boundary. In
particular, the solution represents a precise description of the Luo-Hou
blow-up scenario [G. Luo, T. Hou, Proc. Natl. Acad. Sci. 111(36): 12968-12973,
2014] for 3-dimensional Euler. To the best of the authors' knowledge, the
solution is the first truly multi-dimensional smooth backwards self-similar
profile found for an equation from fluid mechanics. The new numerical framework
is shown to be both robust and readily adaptable to other equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures. Some sign conventions have been changed and
  improved computations were performed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-distribution Generalization with Causal Invariant Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Wang, Mingyang Yi, Zhitang Chen, Shengyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, it is important and desirable to learn a model
that performs well on out-of-distribution (OOD) data. Recently, causality has
become a powerful tool to tackle the OOD generalization problem, with the idea
resting on the causal mechanism that is invariant across domains of interest.
To leverage the generally unknown causal mechanism, existing works assume a
linear form of causal feature or require sufficiently many and diverse training
domains, which are usually restrictive in practice. In this work, we obviate
these assumptions and tackle the OOD problem without explicitly recovering the
causal feature. Our approach is based on transformations that modify the
non-causal feature but leave the causal part unchanged, which can be either
obtained from prior knowledge or learned from the training data in the
multi-domain scenario. Under the setting of invariant causal mechanism, we
theoretically show that if all such transformations are available, then we can
learn a minimax optimal model across the domains using only single domain data.
Noticing that knowing a complete set of these causal invariant transformations
may be impractical, we further show that it suffices to know only a subset of
these transformations. Based on the theoretical findings, a regularized
training procedure is proposed to improve the OOD generalization capability.
Extensive experimental results on both synthetic and real datasets verify the
effectiveness of the proposed algorithm, even with only a few causal invariant
transformations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by cvpr2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scale-invariant representation of machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungyeop Lee, Junghyo Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of machine learning has resulted from its structured
representation of data. Similar data have close internal representations as
compressed codes for classification or emerged labels for clustering. We
observe that the frequency of internal codes or labels follows power laws in
both supervised and unsupervised learning models. This scale-invariant
distribution implies that machine learning largely compresses frequent typical
data, and simultaneously, differentiates many atypical data as outliers. In
this study, we derive the process by which these power laws can naturally arise
in machine learning. In terms of information theory, the scale-invariant
representation corresponds to a maximally uncertain data grouping among
possible representations that guarantee a given learning accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multi-task learning-based optimization approach for finding diverse
  sets of material microstructures with desired properties and its application
  to texture optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.00916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.00916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Iraki, Lukas Morand, Johannes Dornheim, Norbert Link, Dirk Helm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimization along the chain processing-structure-properties-performance
is one of the core objectives in data-driven materials science. In this sense,
processes are supposed to manufacture workpieces with targeted material
microstructures. These microstructures are defined by the material properties
of interest and identifying them is a question of materials design. In the
present paper, we addresse this issue and introduce a generic multi-task
learning-based optimization approach. The approach enables the identification
of sets of highly diverse microstructures for given desired properties and
corresponding tolerances. Basically, the approach consists of an optimization
algorithm that interacts with a machine learning model that combines multi-task
learning with siamese neural networks. The resulting model (1) relates
microstructures and properties, (2) estimates the likelihood of a
microstructure of being producible, and (3) performs a distance preserving
microstructure feature extraction in order to generate a lower dimensional
latent feature space to enable efficient optimization. The proposed approach is
applied on a crystallographic texture optimization problem for rolled steel
sheets given desired properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Robust Convolutional Neural Networks with Relevant Feature
  Focusing via Explanations <span class="chip">ICME 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Adachi, Shin'ya Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing image recognition techniques based on convolutional neural networks
(CNNs) basically assume that the training and test datasets are sampled from
i.i.d distributions. However, this assumption is easily broken in the real
world because of the distribution shift that occurs when the co-occurrence
relations between objects and backgrounds in input images change. Under this
type of distribution shift, CNNs learn to focus on features that are not
task-relevant, such as backgrounds from the training data, and degrade their
accuracy on the test data. To tackle this problem, we propose relevant feature
focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via
explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc
explanation modules, it can be easily applied to off-the-shelf CNNs.
Furthermore, ReFF requires no additional inference cost at test time because it
is only used for regularization while training. We demonstrate that CNNs
trained with ReFF focus on features relevant to the target task and that ReFF
improves the test-time accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query Driven-Graph Neural Networks for Community Search: From
  Non-Attributed, Attributed, to Interactive Attributed <span class="chip">VLDB 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.03583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.03583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Jiang, Yu Rong, Hong Cheng, Xin Huang, Kangfei Zhao, Junzhou Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given one or more query vertices, Community Search (CS) aims to find densely
intra-connected and loosely inter-connected structures containing query
vertices. Attributed Community Search (ACS), a related problem, is more
challenging since it finds communities with both cohesive structures and
homogeneous vertex attributes. However, most methods for the CS task rely on
inflexible pre-defined structures and studies for ACS treat each attribute
independently. Moreover, the most popular ACS strategies decompose ACS into two
separate sub-problems, i.e., the CS task and subsequent attribute filtering
task. However, in real-world graphs, the community structure and the vertex
attributes are closely correlated to each other. This correlation is vital for
the ACS problem. In this paper, we propose Graph Neural Network models for both
CS and ACS problems, i.e., Query Driven-GNN and Attributed Query Driven-GNN. In
QD-GNN, we combine the local query-dependent structure and global graph
embedding. In order to extend QD-GNN to handle attributes, we model vertex
attributes as a bipartite graph and capture the relation between attributes by
constructing GNNs on this bipartite graph. With a Feature Fusion operator,
AQD-GNN processes the structure and attribute simultaneously and predicts
communities according to each attributed query. Experiments on real-world
graphs with ground-truth communities demonstrate that the proposed models
outperform existing CS and ACS algorithms in terms of both efficiency and
effectiveness. More recently, an interactive setting for CS is proposed that
allows users to adjust the predicted communities. We further verify our
approaches under the interactive setting and extend to the attributed context.
Our method achieves 2.37% and 6.29% improvements in F1-score than the
state-of-the-art model without attributes and with attributes respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PVLDB 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and
  Extracting Them from 2D Renderings <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.13450v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.13450v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava, Ce Liu, Peyman Milanfar, Feng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Graph Neural Networks with Shallow Subgraph Samplers <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.01380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.01380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, Viktor Prasanna, Long Jin, Ren Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Graph Neural Networks (GNNs) are powerful models for learning
representations on graphs, most state-of-the-art models do not have significant
accuracy gain beyond two to three layers. Deep GNNs fundamentally need to
address: 1). expressivity challenge due to oversmoothing, and 2). computation
challenge due to neighborhood explosion. We propose a simple "deep GNN, shallow
sampler" design principle to improve both the GNN accuracy and efficiency -- to
generate representation of a target node, we use a deep GNN to pass messages
only within a shallow, localized subgraph. A properly sampled subgraph may
exclude irrelevant or even noisy nodes, and still preserve the critical
neighbor features and graph structures. The deep GNN then smooths the
informative local signals to enhance feature learning, rather than
oversmoothing the global graph signals into just "white noise". We
theoretically justify why the combination of deep GNNs with shallow samplers
yields the best learning performance. We then propose various sampling
algorithms and neural architecture extensions to achieve good empirical
results. On the largest public graph dataset, ogbn-papers100M, we achieve
state-of-the-art accuracy with an order of magnitude reduction in hardware
cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The complete version of this paper is accepted to NeurIPS 2021,
  available on arXiv under the new title "Decoupling the depth and scope of
  graph neural networks" (arXiv:2201.07858). This version, "Deep graph neural
  networks with shallow subgraph samplers", is a short version and we withdraw
  it to avoid confusion. Please always refer to arXiv:2201.07858</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning and Generalization in Overparameterized Normalizing Flows <span class="chip">AISTATS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.10535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.10535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kulin Shah, Amit Deshpande, Navin Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In supervised learning, it is known that overparameterized neural networks
with one hidden layer provably and efficiently learn and generalize, when
trained using stochastic gradient descent with a sufficiently small learning
rate and suitable initialization. In contrast, the benefit of
overparameterization in unsupervised learning is not well understood.
Normalizing flows (NFs) constitute an important class of models in unsupervised
learning for sampling and density estimation. In this paper, we theoretically
and empirically analyze these models when the underlying neural network is a
one-hidden-layer overparametrized network. Our main contributions are two-fold:
(1) On the one hand, we provide theoretical and empirical evidence that for
constrained NFs (this class of NFs underlies many NF constructions) with the
one-hidden-layer network, overparametrization hurts training. (2) On the other
hand, we prove that unconstrained NFs, a recently introduced model, can
efficiently learn any reasonable data distribution under minimal assumptions
when the underlying network is overparametrized and has one hidden-layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>75 pages, Accepted in AISTATS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Adaptation of GAN in Just One CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gihyun Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Image compressed version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Root-aligned SMILES for Molecular Retrosynthesis Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Yao, Min Wu, Tingjun Hou, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrosynthesis prediction is a fundamental problem in organic synthesis,
where the task is to discover precursor molecules that can be used to
synthesize a target molecule. A popular paradigm of existing computational
retrosynthesis methods formulate retrosynthesis prediction as a
sequence-to-sequence translation problem, where the typical SMILES
representations are adopted for both reactants and products. However, the
general-purpose SMILES neglects the characteristics of retrosynthesis that 1)
the search space of the reactants is quite huge, and 2) the molecular graph
topology is largely unaltered from products to reactants, resulting in the
suboptimal performance of SMILES if straightforwardly applied. In this article,
we propose the root-aligned SMILES~(R-SMILES), which specifies a tightly
aligned one-to-one mapping between the product and the reactant SMILES, to
narrow the string representation discrepancy for more efficient retrosynthesis.
As the minimum edit distance between the input and the output is significantly
decreased with the proposed R-SMILES, the computational model is largely
relieved from learning the complex syntax and dedicated to learning the
chemical knowledge for retrosynthesis. We compare the proposed R-SMILES with
various state-of-the-art baselines on different benchmarks and show that it
significantly outperforms them all, demonstrating the superiority of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 15 pages, 5 figures, and 1 table; supplementary
  information: 4 pages, 2 figures and 3 tables. Code repository:
  https://github.com/otori-bird/retrosynthesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Yu Hsieh, Jieyu Zhang, Alexander Ratner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weak Supervision (WS) techniques allow users to efficiently create large
training datasets by programmatically labeling data with heuristic sources of
supervision. While the success of WS relies heavily on the provided labeling
heuristics, the process of how these heuristics are created in practice has
remained under-explored. In this work, we formalize the development process of
labeling heuristics as an interactive procedure, built around the existing
workflow where users draw ideas from a selected set of development data for
designing the heuristic sources. With the formalism, we study two core problems
of how to strategically select the development data to guide users in
efficiently creating informative heuristics, and how to exploit the information
within the development process to contextualize and better learn from the
resultant heuristics. Building upon two novel methodologies that effectively
tackle the respective problems considered, we present Nemo, an end-to-end
interactive system that improves the overall productivity of WS learning
pipeline by an average 20% (and up to 47% in one task) compared to the
prevailing WS approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking Distance Calibration for Cross-Domain Few-Shot Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Li, Shaogang Gong, Chengjie Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in few-shot learning promotes a more realistic cross-domain
setting, where the source and target datasets are from different domains. Due
to the domain gap and disjoint label spaces between source and target datasets,
their shared knowledge is extremely limited. This encourages us to explore more
information in the target domain rather than to overly elaborate training
strategies on the source domain as in many existing methods. Hence, we start
from a generic representation pre-trained by a cross-entropy loss and a
conventional distance-based classifier, along with an image retrieval view, to
employ a re-ranking process for calibrating a target distance matrix by
discovering the reciprocal k-nearest neighbours within the task. Assuming the
pre-trained representation is biased towards the source, we construct a
non-linear subspace to minimise task-irrelevant features therewithin while keep
more transferrable discriminative information by a hyperbolic tangent
transformation. The calibrated distance in this target-aware non-linear
subspace is complementary to that in the pre-trained representation. To impose
such distance calibration information onto the pre-trained representation, a
Kullback-Leibler divergence loss is employed to gradually guide the model
towards the calibrated distance-based distribution. Extensive evaluations on
eight target domains show that this target ranking calibration process can
improve conventional distance-based classifiers in few-shot learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> On the Efficiency of Entropic Regularized Algorithms for Optimal
  Transport <span class="chip">ICML 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1906.01437v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1906.01437v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Lin, Nhat Ho, <span class="highlight-author">Michael I. Jordan</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present several new complexity results for the entropic regularized
algorithms that approximately solve the optimal transport (OT) problem between
two discrete probability measures with at most $n$ atoms. First, we improve the
complexity bound of a greedy variant of Sinkhorn, known as \textit{Greenkhorn},
from $\widetilde{O}(n^2\varepsilon^{-3})$ to
$\widetilde{O}(n^2\varepsilon^{-2})$. Notably, our result can match the best
known complexity bound of Sinkhorn and help clarify why Greenkhorn
significantly outperforms Sinkhorn in practice in terms of row/column updates
as observed by~\citet{Altschuler-2017-Near}. Second, we propose a new
algorithm, which we refer to as \textit{APDAMD} and which generalizes an
adaptive primal-dual accelerated gradient descent (APDAGD)
algorithm~\citep{Dvurechensky-2018-Computational} with a prespecified mirror
mapping $\phi$. We prove that APDAMD achieves the complexity bound of
$\widetilde{O}(n^2\sqrt{\delta}\varepsilon^{-1})$ in which $\delta>0$ stands
for the regularity of $\phi$. In addition, we show by a counterexample that the
complexity bound of $\widetilde{O}(\min\{n^{9/4}\varepsilon^{-1},
n^2\varepsilon^{-2}\})$ proved for APDAGD before is invalid and give a refined
complexity bound of $\widetilde{O}(n^{5/2}\varepsilon^{-1})$. Further, we
develop a \textit{deterministic} accelerated variant of Sinkhorn via appeal to
estimated sequence and prove the complexity bound of
$\widetilde{O}(n^{7/3}\varepsilon^{-4/3})$. As such, we see that accelerated
variant of Sinkhorn outperforms Sinkhorn and Greenkhorn in terms of
$1/\varepsilon$ and APDAGD and accelerated alternating minimization
(AAM)~\citep{Guminov-2021-Combination} in terms of $n$. Finally, we conduct the
experiments on synthetic and real data and the numerical results show the
efficiency of Greenkhorn, APDAMD and accelerated Sinkhorn in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preliminary version [arXiv:1901.06482] of this paper, with a subset
  of the results that are presented here, was presented at ICML 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical <span class="highlight-title">Survey</span> of the Effectiveness of Debiasing Techniques for
  <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Meade, Elinor Poole-Dayan, Siva Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown pre-trained language models capture social biases from
the large amounts of text they are trained on. This has attracted attention to
developing techniques that mitigate such biases. In this work, we perform an
empirical survey of five recently proposed bias mitigation techniques:
Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace
Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of
each technique using three intrinsic bias benchmarks while also measuring the
impact of these techniques on a model's language modeling ability, as well as
its performance on downstream NLU tasks. We experimentally find that: (1)
Self-Debias is the strongest debiasing technique, obtaining improved scores on
all bias benchmarks; (2) Current debiasing techniques perform less consistently
when mitigating non-gender biases; And (3) improvements on bias benchmarks such
as StereoSet and CrowS-Pairs by using debiasing strategies are often
accompanied by a decrease in language modeling ability, making it difficult to
determine whether the bias mitigation was effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Subgraph Recognition with Variational Graph Information
  Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchi Yu, Jie Cao, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subgraph recognition aims at discovering a compressed substructure of a graph
that is most informative to the graph property. It can be formulated by
optimizing Graph Information Bottleneck (GIB) with a mutual information
estimator. However, GIB suffers from training instability since the mutual
information of graph data is intrinsically difficult to estimate. This paper
introduces a noise injection method to compress the information in the
subgraphs, which leads to a novel Variational Graph Information Bottleneck
(VGIB) framework. VGIB allows a tractable variational approximation to its
objective under mild assumptions. Therefore, VGIB enjoys more stable and
efficient training process - we find that VGIB converges 10 times faster than
GIB with improved performances in practice. Extensive experiments on graph
interpretation, explainability of Graph Neural Networks, and graph
classification show that VGIB finds better subgraphs than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Programmable 3D snapshot microscopy with Fourier convolutional networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.10611v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.10611v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diptodip Deb, Zhenfei Jiao, Alex B. Chen, Michael Broxton, Misha B. Ahrens, Kaspar Podgorski, Srinivas C. Turaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D snapshot microscopy enables fast volumetric imaging by capturing a 3D
volume in a single 2D camera image and performing computational reconstruction.
Fast volumetric imaging has a variety of biological applications such as whole
brain imaging of rapid neural activity in larval zebrafish. The optimal
microscope design for this optical 3D-to-2D encoding is both sample- and
task-dependent, with no general solution known. Deep learning based decoders
can be combined with a differentiable simulation of an optical encoder for
end-to-end optimization of both the deep learning decoder and optical encoder.
This technique has been used to engineer local optical encoders for other
problems such as depth estimation, 3D particle localization, and lensless
photography. However, 3D snapshot microscopy is known to require a highly
non-local optical encoder which existing UNet-based decoders are not able to
engineer. We show that a neural network architecture based on global kernel
Fourier convolutional neural networks can efficiently decode information from
multiple depths in a volume, globally encoded across a 3D snapshot image. We
show in simulation that our proposed networks succeed in engineering and
reconstructing optical encoders for 3D snapshot microscopy where the existing
state-of-the-art UNet architecture fails. We also show that our networks
outperform the state-of-the-art learned reconstruction algorithms for a
computational photography dataset collected on a prototype lensless camera
which also uses a highly non-local optical encoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add memory usage tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Study on Robustness to Perturbations for Representations of
  Environmental Sound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangeeta Srivastava, Ho-Hsiang Wu, Joao Rulff, Magdalena Fuentes, Mark Cartwright, Claudio Silva, Anish Arora, Juan Pablo Bello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio applications involving environmental sound analysis increasingly use
general-purpose audio representations, also known as embeddings, for transfer
learning. Recently, Holistic Evaluation of Audio Representations (HEAR)
evaluated twenty-nine embedding models on nineteen diverse tasks. However, the
evaluation's effectiveness depends on the variation already captured within a
given dataset. Therefore, for a given data domain, it is unclear how the
representations would be affected by the variations caused by myriad
microphones' range and acoustic conditions -- commonly known as channel
effects. We aim to extend HEAR to evaluate invariance to channel effects in
this work. To accomplish this, we imitate channel effects by injecting
perturbations to the audio signal and measure the shift in the new (perturbed)
embeddings with three distance measures, making the evaluation domain-dependent
but not task-dependent. Combined with the downstream performance, it helps us
make a more informed prediction of how robust the embeddings are to the channel
effects. We evaluate two embeddings -- YAMNet, and OpenL$^3$ on monophonic
(UrbanSound8K) and polyphonic (SONYC UST) datasets. We show that one distance
measure does not suffice in such task-independent evaluation. Although
Fr\'echet Audio Distance (FAD) correlates with the trend of the performance
drop in the downstream task most accurately, we show that we need to study this
in conjunction with the other distances to get a clear understanding of the
overall effect of the perturbation. In terms of the embedding performance, we
find OpenL$^3$ to be more robust to YAMNet, which aligns with the HEAR
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From calibration to parameter learning: Harnessing the scaling effects
  of big data in geoscientific modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.15751v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.15751v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ping Tsai, Dapeng Feng, Ming Pan, Hylke Beck, Kathryn Lawson, Yuan Yang, Jiangtao Liu, Chaopeng Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The behaviors and skills of models in many geosciences (e.g., hydrology and
ecosystem sciences) strongly depend on spatially-varying parameters that need
calibration. A well-calibrated model can reasonably propagate information from
observations to unobserved variables via model physics, but traditional
calibration is highly inefficient and results in non-unique solutions. Here we
propose a novel differentiable parameter learning (dPL) framework that
efficiently learns a global mapping between inputs (and optionally responses)
and parameters. Crucially, dPL exhibits beneficial scaling curves not
previously demonstrated to geoscientists: as training data increases, dPL
achieves better performance, more physical coherence, and better
generalizability (across space and uncalibrated variables), all with
orders-of-magnitude lower computational cost. We demonstrate examples that
learned from soil moisture and streamflow, where dPL drastically outperformed
existing evolutionary and regionalization methods, or required only ~12.5% of
the training data to achieve similar performance. The generic scheme promotes
the integration of deep learning and process-based models, without mandating
reimplementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble Recognition in Reproducing Kernel Hil<span class="highlight-title">bert</span> Spaces through
  Aggregated Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Miao, Gong Cheng, Jr-Shin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of learning dynamical properties of
ensemble systems from their collective behaviors using statistical approaches
in reproducing kernel Hilbert space (RKHS). Specifically, we provide a
framework to identify and cluster multiple ensemble systems through computing
the maximum mean discrepancy (MMD) between their aggregated measurements in an
RKHS, without any prior knowledge of the system dynamics of ensembles. Then,
leveraging the gradient flow of the newly proposed notion of aggregated Markov
parameters, we present a systematic framework to recognize and identify an
ensemble systems using their linear approximations. Finally, we demonstrate
that the proposed approaches can be extended to cluster multiple unknown
ensembles in RKHS using their aggregated measurements. Numerical experiments
show that our approach is reliable and robust to ensembles with different types
of system dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed
  Bandit with Many Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.10121v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.10121v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a Bayesian $k$-armed bandit problem in many-armed regime, when $k
\geq \sqrt{T}$, with $T$ the time horizon. We first show that subsampling is
critical for designing optimal policies. Specifically, the standard UCB
algorithm is sub-optimal while a subsampled UCB (SS-UCB), which samples
$\Theta(\sqrt{T})$ arms and executes UCB on that subset, is rate-optimal.
Despite theoretically optimal regret, SS-UCB numerically performs worse than a
greedy algorithm that pulls the current empirically best arm each time. These
empirical insights hold in a contextual setting as well, using simulations on
real data. These results suggest a new form of free exploration in the
many-armed regime that benefits greedy algorithms. We theoretically show that
this source of free exploration is deeply connected to the distribution of a
tail event for the prior distribution of arm rewards. This is a fundamentally
distinct phenomenon from free exploration due to variation in covariates, as
discussed in the recent literature on contextual bandits. Building on this
result, we prove that the subsampled greedy algorithm is rate-optimal for
Bernoulli bandits in many armed regime, and achieves sublinear regret with more
general distributions. Taken together, our results suggest that practitioners
may benefit from using greedy algorithms in the many-armed regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Neural Ordinary Differential Equation Model for Visualizing Deep
  Neural Network Behaviors in Multi-Parametric MRI based Glioma Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Yang, Zongsheng Hu, Hangjie Ji, Kyle Lafata, Scott Floyd, Fang-Fang Yin, Chunhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop a neural ordinary differential equation (ODE) model for
visualizing deep neural network (DNN) behavior during multi-parametric MRI
(mp-MRI) based glioma segmentation as a method to enhance deep learning
explainability. Methods: By hypothesizing that deep feature extraction can be
modeled as a spatiotemporally continuous process, we designed a novel deep
learning model, neural ODE, in which deep feature extraction was governed by an
ODE without explicit expression. The dynamics of 1) MR images after
interactions with DNN and 2) segmentation formation can be visualized after
solving ODE. An accumulative contribution curve (ACC) was designed to
quantitatively evaluate the utilization of each MRI by DNN towards the final
segmentation results. The proposed neural ODE model was demonstrated using 369
glioma patients with a 4-modality mp-MRI protocol: T1, contrast-enhanced T1
(T1-Ce), T2, and FLAIR. Three neural ODE models were trained to segment
enhancing tumor (ET), tumor core (TC), and whole tumor (WT). The key MR
modalities with significant utilization by DNN were identified based on ACC
analysis. Segmentation results by DNN using only the key MR modalities were
compared to the ones using all 4 MR modalities. Results: All neural ODE models
successfully illustrated image dynamics as expected. ACC analysis identified
T1-Ce as the only key modality in ET and TC segmentations, while both FLAIR and
T2 were key modalities in WT segmentation. Compared to the U-Net results using
all 4 MR modalities, Dice coefficient of ET (0.784->0.775), TC (0.760->0.758),
and WT (0.841->0.837) using the key modalities only had minimal differences
without significance. Conclusion: The neural ODE model offers a new tool for
optimizing the deep learning model inputs with enhanced explainability. The
presented methodology can be generalized to other medical image-related deep
learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Exploration of Learnt Representations of W Jets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.10919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.10919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack H. Collins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I present a Variational Autoencoder (VAE) trained on collider physics data
(specifically boosted $W$ jets), with reconstruction error given by an
approximation to the Earth Movers Distance (EMD) between input and output jets.
This VAE learns a concrete representation of the data manifold, with
semantically meaningful and interpretable latent space directions which are
hierarchically organized in terms of their relation to physical EMD scales in
the underlying physical generative process. A hyperparameter $\beta$ controls
the resolution at which the VAE is sensitive to structures in the data
manifold. The variation of the latent space structure with $\beta$, and the
scaling of some VAE properties, provide insight into scale dependent structure
of the dataset and its information complexity. I introduce two measures of the
dimensionality of the learnt representation that are calculated from this
scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added fig 5 and altered fig 3. Added citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spoofing Generalization: When Can't You Trust Proprietary Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.08393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.08393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Moitra, Elchanan Mossel, Colin Sandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the computational complexity of determining whether a
machine learning model that perfectly fits the training data will generalizes
to unseen data. In particular, we study the power of a malicious agent whose
goal is to construct a model g that fits its training data and nothing else,
but is indistinguishable from an accurate model f. We say that g strongly
spoofs f if no polynomial-time algorithm can tell them apart. If instead we
restrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g
c-weakly spoofs f. Our main results are
  1. Under cryptographic assumptions, strong spoofing is possible and
  2. For any c> 0, c-weak spoofing is possible unconditionally
  While the assumption of a malicious agent is an extreme scenario (hopefully
companies training large models are not malicious), we believe that it sheds
light on the inherent difficulties of blindly trusting large proprietary models
or data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward the Detection of Polyglot Files 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07561v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07561v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Koch, Sean Oesch, Mary Adkisson, Sam Erwin, Brian Weber, Amul Chaulagain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standardized file formats play a key role in the development and use of
computer software. However, it is possible to abuse standardized file formats
by creating a file that is valid in multiple file formats. The resulting
polyglot (many languages) file can confound file format identification,
allowing elements of the file to evade analysis.This is especially problematic
for malware detection systems that rely on file format identification for
feature extraction. File format identification processes that depend on file
signatures can be easily evaded thanks to flexibility in the format
specifications of certain file formats. Although work has been done to identify
file formats using more comprehensive methods than file signatures, accurate
identification of polyglot files remains an open problem. Since malware
detection systems routinely perform file format-specific feature extraction,
polyglot files need to be filtered out prior to ingestion by these systems.
Otherwise, malicious content could pass through undetected. To address the
problem of polyglot detection we assembled a data set using the mitra tool. We
then evaluated the performance of the most commonly used file identification
tool, file. Finally, we demonstrated the accuracy, precision, recall and F1
score of a range of machine and deep learning models. Malconv2 and Catboost
demonstrated the highest recall on our data set with 95.16% and 95.45%,
respectively. These models can be incorporated into a malware detector's file
processing pipeline to filter out potentially malicious polyglots before file
format-dependent feature extraction takes place.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning for Finite-Horizon Restless Multi-Armed
  Multi-Action Bandits <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guojun Xiong, Jian Li, Rahul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a finite-horizon restless multi-armed bandit problem with multiple
actions, dubbed R(MA)^2B. The state of each arm evolves according to a
controlled Markov decision process (MDP), and the reward of pulling an arm
depends on both the current state of the corresponding MDP and the action
taken. The goal is to sequentially choose actions for arms so as to maximize
the expected value of the cumulative rewards collected. Since finding the
optimal policy is typically intractable, we propose a computationally appealing
index policy which we call Occupancy-Measured-Reward Index Policy. Our policy
is well-defined even if the underlying MDPs are not indexable. We prove that it
is asymptotically optimal when the activation budget and number of arms are
scaled up, while keeping their ratio as a constant. For the case when the
system parameters are unknown, we develop a learning algorithm. Our learning
algorithm uses the principle of optimism in the face of uncertainty and further
uses a generative model in order to fully exploit the structure of
Occupancy-Measured-Reward Index Policy. We call it the R(MA)^2B-UCB algorithm.
As compared with the existing algorithms, R(MA)^2B-UCB performs close to an
offline optimum policy, and also achieves a sub-linear regret with a low
computational complexity. Experimental results show that R(MA)^2B-UCB
outperforms the existing algorithms in both regret and run time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared in AAAI 2022 with title "Reinforcement Learning Augmented
  Asymptotically Optimal Index Policy for Finite-Horizon Restless Bandits"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Kernel-Based Approach to Non-Stationary Reinforcement Learning in
  Metric Spaces <span class="chip">AISTATS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.05078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.05078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Darwiche Domingues, Pierre Ménard, Matteo Pirotta, Emilie Kaufmann, Michal Valko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose KeRNS: an algorithm for episodic reinforcement
learning in non-stationary Markov Decision Processes (MDPs) whose state-action
set is endowed with a metric. Using a non-parametric model of the MDP built
with time-dependent kernels, we prove a regret bound that scales with the
covering dimension of the state-action space and the total variation of the MDP
with time, which quantifies its level of non-stationarity. Our method
generalizes previous approaches based on sliding windows and exponential
discounting used to handle changing environments. We further propose a
practical implementation of KeRNS, we analyze its regret and validate it
experimentally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update following the publication in AISTATS 2021. Fixed typos and
  lemma about runtime</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization
  Problems <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.07508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.07508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangmo Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world decision-making systems are often subject to uncertainties that
have to be resolved through observational data. Therefore, we are frequently
confronted with combinatorial optimization problems of which the objective
function is unknown and thus has to be debunked using empirical evidence. In
contrast to the common practice that relies on a learning-and-optimization
strategy, we consider the regression between combinatorial spaces, aiming to
infer high-quality optimization solutions from samples of input-solution pairs
-- without the need to learn the objective function. Our main deliverable is a
universal solver that is able to handle abstract undetermined stochastic
combinatorial optimization problems. For learning foundations, we present
learning-error analysis under the PAC-Bayesian framework using a new
margin-based analysis. In empirical studies, we demonstrate our design using
proof-of-concept experiments, and compare it with other methods that are
potentially applicable. Overall, we obtain highly encouraging experimental
results for several classic combinatorial problems on both synthetic and
real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation of binary classification trees with binary features by
  quantum circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raoul Heese, Patricia Bickert, Astrid Elisa Niederle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a quantum representation of binary classification trees with
binary features based on a probabilistic approach. By using the quantum
computer as a processor for probability distributions, a probabilistic
traversal of the decision tree can be realized via measurements of a quantum
circuit. We describe how tree inductions and the prediction of class labels of
query data can be integrated into this framework. An on-demand sampling method
enables predictions with a constant number of classical memory slots,
independent of the tree depth. We experimentally study our approach using both
a quantum computing simulator and actual IBM quantum hardware. To our
knowledge, this is the first realization of a decision tree classifier on a
quantum device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 20 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M-SENA: An Integrated Platform for <span class="highlight-title">Multimodal</span> Sentiment Analysis <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huisheng Mao, Ziqi Yuan, Hua Xu, Wenmeng Yu, Yihe Liu, Kai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  M-SENA is an open-sourced platform for Multimodal Sentiment Analysis. It aims
to facilitate advanced research by providing flexible toolkits, reliable
benchmarks, and intuitive demonstrations. The platform features a fully modular
video sentiment analysis framework consisting of data management, feature
extraction, model training, and result analysis modules. In this paper, we
first illustrate the overall architecture of the M-SENA platform and then
introduce features of the core modules. Reliable baseline results of different
modality features and MSA benchmarks are also reported. Moreover, we use model
evaluation and analysis tools provided by M-SENA to present intermediate
representation visualization, on-the-fly instance test, and generalization
ability test results. The source code of the platform is publicly available at
https://github.com/thuiar/M-SENA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, to be published in ACL 2022 System Demonstration
  Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Adversarial Robustness of Large-scale Audio Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng B Li, Shuhui Qu, Xinjian Li,  Po-Yao,  Huang, Florian Metze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As audio-visual systems are being deployed for safety-critical tasks such as
surveillance and malicious content filtering, their robustness remains an
under-studied area. Existing published work on robustness either does not scale
to large-scale dataset, or does not deal with multiple modalities. This work
aims to study several key questions related to multi-modal learning through the
lens of robustness: 1) Are multi-modal models necessarily more robust than
uni-modal models? 2) How to efficiently measure the robustness of multi-modal
learning? 3) How to fuse different modalities to achieve a more robust
multi-modal model? To understand the robustness of the multi-modal model in a
large-scale setting, we propose a density-based metric, and a convexity metric
to efficiently measure the distribution of each modality in high-dimensional
latent space. Our work provides a theoretical intuition together with empirical
evidence showing how multi-modal fusion affects adversarial robustness through
these metrics. We further devise a mix-up strategy based on our metrics to
improve the robustness of the trained model. Our experiments on AudioSet and
Kinetics-Sounds verify our hypothesis that multi-modal models are not
necessarily more robust than their uni-modal counterparts in the face of
adversarial examples. We also observe our mix-up trained method could achieve
as much protection as traditional adversarial training, offering a
computationally cheap alternative. Implementation:
https://github.com/lijuncheng16/AudioSetDoneRight
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An interactive music infilling interface for pop music composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has been widely applied to music generation
topics such as continuation, melody/harmony generation, genre transfer and
music infilling application. Although with the burst interest to apply AI to
music, there are still few interfaces for the musicians to take advantage of
the latest progress of the AI technology. This makes those tools less valuable
in practice and harder to find its advantage/drawbacks without utilizing them
in the real scenario. This work builds a max patch for interactive music
infilling application with different levels of control, including track
density/polyphony/occupation rate and bar tonal tension control. The user can
select the melody/bass/harmony track as the infilling content up to 16 bars.
The infilling algorithm is based on the author's previous work, and the
interface sends/receives messages to the AI system hosted in the cloud. This
interface lowers the barrier of AI technology and can generate different
variations of the selected content. Those results can give several alternatives
to the musicians' composition, and the interactive process realizes the value
of the AI infilling system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Affective Feedback Synthesis Towards <span class="highlight-title">Multimodal</span> Text and Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puneet Kumar, Gaurav Bhat, Omkar Ingle, Daksh Goyal, Balasubramanian Raman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we have defined a novel task of affective feedback synthesis
that deals with generating feedback for input text & corresponding image in a
similar way as humans respond towards the multimodal data. A feedback synthesis
system has been proposed and trained using ground-truth human comments along
with image-text input. We have also constructed a large-scale dataset
consisting of image, text, Twitter user comments, and the number of likes for
the comments by crawling the news articles through Twitter feeds. The proposed
system extracts textual features using a transformer-based textual encoder
while the visual features have been extracted using a Faster region-based
convolutional neural networks model. The textual and visual features have been
concatenated to construct the multimodal features using which the decoder
synthesizes the feedback. We have compared the results of the proposed system
with the baseline models using quantitative and qualitative measures. The
generated feedbacks have been analyzed using automatic and human evaluation.
They have been found to be semantically similar to the ground-truth comments
and relevant to the given text-image input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Transactions on Multimedia Computing,
  Communications, and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static
  Models by Fitting Feature-level Space-time Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR 2022 (Source Code:
  https://github.com/jx-zhong-for-academic-purpose/Kinet )</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-22T00:00:00Z">2022-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span> Array Keeps the Bias Away: Debiasing <span class="highlight-title">Vision-Language</span> Models
  with Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Berg, Siobhan Mackenzie Hall, Yash Bhalgat, Wonsuk Yang, Hannah Rose Kirk, Aleksandar Shtedritski, Max Bain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures. For code and trained token embeddings, see
  https://github.com/oxai/debias-vision-lang</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> based ensemble for emotion detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kane, Shantanu Patankar, Sahil Khose, Neeraja Kirtane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting emotions in languages is important to accomplish a complete
interaction between humans and machines. This paper describes our contribution
to the WASSA 2022 shared task which handles this crucial task of emotion
detection. We have to identify the following emotions: sadness, surprise,
neutral, anger, fear, disgust, joy based on a given essay text. We are using an
ensemble of ELECTRA and BERT models to tackle this problem achieving an F1
score of 62.76%. Our codebase (https://bit.ly/WASSA_shared_task) and our WandB
project (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Computational Approach to Understand Mental Health from Reddit:
  Knowledge-aware Multitask Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usha Lokala, Aseem Srivastava, Triyasha Ghosh Dastidar, Tanmoy Chakraborty, Md Shad Akthar, Maryam Panahiazar, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing gender is critical to study mental health (MH) support in CVD
(cardiovascular disease). The existing studies on using social media for
extracting MH symptoms consider symptom detection and tend to ignore user
context, disease, or gender. The current study aims to design and evaluate a
system to capture how MH symptoms associated with CVD are expressed differently
with the gender on social media. We observe that the reliable detection of MH
symptoms expressed by persons with heart disease in user posts is challenging
because of the co-existence of (dis)similar MH symptoms in one post and due to
variation in the description of symptoms based on gender. We collect a corpus
of $150k$ items (posts and comments) annotated using the subreddit labels and
transfer learning approaches. We propose GeM, a novel task-adaptive multi-task
learning approach to identify the MH symptoms in CVD patients based on gender.
Specifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to
capture CVD-related MH symptoms. Moreover, it enhances the reliability for
differentiating the gender language in MH symptoms when compared to the
state-of-art language models. Our model achieves high (statistically
significant) performance and predicts four labels of MH issues and two gender
labels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom
identification task and by 2.55% on the gender identification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Girl Has A Name, And It's ... Adversarial Authorship Attribution for
  Deobfuscation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyue Zhai, Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in natural language processing have enabled powerful
privacy-invasive authorship attribution. To counter authorship attribution,
researchers have proposed a variety of rule-based and learning-based text
obfuscation approaches. However, existing authorship obfuscation approaches do
not consider the adversarial threat model. Specifically, they are not evaluated
against adversarially trained authorship attributors that are aware of
potential obfuscation. To fill this gap, we investigate the problem of
adversarial authorship attribution for deobfuscation. We show that
adversarially trained authorship attributors are able to degrade the
effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate
the effectiveness of adversarial training when the attributor makes incorrect
assumptions about whether and which obfuscator was used. While there is a a
clear degradation in attribution accuracy, it is noteworthy that this
degradation is still at or above the attribution accuracy of the attributor
that is not adversarially trained at all. Our results underline the need for
stronger obfuscation approaches that are resistant to deobfuscation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 3 tables, ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SU-NLP at SemEval-2022 Task 11: Complex Named Entity Recognition with
  Entity Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buse Çarık, Fatih Beyhan, Reyyan Yeniterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the system proposed by Sabanc{\i} University Natural
Language Processing Group in the SemEval-2022 MultiCoNER task. We developed an
unsupervised entity linking pipeline that detects potential entity mentions
with the help of Wikipedia and also uses the corresponding Wikipedia context to
help the classifier in finding the named entity type of that mention. Our
results showed that our pipeline improved performance significantly, especially
for complex entities in low-context settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Listening to Affected Communities to Define Extreme Speech: <span class="highlight-title">Dataset</span> and
  Experiments <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonis Maronikolakis, Axel Wisiorek, Leah Nann, Haris Jabbar, Sahana Udupa, Hinrich Schuetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on current work on multilingual hate speech (e.g., Ousidhoum et al.
(2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present
XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages
from Brazil, Germany, India and Kenya. The key novelty is that we directly
involve the affected communities in collecting and annotating the data - as
opposed to giving companies and governments control over defining and
combatting hate speech. This inclusive approach results in datasets more
representative of actually occurring online speech and is likely to facilitate
the removal of the social media content that marginalized communities view as
causing the most harm. Based on XTREMESPEECH, we establish novel tasks with
accompanying baselines, provide evidence that cross-country training is
generally not feasible due to cultural differences between countries and
perform an interpretability analysis of BERT's predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">BERT</span>-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning
  in Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Murtadha, Shengfeng Pan, Bo Wen, Jianlin Su, Wenze Zhang, Yunfeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) task aims to associate a piece of text
with a set of aspects and meanwhile infer their respective sentimental
polarities. Up to now, the state-of-the-art approaches are built upon
fine-tuning of various pre-trained language models. They commonly aim to learn
the aspect-specific representation in the corpus. Unfortunately, the aspect is
often expressed implicitly through a set of representatives and thus renders
implicit mapping process unattainable unless sufficient labeled examples.
  In this paper, we propose to jointly address aspect categorization and
aspect-based sentiment subtasks in a unified framework. Specifically, we first
introduce a simple but effective mechanism that collaborates the semantic and
syntactic information to construct auxiliary-sentences for the implicit aspect.
Then, we encourage BERT to learn the aspect-specific representation in response
to the automatically constructed auxiliary-sentence instead of the aspect
itself. Finally, we empirically evaluate the performance of the proposed
solution by a comparative study on real benchmark datasets for both ABSA and
Targeted-ABSA tasks. Our extensive experiments show that it consistently
achieves state-of-the-art performance in terms of aspect categorization and
aspect-based sentiment across all datasets and the improvement margins are
considerable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Meta-learning for Low-resource Text Classification and
  Generation via Memory Imitation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng, Dongkyu Lee, Yiping Song, Jian Sun, Nevin L. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building models of natural language processing (NLP) is challenging in
low-resource scenarios where only limited data are available.
Optimization-based meta-learning algorithms achieve promising results in
low-resource scenarios by adapting a well-generalized model initialization to
handle new tasks. Nonetheless, these approaches suffer from the memorization
overfitting issue, where the model tends to memorize the meta-training tasks
while ignoring support sets when adapting to new tasks. To address this issue,
we propose a memory imitation meta-learning (MemIML) method that enhances the
model's reliance on support sets for task adaptation. Specifically, we
introduce a task-specific memory module to store support set information and
construct an imitation module to force query sets to imitate the behaviors of
some representative support-set samples stored in the memory. A theoretical
analysis is provided to prove the effectiveness of our method, and empirical
results also demonstrate that our method outperforms competitive baselines on
both text classification and generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are You Misinformed? A Study of Covid-Related Fake News in Bengali on
  Facebook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Protik Bose Pranto, Syed Zami-Ul-Haque Navid, Protik Dey, Gias Uddin, Anindya Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our opinions and views of life can be shaped by how we perceive the opinions
of others on social media like Facebook. This dependence has increased during
COVID-19 periods when we have fewer means to connect with others. However, fake
news related to COVID-19 has become a significant problem on Facebook. Bengali
is the seventh most spoken language worldwide, yet we are aware of no previous
research that studied the prevalence of COVID-19 related fake news in Bengali
on Facebook. In this paper, we develop machine learning models to detect fake
news in Bengali automatically. The best performing model is BERT, with an
F1-score of 0.97. We apply BERT on all Facebook Bengali posts related to
COVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into
three categories: System (e.g., medical system), belief (e.g., religious
rituals), and social (e.g., scientific awareness).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Relation-Specific Representations for Few-shot Knowledge Graph
  Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Li, Kui Yu, Yuhong Zhang, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed increasing interest in few-shot knowledge graph
completion (FKGC), which aims to infer unseen query triples for a few-shot
relation using a handful of reference triples of the relation. The primary
focus of existing FKGC methods lies in learning the relation representations
that can reflect the common information shared by the query and reference
triples. To this end, these methods learn the embeddings of entities with their
direct neighbors, and use the concatenation of the entity embeddings as the
relation representations. However, the entity embeddings learned only from
direct neighborhoods may have low expressiveness when the entity has sparse
neighbors or shares a common local neighborhood with other entities. Moreover,
the embeddings of two entities are insufficient to represent the semantic
information of their relationship, especially when they have multiple
relations. To address these issues, we propose a Relation-Specific Context
Learning (RSCL) framework, which exploits graph contexts of triples to capture
the semantic information of relations and entities simultaneously.
Specifically, we first extract graph contexts for each triple, which can
provide long-term entity-relation dependencies. To model the graph contexts, we
then develop a hierarchical relation-specific learner to learn global and local
relation-specific representations for relations by capturing contextualized
information of triples and incorporating local information of entities.
Finally, we utilize the learned representations to predict the likelihood of
the query triples. Experimental results on two public datasets demonstrate that
RSCL outperforms state-of-the-art FKGC methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> HOP: History-and-Order Aware <span class="highlight-title">Pre-train</span>ing for <span class="highlight-title">Vision-and-Language</span>
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, <span class="highlight-author">Qi Wu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training has been adopted in a few of recent works for
Vision-and-Language Navigation (VLN). However, previous pre-training methods
for VLN either lack the ability to predict future actions or ignore the
trajectory contexts, which are essential for a greedy navigation process. In
this work, to promote the learning of spatio-temporal visual-textual
correspondence as well as the agent's capability of decision making, we propose
a novel history-and-order aware pre-training paradigm (HOP) with VLN-specific
objectives that exploit the past observations and support future action
prediction. Specifically, in addition to the commonly used Masked Language
Modeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy
tasks to model temporal order information: Trajectory Order Modeling (TOM) and
Group Order Modeling (GOM). Moreover, our navigation action prediction is also
enhanced by introducing the task of Action Prediction with History (APH), which
takes into account the history visual perceptions. Extensive experimental
results on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the
effectiveness of our proposed method compared against several state-of-the-art
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utterance Rewriting with <span class="highlight-title">Contrastive Learning</span> in Multi-turn <span class="highlight-title">Dialogue</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Wang, Tangjian Duan, Zihao Wang, Minghui Yang, Zujie Wen, Yongliang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context modeling plays a significant role in building multi-turn dialogue
systems. In order to make full use of context information, systems can use
Incomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue
into single-turn by merging current utterance and context information into a
self-contained utterance. However, previous approaches ignore the intent
consistency between the original query and rewritten query. The detection of
omitted or coreferred locations in the original query can be further improved.
In this paper, we introduce contrastive learning and multi-task learning to
jointly model the problem. Our method benefits from carefully designed
self-supervised objectives, which act as auxiliary tasks to capture semantics
at both sentence-level and token-level. The experiments show that our proposed
model achieves state-of-the-art performance on several public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Text-to-Speech Pipeline, Evaluation Methodology, and Initial
  Fine-Tuning Results for Child Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Jain, Mariam Yiwere, Dan Bigioi, Peter Corcoran, Horia Cucu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech synthesis has come a long way as current text-to-speech (TTS) models
can now generate natural human-sounding speech. However, most of the TTS
research focuses on using adult speech data and there has been very limited
work done on child speech synthesis. This study developed and validated a
training pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models
using child speech datasets. This approach adopts a multispeaker TTS retuning
workflow to provide a transfer-learning pipeline. A publicly available child
speech dataset was cleaned to provide a smaller subset of approximately 19
hours, which formed the basis of our fine-tuning experiments. Both subjective
and objective evaluations were performed using a pretrained MOSNet for
objective evaluation and a novel subjective framework for mean opinion score
(MOS) evaluations. Subjective evaluations achieved the MOS of 3.92 for speech
intelligibility, 3.85 for voice naturalness, and 3.96 for voice consistency.
Objective evaluation using a pretrained MOSNet showed a strong correlation
between real and synthetic child voices. The final trained model was able to
synthesize child-like speech from reference audio samples as short as 5
seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ACCESS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factual Consistency of Multilingual <span class="highlight-title">Pretrain</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models can be queried for factual knowledge, with
potential applications in knowledge base acquisition and tasks that require
inference. However, for that, we need to know how reliable this knowledge is,
and recent work has shown that monolingual English language models lack
consistency when predicting factual knowledge, that is, they fill-in-the-blank
differently for paraphrases describing the same fact. In this paper, we extend
the analysis of consistency to a multilingual setting. We introduce a resource,
mParaRel, and investigate (i) whether multilingual language models such as
mBERT and XLM-R are more consistent than their monolingual counterparts; and
(ii) if such models are equally consistent across languages. We find that mBERT
is as inconsistent as English BERT in English paraphrases, but that both mBERT
and XLM-R exhibit a high degree of inconsistency in English and even more so
for all the other 45 languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approaches for Improving the Performance of Fake News Detection in
  Bangla: Imbalance Handling and Model Stacking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Muzakker Hossain, Zahin Awosaf, Md. Salman Hossan Prottoy, Abu Saleh Muhammod Alvy, Md. Kishor Morol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imbalanced datasets can lead to biasedness into the detection of fake news.
In this work, we present several strategies for resolving the imbalance issue
for fake news detection in Bangla with a comparative assessment of proposed
methodologies. Additionally, we propose a technique for improving performance
even when the dataset is imbalanced. We applied our proposed approaches to
BanFakeNews, a dataset developed for the purpose of detecting fake news in
Bangla comprising of 50K instances but is significantly skewed, with 97% of
majority instances. We obtained a 93.1% F1-score using data manipulation
manipulation techniques such as SMOTE, and a 79.1% F1-score using without data
manipulation approaches such as Stacked Generalization. Without implementing
these techniques, the F1-score would have been 67.6% for baseline models. We
see this work as an important step towards paving the way of fake news
detection in Bangla. By implementing these strategies the obstacles of
imbalanced dataset can be removed and improvement in the performance can be
achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, To appear in the Proceedings of the
  International Conference on 4th Industrial Revolution and Beyond (IC4IR),
  10-11 December 2021, Dhaka, Bangladesh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> WuDaoMM: A large-scale <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Dataset</span> for <span class="highlight-title">Pre-train</span>ing models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sha Yuan, Zhao Shuai, Leng Jiahong, Xue Zhao, Zhao Hanyu, <span class="highlight-author">Tang Jie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling speech recognition and synthesis simultaneously: Encoding and
  decoding lexical and sublexical semantic information into speech with no
  direct access to speech data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gašper Beguš, Alan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human speakers encode information into raw speech which is then decoded by
the listeners. This complex relationship between encoding (production) and
decoding (perception) is often modeled separately. Here, we test how decoding
of lexical and sublexical semantic information can emerge automatically from
raw speech in unsupervised generative deep convolutional networks that combine
both the production and perception principle. We introduce, to our knowledge,
the most challenging objective in unsupervised lexical learning: an
unsupervised network that must learn to assign unique representations for
lexical items with no direct access to training data. We train several models
(ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic
lexical items in the unobserved test data. Strong evidence in favor of lexical
learning emerges. The architecture that combines the production and perception
principles is thus able to learn to decode unique information from raw acoustic
data in an unsupervised manner without ever accessing real training data. We
propose a technique to explore lexical and sublexical learned representations
in the classifier network. The results bear implications for both unsupervised
speech synthesis and recognition as well as for unsupervised semantic modeling
as language models increasingly bypass text and operate from raw acoustics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demo of the Linguistic Field Data Management and Analysis System -- LiFE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Ritesh Kumar, Shyam Ratan, Sonal Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the proposed demo, we will present a new software - Linguistic Field Data
Management and Analysis System - LiFE (https://github.com/kmi-linguistics/life)
- an open-source, web-based linguistic data management and analysis application
that allows for systematic storage, management, sharing and usage of linguistic
data collected from the field. The application allows users to store lexical
items, sentences, paragraphs, audio-visual content with rich glossing /
annotation; generate interactive and print dictionaries; and also train and use
natural language processing tools and models for various purposes using this
data. Since its a web-based application, it also allows for seamless
collaboration among multiple persons and sharing the data, models, etc with
each other.
  The system uses the Python-based Flask framework and MongoDB in the backend
and HTML, CSS and Javascript at the frontend. The interface allows creation of
multiple projects that could be shared with the other users. At the backend,
the application stores the data in RDF format so as to allow its release as
Linked Data over the web using semantic web technologies - as of now it makes
use of the OntoLex-Lemon for storing the lexical data and Ligt for storing the
interlinear glossed text and then internally linking it to the other linked
lexicons and databases such as DBpedia and WordNet. Furthermore it provides
support for training the NLP systems using scikit-learn and HuggingFace
Transformers libraries as well as make use of any model trained using these
libraries - while the user interface itself provides limited options for tuning
the system, an externally-trained model could be easily incorporated within the
application; similarly the dataset itself could be easily exported into a
standard machine-readable format like JSON or CSV that could be consumed by
other programs and pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 19th International Conference on Natural Language
  Processing (ICON-2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-guided Disentangled Tuning for <span class="highlight-title">Pretrain</span>ed Language Models <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Zeng, Yufan Jiang, Shuangzhi Wu, Yongjing Yin, Mu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) trained on large-scale unlabeled corpus are
typically fine-tuned on task-specific downstream datasets, which have produced
state-of-the-art results on various NLP tasks. However, the data discrepancy
issue in domain and scale makes fine-tuning fail to efficiently capture
task-specific patterns, especially in the low data regime. To address this
issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which
enhances the generalization of representations by disentangling task-relevant
signals from the entangled representations. For a given task, we introduce a
learnable confidence model to detect indicative guidance from context, and
further propose a disentangled regularization to mitigate the over-reliance
problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives
consistently better results than fine-tuning with different PLMs, and extensive
analysis demonstrates the effectiveness and robustness of our method. Code is
available at https://github.com/lemon0830/TDT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Abstractive Grounded Summarization of Podcast Transcripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Podcasts have recently shown a rapid rise in popularity. Summarization of
podcast transcripts is of practical benefit to both content providers and
consumers. It helps consumers to quickly decide whether they will listen to the
podcasts and reduces the cognitive load of content providers to write
summaries. Nevertheless, podcast summarization faces significant challenges
including factual inconsistencies with respect to the inputs. The problem is
exacerbated by speech disfluencies and recognition errors in transcripts of
spoken language. In this paper, we explore a novel abstractive summarization
method to alleviate these challenges. Specifically, our approach learns to
produce an abstractive summary while grounding summary segments in specific
portions of the transcript to allow for full inspection of summary details. We
conduct a series of analyses of the proposed approach on a large podcast
dataset and show that the approach can achieve promising results. Grounded
summaries bring clear benefits in locating the summary and transcript segments
that contain inconsistent information, and hence significantly improve
summarization quality in both automatic and human evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Confidence for <span class="highlight-title">Transformer</span>-based Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, Mu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence estimation aims to quantify the confidence of the model
prediction, providing an expectation of success. A well-calibrated confidence
estimate enables accurate failure prediction and proper risk measurement when
given noisy samples and out-of-distribution data in real-world settings.
However, this task remains a severe challenge for neural machine translation
(NMT), where probabilities from softmax distribution fail to describe when the
model is probably mistaken. To address this problem, we propose an unsupervised
confidence estimate learning jointly with the training of the NMT model. We
explain confidence as how many hints the NMT model needs to make a correct
prediction, and more hints indicate low confidence. Specifically, the NMT model
is given the option to ask for hints to improve translation accuracy at the
cost of some slight penalty. Then, we approximate their level of confidence by
counting the number of hints the model uses. We demonstrate that our learned
confidence estimate achieves high accuracy on extensive sentence/word-level
quality estimation tasks. Analytical results verify that our confidence
estimate can correctly assess underlying risk in two real-world scenarios: (1)
discovering noisy samples and (2) detecting out-of-domain data. We further
propose a novel confidence-based instance-specific label smoothing approach
based on our learned confidence estimate, which outperforms standard label
smoothing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suum Cuique: Studying Bias in Taboo Detection with a Community
  Perspective <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osama Khalid, Jonathan Rusert, Padmini Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has discussed and illustrated the need to consider linguistic
norms at the community level when studying taboo (hateful/offensive/toxic etc.)
language. However, a methodology for doing so, that is firmly founded on
community language norms is still largely absent. This can lead both to biases
in taboo text classification and limitations in our understanding of the causes
of bias. We propose a method to study bias in taboo classification and
annotation where a community perspective is front and center. This is
accomplished by using special classifiers tuned for each community's language.
In essence, these classifiers represent community level language norms. We use
these to study bias and find, for example, biases are largest against African
Americans (7/10 datasets and all 3 classifiers examined). In contrast to
previous papers we also study other communities and find, for example, strong
biases against South Asians. In a small scale user study we illustrate our key
idea which is that common utterances, i.e., those with high alignment scores
with a community (community classifier confidence scores) are unlikely to be
regarded taboo. Annotators who are community members contradict taboo
classification decisions and annotations in a majority of instances. This paper
is a significant step toward reducing false positive taboo decisions that over
time harm minority communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, Accepted to the Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLSP 2021 Shared Task: Vietnamese Machine Reading Comprehension <span class="chip">SP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiet Van Nguyen, Son Quoc Tran, Luan Thanh Nguyen, Tin Van Huynh, Son T. Luu, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the shared task on Vietnamese MRC at the Eighth Workshop
on Vietnamese Language and Speech Processing (VLSP 2021). This task attracted
77 participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the shared task, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% EM and 67.43% F1-score on the private test set.
The Vietnamese MRC systems proposed by the top 3 teams use XLM-RoBERTa, a
powerful pre-trained language model using the transformer architecture. The
UIT-ViQuAD 2.0 dataset motivates more researchers to explore Vietnamese machine
reading comprehension, question answering, and question generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VLSP 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving <span class="highlight-title">Conversation</span>al Goals with Unsupervised Post-hoc Knowledge
  Injection <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-Kirkpatrick, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A limitation of current neural dialog models is that they tend to suffer from
a lack of specificity and informativeness in generated responses, primarily due
to dependence on training data that covers a limited variety of scenarios and
conveys limited knowledge. One way to alleviate this issue is to extract
relevant knowledge from external sources at decoding time and incorporate it
into the dialog response. In this paper, we propose a post-hoc
knowledge-injection technique where we first retrieve a diverse set of relevant
knowledge snippets conditioned on both the dialog history and an initial
response from an existing dialog model. We construct multiple candidate
responses, individually injecting each retrieved snippet into the initial
response using a gradient-based decoding method, and then select the final
response with an unsupervised ranking step. Our experiments in goal-oriented
and knowledge-grounded dialog settings demonstrate that human annotators judge
the outputs from the proposed method to be more engaging and informative
compared to responses from prior dialog systems. We further show that
knowledge-augmentation promotes success in achieving conversational goals in
both experimental settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Towards Textual Out-of-Domain Detection without In-Domain Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Jin, Shuyang Gao, Seokhwan Kim, <span class="highlight-author">Yang Liu</span>, Dilek Hakkani-Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world settings, machine learning models need to identify user
inputs that are out-of-domain (OOD) so as to avoid performing wrong actions.
This work focuses on a challenging case of OOD detection, where no labels for
in-domain data are accessible (e.g., no intent labels for the intent
classification task). To this end, we first evaluate different language model
based approaches that predict likelihood for a sequence of tokens. Furthermore,
we propose a novel representation learning based method by combining
unsupervised clustering and contrastive learning so that better data
representations for OOD detection can be learned. Through extensive
experiments, we demonstrate that this method can significantly outperform
likelihood-based methods and can be even competitive to the state-of-the-art
supervised approaches with label information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/ACM Transactions on Audio Speech and Language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Robust <span class="highlight-title">Spoken Language Understanding</span> by Cross Attention between
  Phoneme Sequence and ASR Hypothesis <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexun Wang, Yuquan Le, Yi Zhu, Yuming Zhao, Mingchao Feng, Meng Chen, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building Spoken Language Understanding (SLU) robust to Automatic Speech
Recognition (ASR) errors is an essential issue for various voice-enabled
virtual assistants. Considering that most ASR errors are caused by phonetic
confusion between similar-sounding expressions, intuitively, leveraging the
phoneme sequence of speech can complement ASR hypothesis and enhance the
robustness of SLU. This paper proposes a novel model with Cross Attention for
SLU (denoted as CASLU). The cross attention block is devised to catch the
fine-grained interactions between phoneme and word embeddings in order to make
the joint representations catch the phonetic and semantic features of input
simultaneously and for overcoming the ASR errors in downstream natural language
understanding (NLU) tasks. Extensive experiments are conducted on three
datasets, showing the effectiveness and competitiveness of our approach.
Additionally, We also validate the universality of CASLU and prove its
complementarity when combining with other robust SLU techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Transformations in <span class="highlight-title">Contrastive</span> <span class="highlight-title">Self-Supervised</span> Learning: A <span class="highlight-title">Review</span> <span class="chip">IJCAI'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrita Bhattacharjee, Mansooreh Karami, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive self-supervised learning has become a prominent technique in
representation learning. The main step in these methods is to contrast
semantically similar and dissimilar pairs of samples. However, in the domain of
Natural Language, the augmentation methods used in creating similar pairs with
regard to contrastive learning assumptions are challenging. This is because,
even simply modifying a word in the input might change the semantic meaning of
the sentence, and hence, would violate the distributional hypothesis. In this
review paper, we formalize the contrastive learning framework in the domain of
natural language processing. We emphasize the considerations that need to be
addressed in the data transformation step and review the state-of-the-art
methods and evaluations for contrastive representation learning in NLP.
Finally, we describe some challenges and potential directions for learning
better text representations using contrastive methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review at IJCAI'22 Survey Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Vision-and-Language</span> Navigation: A <span class="highlight-title">Survey</span> of Tasks, Methods, and Future
  Directions <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Gu, Eliana Stefani, <span class="highlight-author">Qi Wu</span>, Jesse Thomason, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages. Accepted to ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rewire-then-Probe: A <span class="highlight-title">Contrastive</span> Recipe for Probing Biomedical Knowledge
  of <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su, Charlotte Collins, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge probing is crucial for understanding the knowledge transfer
mechanism behind the pre-trained language models (PLMs). Despite the growing
progress of probing knowledge for PLMs in the general domain, specialised areas
such as biomedical domain are vastly under-explored. To catalyse the research
in this direction, we release a well-curated biomedical knowledge probing
benchmark, MedLAMA, which is constructed based on the Unified Medical Language
System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs
and probing approaches on our benchmark, reaching at most 3% of acc@10. While
highlighting various sources of domain-specific challenges that amount to this
underwhelming performance, we illustrate that the underlying PLMs have a higher
potential for probing tasks. To achieve this, we propose Contrastive-Probe, a
novel self-supervised contrastive probing approach, that adjusts the underlying
PLMs without using any probing data. While Contrastive-Probe pushes the acc@10
to 28%, the performance gap still remains notable. Our human expert evaluation
suggests that the probing performance of our Contrastive-Probe is still
under-estimated as UMLS still does not include the full spectrum of factual
knowledge. We hope MedLAMA and Contrastive-Probe facilitate further
developments of more suited probing techniques for this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022; code and data are released at
  https://github.com/cambridgeltl/medlama</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled Sequence to Sequence Learning for Compositional
  Generalization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zheng, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is mounting evidence that existing neural network models, in particular
the very popular sequence-to-sequence architecture, struggle to systematically
generalize to unseen compositions of seen components. We demonstrate that one
of the reasons hindering compositional generalization relates to
representations being entangled. We propose an extension to
sequence-to-sequence models which encourages disentanglement by adaptively
re-encoding (at each time step) the source input. Specifically, we condition
the source representations on the newly decoded target context which makes it
easier for the encoder to exploit specialized information for each prediction
rather than capturing it all in a single forward pass. Experimental results on
semantic parsing and machine translation empirically show that our proposal
delivers more disentangled representations and better generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Graph <span class="highlight-title">Pre-train</span>ing for AMR Parsing and Generation <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefeng Bai, Yulong Chen, <span class="highlight-author">Yue Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2022 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptation and Multi-Domain Adaptation for Neural Machine
  Translation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.06951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.06951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danielle Saunders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of deep learning techniques has allowed Neural Machine
Translation (NMT) models to become extremely powerful, given sufficient
training data and training time. However, systems struggle when translating
text from a new domain with a distinct style or vocabulary. Fine-tuning on
in-domain data allows good domain adaptation, but requires sufficient relevant
bilingual data. Even if this is available, simple fine-tuning can cause
overfitting to new data and `catastrophic forgetting' of previously learned
behaviour.
  We concentrate on robust approaches to domain adaptation for NMT,
particularly where a system may need to translate across multiple domains. We
divide techniques into those revolving around data selection or generation,
model architecture, parameter adaptation procedure, and inference procedure. We
finally highlight the benefits of domain adaptation and multi-domain adaptation
techniques to other lines of NMT research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages + references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InFillmore: Frame-Guided Language Generation with Bidirectional Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.04941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.04941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiefu Ou, Nathaniel Weir, Anton Belyy, Felix Yu, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a structured extension to bidirectional-context conditional
language generation, or "infilling," inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss in distinguishability from human-generated text.
Our methods flexibly apply to a variety of use scenarios, and we provide a
codebase and interactive demo available from
https://nlp.jhu.edu/demos/infillmore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in *SEM 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Translation from Signed to Spoken Languages: State of the Art
  and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03086v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03086v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu De Coster, Dimitar Shterionov, Mieke Van Herreweghe, Joni Dambre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic translation from signed to spoken languages is an interdisciplinary
research domain, lying on the intersection of computer vision, machine
translation and linguistics. Nevertheless, research in this domain is performed
mostly by computer scientists in isolation. As the domain is becoming
increasingly popular - the majority of scientific papers on the topic of sign
language translation have been published in the past three years - we provide
an overview of the state of the art as well as some required background in the
different related disciplines. We give a high-level introduction to sign
language linguistics and machine translation to illustrate the requirements of
automatic sign language translation. We present a systematic literature review
to illustrate the state of the art in the domain and then, harking back to the
requirements, lay out several challenges for future research. We find that
significant advances have been made on the shoulders of spoken language machine
translation research. However, current approaches are often not linguistically
motivated or are not adapted to the different input modality of sign languages.
We explore challenges related to the representation of sign language data, the
collection of datasets, the need for interdisciplinary research and
requirements for moving beyond research, towards applications. Based on our
findings, we advocate for interdisciplinary research and to base future
research on linguistic analysis of sign languages. Furthermore, the inclusion
of deaf and hearing end users of sign language translation applications in use
case identification, data collection and evaluation is of the utmost importance
in the creation of useful sign language translation models. We recommend
iterative, human-in-the-loop, design and development of sign language
translation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-MORE: <span class="highlight-title">Pretrain</span>ing to Answer Open-Domain Questions by Consulting
  Millions of References <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Yue, Xiaoman Pan, Wenlin Yao, Dian Yu, Dong Yu, Jianshu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of pretraining a two-stage open-domain question
answering (QA) system (retriever + reader) with strong transfer capabilities.
The key challenge is how to construct a large amount of high-quality
question-answer-context triplets without task-specific annotations.
Specifically, the triplets should align well with downstream tasks by: (i)
covering a wide range of domains (for open-domain applications), (ii) linking a
question to its semantically relevant context with supporting evidence (for
training the retriever), and (iii) identifying the correct answer in the
context (for training the reader). Previous pretraining approaches generally
fall short of one or more of these requirements. In this work, we automatically
construct a large-scale corpus that meets all three criteria by consulting
millions of references cited within Wikipedia. The well-aligned pretraining
signals benefit both the retriever and the reader significantly. Our pretrained
retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our
pretrained reader, the entire system improves by up to 4% in exact match.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Jiangyan Yi, Ruibo Fu, Jianhua Tao, Zhengqi Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The text-based speech editor allows the editing of speech through intuitive
cutting, copying, and pasting operations to speed up the process of editing
speech. However, the major drawback of current systems is that edited speech
often sounds unnatural due to cut-copy-paste operation. In addition, it is not
obvious how to synthesize records according to a new word not appearing in the
transcript. This paper proposes a novel end-to-end text-based speech editing
method called context-aware mask prediction network (CampNet). The model can
simulate the text-based speech editing process by randomly masking part of
speech and then predicting the masked region by sensing the speech context. It
can solve unnatural prosody in the edited region and synthesize the speech
corresponding to the unseen words in the transcript. Secondly, for the possible
operation of text-based speech editing, we design three text-based operations
based on CampNet: deletion, insertion, and replacement. These operations can
cover various situations of speech editing. Thirdly, to synthesize the speech
corresponding to long text in insertion and replacement operations, a
word-level autoregressive generation method is proposed. Fourthly, we propose a
speaker adaptation method using only one sentence for CampNet and explore the
ability of few-shot learning based on CampNet, which provides a new idea for
speech forgery tasks. The subjective and objective experiments on VCTK and
LibriTTS datasets show that the speech editing results based on CampNet are
better than TTS technology, manual editing, and VoCo method. We also conduct
detailed ablation experiments to explore the effect of the CampNet structure on
its performance. Finally, the experiment shows that speaker adaptation with
only one sentence can further improve the naturalness of speech. Examples of
generated speech can be found at https://hairuo55.github.io/CampNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review, 14 pages, 14 figures, demo page is available at
  https://hairuo55.github.io/CampNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supporting Land Reuse of Former Open Pit Mining Sites using Text
  Classification and Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.05557v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.05557v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Schröder, Kim Bürgl, Yves Annanias, Andreas Niekler, Lydia Müller, Daniel Wiegreffe, Christian Bender, Christoph Mengs, Gerik Scheuermann, Gerhard Heyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open pit mines left many regions worldwide inhospitable or uninhabitable. To
put these regions back into use, entire stretches of land must be
renaturalized. For the sustainable subsequent use or transfer to a new primary
use, many contaminated sites and soil information have to be permanently
managed. In most cases, this information is available in the form of expert
reports in unstructured data collections or file folders, which in the best
case are digitized. Due to size and complexity of the data, it is difficult for
a single person to have an overview of this data in order to be able to make
reliable statements. This is one of the most important obstacles to the rapid
transfer of these areas to after-use. An information-based approach to this
issue supports fulfilling several Sustainable Development Goals regarding
environment issues, health and climate action. We use a stack of Optical
Character Recognition, Text Classification, Active Learning and Geographic
Information System Visualization to effectively mine and visualize this
information. Subsequently, we link the extracted information to geographic
coordinates and visualize them using a Geographic Information System. Active
Learning plays a vital role because our dataset provides no training data. In
total, we process nine categories and actively learn their representation in
our dataset. We evaluate the OCR, Active Learning and Text Classification
separately to report the performance of the system. Active Learning and text
classification results are twofold: Whereas our categories about restrictions
work sufficient ($>$.85 F1), the seven topic-oriented categories were
complicated for human coders and hence the results achieved mediocre evaluation
scores ($<$.70 F1).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Dual Read/Write Paths for Simultaneous Machine Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation (SiMT) outputs translation while reading
source sentence and hence requires a policy to decide whether to wait for the
next source word (READ) or generate a target word (WRITE), the actions of which
form a read/write path. Although the read/write path is essential to SiMT
performance, no direct supervision is given to the path in the existing
methods. In this paper, we propose a method of dual-path SiMT which introduces
duality constraints to direct the read/write path. According to duality
constraints, the read/write path in source-to-target and target-to-source SiMT
models can be mapped to each other. As a result, the two SiMT models can be
optimized jointly by forcing their read/write paths to satisfy the mapping.
Experiments on En-Vi and De-En tasks show that our method can outperform strong
baselines under all latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2022 main conference. 19 pages, 12 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FastCorrect: Fast Error Correction with Edit Alignment for Automatic
  Speech Recognition <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.03842v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.03842v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiang-Yang Li, Ed Lin, <span class="highlight-author">Tie-Yan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the popular NAR models adopted in neural machine translation and
text edition by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021. Code URL:
  https://github.com/microsoft/NeuralSpeech/tree/master/FastCorrect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ N-Shot Learning for Augmenting <span class="highlight-title">Task-Oriented</span> <span class="highlight-title">Dialogue</span> State Tracking <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.00293v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.00293v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Aksu, Zhengyuan Liu, Min-Yen Kan, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmentation of task-oriented dialogues has followed standard methods used
for plain-text such as back-translation, word-level manipulation, and
paraphrasing despite its richly annotated structure. In this work, we introduce
an augmentation framework that utilizes belief state annotations to match turns
from various dialogues and form new synthetic dialogues in a bottom-up manner.
Unlike other augmentation strategies, it operates with as few as five examples.
Our augmentation strategy yields significant improvements when both adapting a
DST model to a new domain, and when adapting a language model to the DST task,
on evaluations with TRADE and TOD-BERT models. Further analysis shows that our
model performs better on seen values during training, and it is also more
robust to unseen values. We conclude that exploiting belief state annotations
enhances dialogue augmentation and results in improved models in n-shot
training scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> XTREME-S: Evaluating Cross-lingual Speech Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Conneau, Ankur Bapna, Yu Zhang, Min Ma, Patrick von Platen, Anton Lozhkov, Colin Cherry, Ye Jia, Clara Rivera, Mihir Kale, Daan Van Esch, Vera Axelrod, Simran Khanuja, Jonathan H. Clark, Orhan Firat, Michael Auli, <span class="highlight-author">Sebastian Ruder</span>, Jason Riesa, Melvin Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual
speech representations in many languages. XTREME-S covers four task families:
speech recognition, classification, speech-to-text translation and retrieval.
Covering 102 languages from 10+ language families, 3 different domains and 4
task families, XTREME-S aims to simplify multilingual speech representation
evaluation, as well as catalyze research in "universal" speech representation
learning. This paper describes the new benchmark and establishes the first
speech-only and speech-text baselines using XLS-R and mSLAM on all downstream
tasks. We motivate the design choices and detail how to use the benchmark.
Datasets and fine-tuning scripts are made easily accessible at
https://hf.co/datasets/google/xtreme_s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic <span class="highlight-title">Review</span>-based Recommenders <span class="chip">SC21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostadin Cvejoski, Ramses J. Sanchez, Christian Bauckhage, Cesar Ojeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Just as user preferences change with time, item reviews also reflect those
same preference changes. In a nutshell, if one is to sequentially incorporate
review content knowledge into recommender systems, one is naturally led to
dynamical models of text. In the present work we leverage the known power of
reviews to enhance rating predictions in a way that (i) respects the causality
of review generation and (ii) includes, in a bidirectional fashion, the ability
of ratings to inform language review models and vice-versa, language
representations that help predict ratings end-to-end. Moreover, our
representations are time-interval aware and thus yield a continuous-time
representation of the dynamics. We provide experiments on real-world datasets
and show that our methodology is able to outperform several state-of-the-art
models. Source code for all models can be found at [1].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages, Published at International Data Science Conference 2021
  (iDSC21)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ing with Artificial Language: Studying Transferable Knowledge in
  Language Models <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryokan Ri, Yoshimasa Tsuruoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning When to Translate for Streaming Speech <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.07368v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.07368v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Dong, Yaoming Zhu, Mingxuan Wang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to find proper moments to generate partial sentence translation given a
streaming speech input? Existing approaches waiting-and-translating for a fixed
duration often break the acoustic units in speech, since the boundaries between
acoustic units in speech are not even. In this paper, we propose MoSST, a
simple yet effective method for translating streaming speech content. Given a
usually long speech sequence, we develop an efficient monotonic segmentation
module inside an encoder-decoder model to accumulate acoustic information
incrementally and detect proper speech unit boundaries for the input in speech
translation task. Experiments on multiple translation directions of the MuST-C
dataset show that MoSST outperforms existing methods and achieves the best
trade-off between translation quality (BLEU) and latency. Our code is available
at https://github.com/dqqcasia/mosst.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2022 main conference. 15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-Verse: Bidirectional Generation Between Image and Text <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11133v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11133v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Misinfo Reaction Frames: Reasoning about Readers' Reactions to News
  Headlines <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.08790v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.08790v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saadia Gabriel, Skyler Hallinan, Maarten Sap, Pemi Nguyen, Franziska Roesner, Eunsol Choi, <span class="highlight-author">Yejin Choi</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.
feeling distrust), and behaviorally (e.g. sharing the news with their friends).
Such reactions are instantaneous and yet complex, as they rely on factors that
go beyond interpreting factual content of news. We propose Misinfo Reaction
Frames (MRF), a pragmatic formalism for modeling how readers might react to a
news headline. In contrast to categorical schema, our free-text dimensions
provide a more nuanced way of understanding intent beyond being benign or
malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced
dataset of reactions to over 25k news headlines focusing on global crises: the
Covid-19 pandemic, climate change, and cancer. Empirical results confirm that
it is indeed possible for neural models to predict the prominent patterns of
readers' reactions to previously unseen news headlines. Additionally, our user
study shows that displaying machine-generated MRF implications alongside news
headlines to readers can increase their trust in real news while decreasing
their trust in misinformation. Our work demonstrates the feasibility and
importance of pragmatic inferences on news headlines to help enhance AI-guided
misinformation detection and mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query and Extract: Refining Event Extraction as Type-oriented Binary
  Decoding <span class="chip">ACL'2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Wang, Mo Yu, Shiyu Chang, Lichao Sun, Lifu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction is typically modeled as a multi-class classification problem
where event types and argument roles are treated as atomic symbols. These
approaches are usually limited to a set of pre-defined types. We propose a
novel event extraction framework that uses event types and argument roles as
natural language queries to extract candidate triggers and arguments from the
input text. With the rich semantics in the queries, our framework benefits from
the attention mechanisms to better capture the semantic correlation between the
event types or argument roles and the input text. Furthermore, the
query-and-extract formulation allows our approach to leverage all available
event annotations from various ontologies as a unified model. Experiments on
ACE and ERE demonstrate that our approach achieves state-of-the-art performance
on each dataset and significantly outperforms existing methods on zero-shot
event extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages, ACL'2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ g2pW: A Conditional Weighted Softmax <span class="highlight-title">BERT</span> for Polyphone Disambiguation
  in Mandarin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chang Chen, Yu-Chuan Chang, Yen-Cheng Chang, Yi-Ren Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Insterspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Word Translation via Two-Stage <span class="highlight-title">Contrastive Learning</span> <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyiran Li, Fangyu Liu, Nigel Collier, Anna Korhonen, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual
task, aiming to bridge the lexical gap between different languages. In this
work, we propose a robust and effective two-stage contrastive learning
framework for the BLI task. At Stage C1, we propose to refine standard
cross-lingual linear maps between static word embeddings (WEs) via a
contrastive learning objective; we also show how to integrate it into the
self-learning procedure for even more refined cross-lingual maps. In Stage C2,
we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word
translation capability. We also show that static WEs induced from the
`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments
on standard BLI datasets for diverse languages and different experimental
setups demonstrate substantial gains achieved by our framework. While the BLI
method from Stage C1 already yields substantial gains over all state-of-the-art
BLI methods in our comparison, even stronger improvements are met with the full
two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28
language pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRS: Combining Generation and Revision in Unsupervised Sentence
  Simplification <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Dehghan, Dhruv Kumar, Lukasz Golab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose GRS: an unsupervised approach to sentence simplification that
combines text generation and text revision. We start with an iterative
framework in which an input sentence is revised using explicit edit operations,
and add paraphrasing as a new edit operation. This allows us to combine the
advantages of generative and revision-based approaches: paraphrasing captures
complex edit operations, and the use of explicit edit operations in an
iterative manner provides controllability and interpretability. We demonstrate
these advantages of GRS compared to existing methods on the Newsela and ASSET
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical prosody modeling and control in non-autoregressive parallel
  neural TTS <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.02952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.02952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomo Raitio, Jiangchuan Li, Shreyas Seshadri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural text-to-speech (TTS) synthesis can generate speech that is
indistinguishable from natural speech. However, the synthetic speech often
represents the average prosodic style of the database instead of having more
versatile prosodic variation. Moreover, many models lack the ability to control
the output prosody, which does not allow for different styles for the same text
input. In this work, we train a non-autoregressive parallel neural TTS
front-end model hierarchically conditioned on both coarse and fine-grained
acoustic speech features to learn a latent prosody space with intuitive and
meaningful dimensions. Experiments show that a non-autoregressive TTS model
hierarchically conditioned on utterance-wise pitch, pitch range, duration,
energy, and spectral tilt can effectively control each prosodic dimension,
generate a wide variety of speaking styles, and provide word-wise emphasis
control, while maintaining equal or better quality to the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, preprint accepted to ICASSP 2022. arXiv admin
  note: text overlap with arXiv:2009.06775</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ φ-SfT: Shape-from-Template with a Physics-Based Deformation Model <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navami Kairanda, Edith Tretschk, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shape-from-Template (SfT) methods estimate 3D surface deformations from a
single monocular RGB camera while assuming a 3D state known in advance (a
template). This is an important yet challenging problem due to the
under-constrained nature of the monocular setting. Existing SfT techniques
predominantly use geometric and simplified deformation models, which often
limits their reconstruction abilities. In contrast to previous works, this
paper proposes a new SfT approach explaining 2D observations through physical
simulations accounting for forces and material properties. Our differentiable
physics simulator regularises the surface evolution and optimises the material
elastic properties such as bending coefficients, stretching stiffness and
density. We use a differentiable renderer to minimise the dense reprojection
error between the estimated 3D states and the input images and recover the
deformation parameters using an adaptive gradient-based optimisation. For the
evaluation, we record with an RGB-D camera challenging real surfaces exposed to
physical forces with various material properties and textures. Our approach
significantly reduces the 3D reconstruction error compared to multiple
competing methods. For the source code and data, see
https://4dqv.mpi-inf.mpg.de/phi-SfT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures and one table; Computer Vision and Pattern
  Recognition (CVPR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 4D-OR: Semantic Scene Graphs for OR Domain Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Özsoy, Evin Pınar Örnek, Ulrich Eck, Tobias Czempiel, Federico Tombari, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical procedures are conducted in highly complex operating rooms (OR),
comprising different actors, devices, and interactions. To date, only medically
trained human experts are capable of understanding all the links and
interactions in such a demanding environment. This paper aims to bring the
community one step closer to automated, holistic and semantic understanding and
modeling of OR domain. Towards this goal, for the first time, we propose using
semantic scene graphs (SSG) to describe and summarize the surgical scene. The
nodes of the scene graphs represent different actors and objects in the room,
such as medical staff, patients, and medical equipment, whereas edges are the
relationships between them. To validate the possibilities of the proposed
representation, we create the first publicly available 4D surgical SSG dataset,
4D-OR, containing ten simulated total knee replacement surgeries recorded with
six RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734
frames and is richly annotated with SSGs, human and object poses, and clinical
roles. We propose an end-to-end neural network-based SSG generation pipeline,
with a rate of success of 0.75 macro F1, indeed being able to infer semantic
reasoning in the OR. We further demonstrate the representation power of our
scene graphs by using it for the problem of clinical role prediction, where we
achieve 0.85 macro F1. The code and dataset will be made available upon
acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span> Array Keeps the Bias Away: Debiasing <span class="highlight-title">Vision-Language</span> Models
  with Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Berg, Siobhan Mackenzie Hall, Yash Bhalgat, Wonsuk Yang, Hannah Rose Kirk, Aleksandar Shtedritski, Max Bain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures. For code and trained token embeddings, see
  https://github.com/oxai/debias-vision-lang</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from All Vehicles <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dian Chen, Philipp Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a system to train driving policies from experiences
collected not just from the ego-vehicle, but all vehicles that it observes.
This system uses the behaviors of other agents to create more diverse driving
scenarios without collecting additional data. The main difficulty in learning
from other vehicles is that there is no sensor information. We use a set of
supervisory tasks to learn an intermediate representation that is invariant to
the viewpoint of the controlling vehicle. This not only provides a richer
signal at training time but also allows more complex reasoning during
inference. Learning how all vehicles drive helps predict their behavior at test
time and can avoid collisions. We evaluate this system in closed-loop driving
simulations. Our system outperforms all prior methods on the public CARLA
Leaderboard by a wide margin, improving driving score by 25 and route
completion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving
challenge. Demo videos are available at https://dotchen.github.io/LAV/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted to CVPR 2022; Code and data available at
  https://github.com/dotchen/LAV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> <span class="highlight-title">Distillation</span> by Matching Training Trajectories <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, Jun-Yan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 website:
  https://georgecazenavette.github.io/mtt-distillation/ code:
  https://github.com/GeorgeCazenavette/mtt-distillation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Focal Modulation Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Yang, Chunyuan Li, <span class="highlight-author">Jianfeng Gao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose focal modulation network (FocalNet in short), where
self-attention (SA) is completely replaced by a focal modulation module that is
more effective and efficient for modeling token interactions. Focal modulation
comprises three components: $(i)$ hierarchical contextualization, implemented
using a stack of depth-wise convolutional layers, to encode visual contexts
from short to long ranges at different granularity levels, $(ii)$ gated
aggregation to selectively aggregate context features for each visual token
(query) based on its content, and $(iii)$ modulation or element-wise affine
transformation to fuse the aggregated features into the query vector. Extensive
experiments show that FocalNets outperform the state-of-the-art SA counterparts
(e.g., Swin Transformers) with similar time and memory cost on the tasks of
image classification, object detection, and semantic segmentation.
Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9%
top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains
86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224
and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when
transferred to downstream tasks. For object detection with Mask R-CNN, our
FocalNet base trained with 1$\times$ already surpasses Swin trained with
3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet,
FocalNet base evaluated at single-scale outperforms Swin evaluated at
multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable
alternative to SA for effective and efficient visual modeling in real-world
applications. Code is available at https://github.com/microsoft/FocalNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Neural Predictivity in the Visual Cortex with Gated Recurrent
  Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Azeglio, Simone Poetto, Luca Savant Aira, Marco Nurisso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational models of vision have traditionally been developed in a
bottom-up fashion, by hierarchically composing a series of straightforward
operations - i.e. convolution and pooling - with the aim of emulating simple
and complex cells in the visual cortex, resulting in the introduction of deep
convolutional neural networks (CNNs). Nevertheless, data obtained with recent
neuronal recording techniques support that the nature of the computations
carried out in the ventral visual stream is not completely captured by current
deep CNN models. To fill the gap between the ventral visual stream and deep
models, several benchmarks have been designed and organized into the
Brain-Score platform, granting a way to perform multi-layer (V1, V2, V4, IT)
and behavioral comparisons between the two counterparts. In our work, we aim to
shift the focus on architectures that take into account lateral recurrent
connections, a ubiquitous feature of the ventral visual stream, to devise
adaptive receptive fields. Through recurrent connections, the input s
long-range spatial dependencies can be captured in a local multi-step fashion
and, as introduced with Gated Recurrent CNNs (GRCNN), the unbounded expansion
of the neuron s receptive fields can be modulated through the use of gates. In
order to increase the robustness of our approach and the biological fidelity of
the activations, we employ specific data augmentation techniques in line with
several of the scoring benchmarks. Enforcing some form of invariance, through
heuristics, was found to be beneficial for better neural predictivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, BrainScore Workshop 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling faster and more reliable sonographic assessment of gestational
  age through machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chace Lee, Angelica Willis, Christina Chen, Marcin Sieniek, Akib Uddin, Jonny Wong, Rory Pilgrim, Katherine Chou, Daniel Tse, Shravya Shetty, Ryan G. Gomes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fetal ultrasounds are an essential part of prenatal care and can be used to
estimate gestational age (GA). Accurate GA assessment is important for
providing appropriate prenatal care throughout pregnancy and identifying
complications such as fetal growth disorders. Since derivation of GA from
manual fetal biometry measurements (head, abdomen, femur) are
operator-dependent and time-consuming, there have been a number of research
efforts focused on using artificial intelligence (AI) models to estimate GA
using standard biometry images, but there is still room to improve the accuracy
and reliability of these AI systems for widescale adoption. To improve GA
estimates, without significant change to provider workflows, we leverage AI to
interpret standard plane ultrasound images as well as 'fly-to' ultrasound
videos, which are 5-10s videos automatically recorded as part of the standard
of care before the still image is captured. We developed and validated three AI
models: an image model using standard plane images, a video model using fly-to
videos, and an ensemble model (combining both image and video). All three were
statistically superior to standard fetal biometry-based GA estimates derived by
expert sonographers, the ensemble model has the lowest mean absolute error
(MAE) compared to the clinical standard fetal biometry (mean difference: -1.51
$\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404
participants. We showed that our models outperform standard biometry by a more
substantial margin on fetuses that were small for GA. Our AI models have the
potential to empower trained operators to estimate GA with higher accuracy
while reducing the amount of time required and user variability in measurement
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection, Recognition, and Tracking: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyao Chen, Dale Chen-Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For humans, object detection, recognition, and tracking are innate. These
provide the ability for human to perceive their environment and objects within
their environment. This ability however doesn't translate well in computers. In
Computer Vision and Multimedia, it is becoming increasingly more important to
detect, recognize and track objects in images and/or videos. Many of these
applications, such as facial recognition, surveillance, animation, are used for
tracking features and/or people. However, these tasks prove challenging for
computers to do effectively, as there is a significant amount of data to parse
through. Therefore, many techniques and algorithms are needed and therefore
researched to try to achieve human like perception. In this literature review,
we focus on some novel techniques on object detection and recognition, and how
to apply tracking algorithms to the detected features to track the objects'
movements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GradViT: Gradient Inversion of Vision <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hatamizadeh, Hongxu Yin, Holger Roth, Wenqi Li, Jan Kautz, Daguang Xu, Pavlo Molchanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we demonstrate the vulnerability of vision transformers (ViTs)
to gradient-based inversion attacks. During this attack, the original data
batch is reconstructed given model weights and the corresponding gradients. We
introduce a method, named GradViT, that optimizes random noise into naturally
looking images via an iterative process. The optimization objective consists of
(i) a loss on matching the gradients, (ii) image prior in the form of distance
to batch-normalization statistics of a pretrained CNN model, and (iii) a total
variation regularization on patches to guide correct recovery locations. We
propose a unique loss scheduling function to overcome local minima during
optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and
observe unprecedentedly high fidelity and closeness to the original (hidden)
data. During the analysis we find that vision transformers are significantly
more vulnerable than previously studied CNNs due to the presence of the
attention mechanism. Our method demonstrates new state-of-the-art results for
gradient inversion in both qualitative and quantitative metrics. Project page
at https://gradvit.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2021 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Under the Hood of <span class="highlight-title">Transformer</span> Networks for Trajectory Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Franco, Leonardo Placidi, Francesco Giuliari, Irtiza Hasan, Marco Cristani, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer Networks have established themselves as the de-facto
state-of-the-art for trajectory forecasting but there is currently no
systematic study on their capability to model the motion patterns of people,
without interactions with other individuals nor the social context. This paper
proposes the first in-depth study of Transformer Networks (TF) and
Bidirectional Transformers (BERT) for the forecasting of the individual motion
of people, without bells and whistles. We conduct an exhaustive evaluation of
input/output representations, problem formulations and sequence modeling,
including a novel analysis of their capability to predict multi-modal futures.
Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are
top performers in predicting individual motions, definitely overcoming RNNs and
LSTMs. Furthermore, they remain within a narrow margin wrt more complex
techniques, which include both social interactions and scene contexts. Source
code will be released for all conducted experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in Pattern Recognition journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Vocabulary DETR with Conditional Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection, which is concerned with the problem of
detecting novel objects guided by natural language, has gained increasing
attention from the community. Ideally, we would like to extend an
open-vocabulary detector such that it can produce bounding box predictions
based on user inputs in form of either natural language or exemplar image. This
offers great flexibility and user experience for human-computer interaction. To
this end, we propose a novel open-vocabulary detector based on DETR -- hence
the name OV-DETR -- which, once trained, can detect any object given its class
name or an exemplar image. The biggest challenge of turning DETR into an
open-vocabulary detector is that it is impossible to calculate the
classification cost matrix of novel classes without access to their labeled
images. To overcome this challenge, we formulate the learning objective as a
binary matching one between input queries (class name or exemplar image) and
the corresponding objects, which learns useful correspondence to generalize to
unseen queries during testing. For training, we choose to condition the
Transformer decoder on the input embeddings obtained from a pre-trained
vision-language model like CLIP, in order to enable matching for both text and
image queries. With extensive experiments on LVIS and COCO datasets, we
demonstrate that our OV-DETR -- the first end-to-end Transformer-based
open-vocabulary detector -- achieves non-trivial improvements over current
state of the arts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating natural images with direct Patch Distributions Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Elnekave, Yair Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-time Junk Food Recognition System based on Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirajum Munira Shifat, Takitazwar Parthib, Sabikunnahar Talukder Pyaasa, Nila Maitra Chaity, Niloy Kumar, Md. Kishor Morol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $ $As a result of bad eating habits, humanity may be destroyed. People are
constantly on the lookout for tasty foods, with junk foods being the most
common source. As a consequence, our eating patterns are shifting, and we're
gravitating toward junk food more than ever, which is bad for our health and
increases our risk of acquiring health problems. Machine learning principles
are applied in every aspect of our lives, and one of them is object recognition
via image processing. However, because foods vary in nature, this procedure is
crucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a
low accuracy rate. All of these issues were defeated by the Deep Neural
Network. In this work, we created a fresh dataset of 10,000 data points from 20
junk food classifications to try to recognize junk foods. All of the data in
the data set was gathered using the Google search engine, which is thought to
be one-of-a-kind in every way. The goal was achieved using Convolution Neural
Network (CNN) technology, which is well-known for image processing. We achieved
a 98.05\% accuracy rate throughout the research, which was satisfactory. In
addition, we conducted a test based on a real-life event, and the outcome was
extraordinary. Our goal is to advance this research to the next level, so that
it may be applied to a future study. Our ultimate goal is to create a system
that would encourage people to avoid eating junk food and to be
health-conscious. \keywords{ Machine Learning \and junk food \and object
detection \and YOLOv3 \and custom food dataset.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, accepted in ICBBDB conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalization in Federated Learning by Seeking Flat Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Caldarola, Barbara Caputo, Marco Ciccone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Cross-View Panorama Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songsong Wu, <span class="highlight-author">Hao Tan</span>g, Xiao-Yuan Jing, Haifeng Zhao, Jianjun Qian, Nicu Sebe, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of synthesizing a ground-view panorama
image conditioned on a top-view aerial image, which is a challenging problem
due to the large gap between the two image domains with different view-points.
Instead of learning cross-view mapping in a feedforward pass, we propose a
novel adversarial feedback GAN framework named PanoGAN with two key components:
an adversarial feedback module and a dual branch discrimination strategy.
First, the aerial image is fed into the generator to produce a target panorama
image and its associated segmentation map in favor of model training with
layout semantics. Second, the feature responses of the discriminator encoded by
our adversarial feedback module are fed back to the generator to refine the
intermediate representations, so that the generation performance is continually
improved through an iterative generation process. Third, to pursue
high-fidelity and semantic consistency of the generated panorama image, we
propose a pixel-segmentation alignment mechanism under the dual branch
discrimiantion strategy to facilitate cooperation between the generator and the
discriminator. Extensive experimental results on two challenging cross-view
image datasets show that PanoGAN enables high-quality panorama image generation
with more convincing details than state-of-the-art approaches. The source code
and trained models are available at \url{https://github.com/sswuai/PanoGAN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Was that so hard? Estimating human classification difficulty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morten Rieger Hannemose, Josefine Vilsbøll Sundgaard, Niels Kvorning Ternov, Rasmus R. Paulsen, Anders Nymark Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When doctors are trained to diagnose a specific disease, they learn faster
when presented with cases in order of increasing difficulty. This creates the
need for automatically estimating how difficult it is for doctors to classify a
given case. In this paper, we introduce methods for estimating how hard it is
for a doctor to diagnose a case represented by a medical image, both when
ground truth difficulties are available for training, and when they are not.
Our methods are based on embeddings obtained with deep metric learning.
Additionally, we introduce a practical method for obtaining ground truth human
difficulty for each image case in a dataset using self-assessed certainty. We
apply our methods to two different medical datasets, achieving high Kendall
rank correlation coefficients, showing that we outperform existing methods by a
large margin on our problem and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Broad Study of <span class="highlight-title">Pre-train</span>ing for Domain Generalization and Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghyun Kim, Kaihong Wang, Stan Sclaroff, Kate Saenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep models must learn robust and transferable representations in order to
perform well on new domains. While domain transfer methods (e.g., domain
adaptation, domain generalization) have been proposed to learn transferable
representations across domains, they are typically applied to ResNet backbones
pre-trained on ImageNet. Thus, existing works pay little attention to the
effects of pre-training on domain transfer tasks. In this paper, we provide a
broad study and in-depth analysis of pre-training for domain adaptation and
generalization, namely: network architectures, size, pre-training loss, and
datasets. We observe that simply using a state-of-the-art backbone outperforms
existing state-of-the-art domain adaptation baselines and set new baselines on
Office-Home and DomainNet improving by 10.7\% and 5.5\%. We hope that this work
can provide more insights for future domain transfer research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Approach to Improve Learning-based Deepfake Detection in Realistic
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Lu, Touradj Ebrahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional neural networks have achieved exceptional results on
multiple detection and recognition tasks. However, the performance of such
detectors are often evaluated in public benchmarks under constrained and
non-realistic situations. The impact of conventional distortions and processing
operations found in imaging workflows such as compression, noise, and
enhancement are not sufficiently studied. Currently, only a few researches have
been done to improve the detector robustness to unseen perturbations. This
paper proposes a more effective data augmentation scheme based on real-world
image degradation process. This novel technique is deployed for deepfake
detection tasks and has been evaluated by a more realistic assessment
framework. Extensive experiments show that the proposed data augmentation
scheme improves generalization ability to unpredictable data distortions and
unseen datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AP-BSN: <span class="highlight-title">Self-Supervised</span> Denoising for Real-World Images via Asymmetric
  PD and Blind-Spot Network <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseok Lee, Sanghyun Son, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind-spot network (BSN) and its variants have made significant advances in
self-supervised denoising. Nevertheless, they are still bound to synthetic
noisy inputs due to less practical assumptions like pixel-wise independent
noise. Hence, it is challenging to deal with spatially correlated real-world
noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has
been proposed to remove the spatial correlation of real-world noise. However,
it is not trivial to integrate PD and BSN directly, which prevents the fully
self-supervised denoising model on real-world images. We propose an Asymmetric
PD (AP) to address this issue, which introduces different PD stride factors for
training and inference. We systematically demonstrate that the proposed AP can
resolve inherent trade-offs caused by specific PD stride factors and make BSN
applicable to practical scenarios. To this end, we develop AP-BSN, a
state-of-the-art self-supervised denoising method for real-world sRGB images.
We further propose random-replacing refinement, which significantly improves
the performance of our AP-BSN without any additional parameters. Extensive
studies demonstrate that our method outperforms the other self-supervised and
even unpaired denoising methods by a large margin, without using any additional
knowledge, e.g., noise level, regarding the underlying unknown noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Framework for Assessment of Learning-based Detectors in
  Realistic Conditions with Application to Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Lu, Ruizhi Luo, Touradj Ebrahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional neural networks have shown remarkable results on multiple
detection tasks. Despite the significant progress, the performance of such
detectors are often assessed in public benchmarks under non-realistic
conditions. Specifically, impact of conventional distortions and processing
operations such as compression, noise, and enhancement are not sufficiently
studied. This paper proposes a rigorous framework to assess performance of
learning-based detectors in more realistic situations. An illustrative example
is shown under deepfake detection context. Inspired by the assessment results,
a data augmentation strategy based on natural image degradation process is
designed, which significantly improves the generalization ability of two
deepfake detectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring and Evaluating Image Restoration Potential in Dynamic Scenes <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Zhang, Shaolin Su, Yu Zhu, Qingsen Yan, Jinqiu Sun, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dynamic scenes, images often suffer from dynamic blur due to superposition
of motions or low signal-noise ratio resulted from quick shutter speed when
avoiding motions. Recovering sharp and clean results from the captured images
heavily depends on the ability of restoration methods and the quality of the
input. Although existing research on image restoration focuses on developing
models for obtaining better restored results, fewer have studied to evaluate
how and which input image leads to superior restored quality. In this paper, to
better study an image's potential value that can be explored for restoration,
we propose a novel concept, referring to image restoration potential (IRP).
Specifically, We first establish a dynamic scene imaging dataset containing
composite distortions and applied image restoration processes to validate the
rationality of the existence to IRP. Based on this dataset, we investigate
several properties of IRP and propose a novel deep model to accurately predict
IRP values. By gradually distilling and selective fusing the degradation
features, the proposed model shows its superiority in IRP prediction. Thanks to
the proposed model, we are then able to validate how various image restoration
related applications are benefited from IRP prediction. We show the potential
usages of IRP as a filtering principle to select valuable frames, an auxiliary
guidance to improve restoration models, and even an indicator to optimize
camera settings for capturing better images under dynamic scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based
  Motion Segmentation <span class="chip">AAAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Chen, Yang Wang, Yang Cao, Feng Wu, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting
apparent motion of objects with microsecond resolution, and shows great
application potential in monitoring and other fields. However, the output event
stream of existing DVS inevitably contains background activity noise (BA noise)
due to dark current and junction leakage current, which will affect the
temporal correlation of objects, resulting in deteriorated motion estimation
performance. Particularly, the existing filter-based denoising methods cannot
be directly applied to suppress the noise in event stream, since there is no
spatial correlation. To address this issue, this paper presents a novel
progressive framework, in which a Motion Estimation (ME) module and an Event
Denoising (ED) module are jointly optimized in a mutually reinforced manner.
Specifically, based on the maximum sharpness criterion, ME module divides the
input event into several segments by adaptive clustering in a motion
compensating warp field, and captures the temporal correlation of event stream
according to the clustered motion parameters. Taking temporal correlation as
guidance, ED module calculates the confidence that each event belongs to real
activity events, and transmits it to ME module to update energy function of
motion segmentation for noise suppression. The two steps are iteratively
updated until stable motion segmentation results are obtained. Extensive
experimental results on both synthetic and real datasets demonstrate the
superiority of our proposed approaches against the State-Of-The-Art (SOTA)
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Anomaly Detection in Medical Images with a Memory-augmented
  Multi-level Cross-attentional Masked Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Guansong Pang, Yuyuan Liu, Chong Wang, Yuanhong Chen, Fengbei Liu, Rajvinder Singh, Johan W Verjans, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection (UAD) aims to find anomalous images by
optimising a detector using a training set that contains only normal images.
UAD approaches can be based on reconstruction methods, self-supervised
approaches, and Imagenet pre-trained models. Reconstruction methods, which
detect anomalies from image reconstruction errors, are advantageous because
they do not rely on the design of problem-specific pretext tasks needed by
self-supervised approaches, and on the unreliable translation of models
pre-trained from non-medical datasets. However, reconstruction methods may fail
because they can have low reconstruction errors even for anomalous images. In
this paper, we introduce a new reconstruction-based UAD approach that addresses
this low-reconstruction error issue for anomalous images. Our UAD approach, the
memory-augmented multi-level cross-attentional masked autoencoder (MemMC-MAE),
is a transformer-based approach, consisting of a novel memory-augmented
self-attention operator for the encoder and a new multi-level cross-attention
operator for the decoder. MemMC-MAE masks large parts of the input image during
its reconstruction, reducing the risk that it will produce low reconstruction
errors because anomalies are likely to be masked and cannot be reconstructed.
However, when the anomaly is not masked, then the normal patterns stored in the
encoder's memory combined with the decoder's multi-level cross-attention will
constrain the accurate reconstruction of the anomaly. We show that our method
achieves SOTA anomaly detection and localisation on colonoscopy and Covid-19
Chest X-ray datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Neural Network to Restore Low-Dose Digital Breast
  Tomosynthesis Projections in a Variance Stabilization Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo de Barros Vimieiro, Chuang Niu, Hongming Shan, Lucas Rodrigues Borges, Ge Wang, Marcelo Andrade da Costa Vieira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible
radiation dose while maintaining sufficiently good image quality for accurate
medical diagnosis. In this work, we propose a convolution neural network (CNN)
to restore low-dose (LD) DBT projections to achieve an image quality equivalent
to a standard full-dose (FD) acquisition. The proposed network architecture
benefits from priors in terms of layers that were inspired by traditional
model-based (MB) restoration methods, considering a model-based deep learning
approach, where the network is trained to operate in the variance stabilization
transformation (VST) domain. To accurately control the network operation point,
in terms of noise and blur of the restored image, we propose a loss function
that minimizes the bias and matches residual noise between the input and the
output. The training dataset was composed of clinical data acquired at the
standard FD and low-dose pairs obtained by the injection of quantum noise. The
network was tested using real DBT projections acquired with a physical
anthropomorphic breast phantom. The proposed network achieved superior results
in terms of the mean normalized squared error (MNSE), training time and noise
spatial correlation compared with networks trained with traditional data-driven
methods. The proposed approach can be extended for other medical imaging
application that requires LD acquisitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CP2: Copy-Paste <span class="highlight-title">Contrastive</span> <span class="highlight-title">Pretrain</span>ing for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Wang, Huiyu Wang, Chen Wei, Alan Yuille, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervised contrastive learning yield good
image-level representation, which favors classification tasks but usually
neglects pixel-level detailed information, leading to unsatisfactory transfer
performance to dense prediction tasks such as semantic segmentation. In this
work, we propose a pixel-wise contrastive learning method called CP2
(Copy-Paste Contrastive Pretraining), which facilitates both image- and
pixel-level representation learning and therefore is more suitable for
downstream dense prediction tasks. In detail, we copy-paste a random crop from
an image (the foreground) onto different background images and pretrain a
semantic segmentation model with the objective of 1) distinguishing the
foreground pixels from the background pixels, and 2) identifying the composed
images that share the same foreground.Experiments show the strong performance
of CP2 in downstream semantic segmentation: By finetuning CP2 pretrained models
on PASCAL VOC 2012, we obtain 78.6% mIoU with a ResNet-50 and 79.5% with a
ViT-S.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Learned Block-Based Image Compression with Block-Level Masked
  Convolutions and Asymptotic Closed Loop Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Kamisli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression research has achieved state-of-the-art compression
performance with auto-encoder based neural network architectures, where the
image is mapped via convolutional neural networks (CNN) into a latent
representation that is quantized and processed again with CNN to obtain the
reconstructed image. CNN operate on entire input images. On the other hand,
traditional state-of-the-art image and video compression methods process images
with a block-by-block processing approach for various reasons. Very recently,
work on learned image compression with block based approaches have also
appeared, which use the auto-encoder architecture on large blocks of the input
image and introduce additional neural networks that perform intra/spatial
prediction and deblocking/post-processing functions. This paper explores an
alternative learned block-based image compression approach in which neither an
explicit intra prediction neural network nor an explicit deblocking neural
network is used. A single auto-encoder neural network with block-level masked
convolutions is used and the block size is much smaller (8x8). By using
block-level masked convolutions, each block is processed using reconstructed
neighboring left and upper blocks both at the encoder and decoder. Hence, the
mutual information between adjacent blocks is exploited during compression and
each block is reconstructed using neighboring blocks, resolving the need for
explicit intra prediction and deblocking neural networks. Since the explored
system is a closed loop system, a special optimization procedure, the
asymptotic closed loop design, is used with standard stochastic gradient
descent based training. The experimental results indicate competitive image
compression performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-attention for ViT-backed Continual Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengqi Xue, Haofei Zhang, Jie Song, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel <span class="highlight-title">Self-Supervision</span> for Online Knowledge <span class="highlight-title">Distillation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiao Fan, Xuan Cheng, Xiaomin Wang, Chun Yang, Pan Deng, Minghui Liu, Jiali Deng, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, researchers have shown an increased interest in the online
knowledge distillation. Adopting an one-stage and end-to-end training fashion,
online knowledge distillation uses aggregated intermediated predictions of
multiple peer models for training. However, the absence of a powerful teacher
model may result in the homogeneity problem between group peers, affecting the
effectiveness of group distillation adversely. In this paper, we propose a
novel online knowledge distillation method, \textbf{C}hannel
\textbf{S}elf-\textbf{S}upervision for Online Knowledge Distillation (CSS),
which structures diversity in terms of input, target, and network to alleviate
the homogenization problem. Specifically, we construct a dual-network
multi-branch structure and enhance inter-branch diversity through
self-supervised learning, adopting the feature-level transformation and
augmenting the corresponding labels. Meanwhile, the dual network structure has
a larger space of independent parameters to resist the homogenization problem
during distillation. Extensive quantitative experiments on CIFAR-100 illustrate
that our method provides greater diversity than OKDDip and we also give pretty
performance improvement, even over the state-of-the-art such as PCL. The
results on three fine-grained datasets (StanfordDogs, StanfordCars,
CUB-200-211) also show the significant generalization capability of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Fine-Grained Scene Graph Generation with Data Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Zhang, Yuan Yao, Qianyu Chen, Wei Ji, <span class="highlight-author">Zhiyuan Liu</span>, Maosong Sun, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graph generation (SGG) aims to extract (subject, predicate, object)
triplets in images. Recent works have made a steady progress on SGG, and
provide useful tools for high-level vision and language understanding. However,
due to the data distribution problems including long-tail distribution and
semantic ambiguity, the predictions of current SGG models tend to collapse to
several frequent but uninformative predicates (e.g., \textit{on}, \textit{at}),
which limits practical application of these models in downstream tasks. To deal
with the problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a play-and-plug fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the transferred dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The data and code
for this paper are publicly available at
\url{https://github.com/waxnkw/IETrans-SGG.pytorch}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-Supervised Salient Object Detection Using Point Supervison <span class="chip">AAAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyong Gao, Wei Zhang, Yan Wang, Qianyu Guo, Chenglong Zhang, Yangji He, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art saliency detection models rely heavily on large
datasets of accurate pixel-wise annotations, but manually labeling pixels is
time-consuming and labor-intensive. There are some weakly supervised methods
developed for alleviating the problem, such as image label, bounding box label,
and scribble label, while point label still has not been explored in this
field. In this paper, we propose a novel weakly-supervised salient object
detection method using point supervision. To infer the saliency map, we first
design an adaptive masked flood filling algorithm to generate pseudo labels.
Then we develop a transformer-based point-supervised saliency detection model
to produce the first round of saliency maps. However, due to the sparseness of
the label, the weakly supervised model tends to degenerate into a general
foreground detection model. To address this issue, we propose a Non-Salient
Suppression (NSS) method to optimize the erroneous saliency maps generated in
the first round and leverage them for the second round of training. Moreover,
we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS
dataset. In P-DUTS, there is only one labeled point for each salient object.
Comprehensive experiments on five largest benchmark datasets demonstrate our
method outperforms the previous state-of-the-art methods trained with the
stronger supervision and even surpass several fully supervised state-of-the-art
models. The code is available at: https://github.com/shuyonggao/PSOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic State Estimation in Cloth Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgies Tzelepis, Eren Erdal Aksoy, Júlia Borràs, Guillem Alenyà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding of deformable object manipulations such as textiles is a
challenge due to the complexity and high dimensionality of the problem.
Particularly, the lack of a generic representation of semantic states (e.g.,
\textit{crumpled}, \textit{diagonally folded}) during a continuous manipulation
process introduces an obstacle to identify the manipulation type. In this
paper, we aim to solve the problem of semantic state estimation in cloth
manipulation tasks. For this purpose, we introduce a new large-scale
fully-annotated RGB image dataset showing various human demonstrations of
different complicated cloth manipulations. We provide a set of baseline deep
networks and benchmark them on the problem of semantic state estimation using
our proposed dataset. Furthermore, we investigate the scalability of our
semantic state estimation framework in robot monitoring tasks of long and
complex cloth manipulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Look for the Change: Learning Object States and State-Modifying Actions
  from Untrimmed Web Videos <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Souček, <span class="highlight-author">Jean-Baptiste Alayrac</span>, Antoine Miech, <span class="highlight-author">Ivan Laptev</span>, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human actions often induce changes of object states such as "cutting an
apple", "cleaning shoes" or "pouring coffee". In this paper, we seek to
temporally localize object states (e.g. "empty" and "full" cup) together with
the corresponding state-modifying actions ("pouring coffee") in long uncurated
videos with minimal supervision. The contributions of this work are threefold.
First, we develop a self-supervised model for jointly learning state-modifying
actions together with the corresponding object states from an uncurated set of
videos from the Internet. The model is self-supervised by the causal ordering
signal, i.e. initial object state $\rightarrow$ manipulating action
$\rightarrow$ end state. Second, to cope with noisy uncurated training data,
our model incorporates a noise adaptive weighting module supervised by a small
number of annotated still images, that allows to efficiently filter out
irrelevant videos during training. Third, we collect a new dataset with more
than 2600 hours of video and 34 thousand changes of object states, and manually
annotate a part of this data to validate our approach. Our results demonstrate
substantial improvements over prior work in both action and object
state-recognition in video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human
  Motion Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Hong, Xuelin Qian, Simian Luo, Xiangyang Xue, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the task of conditional Human Motion Animation (cHMA).
Given a source image and a driving video, the model should animate the new
frame sequence, in which the person in the source image should perform a
similar motion as the pose sequence from the driving video. Despite the success
of Generative Adversarial Network (GANs) methods in image and video synthesis,
it is still very challenging to conduct cHMA due to the difficulty in
efficiently utilizing the conditional guided information such as images or
poses, and generating images of good visual quality. To this end, this paper
proposes a novel model of learning to Quantize, Scrabble, and Craft (QS-Craft)
for conditional human motion animation. The key novelties come from the newly
introduced three key steps: quantize, scrabble and craft. Particularly, our
QS-Craft employs transformer in its structure to utilize the attention
architectures. The guided information is represented as a pose coordinate
sequence extracted from the driving videos. Extensive experiments on human
motion datasets validate the efficacy of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution Iterative Feedback Network for Camouflaged Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobin Hu, Deng-Ping Fan, Xuebin Qin, Hang Dai, Wenqi Ren, Ying Tai, Chengjie Wang, Ling Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spotting camouflaged objects that are visually assimilated into the
background is tricky for both object detection algorithms and humans who are
usually confused or cheated by the perfectly intrinsic similarities between the
foreground objects and the background surroundings. To tackle this challenge,
we aim to extract the high-resolution texture details to avoid the detail
degradation that causes blurred vision in edges and boundaries. We introduce a
novel HitNet to refine the low-resolution representations by high-resolution
features in an iterative feedback manner, essentially a global loop-based
connection among the multi-scale resolutions. In addition, an iterative
feedback loss is proposed to impose more constraints on each feedback
connection. Extensive experiments on four challenging datasets demonstrate that
our \ourmodel~breaks the performance bottleneck and achieves significant
improvements compared with 29 state-of-the-art methods. To address the data
scarcity in camouflaged scenarios, we provide an application example by
employing cross-domain learning to extract the features that can reflect the
camouflaged object properties and embed the features into salient objects,
thereby generating more camouflaged training samples from the diverse salient
object datasets The code will be available at
https://github.com/HUuxiaobin/HitNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Residual Networks for Gaze Mapping on Indian Roads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaitanya Kapoor, Kshitij Kumar, Soumya Vishnoi, Sriram Ramanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the recent past, greater accessibility to powerful computational resources
has enabled progress in the field of Deep Learning and Computer Vision to grow
by leaps and bounds. This in consequence has lent progress to the domain of
Autonomous Driving and Navigation Systems. Most of the present research work
has been focused on driving scenarios in the European or American roads. Our
paper draws special attention to the Indian driving context. To this effect, we
propose a novel architecture, DR-Gaze, which is used to map the driver's gaze
onto the road. We compare our results with previous works and state-of-the-art
results on the DGAZE dataset. Our code will be made publicly available upon
acceptance of our paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Negative Pair Generation toward Well-discriminative Feature
  Space for Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junuk Jung, Seonhoon Lee, Heung-Seon Oh, Yongjun Park, Joochan Park, Sungbin Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of face recognition (FR) can be viewed as a pair similarity
optimization problem, maximizing a similarity set $\mathcal{S}^p$ over positive
pairs, while minimizing similarity set $\mathcal{S}^n$ over negative pairs.
Ideally, it is expected that FR models form a well-discriminative feature space
(WDFS) that satisfies $\inf{\mathcal{S}^p} > \sup{\mathcal{S}^n}$. With regard
to WDFS, the existing deep feature learning paradigms (i.e., metric and
classification losses) can be expressed as a unified perspective on different
pair generation (PG) strategies. Unfortunately, in the metric loss (ML), it is
infeasible to generate negative pairs taking all classes into account in each
iteration because of the limited mini-batch size. In contrast, in
classification loss (CL), it is difficult to generate extremely hard negative
pairs owing to the convergence of the class weight vectors to their center.
This leads to a mismatch between the two similarity distributions of the
sampled pairs and all negative pairs. Thus, this paper proposes a unified
negative pair generation (UNPG) by combining two PG strategies (i.e., MLPG and
CLPG) from a unified perspective to alleviate the mismatch. UNPG introduces
useful information about negative pairs using MLPG to overcome the CLPG
deficiency. Moreover, it includes filtering the similarities of noisy negative
pairs to guarantee reliable convergence and improved performance. Exhaustive
experiments show the superiority of UNPG by achieving state-of-the-art
performance across recent loss functions on public benchmark datasets. Our code
and pretrained models are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> HOP: History-and-Order Aware <span class="highlight-title">Pre-train</span>ing for <span class="highlight-title">Vision-and-Language</span>
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, <span class="highlight-author">Qi Wu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training has been adopted in a few of recent works for
Vision-and-Language Navigation (VLN). However, previous pre-training methods
for VLN either lack the ability to predict future actions or ignore the
trajectory contexts, which are essential for a greedy navigation process. In
this work, to promote the learning of spatio-temporal visual-textual
correspondence as well as the agent's capability of decision making, we propose
a novel history-and-order aware pre-training paradigm (HOP) with VLN-specific
objectives that exploit the past observations and support future action
prediction. Specifically, in addition to the commonly used Masked Language
Modeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy
tasks to model temporal order information: Trajectory Order Modeling (TOM) and
Group Order Modeling (GOM). Moreover, our navigation action prediction is also
enhanced by introducing the task of Action Prediction with History (APH), which
takes into account the history visual perceptions. Extensive experimental
results on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the
effectiveness of our proposed method compared against several state-of-the-art
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding
  Alignment <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zeng, Yue Qian, Qijian Zhang, Junhui Hou, Yixuan Yuan, Ying He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of temporally interpolating dynamic 3D
point clouds with large non-rigid deformation. We formulate the problem as
estimation of point-wise trajectories (i.e., smooth curves) and further reason
that temporal irregularity and under-sampling are two major challenges. To
tackle the challenges, we propose IDEA-Net, an end-to-end deep learning
framework, which disentangles the problem under the assistance of the
explicitly learned temporal consistency. Specifically, we propose a temporal
consistency learning module to align two consecutive point cloud frames
point-wisely, based on which we can employ linear interpolation to obtain
coarse trajectories/in-between frames. To compensate the high-order nonlinear
components of trajectories, we apply aligned feature embeddings that encode
local geometry properties to regress point-wise increments, which are combined
with the coarse estimations. We demonstrate the effectiveness of our method on
various point cloud sequences and observe large improvement over
state-of-the-art methods both quantitatively and visually. Our framework can
bring benefits to 3D motion data acquisition. The source code is publicly
available at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Patch Exiting for Scalable Single Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizun Wang, Ming Lu, Kaixin Chen, Xiaoqi Li, Jiaming Liu, Yandong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the future of computing is heterogeneous, scalability is a crucial
problem for single image super-resolution. Recent works try to train one
network, which can be deployed on platforms with different capacities. However,
they rely on the pixel-wise sparse convolution, which is not hardware-friendly
and achieves limited practical speedup. As image can be divided into patches,
which have various restoration difficulties, we present a scalable method based
on Adaptive Patch Exiting (APE) to achieve more practical speedup.
Specifically, we propose to train a regressor to predict the incremental
capacity of each layer for the patch. Once the incremental capacity is below
the threshold, the patch can exit at the specific layer. Our method can easily
adjust the trade-off between performance and efficiency by changing the
threshold of incremental capacity. Furthermore, we propose a novel strategy to
enable the network training of our method. We conduct extensive experiments
across various backbones, datasets and scaling factors to demonstrate the
advantages of our method. Code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose
  CT Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xikai Yang, Zhishen Huang, Yong Long, Saiprasad Ravishankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently proposed sparsifying transform models incur low computational
cost and have been applied to medical imaging. Meanwhile, deep models with
nested network structure reveal great potential for learning features in
different layers. In this study, we propose a network-structured sparsifying
transform learning approach for X-ray computed tomography (CT), which we refer
to as multi-layer clustering-based residual sparsifying transform (MCST)
learning. The proposed MCST scheme learns multiple different unitary transforms
in each layer by dividing each layer's input into several classes. We apply the
MCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST
model into the regularizer in penalized weighted least squares (PWLS)
reconstruction. We conducted LDCT reconstruction experiments on XCAT phantom
data and Mayo Clinic data and trained the MCST model with 2 (or 3) layers and
with 5 clusters in each layer. The learned transforms in the same layer showed
rich features while additional information is extracted from representation
residuals. Our simulation results demonstrate that PWLS-MCST achieves better
image reconstruction quality than the conventional FBP method and PWLS with
edge-preserving (EP) regularizer. It also outperformed recent advanced methods
like PWLS with a learned multi-layer residual sparsifying transform prior
(MARS) and PWLS with a union of learned transforms (ULTRA), especially for
displaying clear edges and preserving subtle details.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures, submitted to the Medical Physics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement-based frugal learning for satellite image change detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastien Deschamps, Hichem Sahbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel interactive satellite image change
detection algorithm based on active learning. The proposed approach is
iterative and asks the user (oracle) questions about the targeted changes and
according to the oracle's responses updates change detections. We consider a
probabilistic framework which assigns to each unlabeled sample a relevance
measure modeling how critical is that sample when training change detection
functions. These relevance measures are obtained by minimizing an objective
function mixing diversity, representativity and uncertainty. These criteria
when combined allow exploring different data modes and also refining change
detections. To further explore the potential of this objective function, we
consider a reinforcement learning approach that finds the best combination of
diversity, representativity and uncertainty, through active learning
iterations, leading to better generalization as corroborated through
experiments in interactive satellite image change detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frugal Learning of Virtual Exemplars for Label-Efficient Satellite Image
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hichem Sahbi, Sebastien Deschamps
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we devise a novel interactive satellite image change detection
algorithm based on active learning. The proposed framework is iterative and
relies on a question and answer model which asks the oracle (user) questions
about the most informative display (subset of critical images), and according
to the user's responses, updates change detections. The contribution of our
framework resides in a novel display model which selects the most
representative and diverse virtual exemplars that adversely challenge the
learned change detection functions, thereby leading to highly discriminating
functions in the subsequent iterations of active learning. Extensive
experiments, conducted on the challenging task of interactive satellite image
change detection, show the superiority of the proposed virtual display model
against the related work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mask Usage Recognition using Vision <span class="highlight-title">Transformer</span> with Transfer Learning
  and Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hensel Donato Jahja, Novanto Yudistira,  Sutrisno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has disrupted various levels of society. The use of
masks is essential in preventing the spread of COVID-19 by identifying an image
of a person using a mask. Although only 23.1% of people use masks correctly,
Artificial Neural Networks (ANN) can help classify the use of good masks to
help slow the spread of the Covid-19 virus. However, it requires a large
dataset to train an ANN that can classify the use of masks correctly.
MaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4
class labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth.
Mask classification training utilizes Vision Transformers (ViT) architecture
with transfer learning method using pre-trained weights on ImageNet-21k, with
random augmentation. In addition, the hyper-parameters of training of 20
epochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of
0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation
function, and a Cross-Entropy loss function are used to be applied on the
training of three architectures of ViT, namely Base-16, Large-16, and Huge-14.
Furthermore, comparisons of with and without augmentation and transfer learning
are conducted. This study found that the best classification is transfer
learning and augmentation using ViT Huge-14. Using this method on
MaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training
data, 0.9412 on validation data, and 0.9534 on test data. This research shows
that training the ViT model with data augmentation and transfer learning
improves classification of the mask usage, even better than convolutional-based
Residual Network (ResNet).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Neural Network-based Efficient Dense Point Cloud
  Generation using Unsigned Distance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abol Basher, Jani Boutellier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense point cloud generation from a sparse or incomplete point cloud is a
crucial and challenging problem in 3D computer vision and computer graphics. So
far, the existing methods are either computationally too expensive, suffer from
limited resolution, or both. In addition, some methods are strictly limited to
watertight surfaces -- another major obstacle for a number of applications. To
address these issues, we propose a lightweight Convolutional Neural Network
that learns and predicts the unsigned distance field for arbitrary 3D shapes
for dense point cloud generation using the recently emerged concept of implicit
function learning. Experiments demonstrate that the proposed architecture
achieves slightly better quality results than the state of the art with 87%
less model parameters and 40% less GPU memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Deraining: Where <span class="highlight-title">Contrastive Learning</span> Meets Self-similarity <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Yuntong, Yu Changfeng, Chang Yi, Zhu Lin, Zhao Xile, Yan Luxin, Tian Yonghong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image deraining is a typical low-level image restoration task, which aims at
decomposing the rainy image into two distinguishable layers: the clean image
layer and the rain layer. Most of the existing learning-based deraining methods
are supervisedly trained on synthetic rainy-clean pairs. The domain gap between
the synthetic and real rains makes them less generalized to different real
rainy scenes. Moreover, the existing methods mainly utilize the property of the
two layers independently, while few of them have considered the mutually
exclusive relationship between the two layers. In this work, we propose a novel
non-local contrastive learning (NLCL) method for unsupervised image deraining.
Consequently, we not only utilize the intrinsic self-similarity property within
samples but also the mutually exclusive property between the two layers, so as
to better differ the rain layer from the clean image. Specifically, the
non-local self-similarity image layer patches as the positives are pulled
together and similar rain layer patches as the negatives are pushed away. Thus
the similar positive/negative samples that are close in the original space
benefit us to enrich more discriminative representation. Apart from the
self-similarity sampling strategy, we analyze how to choose an appropriate
feature encoder in NLCL. Extensive experiments on different real rainy datasets
demonstrate that the proposed method obtains state-of-the-art performance in
real deraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, accept to 2022CVPR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rebalanced Siamese <span class="highlight-title">Contrastive</span> Mining for Long-Tailed Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhisheng Zhong, Jiequan Cui, Eric Lo, Zeming Li, Jian Sun, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks perform poorly on heavily class-imbalanced datasets.
Given the promising performance of contrastive learning, we propose
$\mathbf{Re}$balanced $\mathbf{S}$iamese $\mathbf{Co}$ntrastive
$\mathbf{m}$ining ( $\mathbf{ResCom}$) to tackle imbalanced recognition. Based
on the mathematical analysis and simulation results, we claim that supervised
contrastive learning suffers a dual class-imbalance problem at both the
original batch and Siamese batch levels, which is more serious than long-tailed
classification learning. In this paper, at the original batch level, we
introduce a class-balanced supervised contrastive loss to assign adaptive
weights for different classes. At the Siamese batch level, we present a
class-balanced queue, which maintains the same number of keys for all classes.
Furthermore, we note that the contrastive loss gradient with respect to the
contrastive logits can be decoupled into the positives and negatives, and easy
positives and easy negatives will make the contrastive gradient vanish. We
propose supervised hard positive and negative pairs mining to pick up
informative pairs for contrastive computation and improve representation
learning. Finally, to approximately maximize the mutual information between the
two views, we propose Siamese Balanced Softmax and joint it with the
contrastive loss for one-stage training. ResCom outperforms the previous
methods by large margins on multiple long-tailed recognition benchmarks. Our
code will be made publicly available at:
https://github.com/dvlab-research/ResCom.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, Chiew-Lan Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR and camera are two important sensors for 3D object detection in
autonomous driving. Despite the increasing popularity of sensor fusion in this
field, the robustness against inferior image conditions, e.g., bad illumination
and sensor misalignment, is under-explored. Existing fusion methods are easily
affected by such conditions, mainly due to a hard association of LiDAR points
and image pixels, established by calibration matrices. We propose TransFusion,
a robust solution to LiDAR-camera fusion with a soft-association mechanism to
handle inferior image conditions. Specifically, our TransFusion consists of
convolutional backbones and a detection head based on a transformer decoder.
The first layer of the decoder predicts initial bounding boxes from a LiDAR
point cloud using a sparse set of object queries, and its second decoder layer
adaptively fuses the object queries with useful image features, leveraging both
spatial and contextual relationships. The attention mechanism of the
transformer enables our model to adaptively determine where and what
information should be taken from the image, leading to a robust and effective
fusion strategy. We additionally design an image-guided query initialization
strategy to deal with objects that are difficult to detect in point clouds.
TransFusion achieves state-of-the-art performance on large-scale datasets. We
provide extensive experiments to demonstrate its robustness against degenerated
image quality and calibration errors. We also extend the proposed method to the
3D tracking task and achieve the 1st place in the leaderboard of nuScenes
tracking, showing its effectiveness and generalization capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022; Code at
  \url{https://github.com/XuyangBai/TransFusion}; Based on this work, we
  achieve the 1st place in the leaderboard of nuScenes tracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FrameHopper: Selective Processing of Video Frames in Detection-driven
  Real-Time Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Adnan Arefeen, Sumaiya Tabassum Nimi, Md Yusuf Sarwar Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection-driven real-time video analytics require continuous detection of
objects contained in the video frames using deep learning models like YOLOV3,
EfficientDet. However, running these detectors on each and every frame in
resource-constrained edge devices is computationally intensive. By taking the
temporal correlation between consecutive video frames into account, we note
that detection outputs tend to be overlapping in successive frames. Elimination
of similar consecutive frames will lead to a negligible drop in performance
while offering significant performance benefits by reducing overall computation
and communication costs. The key technical questions are, therefore, (a) how to
identify which frames to be processed by the object detector, and (b) how many
successive frames can be skipped (called skip-length) once a frame is selected
to be processed. The overall goal of the process is to keep the error due to
skipping frames as small as possible. We introduce a novel error vs processing
rate optimization problem with respect to the object detection task that
balances between the error rate and the fraction of frames filtering.
Subsequently, we propose an off-line Reinforcement Learning (RL)-based
algorithm to determine these skip-lengths as a state-action policy of the RL
agent from a recorded video and then deploy the agent online for live video
streams. To this end, we develop FrameHopper, an edge-cloud collaborative video
analytics framework, that runs a lightweight trained RL agent on the camera and
passes filtered frames to the server where the object detection model runs for
a set of applications. We have tested our approach on a number of live videos
captured from real-life scenarios and show that FrameHopper processes only a
handful of frames but produces detection results closer to the oracle solution
and outperforms recent state-of-the-art solutions in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The 18th International Conference on Distributed
  Computing in Sensor Systems (DCOSS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSD-KD: A <span class="highlight-title">Self-supervised</span> Diverse Knowledge <span class="highlight-title">Distillation</span> Method for
  Lightweight Skin Lesion Classification Using Dermoscopic Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongwei Wang, Yuheng Wang, Tim K. Lee, Chunyan Miao, Z. Jane Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is one of the most common types of malignancy, affecting a large
population and causing a heavy economic burden worldwide. Over the last few
years, computer-aided diagnosis has been rapidly developed and make great
progress in healthcare and medical practices due to the advances in artificial
intelligence. However, most studies in skin cancer detection keep pursuing high
prediction accuracies without considering the limitation of computing resources
on portable devices. In this case, knowledge distillation (KD) has been proven
as an efficient tool to help improve the adaptability of lightweight models
under limited resources, meanwhile keeping a high-level representation
capability. To bridge the gap, this study specifically proposes a novel method,
termed SSD-KD, that unifies diverse knowledge into a generic KD framework for
skin diseases classification. Our method models an intra-instance relational
feature representation and integrates it with existing KD research. A dual
relational knowledge distillation architecture is self-supervisedly trained
while the weighted softened outputs are also exploited to enable the student
model to capture richer knowledge from the teacher model. To demonstrate the
effectiveness of our method, we conduct experiments on ISIC 2019, a large-scale
open-accessed benchmark of skin diseases dermoscopic images. Experiments show
that our distilled lightweight model can achieve an accuracy as high as 85% for
the classification tasks of 8 different skin diseases with minimal parameters
and computing requirements. Ablation studies confirm the effectiveness of our
intra- and inter-instance relational knowledge integration strategy. Compared
with state-of-the-art knowledge distillation techniques, the proposed method
demonstrates improved performances for multi-diseases classification on the
large-scale dermoscopy database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Stereo Matching via Cascaded Recurrent Network with Adaptive
  Correlation <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of convolutional neural networks, stereo matching algorithms
have recently gained tremendous progress. However, it remains a great challenge
to accurately extract disparities from real-world image pairs taken by
consumer-level devices like smartphones, due to practical complicating factors
such as thin structures, non-ideal rectification, camera module inconsistencies
and various hard-case scenes. In this paper, we propose a set of innovative
designs to tackle the problem of practical stereo matching: 1) to better
recover fine depth details, we design a hierarchical network with recurrent
refinement to update disparities in a coarse-to-fine manner, as well as a
stacked cascaded architecture for inference; 2) we propose an adaptive group
correlation layer to mitigate the impact of erroneous rectification; 3) we
introduce a new synthetic dataset with special attention to difficult cases for
better generalizing to real-world scenes. Our results not only rank 1st on both
Middlebury and ETH3D benchmarks, outperforming existing state-of-the-art
methods by a notable margin, but also exhibit high-quality details for
real-life photos, which clearly demonstrates the efficacy of our contributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted to CVPR2022. The project link is
  https://github.com/megvii-research/CREStereo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed Differential Privacy in Computer Vision <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> WuDaoMM: A large-scale <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Dataset</span> for <span class="highlight-title">Pre-train</span>ing models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sha Yuan, Zhao Shuai, Leng Jiahong, Xue Zhao, Zhao Hanyu, <span class="highlight-author">Tang Jie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remember Intentions: Retrospective-Memory-based Trajectory Prediction <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxin Xu, Weibo Mao, Wenjun Zhang, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To realize trajectory prediction, most previous methods adopt the
parameter-based approach, which encodes all the seen past-future instance pairs
into model parameters. However, in this way, the model parameters come from all
seen instances, which means a huge amount of irrelevant seen instances might
also involve in predicting the current situation, disturbing the performance.
To provide a more explicit link between the current situation and the seen
instances, we imitate the mechanism of retrospective memory in neuropsychology
and propose MemoNet, an instance-based approach that predicts the movement
intentions of agents by looking for similar scenarios in the training data. In
MemoNet, we design a pair of memory banks to explicitly store representative
instances in the training set, acting as prefrontal cortex in the neural
system, and a trainable memory addresser to adaptively search a current
situation with similar instances in the memory bank, acting like basal ganglia.
During prediction, MemoNet recalls previous memory by using the memory
addresser to index related instances in the memory bank. We further propose a
two-step trajectory prediction system, where the first step is to leverage
MemoNet to predict the destination and the second step is to fulfill the whole
trajectory according to the predicted destinations. Experiments show that the
proposed MemoNet improves the FDE by 20.3%/10.2%/28.3% from the previous best
method on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has
the ability to trace back to specific instances during prediction, promoting
more interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ray3D: ray-based 3D human pose estimation for monocular absolute 3D
  localization <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhan, Fenghai Li, Renliang Weng, Wongun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic
  Layouts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidi Li, Yiqun Wang, Zhengda Lu, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited by the computational efficiency and accuracy, generating complex 3D
scenes remains a challenging problem for existing generation networks. In this
work, we propose DepthGAN, a novel method of generating depth maps with only
semantic layouts as input. First, we introduce a well-designed cascade of
transformer blocks as our generator to capture the structural correlations in
depth maps, which makes a balance between global feature aggregation and local
attention. Meanwhile, we propose a cross-attention fusion module to guide edge
preservation efficiently in depth generation, which exploits additional
appearance supervision information. Finally, we conduct extensive experiments
on the perspective views of the Structured3d panorama dataset and demonstrate
that our DepthGAN achieves superior performance both on quantitative results
and visual effects in the depth generation task.Furthermore, 3D indoor scenes
can be reconstructed by our generated depth maps with reasonable structure and
spatial coherency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Textures in Zero-shot Understanding of Fine-Grained Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyun Wu, Subhransu Maji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textures can be used to describe the appearance of objects in a wide range of
fine-grained domains. Textures are localized and one can often refer to their
properties in a manner that is independent of the object identity. Moreover,
there is a rich vocabulary to describe textures corresponding to properties
such as their color, pattern, structure, periodicity, stochasticity, and
others. Motivated by this, we study the effectiveness of large-scale language
and vision models (e.g., CLIP) at recognizing texture attributes in natural
images. We first conduct a systematic study of CLIP on texture datasets where
we find that it has good coverage for a wide range of texture terms. CLIP can
also handle compositional phrases that consist of color and pattern terms
(e.g., red dots or yellow stripes). We then show how these attributes allow for
zero-shot fine-grained categorization on existing datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating UAV Imagery for Satellite Model Training, Calibration and
  <span class="highlight-title">Test</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasper Brown, Cameron Clark, Sabrina Lomax, Khalid Rafique, Salah Sukkarieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern livestock farming is increasingly data driven and frequently relies on
efficient remote sensing to gather data over wide areas. High resolution
satellite imagery is one such data source, which is becoming more accessible
for farmers as coverage increases and cost falls. Such images can be used to
detect and track animals, monitor pasture changes, and understand land use.
Many of the data driven models being applied to these tasks require ground
truthing at resolutions higher than satellites can provide. Simultaneously,
there is a lack of available aerial imagery focused on farmland changes that
occur over days or weeks, such as herd movement. With this goal in mind, we
present a new multi-temporal dataset of high resolution UAV imagery which is
artificially degraded to match satellite data quality. An empirical blurring
metric is used to calibrate the degradation process against actual satellite
imagery of the area. UAV surveys were flown repeatedly over several weeks, for
specific farm locations. This 5cm/pixel data is sufficiently high resolution to
accurately ground truth cattle locations, and other factors such as grass
cover. From 33 wide area UAV surveys, 1869 patches were extracted and
artificially degraded using an accurate satellite optical model to simulate
satellite data. Geographic patches from multiple time periods are aligned and
presented as sets, providing a multi-temporal dataset that can be used for
detecting changes on farms. The geo-referenced images and 27,853 manually
annotated cattle labels are made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Associating Objects with Scalable <span class="highlight-title">Transformer</span>s for Video Object
  Segmentation <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxin Yang, Jiaxu Miao, Xiaohan Wang, Yunchao Wei, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computation resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
jointly and collaboratively. In detail, AOT employs an identification mechanism
to associate multiple targets into the same high-dimensional embedding space.
Thus, we can simultaneously process multiple objects' matching and segmentation
decoding as efficiently as processing a single object. To sufficiently model
multi-object association, a Long Short-Term Transformer (LSTT) is devised to
construct hierarchical matching and propagation. Based on AOT, we further
propose a more flexible and robust framework, Associating Objects with Scalable
Transformers (AOST), in which a scalable version of LSTT is designed to enable
run-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces
a better layer-wise manner to couple identification and vision embeddings. We
conduct extensive experiments on multi-object and single-object benchmarks to
examine AOT series frameworks. Compared to the state-of-the-art competitors,
our methods can maintain times of run-time efficiency with superior
performance. Notably, we achieve new state-of-the-art performance on three
popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test
(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:
https://github.com/z-x-yang/AOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of arXiv:2106.02638 (NeurIPS 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-Modal</span> Learning for AU Detection Based on Multi-Head Fused
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Lijun Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal learning has been intensified in recent years, especially for
applications in facial analysis and action unit detection whilst there still
exist two main challenges in terms of 1) relevant feature learning for
representation and 2) efficient fusion for multi-modalities. Recently, there
are a number of works have shown the effectiveness in utilizing the attention
mechanism for AU detection, however, most of them are binding the region of
interest (ROI) with features but rarely apply attention between features of
each AU. On the other hand, the transformer, which utilizes a more efficient
self-attention mechanism, has been widely used in natural language processing
and computer vision tasks but is not fully explored in AU detection tasks. In
this paper, we propose a novel end-to-end Multi-Head Fused Transformer (MFT)
method for AU detection, which learns AU encoding features representation from
different modalities by transformer encoder and fuses modalities by another
fusion transformer module. Multi-head fusion attention is designed in the
fusion transformer module for the effective fusion of multiple modalities. Our
approach is evaluated on two public multi-modal AU databases, BP4D, and BP4D+,
and the results are superior to the state-of-the-art algorithms and baseline
models. We further analyze the performance of AU detection from different
modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Representation Learning as <span class="highlight-title">Multimodal</span> Variational
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Nakamura, Masashi Okada, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a probabilistic extension of SimSiam, a recent
self-supervised learning (SSL) method. SimSiam trains a model by maximizing the
similarity between image representations of different augmented views of the
same image. Although uncertainty-aware machine learning has been getting
general like deep variational inference, SimSiam and other SSL are
insufficiently uncertainty-aware, which could lead to limitations on its
potential. The proposed extension is to make SimSiam uncertainty-aware based on
variational inference. Our main contributions are twofold: Firstly, we clarify
the theoretical relationship between non-contrastive SSL and multimodal
variational inference. Secondly, we introduce a novel SSL called variational
inference SimSiam (VI-SimSiam), which incorporates the uncertainty by involving
spherical posterior distributions. Our experiment shows that VI-SimSiam
outperforms SimSiam in classification tasks in ImageNette and ImageWoof by
successfully estimating the representation uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making DeepFakes more spurious: evading deep face forgery detection via
  trace removal attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Liu, Huajie Chen, Tianqing Zhu, Jun Zhang, Wanlei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DeepFakes are raising significant social concerns. Although various DeepFake
detectors have been developed as forensic countermeasures, these detectors are
still vulnerable to attacks. Recently, a few attacks, principally adversarial
attacks, have succeeded in cloaking DeepFake images to evade detection.
However, these attacks have typical detector-specific designs, which require
prior knowledge about the detector, leading to poor transferability. Moreover,
these attacks only consider simple security scenarios. Less is known about how
effective they are in high-level scenarios where either the detectors or the
attacker's knowledge varies. In this paper, we solve the above challenges with
presenting a novel detector-agnostic trace removal attack for DeepFake
anti-forensics. Instead of investigating the detector side, our attack looks
into the original DeepFake creation pipeline, attempting to remove all
detectable natural DeepFake traces to render the fake images more "authentic".
To implement this attack, first, we perform a DeepFake trace discovery,
identifying three discernible traces. Then a trace removal network (TR-Net) is
proposed based on an adversarial learning framework involving one generator and
multiple discriminators. Each discriminator is responsible for one individual
trace representation to avoid cross-trace interference. These discriminators
are arranged in parallel, which prompts the generator to remove various traces
simultaneously. To evaluate the attack efficacy, we crafted heterogeneous
security scenarios where the detectors were embedded with different levels of
defense and the attackers' background knowledge of data varies. The
experimental results show that the proposed attack can significantly compromise
the detection accuracy of six state-of-the-art DeepFake detectors while causing
only a negligible loss in visual quality to the original DeepFake samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Zero-Shot Out-of-Distribution Detection Based on the <span class="highlight-title">Pre-train</span>ed Model
  CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepideh Esmaeilpour, <span class="highlight-author">Bing Liu</span>, Eric Robertson, Lei Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an out-of-distribution (OOD) detection problem, samples of known
classes(also called in-distribution classes) are used to train a special
classifier. In testing, the classifier can (1) classify the test samples of
known classes to their respective classes and also (2) detect samples that do
not belong to any of the known classes (i.e., they belong to some unknown or
OOD classes). This paper studies the problem of zero-shot
out-of-distribution(OOD) detection, which still performs the same two tasks in
testing but has no training except using the given known class names. This
paper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC
builds on top of the recent advances in zero-shot classification through
multi-modal representation learning. It first extends the pre-trained
language-vision model CLIP by training a text-based image description generator
on top of CLIP. In testing, it uses the extended model to generate candidate
unknown class names for each test sample and computes a confidence score based
on both the known class names and candidate unknown class names for zero-shot
OOD detection. Experimental results on 5 benchmark datasets for OOD detection
demonstrate that ZOC outperforms the baselines by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A range characterization of the single-quadrant ADRT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.05360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.05360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Li, Kui Ren, Donsub Rim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work characterizes the range of the single-quadrant approximate discrete
Radon transform (ADRT) of square images. The characterization follows from a
set of linear constraints on the codomain. We show that for data satisfying
these constraints, the exact and fast inversion formula [Rim, Appl. Math. Lett.
102 106159, 2020] yields a square image in a stable manner. The range
characterization is obtained by first showing that the ADRT is a bijection
between images supported on infinite half-strips, then identifying the linear
subspaces that stay finitely supported under the inversion formula.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A hybrid 2-stage vision <span class="highlight-title">transformer</span> for artificial intelligence-assisted
  5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic
  tool for guiding gastric cancer treatment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Oh, Go Eun Bae, Kyung-Hee Kim, Min-Kyung Yeo, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
automatic classification systems for guiding proper GC treatment based on
clinical guideline are still lacking. We propose an AI system classifying 5
classes of GC histology, which can be perfectly matched to general GC treatment
guidance. The AI system was designed to mimic the way pathologist understand
slides through multi-scale self-attention mechanism using a 2-stage Vision
Transformer network. The AI system performance was evaluated on 876 internal
endoscopic slides and 336 external endoscopic slides from clinical cohort. We
further evaluated practical usability of the AI system on observation of
AI-assisted 6 pathologist performance. The AI system demonstrates clinical
capability by achieving class-average diagnostic sensitivity of above 85% for
both internal and external cohort analysis. Furthermore, AI-assisted
pathologists showed significantly improved diagnostic sensitivity by 10% within
18% saved screening time compared to human pathologists (p-values of 0.006 and
0.030, respectively). The reliable performance of the AI system in multi-center
cohort testing and its clinical applicability demonstrate that AI-assisted
endoscopic CG screening would help reduce the workload of limited pathologists.
Furthermore, the AI system has a great potential for providing presumptive
pathologic opinion for deciding proper treatment for early GC patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual MLP for Scoring Sport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfei Xia, Mingchen Zhuge, Tiantian Geng, Shun Fan, Yuantai Wei, Zhenyu He, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figure skating scoring is a challenging task because it requires judging
players' technical moves as well as coordination with the background music.
Prior learning-based work cannot solve it well for two reasons: 1) each move in
figure skating changes quickly, hence simply applying traditional frame
sampling will lose a lot of valuable information, especially in a 3-5 minutes
lasting video, so an extremely long-range representation learning is necessary;
2) prior methods rarely considered the critical audio-visual relationship in
their models. Thus, we introduce a multimodal MLP architecture, named
Skating-Mixer. It extends the MLP-Mixer-based framework into a multimodal
fashion and effectively learns long-term representations through our designed
memory recurrent unit (MRU). Aside from the model, we also collected a
high-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8
types of programs with 7 different rating metrics, overtaking other datasets in
both quantity and diversity. Experiments show the proposed method outperforms
SOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In
addition, we include an analysis applying our method to recent competitions
that occurred in Beijing 2022 Winter Olympic Games, proving our method has
strong robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision <span class="highlight-title">Transformer</span> Hashing for Image Retrieval <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.12564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.12564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiv Ram Dubey, Satish Kumar Singh, Wei-Ta Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown a tremendous growth in hashing techniques for image
retrieval. Recently, Transformer has emerged as a new architecture by utilizing
self-attention without convolution. Transformer is also extended to Vision
Transformer (ViT) for the visual recognition with a promising performance on
ImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)
for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone
network and add the hashing head. The proposed VTS model is fine tuned for
hashing under six different image retrieval frameworks, including Deep
Supervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network
(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)
with their objective functions. We perform the extensive experiments on
CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image
retrieval outperforms the recent state-of-the-art hashing techniques with a
great margin. We also find the proposed VTS model as the backbone network is
better than the existing networks, such as AlexNet and ResNet. The code is
released at \url{https://github.com/shivram1987/VisionTransformerHashing}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE International Conference on Multimedia and Expo
  (ICME), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch
  Feature Swapping for Bodies and Faces <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12448v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12448v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Foti, Bongjin Koo, Danail Stoyanov, Matthew J. Clarkson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a disentangled, interpretable, and structured latent representation
in 3D generative models of faces and bodies is still an open problem. The
problem is particularly acute when control over identity features is required.
In this paper, we propose an intuitive yet effective self-supervised approach
to train a 3D shape variational autoencoder (VAE) which encourages a
disentangled latent representation of identity features. Curating the
mini-batch generation by swapping arbitrary features across different shapes
allows to define a loss function leveraging known differences and similarities
in the latent representations. Experimental results conducted on 3D meshes show
that state-of-the-art methods for latent disentanglement are not able to
disentangle identity features of faces and bodies. Our proposed method properly
decouples the generation of such features while maintaining good representation
and reconstruction capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Efficient Multi-Agent Cooperative Visual Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of cooperative visual exploration where multiple agents
need to jointly explore unseen regions as fast as possible based on visual
signals. Classical planning-based methods often suffer from expensive
computation overhead at each step and a limited expressiveness of complex
cooperation strategy. By contrast, reinforcement learning (RL) has recently
become a popular paradigm for tackling this challenge due to its modeling
capability of arbitrarily complex strategies and minimal inference overhead. In
this paper, we extend the state-of-the-art single-agent visual navigation
method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a
novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages
a transformer-based architecture, Spatial-TeamFormer, which effectively
captures spatial relations and intra-agent interactions via hierarchical
spatial self-attentions. In addition, we also implement a few multi-agent
enhancements to process local information from each agent for an aligned
spatial representation and more precise planning. Finally, we perform policy
distillation to extract a meta policy to significantly improve the
generalization capability of final policy. We call this overall solution,
Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms
classical planning-based baselines for the first time in a photo-realistic 3D
simulator, Habitat. Code and videos can be found at
https://sites.google.com/view/maans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First three authors share equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oriented RepPoints for Aerial Object Detection <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11111v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11111v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentong Li, Yijie Chen, Kaixuan Hu, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to the generic object, aerial targets are often non-axis aligned
with arbitrary orientations having the cluttered surroundings. Unlike the
mainstreamed approaches regressing the bounding box orientations, this paper
proposes an effective adaptive points learning approach to aerial object
detection by taking advantage of the adaptive points representation, which is
able to capture the geometric information of the arbitrary-oriented instances.
To this end, three oriented conversion functions are presented to facilitate
the classification and localization with accurate orientation. Moreover, we
propose an effective quality assessment and sample assignment scheme for
adaptive points learning toward choosing the representative oriented reppoints
samples during training, which is able to capture the non-axis aligned features
from adjacent objects or background noises. A spatial constraint is introduced
to penalize the outlier points for roust adaptive learning. Experimental
results on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD
and DIOR-R, demonstrate the efficacy of our proposed approach. The source code
is availabel at: https://github.com/LiWentomng/OrientedRepPoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape-invariant 3D Adversarial Point Clouds <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversary and invisibility are two fundamental but conflict characters of
adversarial perturbations. Previous adversarial attacks on 3D point cloud
recognition have often been criticized for their noticeable point outliers,
since they just involve an "implicit constrain" like global distance loss in
the time-consuming optimization to limit the generated noise. While point cloud
is a highly structured data format, it is hard to constrain its perturbation
with a simple loss or metric properly. In this paper, we propose a novel
Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility
of point perturbations. This map reveals the vulnerability of point cloud
recognition models when encountering shape-invariant adversarial noises. These
noises are designed along the shape surface with an "explicit constrain"
instead of extra distance loss. Specifically, we first apply a reversible
coordinate transformation on each point of the point cloud input, to reduce one
degree of point freedom and limit its movement on the tangent plane. Then we
calculate the best attacking direction with the gradients of the transformed
point cloud obtained on the white-box model. Finally we assign each point with
a non-negative score to construct the sensitivity map, which benefits both
white-box adversarial invisibility and black-box query-efficiency extended in
our work. Extensive evaluations prove that our method can achieve the superior
performance on various point cloud recognition models, with its satisfying
adversarial imperceptibility and strong resistance to different point cloud
defense settings. Our code is available at: https://github.com/shikiw/SI-Adv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Colar: Effective and Efficient Online Action Detection by Consulting
  Exemplars <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Yang, Junwei Han, Dingwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online action detection has attracted increasing research interests in recent
years. Current works model historical dependencies and anticipate the future to
perceive the action evolution within a video segment and improve the detection
accuracy. However, the existing paradigm ignores category-level modeling and
does not pay sufficient attention to efficiency. Considering a category, its
representative frames exhibit various characteristics. Thus, the category-level
modeling can provide complimentary guidance to the temporal dependencies
modeling. This paper develops an effective exemplar-consultation mechanism that
first measures the similarity between a frame and exemplary frames, and then
aggregates exemplary features based on the similarity weights. This is also an
efficient mechanism, as both similarity measurement and feature aggregation
require limited computations. Based on the exemplar-consultation mechanism, the
long-term dependencies can be captured by regarding historical frames as
exemplars, while the category-level modeling can be achieved by regarding
representative frames from a category as exemplars. Due to the complementarity
from the category-level modeling, our method employs a lightweight architecture
but achieves new high performance on three benchmarks. In addition, using a
spatio-temporal network to tackle video frames, our method makes a good
trade-off between effectiveness and efficiency. Code is available at
https://github.com/VividLe/Online-Action-Detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transform your Smartphone into a DSLR Camera: Learning the ISP in the
  Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ardhendu Shekhar Tripathi, Martin Danelljan, Samarth Shukla, Radu Timofte, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a trainable Image Signal Processing (ISP) framework that produces
DSLR quality images given RAW images captured by a smartphone. To address the
color misalignments between training image pairs, we employ a color-conditional
ISP network and optimize a novel parametric color mapping between each input
RAW and reference DSLR image. During inference, we predict the target color
image by designing a color prediction network with efficient Global Context
Transformer modules. The latter effectively leverage global information to
learn consistent color and tone mappings. We further propose a robust masked
aligned loss to identify and discard regions with inaccurate motion estimation
during training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,
consisting of weakly paired phone RAW and DSLR sRGB images. We extensively
evaluate our method, setting a new state-of-the-art on two datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimAN: Exploring <span class="highlight-title">Self-Supervised</span> Representation Learning of Scene Text
  via Similarity-Aware Normalization <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canjie Luo, Lianwen Jin, Jingdong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abandoning the Bayer-Filter to See in the Dark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang, Zhe Jin, Andrew Beng Jin Teoh, Jiajun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement - a pervasive but challenging problem, plays a
central role in enhancing the visibility of an image captured in a poor
illumination environment. Due to the fact that not all photons can pass the
Bayer-Filter on the sensor of the color camera, in this work, we first present
a De-Bayer-Filter simulator based on deep neural networks to generate a
monochrome raw image from the colored raw image. Next, a fully convolutional
network is proposed to achieve the low-light image enhancement by fusing
colored raw data with synthesized monochrome raw data. Channel-wise attention
is also introduced to the fusion process to establish a complementary
interaction between features from colored and monochrome raw images. To train
the convolutional networks, we propose a dataset with monochrome and color raw
pairs named Mono-Colored Raw paired dataset (MCR) collected by using a
monochrome camera without Bayer-Filter and a color camera with Bayer-Filter.
The proposed pipeline take advantages of the fusion of the virtual monochrome
and the color raw images and our extensive experiments indicate that
significant improvement can be achieved by leveraging raw sensor data and
data-driven learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shaping Visual Representations with Attributes for Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxing Chen, Huaxiong Li, Yaohui Li, Chunlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot recognition aims to recognize novel categories under low-data
regimes. Some recent few-shot recognition methods introduce auxiliary semantic
modality, i.e., category attribute information, into representation learning,
which enhances the feature discrimination and improves the recognition
performance. Most of these existing methods only consider the attribute
information of support set while ignoring the query set, resulting in a
potential loss of performance. In this letter, we propose a novel
attribute-shaped learning (ASL) framework, which can jointly perform query
attributes generation and discriminative visual representation learning for
few-shot recognition. Specifically, a visual-attribute generator (VAG) is
constructed to predict the attributes of queries. By leveraging the attributes
information, an attribute-visual attention module (AVAM) is designed, which can
adaptively utilize attributes and visual representations to learn more
discriminative features. Under the guidance of attribute modality, our method
can learn enhanced semantic-aware representation for classification.
Experiments demonstrate that our method can achieve competitive results on CUB
and SUN benchmarks. Our source code is available at:
\url{https://github.com/chenhaoxing/ASL}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IDR: <span class="highlight-title">Self-Supervised</span> Image Denoising via Iterative Data Refinement <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.14358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.14358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Dasong Li, Ka Lung Law, Xiaogang Wang, Hongwei Qin, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of large-scale noisy-clean image pairs restricts supervised
denoising methods' deployment in actual applications. While existing
unsupervised methods are able to learn image denoising without ground-truth
clean images, they either show poor performance or work under impractical
settings (e.g., paired noisy images). In this paper, we present a practical
unsupervised image denoising method to achieve state-of-the-art denoising
performance. Our method only requires single noisy images and a noise model,
which is easily accessible in practical raw image denoising. It performs two
steps iteratively: (1) Constructing a noisier-noisy dataset with random noise
from the noise model; (2) training a model on the noisier-noisy dataset and
using the trained model to refine noisy images to obtain the targets used in
the next round. We further approximate our full iterative method with a fast
algorithm for more efficient training while keeping its original high
performance. Experiments on real-world, synthetic, and correlated noise show
that our proposed unsupervised denoising approach has superior performances
over existing unsupervised methods and competitive performance with supervised
methods. In addition, we argue that existing denoising datasets are of low
quality and contain only a small number of scenes. To evaluate raw image
denoising performance in real-world applications, we build a high-quality raw
image dataset SenseNoise-500 that contains 500 real-life scenes. The dataset
can serve as a strong benchmark for better evaluating raw image denoising. Code
and dataset will be released at https://github.com/zhangyi-3/IDR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2022; code & dataset: https://github.com/zhangyi-3/IDR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Than Meets the Eye: <span class="highlight-title">Self-Supervised</span> Depth Reconstruction From Brain
  Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.05113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.05113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Gaziv, Michal Irani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as "paired" data), and (ii) a very large number of natural images
with no fMRI recordings ("unpaired data"). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available "paired"
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many "unpaired" data (natural
images & depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/WeizmannVision/SelfSuperReconst</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "If you could see me through my eyes": Predicting Pedestrian Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Petzold, Mostafa Wahby, Franek Stark, Ulrich Behrje, Heiko Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrians are particularly vulnerable road users in urban traffic. With the
arrival of autonomous driving, novel technologies can be developed specifically
to protect pedestrians. We propose a machine learning toolchain to train
artificial neural networks as models of pedestrian behavior. In a preliminary
study, we use synthetic data from simulations of a specific pedestrian crossing
scenario to train a variational autoencoder and a long short-term memory
network to predict a pedestrian's future visual perception. We can accurately
predict a pedestrian's future perceptions within relevant time horizons. By
iteratively feeding these predicted frames into these networks, they can be
used as simulations of pedestrians as indicated by our results. Such trained
networks can later be used to predict pedestrian behaviors even from the
perspective of the autonomous car. Another future extension will be to re-train
these networks with real-world video data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Operator Sketching for Deep Unrolling Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Self-Ensemble for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Bousselham, Guillaume Thibault, Lucas Pagano, Archana Machireddy, Joe Gray, Young Hwan Chang, Xubo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble of predictions is known to perform better than individual
predictions taken separately. However, for tasks that require heavy
computational resources, e.g. semantic segmentation, creating an ensemble of
learners that needs to be trained separately is hardly tractable. In this work,
we propose to leverage the performance boost offered by ensemble methods to
enhance the semantic segmentation, while avoiding the traditional heavy
training cost of the ensemble. Our self-ensemble approach takes advantage of
the multi-scale features set produced by feature pyramid network methods to
feed independent decoders, thus creating an ensemble within a single model.
Similar to the ensemble, the final prediction is the aggregation of the
prediction made by each learner. In contrast to previous works, our model can
be trained end-to-end, alleviating the traditional cumbersome multi-stage
training of ensembles. Our self-ensemble approach outperforms the current
state-of-the-art on the benchmark datasets Pascal Context and COCO-Stuff-10K
for semantic segmentation and is competitive on ADE20K and Cityscapes. Code is
publicly available at github.com/WalBouss/SenFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/WalBouss/SenFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PTTR: Relational 3D Point Cloud Object Tracking with <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02857v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02857v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changqing Zhou, Zhipeng Luo, Yueru Luo, Tianrui Liu, Liang Pan, Zhongang Cai, Haiyu Zhao, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a point cloud sequence, 3D object tracking aims to predict the location
and orientation of an object in the current search point cloud given a template
point cloud. Motivated by the success of transformers, we propose Point
Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D
tracking results in a coarse-to-fine manner with the help of transformer
operations. PTTR consists of three novel designs. 1) Instead of random
sampling, we design Relation-Aware Sampling to preserve relevant points to
given templates during subsampling. 2) Furthermore, we propose a Point Relation
Transformer (PRT) consisting of a self-attention and a cross-attention module.
The global self-attention operation captures long-range dependencies to enhance
encoded point features for the search area and the template, respectively.
Subsequently, we generate the coarse tracking results by matching the two sets
of point features via cross-attention. 3) Based on the coarse tracking results,
we employ a novel Prediction Refinement Module to obtain the final refined
prediction. In addition, we create a large-scale point cloud single object
tracking benchmark based on the Waymo Open Dataset. Extensive experiments show
that PTTR achieves superior point cloud tracking in both accuracy and
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pho(SC)-CTC -- A Hybrid Approach Towards Zero-shot Word Image
  Recognition <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.15093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.15093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Bhatt, Anuj Rai, Narayanan C. Krishnan, Sukalpa Chanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating words in a historical document image archive for word image
recognition purpose demands time and skilled human resource (like historians,
paleographers). In a real-life scenario, obtaining sample images for all
possible words is also not feasible. However, Zero-shot learning methods could
aptly be used to recognize unseen/out-of-lexicon words in such historical
document images. Based on previous state-of-the-art method for zero-shot word
recognition Pho(SC)Net, we propose a hybrid model based on the CTC framework
(Pho(SC)-CTC) that takes advantage of the rich features learned by Pho(SC)Net
followed by a connectionist temporal classification (CTC) framework to perform
the final classification. Encouraging results were obtained on two publicly
available historical document datasets and one synthetic handwritten dataset,
which justifies the efficacy of Pho(SC)-CTC and Pho(SC)Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review (International Journal on Document Analysis and
  Recognition). This paper is the extension of the paper titled "Pho(SC)Net: An
  Approach Towards Zero-shot Word Image Recognition in Historical Documents"
  published in ICDAR 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperbolic Vision <span class="highlight-title">Transformer</span>s: Combining Improvements in Metric
  Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRISPnet: Color Rendition ISP Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus Souza, Wolfgang Heidrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image signal processors (ISPs) are historically grown legacy software systems
for reconstructing color images from noisy raw sensor measurements. They are
usually composited of many heuristic blocks for denoising, demosaicking, and
color restoration. Color reproduction in this context is of particular
importance, since the raw colors are often severely distorted, and each smart
phone manufacturer has developed their own characteristic heuristics for
improving the color rendition, for example of skin tones and other visually
important colors.
  In recent years there has been strong interest in replacing the historically
grown ISP systems with deep learned pipelines. Much progress has been made in
approximating legacy ISPs with such learned models. However, so far the focus
of these efforts has been on reproducing the structural features of the images,
with less attention paid to color rendition.
  Here we present CRISPnet, the first learned ISP model to specifically target
color rendition accuracy relative to a complex, legacy smart phone ISP. We
achieve this by utilizing both image metadata (like a legacy ISP would), as
well as by learning simple global semantics based on image classification --
similar to what a legacy ISP does to determine the scene type. We also
contribute a new ISP image dataset consisting of both high dynamic range
monitor data, as well as real-world data, both captured with an actual cell
phone ISP pipeline under a variety of lighting conditions, exposure times, and
gain settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Occlusion Fields: An Implicit Representation for Non-Line-of-Sight
  Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Grau, Markus Plack, Patrick Haehn, Michael Weinmann, Matthias Hullin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality
that aims to recover objects or scene parts outside the field of view from
measurements of light that is indirectly scattered off a directly visible,
diffuse wall. Despite recent advances in acquisition and reconstruction
techniques, the well-posedness of the problem at large, and the recoverability
of objects and their shapes in particular, remains an open question. The
commonly employed Fermat path criterion is rather conservative with this
regard, as it classifies some surfaces as unrecoverable, although they
contribute to the signal.
  In this paper, we use a simpler necessary criterion for an opaque surface
patch to be recoverable. Such piece of surface must be directly visible from
some point on the wall, and it must occlude the space behind itself. Inspired
by recent advances in neural implicit representations, we devise a new
representation and reconstruction technique for NLoS scenes that unifies the
treatment of recoverability with the reconstruction itself. Our approach, which
we validate on various synthetic and experimental datasets, exhibits
interesting properties. Unlike memory-inefficient volumetric representations,
ours allows to infer adaptively tessellated surfaces from time-of-flight
measurements of moderate resolution. It can further recover features beyond the
Fermat path criterion, and it is robust to significant amounts of
self-occlusion. We believe that this is the first time that these properties
have been achieved in one system that, as an additional benefit, is trainable
and hence suited for data-driven approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bi-directional Object-context Prioritization Learning for Saliency
  Ranking <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Tian, Ke Xu, Xin Yang, Lin Du, Baocai Yin, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The saliency ranking task is recently proposed to study the visual behavior
that humans would typically shift their attention over different objects of a
scene based on their degrees of saliency. Existing approaches focus on learning
either object-object or object-scene relations. Such a strategy follows the
idea of object-based attention in Psychology, but it tends to favor those
objects with strong semantics (e.g., humans), resulting in unrealistic saliency
ranking. We observe that spatial attention works concurrently with object-based
attention in the human visual recognition system. During the recognition
process, the human spatial attention mechanism would move, engage, and
disengage from region to region (i.e., context to context). This inspires us to
model the region-level interactions, in addition to the object-level reasoning,
for saliency ranking. To this end, we propose a novel bi-directional method to
unify spatial attention and object-based attention for saliency ranking. Our
model includes two novel modules: (1) a selective object saliency (SOS) module
that models objectbased attention via inferring the semantic representation of
the salient object, and (2) an object-context-object relation (OCOR) module
that allocates saliency ranks to objects by jointly modeling the object-context
and context-object interactions of the salient objects. Extensive experiments
show that our approach outperforms existing state-of-theart methods. Our code
and pretrained model are available at https://github.com/GrassBro/OCOR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational ergonomics for task delegation in Human-Robot
  Collaboration: spatiotemporal adaptation of the robot to the human through
  contactless gesture recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brenda Elizabeth Olivas-Padilla, Dimitris Papanagiotou, Gavriela Senteri, Sotiris Manitsaris, Alina Glushkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high prevalence of work-related musculoskeletal disorders (WMSDs) could
be addressed by optimizing Human-Robot Collaboration (HRC) frameworks for
manufacturing applications. In this context, this paper proposes two hypotheses
for ergonomically effective task delegation and HRC. The first hypothesis
states that it is possible to quantify ergonomically professional tasks using
motion data from a reduced set of sensors. Then, the most dangerous tasks can
be delegated to a collaborative robot. The second hypothesis is that by
including gesture recognition and spatial adaptation, the ergonomics of an HRC
scenario can be improved by avoiding needless motions that could expose
operators to ergonomic risks and by lowering the physical effort required of
operators. An HRC scenario for a television manufacturing process is optimized
to test both hypotheses. For the ergonomic evaluation, motion primitives with
known ergonomic risks were modeled for their detection in professional tasks
and to estimate a risk score based on the European Assembly Worksheet (EAWS). A
Deep Learning gesture recognition module trained with egocentric television
assembly data was used to complement the collaboration between the human
operator and the robot. Additionally, a skeleton-tracking algorithm provided
the robot with information about the operator's pose, allowing it to spatially
adapt its motion to the operator's anthropometrics. Three experiments were
conducted to determine the effect of gesture recognition and spatial adaptation
on the operator's range of motion. The rate of spatial adaptation was used as a
key performance indicator (KPI), and a new KPI for measuring the reduction in
the operator's motion is presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Know your sensORs -- A Modality Study For Surgical Action Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Bastian, Tobias Czempiel, Christian Heiliger, Konrad Karcz, Ulrich Eck, Benjamin Busam, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surgical operating room (OR) presents many opportunities for automation
and optimization. Videos from various sources in the OR are becoming
increasingly available. The medical community seeks to leverage this wealth of
data to develop automated methods to advance interventional care, lower costs,
and improve overall patient outcomes. Existing datasets from OR room cameras
are thus far limited in size or modalities acquired, leaving it unclear which
sensor modalities are best suited for tasks such as recognizing surgical action
from videos. This study demonstrates that surgical action recognition
performance can vary depending on the image modalities used. We perform a
methodical analysis on several commonly available sensor modalities, presenting
two fusion approaches that improve classification performance. The analyses are
carried out on a set of multi-view RGB-D video recordings of 18 laparoscopic
procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Karaali, Rozenn Dahyot, Donal J. Sexton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate retinal vessel segmentation is an important task for many
computer-aided diagnosis systems. Yet, it is still a challenging problem due to
the complex vessel structures of an eye. Numerous vessel segmentation methods
have been proposed recently, however more research is needed to deal with poor
segmentation of thin and tiny vessels. To address this, we propose a new deep
learning pipeline combining the efficiency of residual dense net blocks and,
residual squeeze and excitation blocks. We validate experimentally our approach
on three datasets and show that our pipeline outperforms current state of the
art techniques on the sensitivity metric relevant to assess capture of small
vessels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICPRAI 2022 - 3rd International Conference on Pattern
  Recognition and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12329v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12329v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present in this paper a novel query formulation using dynamic anchor boxes
for DETR (DEtection TRansformer) and offer a deeper understanding of the role
of queries in DETR. This new formulation directly uses box coordinates as
queries in Transformer decoders and dynamically updates them layer-by-layer.
Using box coordinates not only helps using explicit positional priors to
improve the query-to-feature similarity and eliminate the slow training
convergence issue in DETR, but also allows us to modulate the positional
attention map using the box width and height information. Such a design makes
it clear that queries in DETR can be implemented as performing soft ROI pooling
layer-by-layer in a cascade manner. As a result, it leads to the best
performance on MS-COCO benchmark among the DETR-like detection models under the
same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50
epochs. We also conducted extensive experiments to confirm our analysis and
verify the effectiveness of our methods. Code is available at
\url{https://github.com/SlongLiu/DAB-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Annotator Preference and Stochastic Annotation Error for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13410v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13410v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Liao, Shishuai Hu, Yutong Xie, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual annotation of medical images is highly subjective, leading to
inevitable and huge annotation biases. Deep learning models may surpass human
performance on a variety of tasks, but they may also mimic or amplify these
biases. Although we can have multiple annotators and fuse their annotations to
reduce stochastic errors, we cannot use this strategy to handle the bias caused
by annotators' preferences. In this paper, we highlight the issue of
annotator-related biases on medical image segmentation tasks, and propose a
Preference-involved Annotation Distribution Learning (PADL) framework to
address it from the perspective of disentangling an annotator's preference from
stochastic errors using distribution learning so as to produce not only a meta
segmentation but also the segmentation possibly made by each annotator. Under
this framework, a stochastic error modeling (SEM) module estimates the meta
segmentation map and average stochastic error map, and a series of human
preference modeling (HPM) modules estimate each annotator's segmentation and
the corresponding stochastic error. We evaluated our PADL framework on two
medical image benchmarks with different imaging modalities, which have been
annotated by multiple medical professionals, and achieved promising performance
on all five medical image segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DS-Net: Dynamic Spatiotemporal Network for Video Salient Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.04886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.04886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liu, Jiaxiang Wang, Weikang Wang, Yuting Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As moving objects always draw more attention of human eyes, the temporal
motive information is always exploited complementarily with spatial information
to detect salient objects in videos. Although efficient tools such as optical
flow have been proposed to extract temporal motive information, it often
encounters difficulties when used for saliency detection due to the movement of
camera or the partial movement of salient objects. In this paper, we
investigate the complimentary roles of spatial and temporal information and
propose a novel dynamic spatiotemporal network (DS-Net) for more effective
fusion of spatiotemporal information. We construct a symmetric two-bypass
network to explicitly extract spatial and temporal features. A dynamic weight
generator (DWG) is designed to automatically learn the reliability of
corresponding saliency branch. And a top-down cross attentive aggregation (CAA)
procedure is designed so as to facilitate dynamic complementary aggregation of
spatiotemporal features. Finally, the features are modified by spatial
attention with the guidance of coarse saliency map and then go through decoder
part for final saliency map. Experimental results on five benchmarks VOS,
DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method
achieves superior performance than state-of-the-art algorithms. The source code
is available at https://github.com/TJUMMG/DS-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has made some format changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TVConv: Efficient Translation Variant Convolution for Layout-aware
  Visual Processing <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierun Chen, Tianlang He, Weipeng Zhuo, Li Ma, Sangtae Ha, S. -H. Gary Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As convolution has empowered many smart applications, dynamic convolution
further equips it with the ability to adapt to diverse inputs. However, the
static and dynamic convolutions are either layout-agnostic or
computation-heavy, making it inappropriate for layout-specific applications,
e.g., face recognition and medical image segmentation. We observe that these
applications naturally exhibit the characteristics of large intra-image
(spatial) variance and small cross-image variance. This observation motivates
our efficient translation variant convolution (TVConv) for layout-aware visual
processing. Technically, TVConv is composed of affinity maps and a
weight-generating block. While affinity maps depict pixel-paired relationships
gracefully, the weight-generating block can be explicitly overparameterized for
better training while maintaining efficient inference. Although conceptually
simple, TVConv significantly improves the efficiency of the convolution and can
be readily plugged into various network architectures. Extensive experiments on
face recognition show that TVConv reduces the computational cost by up to 3.1x
and improves the corresponding throughput by 2.3x while maintaining a high
accuracy compared to the depthwise convolution. Moreover, for the same
computation cost, we boost the mean accuracy by up to 4.21%. We also conduct
experiments on the optic disc/cup segmentation task and obtain better
generalization performance, which helps mitigate the critical data scarcity
issue. Code is available at https://github.com/JierunChen/TVConv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PanFormer: a <span class="highlight-title">Transformer</span> Based Model for Pan-sharpening <span class="chip">ICME 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Zhou, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)
image from a low-resolution (LR) multi-spectral (MS) image and its
corresponding panchromatic (PAN) image acquired by a same satellite. Inspired
by a new fashion in recent deep learning community, we propose a novel
Transformer based model for pan-sharpening. We explore the potential of
Transformer in image feature extraction and fusion. Following the successful
development of vision transformers, we design a two-stream network with the
self-attention to extract the modality-specific features from the PAN and MS
modalities and apply a cross-attention module to merge the spectral and spatial
features. The pan-sharpened image is produced from the enhanced fused features.
Extensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our
Transformer based model achieves impressive results and outperforms many
existing CNN based methods, which shows the great potential of introducing
Transformer to the pan-sharpening task. Codes are available at
https://github.com/zhysora/PanFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of Abnormal Hand Movement for Aiding in Autism Detection:
  Machine Learning Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.07917v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.07917v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Lakkapragada, Aaron Kline, Onur Cezmi Mutlu, Kelley Paskov, Brianna Chrisman, Nate Stockham, Peter Washington, Dennis Wall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A formal autism diagnosis can be an inefficient and lengthy process. Families
may wait months or longer before receiving a diagnosis for their child despite
evidence that earlier intervention leads to better treatment outcomes. Digital
technologies which detect the presence of behaviors related to autism can scale
access to pediatric diagnoses. This work aims to demonstrate the feasibility of
deep learning technologies for detecting hand flapping from unstructured home
videos as a first step towards validating whether models and digital
technologies can be leveraged to aid with autism diagnoses. We used the
Self-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand
flapping, head banging, and spinning exhibited by children. From all the hand
flapping videos, we extracted 100 positive and control videos of hand flapping,
each between 2 to 5 seconds in duration. Utilizing both
landmark-driven-approaches and MobileNet V2's pretrained convolutional layers,
our highest performing model achieved a testing F1 score of 84% (90% precision
and 80% recall) when evaluating with 5-fold cross validation 100 times. This
work provides the first step towards developing precise deep learning methods
for activity detection of autism-related behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and
  Scene Flow Estimation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haisong Liu, Tao Lu, Yihui Xu, Jia Liu, Wenjie Li, Lijun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of jointly estimating the optical flow
and scene flow from synchronized 2D and 3D data. Previous methods either employ
a complex pipeline that splits the joint task into independent stages, or fuse
2D and 3D information in an "early-fusion" or "late-fusion" manner. Such
one-size-fits-all approaches suffer from a dilemma of failing to fully utilize
the characteristic of each modality or to maximize the inter-modality
complementarity. To address the problem, we propose a novel end-to-end
framework, called CamLiFlow. It consists of 2D and 3D branches with multiple
bidirectional connections between them in specific layers. Different from
previous work, we apply a point-based 3D branch to better extract the geometric
features and design a symmetric learnable operator to fuse dense image features
and sparse point features. Experiments show that CamLiFlow achieves better
performance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow
benchmark, outperforming the previous art with 1/7 parameters. Code is
available at https://github.com/MCG-NJU/CamLiFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-Verse: Bidirectional Generation Between Image and Text <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11133v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11133v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Shape Analysis of Brain Arterial Networks (BAN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.04793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.04793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Guo, Aditi Basu Bal, Tom Needham, Anuj Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structures of brain arterial networks (BANs) - that are complex arrangements
of individual arteries, their branching patterns, and inter-connectivities -
play an important role in characterizing and understanding brain physiology.
One would like tools for statistically analyzing the shapes of BANs, i.e.
quantify shape differences, compare population of subjects, and study the
effects of covariates on these shapes. This paper mathematically represents and
statistically analyzes BAN shapes as elastic shape graphs. Each elastic shape
graph is made up of nodes that are connected by a number of 3D curves, and
edges, with arbitrary shapes. We develop a mathematical representation, a
Riemannian metric and other geometrical tools, such as computations of
geodesics, means and covariances, and PCA for analyzing elastic graphs and
BANs. This analysis is applied to BANs after separating them into four
components -- top, bottom, left, and right. This framework is then used to
generate shape summaries of BANs from 92 subjects, and to study the effects of
age and gender on shapes of BAN components. We conclude that while gender
effects require further investigation, the age has a clear, quantifiable effect
on BAN shapes. Specifically, we find an increased variance in BAN shapes as age
increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2003.00287</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Liang, Tiancai Wang, Xiangyu Zhang, Jian Sun, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megvii-research/TreeEnergyLoss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Recommender Systems Forget: Learning and Unlearning for Erasable
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyuan Li, Xiaolin Zheng, Chaochao Chen, Junlin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy laws and regulations enforce data-driven systems, e.g., recommender
systems, to erase the data that concern individuals. As machine learning models
potentially memorize the training data, data erasure should also unlearn the
data lineage in models, which raises increasing interest in the problem of
Machine Unlearning (MU). However, existing MU methods cannot be directly
applied into recommendation. The basic idea of most recommender systems is
collaborative filtering, but existing MU methods ignore the collaborative
information across users and items. In this paper, we propose a general
erasable recommendation framework, namely LASER, which consists of Group module
and SeqTrain module. Firstly, Group module partitions users into balanced
groups based on their similarity of collaborative embedding learned via
hypergraph. Then SeqTrain module trains the model sequentially on all groups
with curriculum learning. Both theoretical analysis and experiments on two
real-world datasets demonstrate that LASER can not only achieve efficient
unlearning, but also outperform the state-of-the-art unlearning framework in
terms of model utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommender systems based on graph embedding techniques: A comprehensive
  <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal tool to alleviate the information overload problem, recommender
systems aim to predict user's preferred items from millions of candidates by
analyzing observed user-item relations. As for alleviating the sparsity and
cold start problems encountered by recommender systems, researchers resort to
employing side information or knowledge in recommendation as a strategy for
uncovering hidden (indirect) user-item relations, aiming to enrich observed
information (or data) for recommendation. However, in the face of the high
complexity and large scale of side information and knowledge, this strategy
relies for efficient implementation on the scalability of recommendation
models. Not until after the prevalence of machine learning did graph embedding
techniques be a concentration, which can efficiently utilize complex and
large-scale data. In light of that, equipping recommender systems with graph
embedding techniques has been widely studied these years, appearing to
outperform conventional recommendation implemented directly based on graph
topological analysis. As the focus, this article retrospects graph
embedding-based recommendation from embedding techniques for bipartite graphs,
general graphs and knowledge graphs, and proposes a general design pipeline of
that. In addition, after comparing several representative graph embedding-based
recommendation models with the most common-used conventional recommendation
models on simulations, this article manifests that the conventional models can
overall outperform the graph embedding-based ones in predicting implicit
user-item interactions, revealing the comparative weakness of graph
embedding-based recommendation in these tasks. To foster future research, this
article proposes suggestions on making a trade-off between graph
embedding-based recommendation and conventional recommendation in different
tasks, and puts forward open questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation <span class="chip">KDD 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, Binqiang Zhao, Ziyang Zheng, Shirui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale e-commercial platforms in the real-world usually contain various
recommendation scenarios (domains) to meet demands of diverse customer groups.
Multi-Domain Recommendation (MDR), which aims to jointly improve
recommendations on all domains, has attracted increasing attention from
practitioners and researchers. Existing MDR methods often employ a shared
structure to leverage reusable features for all domains and several specific
parts to capture domain-specific information. However, data from different
domains may conflict with each other and cause shared parameters to stay at a
compromised position on the optimization landscape. This could deteriorate the
overall performance. Despite the specific parameters are separately learned for
each domain, they can easily overfit on data sparsity domains. Furthermore,
data distribution differs across domains, making it challenging to develop a
general model that can be applied to all circumstances. To address these
problems, we propose a novel model agnostic learning method, namely MAMDR, for
the multi-domain recommendation. Specifically, we first propose a Domain
Negotiation (DN) strategy to alleviate the conflict between domains and learn
better shared parameters. Then, we develop a Domain Regularization (DR) scheme
to improve the generalization ability of specific parameters by learning from
other domains. Finally, we integrate these components into a unified framework
and present MAMDR which can be applied to any model structure to perform
multi-domain recommendation. Extensive experiments on various real-world
datasets and online applications demonstrate both the effectiveness and
generalizability of MAMDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to KDD 2022 ADS Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic <span class="highlight-title">Review</span>-based Recommenders <span class="chip">SC21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostadin Cvejoski, Ramses J. Sanchez, Christian Bauckhage, Cesar Ojeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Just as user preferences change with time, item reviews also reflect those
same preference changes. In a nutshell, if one is to sequentially incorporate
review content knowledge into recommender systems, one is naturally led to
dynamical models of text. In the present work we leverage the known power of
reviews to enhance rating predictions in a way that (i) respects the causality
of review generation and (ii) includes, in a bidirectional fashion, the ability
of ratings to inform language review models and vice-versa, language
representations that help predict ratings end-to-end. Moreover, our
representations are time-interval aware and thus yield a continuous-time
representation of the dynamics. We provide experiments on real-world datasets
and show that our methodology is able to outperform several state-of-the-art
models. Source code for all models can be found at [1].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages, Published at International Data Science Conference 2021
  (iDSC21)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Word Translation via Two-Stage <span class="highlight-title">Contrastive Learning</span> <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoyiran Li, Fangyu Liu, Nigel Collier, Anna Korhonen, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual
task, aiming to bridge the lexical gap between different languages. In this
work, we propose a robust and effective two-stage contrastive learning
framework for the BLI task. At Stage C1, we propose to refine standard
cross-lingual linear maps between static word embeddings (WEs) via a
contrastive learning objective; we also show how to integrate it into the
self-learning procedure for even more refined cross-lingual maps. In Stage C2,
we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word
translation capability. We also show that static WEs induced from the
`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments
on standard BLI datasets for diverse languages and different experimental
setups demonstrate substantial gains achieved by our framework. While the BLI
method from Stage C1 already yields substantial gains over all state-of-the-art
BLI methods in our comparison, even stronger improvements are met with the full
two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28
language pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Main</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span> Array Keeps the Bias Away: Debiasing <span class="highlight-title">Vision-Language</span> Models
  with Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Berg, Siobhan Mackenzie Hall, Yash Bhalgat, Wonsuk Yang, Hannah Rose Kirk, Aleksandar Shtedritski, Max Bain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures. For code and trained token embeddings, see
  https://github.com/oxai/debias-vision-lang</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from All Vehicles <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dian Chen, Philipp Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a system to train driving policies from experiences
collected not just from the ego-vehicle, but all vehicles that it observes.
This system uses the behaviors of other agents to create more diverse driving
scenarios without collecting additional data. The main difficulty in learning
from other vehicles is that there is no sensor information. We use a set of
supervisory tasks to learn an intermediate representation that is invariant to
the viewpoint of the controlling vehicle. This not only provides a richer
signal at training time but also allows more complex reasoning during
inference. Learning how all vehicles drive helps predict their behavior at test
time and can avoid collisions. We evaluate this system in closed-loop driving
simulations. Our system outperforms all prior methods on the public CARLA
Leaderboard by a wide margin, improving driving score by 25 and route
completion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving
challenge. Demo videos are available at https://dotchen.github.io/LAV/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted to CVPR 2022; Code and data available at
  https://github.com/dotchen/LAV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> <span class="highlight-title">Distillation</span> by Matching Training Trajectories <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, Jun-Yan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022 website:
  https://georgecazenavette.github.io/mtt-distillation/ code:
  https://github.com/GeorgeCazenavette/mtt-distillation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MetaMorph: Learning Universal Controllers with <span class="highlight-title">Transformer</span>s <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agrim Gupta, Linxi Fan, Surya Ganguli, <span class="highlight-author">Li Fei-Fei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple domains like vision, natural language, and audio are witnessing
tremendous progress by leveraging Transformers for large scale pre-training
followed by task specific fine tuning. In contrast, in robotics we primarily
train a single robot for a single task. However, modular robot systems now
allow for the flexible combination of general-purpose building blocks into task
optimized morphologies. However, given the exponentially large number of
possible robot morphologies, training a controller for each new design is
impractical. In this work, we propose MetaMorph, a Transformer based approach
to learn a universal controller over a modular robot design space. MetaMorph is
based on the insight that robot morphology is just another modality on which we
can condition the output of a Transformer. Through extensive experiments we
demonstrate that large scale pre-training on a variety of robot morphologies
results in policies with combinatorial generalization capabilities, including
zero shot generalization to unseen robot morphologies. We further demonstrate
that our pre-trained policy can be used for sample-efficient transfer to
completely new robot morphologies and tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Focal Modulation Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Yang, Chunyuan Li, <span class="highlight-author">Jianfeng Gao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose focal modulation network (FocalNet in short), where
self-attention (SA) is completely replaced by a focal modulation module that is
more effective and efficient for modeling token interactions. Focal modulation
comprises three components: $(i)$ hierarchical contextualization, implemented
using a stack of depth-wise convolutional layers, to encode visual contexts
from short to long ranges at different granularity levels, $(ii)$ gated
aggregation to selectively aggregate context features for each visual token
(query) based on its content, and $(iii)$ modulation or element-wise affine
transformation to fuse the aggregated features into the query vector. Extensive
experiments show that FocalNets outperform the state-of-the-art SA counterparts
(e.g., Swin Transformers) with similar time and memory cost on the tasks of
image classification, object detection, and semantic segmentation.
Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9%
top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains
86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224
and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when
transferred to downstream tasks. For object detection with Mask R-CNN, our
FocalNet base trained with 1$\times$ already surpasses Swin trained with
3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet,
FocalNet base evaluated at single-scale outperforms Swin evaluated at
multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable
alternative to SA for effective and efficient visual modeling in real-world
applications. Code is available at https://github.com/microsoft/FocalNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Supervised Feature Selection from High Dimensional Feature Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijing Yang, Wei Wang, Hongyu Fu, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of machine learning to image and video data often yields a
high dimensional feature space. Effective feature selection techniques identify
a discriminant feature subspace that lowers computational and modeling costs
with little performance degradation. A novel supervised feature selection
methodology is proposed for machine learning decisions in this work. The
resulting tests are called the discriminant feature test (DFT) and the relevant
feature test (RFT) for the classification and regression problems,
respectively. The DFT and RFT procedures are described in detail. Furthermore,
we compare the effectiveness of DFT and RFT with several classic feature
selection methods. To this end, we use deep features obtained by LeNet-5 for
MNIST and Fashion-MNIST datasets as illustrative examples. It is shown by
experimental results that DFT and RFT can select a lower dimensional feature
subspace distinctly and robustly while maintaining high decision performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPRITE: A Scalable Privacy-Preserving and Verifiable Collaborative
  Learning for Industrial IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayasree Sengupta, Sushmita Ruj, Sipra Das Bit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently collaborative learning is widely applied to model sensitive data
generated in Industrial IoT (IIoT). It enables a large number of devices to
collectively train a global model by collaborating with a server while keeping
the datasets on their respective premises. However, existing approaches are
limited by high overheads and may also suffer from falsified aggregated results
returned by a malicious server. Hence, we propose a Scalable,
Privacy-preserving and veRIfiable collaboraTive lEarning (SPRITE) algorithm to
train linear and logistic regression models for IIoT. We aim to reduce burden
from resource-constrained IIoT devices and trust dependence on cloud by
introducing fog as a middleware. SPRITE employs threshold secret sharing to
guarantee privacy-preservation and robustness to IIoT device dropout whereas
verifiable additive homomorphic secret sharing to ensure verifiability during
model aggregation. We prove the security of SPRITE in an honest-but-curious
setting where the cloud is untrustworthy. We validate SPRITE to be scalable and
lightweight through theoretical overhead analysis and extensive testbed
experimentation on an IIoT use-case with two real-world industrial datasets.
For a large-scale industrial setup, SPRITE records 65% and 55% improved
performance over its competitor for linear and logistic regressions
respectively while reducing communication overhead for an IIoT device by 90%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at The 22nd IEEE/ACM International Symposium
  on Cluster, Cloud and Internet Computing (CCGrid 2022). 5 figures and 6
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling faster and more reliable sonographic assessment of gestational
  age through machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chace Lee, Angelica Willis, Christina Chen, Marcin Sieniek, Akib Uddin, Jonny Wong, Rory Pilgrim, Katherine Chou, Daniel Tse, Shravya Shetty, Ryan G. Gomes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fetal ultrasounds are an essential part of prenatal care and can be used to
estimate gestational age (GA). Accurate GA assessment is important for
providing appropriate prenatal care throughout pregnancy and identifying
complications such as fetal growth disorders. Since derivation of GA from
manual fetal biometry measurements (head, abdomen, femur) are
operator-dependent and time-consuming, there have been a number of research
efforts focused on using artificial intelligence (AI) models to estimate GA
using standard biometry images, but there is still room to improve the accuracy
and reliability of these AI systems for widescale adoption. To improve GA
estimates, without significant change to provider workflows, we leverage AI to
interpret standard plane ultrasound images as well as 'fly-to' ultrasound
videos, which are 5-10s videos automatically recorded as part of the standard
of care before the still image is captured. We developed and validated three AI
models: an image model using standard plane images, a video model using fly-to
videos, and an ensemble model (combining both image and video). All three were
statistically superior to standard fetal biometry-based GA estimates derived by
expert sonographers, the ensemble model has the lowest mean absolute error
(MAE) compared to the clinical standard fetal biometry (mean difference: -1.51
$\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404
participants. We showed that our models outperform standard biometry by a more
substantial margin on fetuses that were small for GA. Our AI models have the
potential to empower trained operators to estimate GA with higher accuracy
while reducing the amount of time required and user variability in measurement
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GradViT: Gradient Inversion of Vision <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hatamizadeh, Hongxu Yin, Holger Roth, Wenqi Li, Jan Kautz, Daguang Xu, Pavlo Molchanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we demonstrate the vulnerability of vision transformers (ViTs)
to gradient-based inversion attacks. During this attack, the original data
batch is reconstructed given model weights and the corresponding gradients. We
introduce a method, named GradViT, that optimizes random noise into naturally
looking images via an iterative process. The optimization objective consists of
(i) a loss on matching the gradients, (ii) image prior in the form of distance
to batch-normalization statistics of a pretrained CNN model, and (iii) a total
variation regularization on patches to guide correct recovery locations. We
propose a unique loss scheduling function to overcome local minima during
optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and
observe unprecedentedly high fidelity and closeness to the original (hidden)
data. During the analysis we find that vision transformers are significantly
more vulnerable than previously studied CNNs due to the presence of the
attention mechanism. Our method demonstrates new state-of-the-art results for
gradient inversion in both qualitative and quantitative metrics. Project page
at https://gradvit.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2021 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insights From the NeurIPS 2021 NetHack Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Hambro, Sharada Mohanty, Dmitrii Babaev, Minwoo Byeon, Dipam Chakraborty, Edward Grefenstette, Minqi Jiang, Daejin Jo, Anssi Kanervisto, Jongmin Kim, Sungwoong Kim, Robert Kirk, Vitaly Kurin, Heinrich Küttler, Taehwon Kwon, Donghoon Lee, Vegard Mella, Nantas Nardelli, Ivan Nazarov, Nikita Ovsov, Jack Parker-Holder, Roberta Raileanu, Karolis Ramanauskas, Tim Rocktäschel, Danielle Rothermel, Mikayel Samvelyan, Dmitry Sorokin, Maciej Sypetkowski, Michał Sypetkowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we summarize the takeaways from the first NeurIPS 2021
NetHack Challenge. Participants were tasked with developing a program or agent
that can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by
interacting with the NetHack Learning Environment (NLE), a scalable,
procedurally generated, and challenging Gym environment for reinforcement
learning (RL). The challenge showcased community-driven progress in AI with
many diverse approaches significantly beating the previously best results on
NetHack. Furthermore, it served as a direct comparison between neural (e.g.,
deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on
NetHack symbolic bots currently outperform deep RL by a large margin. Lastly,
no agent got close to winning the game, illustrating NetHack's suitability as a
long-term benchmark for AI research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at PMLR for the NeuRIPS 2021 Competition Workshop Track,
  10 pages + 10 in appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance of long short-term memory artificial neural networks in
  nowcasting during the COVID-19 crisis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Hopp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has demonstrated the increasing need of policymakers
for timely estimates of macroeconomic variables. A prior UNCTAD research paper
examined the suitability of long short-term memory artificial neural networks
(LSTM) for performing economic nowcasting of this nature. Here, the LSTM's
performance during the COVID-19 pandemic is compared and contrasted with that
of the dynamic factor model (DFM), a commonly used methodology in the field.
Three separate variables, global merchandise export values and volumes and
global services exports, were nowcast with actual data vintages and performance
evaluated for the second, third, and fourth quarters of 2020 and the first and
second quarters of 2021. In terms of both mean absolute error and root mean
square error, the LSTM obtained better performance in two-thirds of
variable/quarter combinations, as well as displayed more gradual forecast
evolutions with more consistent narratives and smaller revisions. Additionally,
a methodology to introduce interpretability to LSTMs is introduced and made
available in the accompanying nowcast_lstm Python library, which is now also
available in R, MATLAB, and Julia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the (Non-)Robustness of Two-Layer Neural Networks in Different
  Learning Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Dohmatob, Alberto Bietti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are known to be highly sensitive to adversarial examples.
These may arise due to different factors, such as random initialization, or
spurious correlations in the learning problem. To better understand these
factors, we provide a precise study of robustness and generalization in
different scenarios, from initialization to the end of training in different
regimes, as well as intermediate scenarios, where initialization still plays a
role due to "lazy" training. We consider over-parameterized networks in high
dimensions with quadratic targets and infinite samples. Our analysis allows us
to identify new trade-offs between generalization and robustness, whereby
robustness can only get worse when generalization improves, and vice versa. We
also show how linearized lazy training regimes can worsen robustness, due to
improperly scaled random initialization. Our theoretical results are
illustrated with numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical tradeoffs between memory, compute, and performance in learned
  optimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Metz, C. Daniel Freeman, James Harrison, Niru Maheswaranathan, Jascha Sohl-Dickstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization plays a costly and crucial role in developing machine learning
systems. In learned optimizers, the few hyperparameters of commonly used
hand-designed optimizers, e.g. Adam or SGD, are replaced with flexible
parametric functions. The parameters of these functions are then optimized so
that the resulting learned optimizer minimizes a target loss on a chosen class
of models. Learned optimizers can both reduce the number of required training
steps and improve the final test loss. However, they can be expensive to train,
and once trained can be expensive to use due to computational and memory
overhead for the optimizer itself. In this work, we identify and quantify the
design features governing the memory, compute, and performance trade-offs for
many learned and hand-designed optimizers. We further leverage our analysis to
construct a learned optimizer that is both faster and more memory efficient
than previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sionna: An Open-Source Library for Next-Generation Physical Layer
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Hoydis, Sebastian Cammerer, Fayçal Ait Aoudia, Avinash Vem, Nikolaus Binder, Guillermo Marcus, Alexander Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sionna is a GPU-accelerated open-source library for link-level simulations
based on TensorFlow. It enables the rapid prototyping of complex communication
system architectures and provides native support for the integration of neural
networks. Sionna implements a wide breadth of carefully tested state-of-the-art
algorithms that can be used for benchmarking and end-to-end performance
evaluation. This allows researchers to focus on their research, making it more
impactful and reproducible, while saving time implementing components outside
their area of expertise. This white paper provides a brief introduction to
Sionna, explains its design principles and features, as well as future
extensions, such as integrated ray tracing and custom CUDA kernels. We believe
that Sionna is a valuable tool for research on next-generation communication
systems, such as 6G, and we welcome contributions from our community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 4 code listings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Techniques for Identifying and Resolving Representation Bias
  in Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Shahbazi, Yin Lin, Abolfazl Asudeh, H. V. Jagadish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The grand goal of data-driven decision-making is to help humans make
decisions, not only easily and at scale but also wisely, accurately, and just.
However, data-driven algorithms are only as good as the data they work with,
while data sets, especially social data, often miss representing minorities.
Representation Bias in data can happen due to various reasons ranging from
historical discrimination to selection and sampling biases in the data
acquisition and preparation methods. One cannot expect AI-based societal
solutions to have equitable outcomes without addressing the representation
bias. This paper surveys the existing literature on representation bias in the
data. It presents a taxonomy to categorize the studied techniques based on
multiple design dimensions and provide a side-by-side comparison of their
properties. There is still a long way to fully address representation bias
issues in data. The authors hope that this survey motivates researchers to
approach these challenges in the future by observing existing work within their
respective domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Girl Has A Name, And It's ... Adversarial Authorship Attribution for
  Deobfuscation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyue Zhai, Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in natural language processing have enabled powerful
privacy-invasive authorship attribution. To counter authorship attribution,
researchers have proposed a variety of rule-based and learning-based text
obfuscation approaches. However, existing authorship obfuscation approaches do
not consider the adversarial threat model. Specifically, they are not evaluated
against adversarially trained authorship attributors that are aware of
potential obfuscation. To fill this gap, we investigate the problem of
adversarial authorship attribution for deobfuscation. We show that
adversarially trained authorship attributors are able to degrade the
effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate
the effectiveness of adversarial training when the attributor makes incorrect
assumptions about whether and which obfuscator was used. While there is a a
clear degradation in attribution accuracy, it is noteworthy that this
degradation is still at or above the attribution accuracy of the attributor
that is not adversarially trained at all. Our results underline the need for
stronger obfuscation approaches that are resistant to deobfuscation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 3 tables, ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-MEN: Guaranteed XOR-Maximum Entropy Constrained Inverse Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Ding, Yeiang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse Reinforcement Learning (IRL) is a powerful way of learning from
demonstrations. In this paper, we address IRL problems with the availability of
prior knowledge that optimal policies will never violate certain constraints.
Conventional approaches ignoring these constraints need many demonstrations to
converge. We propose XOR-Maximum Entropy Constrained Inverse Reinforcement
Learning (X-MEN), which is guaranteed to converge to the optimal policy in
linear rate w.r.t. the number of learning iterations. X-MEN embeds XOR-sampling
-- a provable sampling approach that transforms the #P complete sampling
problem into queries to NP oracles -- into the framework of maximum entropy
IRL. X-MEN also guarantees the learned policy will never generate trajectories
that violate constraints. Empirical results in navigation demonstrate that
X-MEN converges faster to the optimal policies compared to baseline approaches
and always generates trajectories that satisfy multi-state combinatorial
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-time Junk Food Recognition System based on Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirajum Munira Shifat, Takitazwar Parthib, Sabikunnahar Talukder Pyaasa, Nila Maitra Chaity, Niloy Kumar, Md. Kishor Morol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $ $As a result of bad eating habits, humanity may be destroyed. People are
constantly on the lookout for tasty foods, with junk foods being the most
common source. As a consequence, our eating patterns are shifting, and we're
gravitating toward junk food more than ever, which is bad for our health and
increases our risk of acquiring health problems. Machine learning principles
are applied in every aspect of our lives, and one of them is object recognition
via image processing. However, because foods vary in nature, this procedure is
crucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a
low accuracy rate. All of these issues were defeated by the Deep Neural
Network. In this work, we created a fresh dataset of 10,000 data points from 20
junk food classifications to try to recognize junk foods. All of the data in
the data set was gathered using the Google search engine, which is thought to
be one-of-a-kind in every way. The goal was achieved using Convolution Neural
Network (CNN) technology, which is well-known for image processing. We achieved
a 98.05\% accuracy rate throughout the research, which was satisfactory. In
addition, we conducted a test based on a real-life event, and the outcome was
extraordinary. Our goal is to advance this research to the next level, so that
it may be applied to a future study. Our ultimate goal is to create a system
that would encourage people to avoid eating junk food and to be
health-conscious. \keywords{ Machine Learning \and junk food \and object
detection \and YOLOv3 \and custom food dataset.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, accepted in ICBBDB conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalization in Federated Learning by Seeking Flat Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Caldarola, Barbara Caputo, Marco Ciccone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Constrained Stochastic Convex Optimization with XOR-Projected
  Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Ding, Yijie Wang, Jianzhu Ma, Yexiang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Provably solving stochastic convex optimization problems with constraints is
essential for various problems in science, business, and statistics. Recently
proposed XOR-Stochastic Gradient Descent (XOR-SGD) provides a convergence rate
guarantee solving the constraints-free version of the problem by leveraging
XOR-Sampling. However, the task becomes more difficult when additional equality
and inequality constraints are needed to be satisfied. Here we propose XOR-PGD,
a novel algorithm based on Projected Gradient Descent (PGD) coupled with the
XOR sampler, which is guaranteed to solve the constrained stochastic convex
optimization problem still in linear convergence rate by choosing proper step
size. We show on both synthetic stochastic inventory management and real-world
road network design problems that the rate of constraints satisfaction of the
solutions optimized by XOR-PGD is $10\%$ more than the competing approaches in
a very large searching space. The improved XOR-PGD algorithm is demonstrated to
be more accurate and efficient than both XOR-SGD and SGD coupled with MCMC
based samplers. It is also shown to be more scalable with respect to the number
of samples and processor cores via experiments with large dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Landscape Analysis in Automated Algorithm Performance
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Risto Trajanov, Stefan Dimeski, Martin Popovski, Peter Korošec, Tome Eftimov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the performance of an optimization algorithm on a new problem
instance is crucial in order to select the most appropriate algorithm for
solving that problem instance. For this purpose, recent studies learn a
supervised machine learning (ML) model using a set of problem landscape
features linked to the performance achieved by the optimization algorithm.
However, these models are black-box with the only goal of achieving good
predictive performance, without providing explanations which landscape features
contribute the most to the prediction of the performance achieved by the
optimization algorithm. In this study, we investigate the expressiveness of
problem landscape features utilized by different supervised ML models in
automated algorithm performance prediction. The experimental results point out
that the selection of the supervised ML method is crucial, since different
supervised ML regression models utilize the problem landscape features
differently and there is no common pattern with regard to which landscape
features are the most informative.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in International Conference on the Applications of
  Evolutionary Computation 2022 (Part of EvoStar 2022). arXiv admin note: text
  overlap with arXiv:2110.11633</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Was that so hard? Estimating human classification difficulty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morten Rieger Hannemose, Josefine Vilsbøll Sundgaard, Niels Kvorning Ternov, Rasmus R. Paulsen, Anders Nymark Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When doctors are trained to diagnose a specific disease, they learn faster
when presented with cases in order of increasing difficulty. This creates the
need for automatically estimating how difficult it is for doctors to classify a
given case. In this paper, we introduce methods for estimating how hard it is
for a doctor to diagnose a case represented by a medical image, both when
ground truth difficulties are available for training, and when they are not.
Our methods are based on embeddings obtained with deep metric learning.
Additionally, we introduce a practical method for obtaining ground truth human
difficulty for each image case in a dataset using self-assessed certainty. We
apply our methods to two different medical datasets, achieving high Kendall
rank correlation coefficients, showing that we outperform existing methods by a
large margin on our problem and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering units in neural networks: upstream vs downstream information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard D. Lange, David S. Rolnick, Konrad P. Kording
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been hypothesized that some form of "modular" structure in artificial
neural networks should be useful for learning, compositionality, and
generalization. However, defining and quantifying modularity remains an open
problem. We cast the problem of detecting functional modules into the problem
of detecting clusters of similar-functioning units. This begs the question of
what makes two units functionally similar. For this, we consider two broad
families of methods: those that define similarity based on how units respond to
structured variations in inputs ("upstream"), and those based on how variations
in hidden unit activations affect outputs ("downstream"). We conduct an
empirical study quantifying modularity of hidden layer representations of
simple feedforward, fully connected networks, across a range of
hyperparameters. For each model, we quantify pairwise associations between
hidden units in each layer using a variety of both upstream and downstream
measures, then cluster them by maximizing their "modularity score" using
established tools from network science. We find two surprising results: first,
dropout dramatically increased modularity, while other forms of weight
regularization had more modest effects. Second, although we observe that there
is usually good agreement about clusters within both upstream methods and
downstream methods, there is little agreement about the cluster assignments
across these two families of methods. This has important implications for
representation-learning, as it suggests that finding modular representations
that reflect structure in inputs (e.g. disentanglement) may be a distinct goal
from learning modular representations that reflect structure in outputs (e.g.
compositionality).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 main text pages, 4 main figures, 5 supplemental figures. Will be
  submitted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural System Level Synthesis: Learning over All Stabilizing Policies
  for Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Furieri, Clara Lucía Galimberti, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of designing stabilizing control policies for
nonlinear systems in discrete-time, while minimizing an arbitrary cost
function. When the system is linear and the cost is convex, the System Level
Synthesis (SLS) approach offers an exact solution based on convex programming.
Beyond this case, a globally optimal solution cannot be found in a tractable
way, in general. In this paper, we develop a parametrization of all and only
the control policies stabilizing a given time-varying nonlinear system in terms
of the combined effect of 1) a strongly stabilizing base controller and 2) a
stable SLS operator to be freely designed. Based on this result, we propose a
Neural SLS (Neur-SLS) approach guaranteeing closed-loop stability during and
after parameter optimization, without requiring any constraints to be
satisfied. We exploit recent Deep Neural Network (DNN) models based on
Recurrent Equilibrium Networks (RENs) to learn over a rich class of nonlinear
stable operators, and demonstrate the effectiveness of the proposed approach in
numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Robust Classification using Contractive Hamiltonian Neural ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Zakwan, Liang Xu, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks can be fragile and sensitive to small input
perturbations that might cause a significant change in the output. In this
paper, we employ contraction theory to improve the robustness of neural ODEs
(NODEs). A dynamical system is contractive if all solutions with different
initial conditions converge to each other asymptotically. As a consequence,
perturbations in initial conditions become less and less relevant over time.
Since in NODEs, the input data corresponds to the initial condition of
dynamical systems, we show contractivity can mitigate the effect of input
perturbations. More precisely, inspired by NODEs with Hamiltonian dynamics, we
propose a class of contractive Hamiltonian NODEs (CH-NODEs). By properly tuning
a scalar parameter, CH-NODEs ensure contractivity by design and can be trained
using standard backpropagation and gradient descent algorithms. Moreover,
CH-NODEs enjoy built-in guarantees of non-exploding gradients, which ensures a
well-posed training process. Finally, we demonstrate the robustness of CH-NODEs
on the MNIST image classification problem with noisy test datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Perspective on Neural Capacity Estimation: Viability and Reliability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Mirkarimi, Stefano Rini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, several methods have been proposed for estimating the mutual
information from sample data using deep neural networks and without the
knowledge of closed-form distribution of the data. This class of estimators is
referred to as neural mutual information estimators (NMIE). In this paper, we
investigate the performance of different NMIE proposed in the literature when
applied to the capacity estimation problem. In particular, we study the
performance of mutual information neural estimator (MINE), smoothed mutual
information lower-bound estimator (SMILE), and directed information neural
estimator (DINE). For the NMIE above, capacity estimation relies on two deep
neural networks (DNN): (i) one DNN generates samples from a distribution that
is learned, and (ii) a DNN to estimate the MI between the channel input and the
channel output. We benchmark these NMIE in three scenarios: (i) AWGN channel
capacity estimation and (ii) channels with unknown capacity and continuous
inputs i.e., optical intensity and peak-power constrained AWGN channel (iii)
channels with unknown capacity and a discrete number of mass points i.e.,
Poisson channel. Additionally, we also (iv) consider the extension to the MAC
capacity problem by considering the AWGN and optical MAC models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 8 figures, submitted for possible journal publication.
  arXiv admin note: text overlap with arXiv:2111.07401</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Program Semantics with Code Representations: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Kai Siow, Shangqing Liu, Xiaofei Xie, Guozhu Meng, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program semantics learning is the core and fundamental for various code
intelligent tasks e.g., vulnerability detection, clone detection. A
considerable amount of existing works propose diverse approaches to learn the
program semantics for different tasks and these works have achieved
state-of-the-art performance. However, currently, a comprehensive and
systematic study on evaluating different program representation techniques
across diverse tasks is still missed.
  From this starting point, in this paper, we conduct an empirical study to
evaluate different program representation techniques. Specifically, we
categorize current mainstream code representation techniques into four
categories i.e., Feature-based, Sequence-based, Tree-based, and Graph-based
program representation technique and evaluate its performance on three diverse
and popular code intelligent tasks i.e., {Code Classification}, Vulnerability
Detection, and Clone Detection on the public released benchmark. We further
design three {research questions (RQs)} and conduct a comprehensive analysis to
investigate the performance. By the extensive experimental results, we conclude
that (1) The graph-based representation is superior to the other selected
techniques across these tasks. (2) Compared with the node type information used
in tree-based and graph-based representations, the node textual information is
more critical to learning the program semantics. (3) Different tasks require
the task-specific semantics to achieve their highest performance, however
combining various program semantics from different dimensions such as control
dependency, data dependency can still produce promising results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 29th edition IEEE International Conference on Software
  Analysis, Evolution and Reengineering (SANER 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of speaker age and height from speech signal using bi-encoder
  <span class="highlight-title">transformer</span> mixture model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Gupta, Duc-Tuan Truong, Tran The Anh, Chng Eng Siong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of speaker characteristics such as age and height is a
challenging task, having numerous applications in voice forensic analysis. In
this work, we propose a bi-encoder transformer mixture model for speaker age
and height estimation. Considering the wide differences in male and female
voice characteristics such as differences in formant and fundamental
frequencies, we propose the use of two separate transformer encoders for the
extraction of specific voice features in the male and female gender, using
wav2vec 2.0 as a common-level feature extractor. This architecture reduces the
interference effects during backpropagation and improves the generalizability
of the model. We perform our experiments on the TIMIT dataset and significantly
outperform the current state-of-the-art results on age estimation.
Specifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49
years for male and female age estimation, respectively. Further experiment to
evaluate the relative importance of different phonetic types for our task
demonstrate that vowel sounds are the most distinguishing for age estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear convergence of a policy gradient method for finite horizon
  continuous time stochastic control problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Reisinger, Wolfgang Stockinger, Yufei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its popularity in the reinforcement learning community, a provably
convergent policy gradient method for general continuous space-time stochastic
control problems has been elusive. This paper closes the gap by proposing a
proximal gradient algorithm for feedback controls of finite-time horizon
stochastic control problems. The state dynamics are continuous time nonlinear
diffusions with controlled drift and possibly degenerate noise, and the
objectives are nonconvex in the state and nonsmooth in the control. We prove
under suitable conditions that the algorithm converges linearly to a stationary
point of the control problem, and is stable with respect to policy updates by
approximate gradient steps. The convergence result justifies the recent
reinforcement learning heuristics that adding entropy regularization to the
optimization objective accelerates the convergence of policy gradient methods.
The proof exploits careful regularity estimates of backward stochastic
differential equations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling
  and Correction <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, Cheng-Zhong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) allows multiple clients to collectively train a
high-performance global model without sharing their private data. However, the
key challenge in federated learning is that the clients have significant
statistical heterogeneity among their local data distributions, which would
cause inconsistent optimized local models on the client-side. To address this
fundamental dilemma, we propose a novel federated learning algorithm with local
drift decoupling and correction (FedDC). Our FedDC only introduces lightweight
modifications in the local training phase, in which each client utilizes an
auxiliary local drift variable to track the gap between the local model
parameter and the global model parameters. The key idea of FedDC is to utilize
this learned local drift variable to bridge the gap, i.e., conducting
consistency in parameter-level. The experiment results and analysis demonstrate
that FedDC yields expediting convergence and better performance on various
image classification tasks, robust in partial participation settings, non-iid
data, and heterogeneous clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, to be published in CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Neural Network to Restore Low-Dose Digital Breast
  Tomosynthesis Projections in a Variance Stabilization Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo de Barros Vimieiro, Chuang Niu, Hongming Shan, Lucas Rodrigues Borges, Ge Wang, Marcelo Andrade da Costa Vieira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible
radiation dose while maintaining sufficiently good image quality for accurate
medical diagnosis. In this work, we propose a convolution neural network (CNN)
to restore low-dose (LD) DBT projections to achieve an image quality equivalent
to a standard full-dose (FD) acquisition. The proposed network architecture
benefits from priors in terms of layers that were inspired by traditional
model-based (MB) restoration methods, considering a model-based deep learning
approach, where the network is trained to operate in the variance stabilization
transformation (VST) domain. To accurately control the network operation point,
in terms of noise and blur of the restored image, we propose a loss function
that minimizes the bias and matches residual noise between the input and the
output. The training dataset was composed of clinical data acquired at the
standard FD and low-dose pairs obtained by the injection of quantum noise. The
network was tested using real DBT projections acquired with a physical
anthropomorphic breast phantom. The proposed network achieved superior results
in terms of the mean normalized squared error (MNSE), training time and noise
spatial correlation compared with networks trained with traditional data-driven
methods. The proposed approach can be extended for other medical imaging
application that requires LD acquisitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bayesian Approach for Shaft Centre Localisation in Journal Bearings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher A. Lindley, Scott Beamish, Rob Dwyer-Joyce, Nikolaos Dervilis, Keith Worden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been shown that ultrasonic techniques work well for online measuring
of circumferential oil film thickness profile in journal bearings;
unfortunately, they can be limited by their measuring range and unable to
capture details of the film all around the bearing circumference. Attempts to
model the film thickness over the full range of the bearing rely on
deterministic approaches, which assume the observations to be true with
absolute certainty. Unaccounted uncertainties of the film thickness may lead to
a cascade of inaccurate predictions for subsequent calculations of hydrodynamic
parameters. In the present work, a probabilistic framework is proposed to model
the film thickness with Gaussian Processes. The results are then used to
estimate the location of the bearing shaft under various operational
conditions. A further step in the process involves using the newly-constructed
dataset to generate likelihood maps displaying the probable location of the
shaft centre, given the bearing rotational speed and applied static load. The
results offer the possibility to visualise the confidence of the predictions
and allow the true location to be found within an area of high probability
within the bearing's bore.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">BERT</span>-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning
  in Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Murtadha, Shengfeng Pan, Bo Wen, Jianlin Su, Wenze Zhang, Yunfeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) task aims to associate a piece of text
with a set of aspects and meanwhile infer their respective sentimental
polarities. Up to now, the state-of-the-art approaches are built upon
fine-tuning of various pre-trained language models. They commonly aim to learn
the aspect-specific representation in the corpus. Unfortunately, the aspect is
often expressed implicitly through a set of representatives and thus renders
implicit mapping process unattainable unless sufficient labeled examples.
  In this paper, we propose to jointly address aspect categorization and
aspect-based sentiment subtasks in a unified framework. Specifically, we first
introduce a simple but effective mechanism that collaborates the semantic and
syntactic information to construct auxiliary-sentences for the implicit aspect.
Then, we encourage BERT to learn the aspect-specific representation in response
to the automatically constructed auxiliary-sentence instead of the aspect
itself. Finally, we empirically evaluate the performance of the proposed
solution by a comparative study on real benchmark datasets for both ABSA and
Targeted-ABSA tasks. Our extensive experiments show that it consistently
achieves state-of-the-art performance in terms of aspect categorization and
aspect-based sentiment across all datasets and the improvement margins are
considerable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Linear Feature Disentanglement For Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian He, Zhibin Li, Yongshun Gong, Yazhou Yao, Xiushan Nie, Yilong Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-linear activation functions, e.g., Sigmoid, ReLU, and Tanh, have achieved
great success in neural networks (NNs). Due to the complex non-linear
characteristic of samples, the objective of those activation functions is to
project samples from their original feature space to a linear separable feature
space. This phenomenon ignites our interest in exploring whether all features
need to be transformed by all non-linear functions in current typical NNs,
i.e., whether there exists a part of features arriving at the linear separable
feature space in the intermediate layers, that does not require further
non-linear variation but an affine transformation instead. To validate the
above hypothesis, we explore the problem of linear feature disentanglement for
neural networks in this paper. Specifically, we devise a learnable mask module
to distinguish between linear and non-linear features. Through our designed
experiments we found that some features reach the linearly separable space
earlier than the others and can be detached partly from the NNs. The explored
method also provides a readily feasible pruning strategy which barely affects
the performance of the original model. We conduct our experiments on four
datasets and present promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Twin Weisfeiler-Lehman: High Expressive GNNs for Graph Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Wang, Qi Cao, Huawei Shen, Bingbing Xu, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expressive power of message passing GNNs is upper-bounded by
Weisfeiler-Lehman (WL) test. To achieve high expressive GNNs beyond WL test, we
propose a novel graph isomorphism test method, namely Twin-WL, which
simultaneously passes node labels and node identities rather than only passes
node label as WL. The identity-passing mechanism encodes complete structure
information of rooted subgraph, and thus Twin-WL can offer extra power beyond
WL at distinguishing graph structures. Based on Twin-WL, we implement two
Twin-GNNs for graph classification via defining readout function over rooted
subgraph: one simply readouts the size of rooted subgraph and the other
readouts rich structure information of subgraph following a GNN-style. We prove
that the two Twin-GNNs both have higher expressive power than traditional
message passing GNNs. Experiments also demonstrate the Twin-GNNs significantly
outperform state-of-the-art methods at the task of graph classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Meta-learning for Low-resource Text Classification and
  Generation via Memory Imitation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng, Dongkyu Lee, Yiping Song, Jian Sun, Nevin L. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building models of natural language processing (NLP) is challenging in
low-resource scenarios where only limited data are available.
Optimization-based meta-learning algorithms achieve promising results in
low-resource scenarios by adapting a well-generalized model initialization to
handle new tasks. Nonetheless, these approaches suffer from the memorization
overfitting issue, where the model tends to memorize the meta-training tasks
while ignoring support sets when adapting to new tasks. To address this issue,
we propose a memory imitation meta-learning (MemIML) method that enhances the
model's reliance on support sets for task adaptation. Specifically, we
introduce a task-specific memory module to store support set information and
construct an imitation module to force query sets to imitate the behaviors of
some representative support-set samples stored in the memory. A theoretical
analysis is provided to prove the effectiveness of our method, and empirical
results also demonstrate that our method outperforms competitive baselines on
both text classification and generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are You Misinformed? A Study of Covid-Related Fake News in Bengali on
  Facebook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Protik Bose Pranto, Syed Zami-Ul-Haque Navid, Protik Dey, Gias Uddin, Anindya Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our opinions and views of life can be shaped by how we perceive the opinions
of others on social media like Facebook. This dependence has increased during
COVID-19 periods when we have fewer means to connect with others. However, fake
news related to COVID-19 has become a significant problem on Facebook. Bengali
is the seventh most spoken language worldwide, yet we are aware of no previous
research that studied the prevalence of COVID-19 related fake news in Bengali
on Facebook. In this paper, we develop machine learning models to detect fake
news in Bengali automatically. The best performing model is BERT, with an
F1-score of 0.97. We apply BERT on all Facebook Bengali posts related to
COVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into
three categories: System (e.g., medical system), belief (e.g., religious
rituals), and social (e.g., scientific awareness).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Vanilla Policy Gradient Overlooked? Analyzing Deep Reinforcement
  Learning for Hanabi <span class="chip">AAMAS
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bram Grooten, Jelle Wemmenhove, Maurice Poot, Jim Portegies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In pursuit of enhanced multi-agent collaboration, we analyze several
on-policy deep reinforcement learning algorithms in the recently published
Hanabi benchmark. Our research suggests a perhaps counter-intuitive finding,
where Proximal Policy Optimization (PPO) is outperformed by Vanilla Policy
Gradient over multiple random seeds in a simplified environment of the
multi-agent cooperative card game. In our analysis of this behavior we look
into Hanabi-specific metrics and hypothesize a reason for PPO's plateau. In
addition, we provide proofs for the maximum length of a perfect game (71 turns)
and any game (89 turns). Our code can be found at:
https://github.com/bramgrooten/DeepRL-for-Hanabi
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ALA 2022 (Adaptive and Learning Agents Workshop at AAMAS
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Evaluation of Machine Learning-based Algorithm and Taguchi
  Algorithm for the Determination of the Hardness Value of the Friction Stir
  Welded AA 6262 Joints at a Nugget Zone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshansh Mishra, Eyob Messele Sefene, Gopikrishna Nidigonda, Assefa Asmare Tsegaw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, industry 4.0 plays a tremendous role in the manufacturing
industries for increasing the amount of data and accuracy in modern
manufacturing systems. Thanks to artificial intelligence, particularly machine
learning, big data analytics have dramatically amended, and manufacturers
easily exploit organized and unorganized data. This study utilized hybrid
optimization algorithms to find friction stir welding and optimal hardness
value at the nugget zone. A similar AA 6262 material was used and welded in a
butt joint configuration. Tool rotational speed (RPM), tool traverse speed
(mm/min), and the plane depth (mm) are used as controllable parameters and
optimized using Taguchi L9, Random Forest, and XG Boost machine learning tools.
Analysis of variance was also conducted at a 95% confidence interval for
identifying the significant parameters. The result indicated that the
coefficient of determination from Taguchi L9 orthogonal array is 0.91 obtained
while Random Forest and XG Boost algorithm imparted 0.62 and 0.65,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Source Domain Adaptation Based on Federated Knowledge Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Sun, Ng Chong, Ochiai Hideya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) facilitates distributed model learning to protect
users' privacy. In the absence of labels for a new user's data, the knowledge
transfer in FL allows a learned global model to adapt to the new samples
quickly. The multi-source domain adaptation in FL aims to improve the model's
generality in a target domain by learning domain-invariant features from
different clients. In this paper, we propose Federated Knowledge Alignment
(FedKA) that aligns features from different clients and those of the target
task. We identify two types of negative transfer arising in multi-source domain
adaptation of FL and demonstrate how FedKA can alleviate such negative
transfers with the help of a global features disentangler enhanced by embedding
matching. To further facilitate representation learning of the target task, we
devise a federated voting mechanism to provide labels for samples from the
target domain via a consensus from querying local models and fine-tune the
global model with these labeled samples. Extensive experiments, including an
ablation study, on an image classification task of Digit-Five and a text
sentiment classification task of Amazon Review, show that FedKA could be
augmented to existing FL algorithms to improve the generality of the learned
model for tackling a new task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Targeted Model Poisoning Attack on Federated Learning via Backward
  Error Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Sun, Hideya Ochiai, Jun Sakuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model poisoning attacks on federated learning (FL) intrude in the entire
system via compromising an edge model, resulting in malfunctioning of machine
learning models. Such compromised models are tampered with to perform
adversary-desired behaviors. In particular, we considered a semi-targeted
situation where the source class is predetermined however the target class is
not. The goal is to cause the global classifier to misclassify data of the
source class. Though approaches such as label flipping have been adopted to
inject poisoned parameters into FL, it has been shown that their performances
are usually class-sensitive varying with different target classes applied.
Typically, an attack can become less effective when shifting to a different
target class. To overcome this challenge, we propose the Attacking
Distance-aware Attack (ADA) to enhance a poisoning attack by finding the
optimized target class in the feature space. Moreover, we studied a more
challenging situation where an adversary had limited prior knowledge about a
client's data. To tackle this problem, ADA deduces pair-wise distances between
different classes in the latent feature space from shared model parameters
based on the backward error analysis. We performed extensive empirical
evaluations on ADA by varying the factor of attacking frequency in three
different image classification tasks. As a result, ADA succeeded in increasing
the attack performance by 1.8 times in the most challenging case with an
attacking frequency of 0.01.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Neural Network Equivalence Checking using SMT Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charis Eleftheriadis, Nikolaos Kekatos, Panagiotis Katsaros, Stavros Tripakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two pretrained neural networks are deemed equivalent if they yield similar
outputs for the same inputs. Equivalence checking of neural networks is of
great importance, due to its utility in replacing learning-enabled components
with equivalent ones, when there is need to fulfill additional requirements or
to address security threats, as is the case for example when using knowledge
distillation, adversarial training etc. SMT solvers can potentially provide
solutions to the problem of neural network equivalence checking that will be
sound and complete, but as it is expected any such solution is associated with
significant limitations with respect to the size of neural networks to be
checked. This work presents a first SMT-based encoding of the equivalence
checking problem, explores its utility and limitations and proposes avenues for
future research and improvements towards more scalable and practically
applicable solutions. We present experimental results that shed light to the
aforementioned issues, for diverse types of neural network models (classifiers
and regression networks) and equivalence criteria, towards a general and
application-independent equivalence checking approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosis of Schizophrenia: A comprehensive evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Tanveer, Jatin Jangir, M. A. Ganaie, Iman Beheshti, M. Tabish, Nikunj Chhabra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have been successfully employed in the diagnosis of
Schizophrenia disease. The impact of classification models and the feature
selection techniques on the diagnosis of Schizophrenia have not been evaluated.
Here, we sought to access the performance of classification models along with
different feature selection approaches on the structural magnetic resonance
imaging data. The data consist of 72 subjects with Schizophrenia and 74 healthy
control subjects. We evaluated different classification algorithms based on
support vector machine (SVM), random forest, kernel ridge regression and
randomized neural networks. Moreover, we evaluated T-Test, Receiver Operator
Characteristics (ROC), Wilcoxon, entropy, Bhattacharyya, Minimum Redundancy
Maximum Relevance (MRMR) and Neighbourhood Component Analysis (NCA) as the
feature selection techniques. Based on the evaluation, SVM based models with
Gaussian kernel proved better compared to other classification models and
Wilcoxon feature selection emerged as the best feature selection approach.
Moreover, in terms of data modality the performance on integration of the grey
matter and white matter proved better compared to the performance on the grey
and white matter individually. Our evaluation showed that classification
algorithms along with the feature selection approaches impact the diagnosis of
Schizophrenia disease. This indicates that proper selection of the features and
the classification models can improve the diagnosis of Schizophrenia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Knowledge Aids in Signal Disaggregation; the Example of the
  Cumulative Water Heater 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Belikov, Guillaume Matheron, Johan Sassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we present an unsupervised low-frequency method aimed at
detecting and disaggregating the power used by Cumulative Water Heaters (CWH)
in residential homes. Our model circumvents the inherent difficulty of
unsupervised signal disaggregation by using both the shape of a power spike and
its time of occurrence to identify the contribution of CWH reliably. Indeed,
many CHWs in France are configured to turn on automatically during off-peak
hours only, and we are able to use this domain knowledge to aid peak
identification despite the low sampling frequency. In order to test our model,
we equipped a home with sensors to record the ground-truth consumption of a
water heater. We then apply the model to a larger dataset of energy consumption
of Hello Watt users consisting of one month of consumption data for 5k homes at
30-minute resolution. In this dataset we successfully identified CWHs in the
majority of cases where consumers declared using them. The remaining part is
likely due to possible misconfiguration of CWHs, since triggering them during
off-peak hours requires specific wiring in the electrical panel of the house.
Our model, despite its simplicity, offers promising applications: detection of
mis-configured CWHs on off-peak contracts and slow performance degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utterance Rewriting with <span class="highlight-title">Contrastive Learning</span> in Multi-turn <span class="highlight-title">Dialogue</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Wang, Tangjian Duan, Zihao Wang, Minghui Yang, Zujie Wen, Yongliang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context modeling plays a significant role in building multi-turn dialogue
systems. In order to make full use of context information, systems can use
Incomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue
into single-turn by merging current utterance and context information into a
self-contained utterance. However, previous approaches ignore the intent
consistency between the original query and rewritten query. The detection of
omitted or coreferred locations in the original query can be further improved.
In this paper, we introduce contrastive learning and multi-task learning to
jointly model the problem. Our method benefits from carefully designed
self-supervised objectives, which act as auxiliary tasks to capture semantics
at both sentence-level and token-level. The experiments show that our proposed
model achieves state-of-the-art performance on several public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Stochastic Factored Gradient Descent for Distributed Quantum State
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyung Lyle Kim, Mohammad Taha Toghani, César A. Uribe, Anastasios Kyrillidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a distributed Quantum State Tomography (QST) protocol, named Local
Stochastic Factored Gradient Descent (Local SFGD), to learn the low-rank factor
of a density matrix over a set of local machines. QST is the canonical
procedure to characterize the state of a quantum system, which we formulate as
a stochastic nonconvex smooth optimization problem. Physically, the estimation
of a low-rank density matrix helps characterizing the amount of noise
introduced by quantum computation. Theoretically, we prove the local
convergence of Local SFGD for a general class of restricted strongly
convex/smooth loss functions, i.e., Local SFGD converges locally to a small
neighborhood of the global optimum at a linear rate with a constant step size,
while it locally converges exactly at a sub-linear rate with diminishing step
sizes. With a proper initialization, local convergence results imply global
convergence. We validate our theoretical findings with numerical simulations of
QST on the Greenberger-Horne-Zeilinger (GHZ) state.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Multi-view Clustering via Ensembles: Towards Scalability,
  Superiority, and Simplicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Chang-Dong Wang, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress, there remain three limitations to the previous
multi-view clustering algorithms. First, they often suffer from high
computational complexity, restricting their feasibility for large-scale
datasets. Second, they typically fuse multi-view information via one-stage
fusion, neglecting the possibilities in multi-stage fusions. Third,
dataset-specific hyperparameter-tuning is frequently required, further
undermining their practicability. In light of this, we propose a fast
multi-view clustering via ensembles (FastMICE) approach. Particularly, the
concept of random view groups is presented to capture the versatile view-wise
relationships, through which the hybrid early-late fusion strategy is designed
to enable efficient multi-stage fusions. With multiple views extended to many
view groups, three levels of diversity (w.r.t. features, anchors, and
neighbors, respectively) are jointly leveraged for constructing the
view-sharing bipartite graphs in the early-stage fusion. Then, a set of
diversified base clusterings for different view groups are obtained via fast
graph partitioning, which are further formulated into a unified bipartite graph
for final clustering in the late-stage fusion. Remarkably, FastMICE has almost
linear time and space complexity, and is free of dataset-specific tuning.
Experiments on twenty multi-view datasets demonstrate its advantages in
scalability (for extremely large datasets), superiority (in clustering
performance), and simplicity (to be applied) over the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Generative Data Augmentation for Clinical Audio <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Seibold, Armando Hoch, Mazda Farshad, Nassir Navab, Philipp Fürnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel data augmentation method for clinical audio
datasets based on a conditional Wasserstein Generative Adversarial Network with
Gradient Penalty (cWGAN-GP), operating on log-mel spectrograms. To validate our
method, we created a clinical audio dataset which was recorded in a real-world
operating room during Total Hip Arthroplasty (THA) procedures and contains
typical sounds which resemble the different phases of the intervention. We
demonstrate the capability of the proposed method to generate realistic
class-conditioned samples from the dataset distribution and show that training
with the generated augmented samples outperforms classical audio augmentation
methods in terms of classification accuracy. The performance was evaluated
using a ResNet-18 classifier which shows a mean per-class accuracy improvement
of 1.51% in a 5-fold cross validation experiment using the proposed
augmentation method. Because clinical data is often expensive to acquire, the
development of realistic and high-quality data augmentation methods is crucial
to improve the robustness and generalization capabilities of learning-based
algorithms which is especially important for safety-critical medical
applications. Therefore, the proposed data augmentation method is an important
step towards improving the data bottleneck for clinical audio-based machine
learning systems. The code and dataset will be published upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling continual learning in humans with Hebbian context gating and
  exponentially decaying task signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Flesch, David G. Nagy, Andrew Saxe, Christopher Summerfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can learn several tasks in succession with minimal mutual interference
but perform more poorly when trained on multiple tasks at once. The opposite is
true for standard deep neural networks. Here, we propose novel computational
constraints for artificial neural networks, inspired by earlier work on gating
in the primate prefrontal cortex, that capture the cost of interleaved training
and allow the network to learn two tasks in sequence without forgetting. We
augment standard stochastic gradient descent with two algorithmic motifs,
so-called "sluggish" task units and a Hebbian training step that strengthens
connections between task units and hidden units that encode task-relevant
information. We found that the "sluggish" units introduce a switch-cost during
training, which biases representations under interleaved training towards a
joint representation that ignores the contextual cue, while the Hebbian step
promotes the formation of a gating scheme from task units to the hidden layer
that produces orthogonal representations which are perfectly guarded against
interference. Validating the model on previously published human behavioural
data revealed that it matches performance of participants who had been trained
on blocked or interleaved curricula, and that these performance differences
were driven by misestimation of the true category boundary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VQ-Flows: Vector Quantized Local Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Sidheekh, Chris B. Dock, Tushar Jain, Radu Balan, Maneesh K. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flows provide an elegant approach to generative modeling that
allows for efficient sampling and exact density evaluation of unknown data
distributions. However, current techniques have significant limitations in
their expressivity when the data distribution is supported on a low-dimensional
manifold or has a non-trivial topology. We introduce a novel statistical
framework for learning a mixture of local normalizing flows as "chart maps"
over the data manifold. Our framework augments the expressivity of recent
approaches while preserving the signature property of normalizing flows, that
they admit exact density evaluation. We learn a suitable atlas of charts for
the data manifold via a vector quantized auto-encoder (VQ-AE) and the
distributions over them using a conditional flow. We validate experimentally
that our probabilistic framework enables existing approaches to better model
data distributions over complex manifolds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient flows and randomised thresholding: sparse inversion and
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Latz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse inversion and classification problems are ubiquitous in modern data
science and imaging. They are often formulated as non-smooth minimisation
problems. In sparse inversion, we minimise, e.g., the sum of a data fidelity
term and an L1/LASSO regulariser. In classification, we consider, e.g., the sum
of a data fidelity term and a non-smooth Ginzburg--Landau energy. Standard
(sub)gradient descent methods have shown to be inefficient when approaching
such problems. Splitting techniques are much more useful: here, the target
function is partitioned into a sum of two subtarget functions -- each of which
can be efficiently optimised. Splitting proceeds by performing optimisation
steps alternately with respect to each of the two subtarget functions.
  In this work, we study splitting from a stochastic continuous-time
perspective. Indeed, we define a differential inclusion that follows one of the
two subtarget function's negative subgradient at each point in time. The choice
of the subtarget function is controlled by a binary continuous-time Markov
process. The resulting dynamical system is a stochastic approximation of the
underlying subgradient flow. We investigate this stochastic approximation for
an L1-regularised sparse inversion flow and for a discrete Allen-Cahn equation
minimising a Ginzburg--Landau energy. In both cases, we study the longtime
behaviour of the stochastic dynamical system and its ability to approximate the
underlying subgradient flow at any accuracy. We illustrate our theoretical
findings in a simple sparse estimation problem and also in a low-dimensional
classification problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factual Consistency of Multilingual <span class="highlight-title">Pretrain</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models can be queried for factual knowledge, with
potential applications in knowledge base acquisition and tasks that require
inference. However, for that, we need to know how reliable this knowledge is,
and recent work has shown that monolingual English language models lack
consistency when predicting factual knowledge, that is, they fill-in-the-blank
differently for paraphrases describing the same fact. In this paper, we extend
the analysis of consistency to a multilingual setting. We introduce a resource,
mParaRel, and investigate (i) whether multilingual language models such as
mBERT and XLM-R are more consistent than their monolingual counterparts; and
(ii) if such models are equally consistent across languages. We find that mBERT
is as inconsistent as English BERT in English paraphrases, but that both mBERT
and XLM-R exhibit a high degree of inconsistency in English and even more so
for all the other 45 languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mask Usage Recognition using Vision <span class="highlight-title">Transformer</span> with Transfer Learning
  and Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hensel Donato Jahja, Novanto Yudistira,  Sutrisno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has disrupted various levels of society. The use of
masks is essential in preventing the spread of COVID-19 by identifying an image
of a person using a mask. Although only 23.1% of people use masks correctly,
Artificial Neural Networks (ANN) can help classify the use of good masks to
help slow the spread of the Covid-19 virus. However, it requires a large
dataset to train an ANN that can classify the use of masks correctly.
MaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4
class labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth.
Mask classification training utilizes Vision Transformers (ViT) architecture
with transfer learning method using pre-trained weights on ImageNet-21k, with
random augmentation. In addition, the hyper-parameters of training of 20
epochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of
0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation
function, and a Cross-Entropy loss function are used to be applied on the
training of three architectures of ViT, namely Base-16, Large-16, and Huge-14.
Furthermore, comparisons of with and without augmentation and transfer learning
are conducted. This study found that the best classification is transfer
learning and augmentation using ViT Huge-14. Using this method on
MaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training
data, 0.9412 on validation data, and 0.9534 on test data. This research shows
that training the ViT model with data augmentation and transfer learning
improves classification of the mask usage, even better than convolutional-based
Residual Network (ResNet).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale-out Systolic Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Caner Yüzügüler, Canberk Sönmez, Mario Drumond, Yunho Oh, Babak Falsafi, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-pod systolic arrays are emerging as the architecture of choice in DNN
inference accelerators. Despite their potential, designing multi-pod systolic
arrays to maximize effective throughput/Watt (i.e., throughput/Watt adjusted
when accounting for array utilization) poses a unique set of challenges. In
this work, we study three key pillars in multi-pod systolic array designs,
namely array granularity, interconnect, and tiling. We identify optimal array
granularity across workloads and show that state-of-the-art commercial
accelerators use suboptimal array sizes for single-tenancy workloads. We, then
evaluate the bandwidth/latency trade-offs in interconnects and show that
Butterfly networks offer a scalable topology for accelerators with a large
number of pods. Finally, we introduce a novel data tiling scheme with custom
partition size to maximize utilization in optimally sized pods. We propose
Scale-out Systolic Arrays, a multi-pod inference accelerator for both single-
and multi-tenancy based on these three pillars. We show that SOSA exhibits
scaling of up to 600 TeraOps/s in effective throughput for state-of-the-art DNN
inference workloads, and outperforms state-of-the-art multi-pod accelerators by
a factor of 1.5x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-distribution Generalization with Causal Invariant Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Wang, Mingyang Yi, Zhitang Chen, Shengyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, it is important and desirable to learn a model
that performs well on out-of-distribution (OOD) data. Recently, causality has
become a powerful tool to tackle the OOD generalization problem, with the idea
resting on the causal mechanism that is invariant across domains of interest.
To leverage the generally unknown causal mechanism, existing works assume a
linear form of causal feature or require sufficiently many and diverse training
domains, which are usually restrictive in practice. In this work, we obviate
these assumptions and tackle the OOD problem without explicitly recovering the
causal feature. Our approach is based on transformations that modify the
non-causal feature but leave the causal part unchanged, which can be either
obtained from prior knowledge or learned from the training data in the
multi-domain scenario. Under the setting of invariant causal mechanism, we
theoretically show that if all such transformations are available, then we can
learn a minimax optimal model across the domains using only single domain data.
Noticing that knowing a complete set of these causal invariant transformations
may be impractical, we further show that it suffices to know only a subset of
these transformations. Based on the theoretical findings, a regularized
training procedure is proposed to improve the OOD generalization capability.
Extensive experimental results on both synthetic and real datasets verify the
effectiveness of the proposed algorithm, even with only a few causal invariant
transformations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action Candidate Driven Clipped Double Q-learning for Discrete and
  Continuous Action Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Jiang, Jin Xie, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Double Q-learning is a popular reinforcement learning algorithm in Markov
decision process (MDP) problems. Clipped Double Q-learning, as an effective
variant of Double Q-learning, employs the clipped double estimator to
approximate the maximum expected action value. Due to the underestimation bias
of the clipped double estimator, the performance of clipped Double Q-learning
may be degraded in some stochastic environments. In this paper, in order to
reduce the underestimation bias, we propose an action candidate-based clipped
double estimator for Double Q-learning. Specifically, we first select a set of
elite action candidates with high action values from one set of estimators.
Then, among these candidates, we choose the highest valued action from the
other set of estimators. Finally, we use the maximum value in the second set of
estimators to clip the action value of the chosen action in the first set of
estimators and the clipped value is used for approximating the maximum expected
action value. Theoretically, the underestimation bias in our clipped Double
Q-learning decays monotonically as the number of action candidates decreases.
Moreover, the number of action candidates controls the trade-off between the
overestimation and underestimation biases. In addition, we also extend our
clipped Double Q-learning to continuous action tasks via approximating the
elite continuous action candidates. We empirically verify that our algorithm
can more accurately estimate the maximum expected action value on some toy
environments and yield good performance on several benchmark problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2105.00704</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual-Guided Non-Intrusive Speech Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Ye, Jiahao Chen, Diqun Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an approach to improve Non-Intrusive speech quality
assessment(NI-SQA) based on the residuals between impaired speech and enhanced
speech. The difficulty in our task is particularly lack of information, for
which the corresponding reference speech is absent. We generate an enhanced
speech on the impaired speech to compensate for the absence of the reference
audio, then pair the information of residuals with the impaired speech.
Compared to feeding the impaired speech directly into the model, residuals
could bring some extra helpful information from the contrast in enhancement.
The human ear is sensitive to certain noises but different to deep learning
model. Causing the Mean Opinion Score(MOS) the model predicted is not enough to
fit our subjective sensitive well and causes deviation. These residuals have a
close relationship to reference speech and then improve the ability of the deep
learning models to predict MOS. During the training phase, experimental results
demonstrate that paired with residuals can quickly obtain better evaluation
indicators under the same conditions. Furthermore, our final results improved
31.3 percent and 14.1 percent, respectively, in PLCC and RMSE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring High-Order Structure for Robust Graph Structure Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangqian Yang, Yibing Zhan, Jinlong Li, Baosheng Yu, Liu Liu, Fengxiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that Graph Neural Networks (GNNs) are vulnerable to
adversarial attack, i.e., an imperceptible structure perturbation can fool GNNs
to make wrong predictions. Some researches explore specific properties of clean
graphs such as the feature smoothness to defense the attack, but the analysis
of it has not been well-studied. In this paper, we analyze the adversarial
attack on graphs from the perspective of feature smoothness which further
contributes to an efficient new adversarial defensive algorithm for GNNs. We
discover that the effect of the high-order graph structure is a smoother filter
for processing graph structures. Intuitively, the high-order graph structure
denotes the path number between nodes, where larger number indicates closer
connection, so it naturally contributes to defense the adversarial
perturbation. Further, we propose a novel algorithm that incorporates the
high-order structural information into the graph structure learning. We perform
experiments on three popular benchmark datasets, Cora, Citeseer and Polblogs.
Extensive experiments demonstrate the effectiveness of our method for defending
against graph adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Recommender Systems Forget: Learning and Unlearning for Erasable
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyuan Li, Xiaolin Zheng, Chaochao Chen, Junlin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy laws and regulations enforce data-driven systems, e.g., recommender
systems, to erase the data that concern individuals. As machine learning models
potentially memorize the training data, data erasure should also unlearn the
data lineage in models, which raises increasing interest in the problem of
Machine Unlearning (MU). However, existing MU methods cannot be directly
applied into recommendation. The basic idea of most recommender systems is
collaborative filtering, but existing MU methods ignore the collaborative
information across users and items. In this paper, we propose a general
erasable recommendation framework, namely LASER, which consists of Group module
and SeqTrain module. Firstly, Group module partitions users into balanced
groups based on their similarity of collaborative embedding learned via
hypergraph. Then SeqTrain module trains the model sequentially on all groups
with curriculum learning. Both theoretical analysis and experiments on two
real-world datasets demonstrate that LASER can not only achieve efficient
unlearning, but also outperform the state-of-the-art unlearning framework in
terms of model utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Note on Target Q-learning For Solving Finite MDPs with A Generative
  Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziniu Li, Tian Xu, Yang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Q-learning with function approximation could diverge in the off-policy
setting and the target network is a powerful technique to address this issue.
In this manuscript, we examine the sample complexity of the associated target
Q-learning algorithm in the tabular case with a generative oracle. We point out
a misleading claim in [Lee and He, 2020] and establish a tight analysis. In
particular, we demonstrate that the sample complexity of the target Q-learning
algorithm in [Lee and He, 2020] is $\widetilde{\mathcal O}(|\mathcal
S|^2|\mathcal A|^2 (1-\gamma)^{-5}\varepsilon^{-2})$. Furthermore, we show that
this sample complexity is improved to $\widetilde{\mathcal O}(|\mathcal
S||\mathcal A| (1-\gamma)^{-5}\varepsilon^{-2})$ if we can sequentially update
all state-action pairs and $\widetilde{\mathcal O}(|\mathcal S||\mathcal A|
(1-\gamma)^{-4}\varepsilon^{-2})$ if $\gamma$ is further in $(1/2, 1)$.
Compared with the vanilla Q-learning, our results conclude that the
introduction of a periodically-frozen target Q-function does not sacrifice the
sample complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approaches for Improving the Performance of Fake News Detection in
  Bangla: Imbalance Handling and Model Stacking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Muzakker Hossain, Zahin Awosaf, Md. Salman Hossan Prottoy, Abu Saleh Muhammod Alvy, Md. Kishor Morol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imbalanced datasets can lead to biasedness into the detection of fake news.
In this work, we present several strategies for resolving the imbalance issue
for fake news detection in Bangla with a comparative assessment of proposed
methodologies. Additionally, we propose a technique for improving performance
even when the dataset is imbalanced. We applied our proposed approaches to
BanFakeNews, a dataset developed for the purpose of detecting fake news in
Bangla comprising of 50K instances but is significantly skewed, with 97% of
majority instances. We obtained a 93.1% F1-score using data manipulation
manipulation techniques such as SMOTE, and a 79.1% F1-score using without data
manipulation approaches such as Stacked Generalization. Without implementing
these techniques, the F1-score would have been 67.6% for baseline models. We
see this work as an important step towards paving the way of fake news
detection in Bangla. By implementing these strategies the obstacles of
imbalanced dataset can be removed and improvement in the performance can be
achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, To appear in the Proceedings of the
  International Conference on 4th Industrial Revolution and Beyond (IC4IR),
  10-11 December 2021, Dhaka, Bangladesh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Class-Incremental Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has attracted growing attention via data-private
collaborative training on decentralized clients. However, most existing methods
unrealistically assume object classes of the overall framework are fixed over
time. It makes the global model suffer from significant catastrophic forgetting
on old classes in real-world scenarios, where local clients often collect new
classes continuously and have very limited storage memory to store old classes.
Moreover, new clients with unseen new classes may participate in the FL
training, further aggravating the catastrophic forgetting of the global model.
To address these challenges, we develop a novel Global-Local Forgetting
Compensation (GLFC) model, to learn a global class incremental model for
alleviating the catastrophic forgetting from both local and global
perspectives. Specifically, to address local forgetting caused by class
imbalance at the local clients, we design a class-aware gradient compensation
loss and a class-semantic relation distillation loss to balance the forgetting
of old classes and distill consistent inter-class relations across tasks. To
tackle the global forgetting brought by the non-i.i.d class imbalance across
clients, we propose a proxy server that selects the best old global model to
assist the local relation distillation. Moreover, a prototype gradient-based
communication mechanism is developed to protect privacy. Our model outperforms
state-of-the-art methods by 4.4%-15.1% in terms of average accuracy on
representative benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022, the first two authors contribute equally and they are
  ordered alphabetically</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BigBird: Big Data Storage and Analytics at Scale in Hybrid Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Deochake, Vrushali Channapattan, Gary Steelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing big data storage at scale is a complex and arduous task that
requires an advanced infrastructure. With the rise of public cloud computing,
various big data management services can be readily leveraged. As a critical
part of Twitter's "Project Partly Cloudy", the cold storage data and analytics
systems are being moved to the public cloud. This paper showcases our approach
in designing a scalable big data storage and analytics management framework
using BigQuery in Google Cloud Platform while ensuring security, privacy, and
data protection. The paper also discusses the limitations on the public cloud
resources and how they can be effectively overcome when designing a big data
storage and analytics solution at scale. Although the paper discusses the
framework implementation in Google Cloud Platform, it can easily be applied to
all major cloud providers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Graph Representation Learning for the Prediction of
  Drug-Target Binding Affinity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Chu, Shichao Liu, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of drug-target binding affinity (DTA) has attracted
increasing attention in the drug discovery process due to the more specific
interpretation than binary interaction prediction. Recently, numerous deep
learning-based computational methods have been proposed to predict the binding
affinities between drugs and targets benefiting from their satisfactory
performance. However, the previous works mainly focus on encoding biological
features and chemical structures of drugs and targets, with a lack of
exploiting the essential topological information from the drug-target affinity
network. In this paper, we propose a novel hierarchical graph representation
learning model for the drug-target binding affinity prediction, namely
HGRL-DTA. The main contribution of our model is to establish a hierarchical
graph learning architecture to incorporate the intrinsic properties of
drug/target molecules and the topological affinities of drug-target pairs. In
this architecture, we adopt a message broadcasting mechanism to integrate the
hierarchical representations learned from the global-level affinity graph and
the local-level molecular graph. Besides, we design a similarity-based
embedding map to solve the cold start problem of inferring representations for
unseen drugs and targets. Comprehensive experimental results under different
scenarios indicate that HGRL-DTA significantly outperforms the state-of-the-art
models and shows better model generalization among all the scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Root-aligned SMILES for Molecular Retrosynthesis Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Liu, Min Wu, Tingjun Hou, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrosynthesis prediction is a fundamental problem in organic synthesis,
where the task is to discover precursor molecules that can be used to
synthesize a target molecule. A popular paradigm of existing computational
retrosynthesis methods formulate retrosynthesis prediction as a
sequence-to-sequence translation problem, where the typical SMILES
representations are adopted for both reactants and products. However, the
general-purpose SMILES neglects the characteristics of retrosynthesis that 1)
the search space of the reactants is quite huge, and 2) the molecular graph
topology is largely unaltered from products to reactants, resulting in the
suboptimal performance of SMILES if straightforwardly applied. In this article,
we propose the root-aligned SMILES~(R-SMILES), which specifies a tightly
aligned one-to-one mapping between the product and the reactant SMILES, to
narrow the string representation discrepancy for more efficient retrosynthesis.
As the minimum edit distance between the input and the output is significantly
decreased with the proposed R-SMILES, the computational model is largely
relieved from learning the complex syntax and dedicated to learning the
chemical knowledge for retrosynthesis. We compare the proposed R-SMILES with
various state-of-the-art baselines on different benchmarks and show that it
significantly outperforms them all, demonstrating the superiority of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 15 pages, 5 figures, and 1 table; supplementary
  information: 4 pages, 2 figures and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Zero-Shot Out-of-Distribution Detection Based on the <span class="highlight-title">Pre-train</span>ed Model
  CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.02748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.02748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepideh Esmaeilpour, <span class="highlight-author">Bing Liu</span>, Eric Robertson, Lei Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an out-of-distribution (OOD) detection problem, samples of known
classes(also called in-distribution classes) are used to train a special
classifier. In testing, the classifier can (1) classify the test samples of
known classes to their respective classes and also (2) detect samples that do
not belong to any of the known classes (i.e., they belong to some unknown or
OOD classes). This paper studies the problem of zero-shot
out-of-distribution(OOD) detection, which still performs the same two tasks in
testing but has no training except using the given known class names. This
paper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC
builds on top of the recent advances in zero-shot classification through
multi-modal representation learning. It first extends the pre-trained
language-vision model CLIP by training a text-based image description generator
on top of CLIP. In testing, it uses the extended model to generate candidate
unknown class names for each test sample and computes a confidence score based
on both the known class names and candidate unknown class names for zero-shot
OOD detection. Experimental results on 5 benchmark datasets for OOD detection
demonstrate that ZOC outperforms the baselines by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral
  Images using a Hybrid Deep Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.04631v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.04631v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangcao Xu, Jian Sun, Guido Cervone, Mark Salvador
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atmospheric correction is a fundamental task in remote sensing because
observations are taken either of the atmosphere or looking through the
atmosphere. Atmospheric correction errors can significantly alter the spectral
signature of the observations, and lead to invalid classifications or target
detection. This is even more crucial when working with hyperspectral data,
where a precise measurement of spectral properties is required.
State-of-the-art physics-based atmospheric correction approaches require
extensive prior knowledge about sensor characteristics, collection geometry,
and environmental characteristics of the scene being collected. These
approaches are computationally expensive, prone to inaccuracy due to lack of
sufficient environmental and collection information, and often impossible for
real-time applications. In this paper, a geometry-dependent hybrid neural
network is proposed for automatic atmospheric correction using multi-scan
hyperspectral data collected from different geometries. The proposed network
can characterize the atmosphere without any additional meteorological data. A
grid-search method is also proposed to solve the temperature emissivity
separation problem. Results show that the proposed network has the capacity to
accurately characterize the atmosphere and estimate target emissivity spectra
with a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This
solution can lead to accurate atmospheric correction to improve target
detection for real time applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-separable Spatio-temporal Graph Kernels via SPDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.08524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.08524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Nikitin, ST John, Arno Solin, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes (GPs) provide a principled and direct approach for
inference and learning on graphs. However, the lack of justified graph kernels
for spatio-temporal modelling has held back their use in graph problems. We
leverage an explicit link between stochastic partial differential equations
(SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via
SPDEs, and derive non-separable spatio-temporal graph kernels that capture
interaction across space and time. We formulate the graph kernels for the
stochastic heat equation and wave equation. We show that by providing novel
tools for spatio-temporal GP modelling on graphs, we outperform pre-existing
graph kernels in real-world applications that feature diffusion, oscillation,
and other complicated interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A hybrid 2-stage vision <span class="highlight-title">transformer</span> for artificial intelligence-assisted
  5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic
  tool for guiding gastric cancer treatment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Oh, Go Eun Bae, Kyung-Hee Kim, Min-Kyung Yeo, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
automatic classification systems for guiding proper GC treatment based on
clinical guideline are still lacking. We propose an AI system classifying 5
classes of GC histology, which can be perfectly matched to general GC treatment
guidance. The AI system was designed to mimic the way pathologist understand
slides through multi-scale self-attention mechanism using a 2-stage Vision
Transformer network. The AI system performance was evaluated on 876 internal
endoscopic slides and 336 external endoscopic slides from clinical cohort. We
further evaluated practical usability of the AI system on observation of
AI-assisted 6 pathologist performance. The AI system demonstrates clinical
capability by achieving class-average diagnostic sensitivity of above 85% for
both internal and external cohort analysis. Furthermore, AI-assisted
pathologists showed significantly improved diagnostic sensitivity by 10% within
18% saved screening time compared to human pathologists (p-values of 0.006 and
0.030, respectively). The reliable performance of the AI system in multi-center
cohort testing and its clinical applicability demonstrate that AI-assisted
endoscopic CG screening would help reduce the workload of limited pathologists.
Furthermore, the AI system has a great potential for providing presumptive
pathologic opinion for deciding proper treatment for early GC patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPPIN: A Biological Repository of Dynamic Protein-Protein Interaction
  Network Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02168v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02168v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Fu, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the big data era, the relationship between entries becomes more and more
complex. Effective graph (or network) representation learning and mining
algorithms pave the way for many applications, such as recommendation, fraud
detection, social search, and bioinformation retrieval. Nowadays, many graph
(or network) algorithms have already paid attention to dynamic or temporal
networks, which are more suitable than static ones for fitting the complex
real-world scenarios with evolving patterns of graph topology and node
features. To contribute to the network representation learning and network
mining research community, we provide a bunch of label-adequate,
dynamics-meaningful, and attribute-sufficient dynamic networks from the health
domain. To be specific, in our repository DPPIN, we totally have 12 dynamic
network datasets at different scales, and each dataset is a dynamic
protein-protein interaction network describing protein-level interactions of
yeast cells. These domain-specific node features, graph evolution patterns, and
node and graph labels could serve as the benchmarks and constraints to help the
training and learning manners of graph algorithms. All resources of this work
are deployed and publicly available at https://github.com/DongqiFu/DPPIN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EigenGame Unloaded: When playing games is better than optimizing <span class="chip">ICLR '22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.04152v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.04152v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Gemp, Brian McWilliams, Claire Vernade, Thore Graepel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We build on the recently proposed EigenGame that views eigendecomposition as
a competitive game. EigenGame's updates are biased if computed using
minibatches of data, which hinders convergence and more sophisticated
parallelism in the stochastic setting. In this work, we propose an unbiased
stochastic update that is asymptotically equivalent to EigenGame, enjoys
greater parallelism allowing computation on datasets of larger sample sizes,
and outperforms EigenGame in experiments. We present applications to finding
the principal components of massive datasets and performing spectral clustering
of graphs. We analyze and discuss our proposed update in the context of
EigenGame and the shift in perspective from optimization to games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR '22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Training of Neural Networks with Partial Gradients <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.08895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.08895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirkeivan Mohtashami, Martin Jaggi, Sebastian U. Stich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art training algorithms for deep learning models are based on
stochastic gradient descent (SGD). Recently, many variations have been
explored: perturbing parameters for better accuracy (such as in Extragradient),
limiting SGD updates to a subset of parameters for increased efficiency (such
as meProp) or a combination of both (such as Dropout). However, the convergence
of these methods is often not studied in theory. We propose a unified
theoretical framework to study such SGD variants -- encompassing the
aforementioned algorithms and additionally a broad variety of methods used for
communication efficient training or model compression. Our insights can be used
as a guide to improve the efficiency of such methods and facilitate
generalization to new applications. As an example, we tackle the task of
jointly training networks, a version of which (limited to sub-networks) is used
to create Slimmable Networks. By training a low-rank Transformer jointly with a
standard one we obtain superior performance than when it is trained separately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 25th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch
  Feature Swapping for Bodies and Faces <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12448v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12448v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Foti, Bongjin Koo, Danail Stoyanov, Matthew J. Clarkson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a disentangled, interpretable, and structured latent representation
in 3D generative models of faces and bodies is still an open problem. The
problem is particularly acute when control over identity features is required.
In this paper, we propose an intuitive yet effective self-supervised approach
to train a 3D shape variational autoencoder (VAE) which encourages a
disentangled latent representation of identity features. Curating the
mini-batch generation by swapping arbitrary features across different shapes
allows to define a loss function leveraging known differences and similarities
in the latent representations. Experimental results conducted on 3D meshes show
that state-of-the-art methods for latent disentanglement are not able to
disentangle identity features of faces and bodies. Our proposed method properly
decouples the generation of such features while maintaining good representation
and reconstruction capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benign Overfitting without Linearity: Neural Network Classifiers Trained
  by Gradient Descent for Noisy Linear Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Frei, Niladri S. Chatterji, Peter L. Bartlett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benign overfitting, the phenomenon where interpolating models generalize well
in the presence of noisy data, was first observed in neural network models
trained with gradient descent. To better understand this empirical observation,
we consider the generalization error of two-layer neural networks trained to
interpolation by gradient descent on the logistic loss following random
initialization. We assume the data comes from well-separated class-conditional
log-concave distributions and allow for a constant fraction of the training
labels to be corrupted by an adversary. We show that in this setting, neural
networks exhibit benign overfitting: they can be driven to zero training error,
perfectly fitting any noisy training labels, and simultaneously achieve test
error close to the Bayes-optimal error. In contrast to previous work on benign
overfitting that require linear or kernel-based predictors, our analysis holds
in a setting where both the model and learning dynamics are fundamentally
nonlinear.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages; fixed typos and clarified an assumption on the activation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommender systems based on graph embedding techniques: A comprehensive
  <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal tool to alleviate the information overload problem, recommender
systems aim to predict user's preferred items from millions of candidates by
analyzing observed user-item relations. As for alleviating the sparsity and
cold start problems encountered by recommender systems, researchers resort to
employing side information or knowledge in recommendation as a strategy for
uncovering hidden (indirect) user-item relations, aiming to enrich observed
information (or data) for recommendation. However, in the face of the high
complexity and large scale of side information and knowledge, this strategy
relies for efficient implementation on the scalability of recommendation
models. Not until after the prevalence of machine learning did graph embedding
techniques be a concentration, which can efficiently utilize complex and
large-scale data. In light of that, equipping recommender systems with graph
embedding techniques has been widely studied these years, appearing to
outperform conventional recommendation implemented directly based on graph
topological analysis. As the focus, this article retrospects graph
embedding-based recommendation from embedding techniques for bipartite graphs,
general graphs and knowledge graphs, and proposes a general design pipeline of
that. In addition, after comparing several representative graph embedding-based
recommendation models with the most common-used conventional recommendation
models on simulations, this article manifests that the conventional models can
overall outperform the graph embedding-based ones in predicting implicit
user-item interactions, revealing the comparative weakness of graph
embedding-based recommendation in these tasks. To foster future research, this
article proposes suggestions on making a trade-off between graph
embedding-based recommendation and conventional recommendation in different
tasks, and puts forward open questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The activity-weight duality in feed forward neural networks: The
  geometric determinants of generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Feng, Yuhai Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the fundamental problems in machine learning is generalization. In
neural network models with a large number of weights (parameters), many
solutions can be found to fit the training data equally well. The key question
is which solution can describe testing data not in the training set. Here, we
report the discovery of an exact duality (equivalence) between changes in
activities in a given layer of neurons and changes in weights that connect to
the next layer of neurons in a densely connected layer in any feed forward
neural network. The activity-weight (A-W) duality allows us to map variations
in inputs (data) to variations of the corresponding dual weights. By using this
mapping, we show that the generalization loss can be decomposed into a sum of
contributions from different eigen-directions of the Hessian matrix of the loss
function at the solution in weight space. The contribution from a given
eigen-direction is the product of two geometric factors (determinants): the
sharpness of the loss landscape and the standard deviation of the dual weights,
which is found to scale with the weight norm of the solution. Our results
provide an unified framework, which we used to reveal how different
regularization schemes (weight decay, stochastic gradient descent with
different batch sizes and learning rates, dropout), training data size, and
labeling noise affect generalization performance by controlling either one or
both of these two geometric determinants for generalization. These insights can
be used to guide development of algorithms for finding more generalizable
solutions in overparametrized neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Efficient Multi-Agent Cooperative Visual Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of cooperative visual exploration where multiple agents
need to jointly explore unseen regions as fast as possible based on visual
signals. Classical planning-based methods often suffer from expensive
computation overhead at each step and a limited expressiveness of complex
cooperation strategy. By contrast, reinforcement learning (RL) has recently
become a popular paradigm for tackling this challenge due to its modeling
capability of arbitrarily complex strategies and minimal inference overhead. In
this paper, we extend the state-of-the-art single-agent visual navigation
method, Active Neural SLAM (ANS), to the multi-agent setting by introducing a
novel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages
a transformer-based architecture, Spatial-TeamFormer, which effectively
captures spatial relations and intra-agent interactions via hierarchical
spatial self-attentions. In addition, we also implement a few multi-agent
enhancements to process local information from each agent for an aligned
spatial representation and more precise planning. Finally, we perform policy
distillation to extract a meta policy to significantly improve the
generalization capability of final policy. We call this overall solution,
Multi-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms
classical planning-based baselines for the first time in a photo-realistic 3D
simulator, Habitat. Code and videos can be found at
https://sites.google.com/view/maans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First three authors share equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards noise robust trigger-word detection with <span class="highlight-title">contrastive learning</span>
  pre-task for fast on-boarding of new trigger-words <span class="chip">INTERSPEECH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sivakumar Balasubramanian, Aditya Jajodia, Gowtham Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trigger-word detection plays an important role as the entry point of user's
communication with voice assistants. But supporting a particular word as a
trigger-word involves huge amount of data collection, augmentation and
labelling for that word. This makes supporting new trigger-words a tedious and
time consuming process. To combat this, we explore the use of contrastive
learning as a pre-training task that helps the detection model to generalize to
different words and noise conditions. We explore supervised contrastive
techniques and also propose a novel self-supervised training technique using
chunked words from long sentence audios. We show that both supervised and the
new self-supervised contrastive pre-training techniques have comparable results
to a traditional classification pre-training on new trigger words with less
data availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to INTERSPEECH</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneFlow: Redesign the Distributed Deep Learning Framework from Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15032v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15032v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhui Yuan, Xinqi Li, Cheng Cheng, Juncheng Liu, Ran Guo, Shenghang Cai, Chi Yao, Fei Yang, Xiaodong Yi, Chuan Wu, Haoran Zhang, Jie Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning frameworks such as TensorFlow and PyTorch provide a productive
interface for expressing and training a deep neural network (DNN) model on a
single device or using data parallelism. Still, they may not be flexible or
efficient enough in training emerging large models on distributed devices,
which require more sophisticated parallelism beyond data parallelism. Plugins
or wrappers have been developed to strengthen these frameworks for model or
pipeline parallelism, but they complicate the usage and implementation of
distributed deep learning. Aiming at a simple, neat redesign of distributed
deep learning frameworks for various parallelism paradigms, we present OneFlow,
a novel distributed training framework based on an SBP (split, broadcast and
partial-value) abstraction and the actor model. SBP enables much easier
programming of data parallelism and model parallelism than existing frameworks,
and the actor model provides a succinct runtime mechanism to manage the complex
dependencies imposed by resource constraints, data movement and computation in
distributed deep learning. We demonstrate the general applicability and
efficiency of OneFlow for training various large DNN models with case studies
and extensive experiments. The results show that OneFlow outperforms many
well-known customized libraries built on top of the state-of-the-art
frameworks. The code of OneFlow is available at:
https://github.com/Oneflow-Inc/oneflow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Selection for Efficient Model Update in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Shi, Valentin Radu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Federated Learning (FL) workflow of training a centralized model with
distributed data is growing in popularity. However, until recently, this was
the realm of contributing clients with similar computing capability. The fast
expanding IoT space and data being generated and processed at the edge are
encouraging more effort into expanding federated learning to include
heterogeneous systems. Previous approaches distribute light-weight models to
clients are rely on knowledge transfer to distil the characteristic of local
data in partitioned updates. However, their additional knowledge exchange
transmitted through the network degrades the communication efficiency of FL. We
propose to reduce the size of knowledge exchanged in these FL setups by
clustering and selecting only the most representative bits of information from
the clients. The partitioned global update adopted in our work splits the
global deep neural network into a lower part for generic feature extraction and
an upper part that is more sensitive to this selected client knowledge. Our
experiments show that only 1.6% of the initially exchanged data can effectively
transfer the characteristic of the client data to the global model in our FL
approach, using split networks. These preliminary results evolve our
understanding of federated learning by demonstrating efficient training using
strategically selected training samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-assisted deep learning of rare extreme events from partial
  observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Asch, Ethan Brady, Hugo Gallardo, John Hood, Bryan Chu, Mohammad Farazmand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To predict rare extreme events using deep neural networks, one encounters the
so-called small data problem because even long-term observations often contain
few extreme events. Here, we investigate a model-assisted framework where the
training data is obtained from numerical simulations, as opposed to
observations, with adequate samples from extreme events. However, to ensure the
trained networks are applicable in practice, the training is not performed on
the full simulation data; instead we only use a small subset of observable
quantities which can be measured in practice. We investigate the feasibility of
this model-assisted framework on three different dynamical systems (Rossler
attractor, FitzHugh-Nagumo model, and a turbulent fluid flow) and three
different deep neural network architectures (feedforward, long short-term
memory, and reservoir computing). In each case, we study the prediction
accuracy, robustness to noise, reproducibility under repeated training, and
sensitivity to the type of input data. In particular, we find long short-term
memory networks to be most robust to noise and to yield relatively accurate
predictions, while requiring minimal fine-tuning of the hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Chaos: An Interdisciplinary Journal of
  Nonlinear Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Collection Programming with Semi-Ring Dictionaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.06376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.06376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shaikhha, Mathieu Huot, Jaclyn Smith, Dan Olteanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces semi-ring dictionaries, a powerful class of
compositional and purely functional collections that subsume other collection
types such as sets, multisets, arrays, vectors, and matrices. We developed
SDQL, a statically typed language that can express relational algebra with
aggregations, linear algebra, and functional collections over data such as
relations and matrices using semi-ring dictionaries. Furthermore, thanks to the
algebraic structure behind these dictionaries, SDQL unifies a wide range of
optimizations commonly used in databases (DB) and linear algebra (LA). As a
result, SDQL enables efficient processing of hybrid DB and LA workloads, by
putting together optimizations that are otherwise confined to either DB systems
or LA frameworks. We show experimentally that a handful of DB and LA workloads
can take advantage of the SDQL language and optimizations. SDQL can be
competitive with or outperforms a host of systems that are state of the art in
their own domain: in-memory DB systems Typer and Tectorwise for (flat, not
nested) relational data; SciPy for LA workloads; sparse tensor compiler taco;
the Trance nested relational engine; and the in-database machine learning
engines LMFAO and Morpheus for hybrid DB/LA workloads over relational data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spline-PINN: Approaching PDEs without Data using Fast, Physics-Informed
  Hermite-Spline CNNs <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.07143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.07143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Wandel, Michael Weinmann, Michael Neidlin, Reinhard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial Differential Equations (PDEs) are notoriously difficult to solve. In
general, closed-form solutions are not available and numerical approximation
schemes are computationally expensive. In this paper, we propose to approach
the solution of PDEs based on a novel technique that combines the advantages of
two recently emerging machine learning based approaches. First,
physics-informed neural networks (PINNs) learn continuous solutions of PDEs and
can be trained with little to no ground truth data. However, PINNs do not
generalize well to unseen domains. Second, convolutional neural networks
provide fast inference and generalize but either require large amounts of
training data or a physics-constrained loss based on finite differences that
can lead to inaccuracies and discretization artifacts. We leverage the
advantages of both of these approaches by using Hermite spline kernels in order
to continuously interpolate a grid-based state representation that can be
handled by a CNN. This allows for training without any precomputed training
data using a physics-informed loss function only and provides fast, continuous
solutions that generalize to unseen domains. We demonstrate the potential of
our method at the examples of the incompressible Navier-Stokes equation and the
damped wave equation. Our models are able to learn several intriguing phenomena
such as Karman vortex streets, the Magnus effect, Doppler effect, interference
patterns and wave reflections. Our quantitative assessment and an interactive
real-time demo show that we are narrowing the gap in accuracy of unsupervised
ML based methods to industrial CFD solvers while being orders of magnitude
faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FastCorrect: Fast Error Correction with Edit Alignment for Automatic
  Speech Recognition <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.03842v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.03842v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiang-Yang Li, Ed Lin, <span class="highlight-author">Tie-Yan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the popular NAR models adopted in neural machine translation and
text edition by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021. Code URL:
  https://github.com/microsoft/NeuralSpeech/tree/master/FastCorrect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Than Meets the Eye: <span class="highlight-title">Self-Supervised</span> Depth Reconstruction From Brain
  Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.05113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.05113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Gaziv, Michal Irani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as "paired" data), and (ii) a very large number of natural images
with no fMRI recordings ("unpaired data"). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available "paired"
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many "unpaired" data (natural
images & depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/WeizmannVision/SelfSuperReconst</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inductive logic programming at 30: a new introduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.07912v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.07912v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Cropper, Sebastijan Dumančić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive logic programming (ILP) is a form of machine learning. The goal of
ILP is to induce a hypothesis (a set of logical rules) that generalises
training examples. As ILP turns 30, we provide a new introduction to the field.
We introduce the necessary logical notation and the main learning settings;
describe the building blocks of an ILP system; compare several systems on
several dimensions; describe four systems (Aleph, TILDE, ASPAL, and Metagol);
highlight key application areas; and, finally, summarise current limitations
and directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper accepted for JAIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Operator Sketching for Deep Unrolling Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperbolic Vision <span class="highlight-title">Transformer</span>s: Combining Improvements in Metric
  Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Occlusion Fields: An Implicit Representation for Non-Line-of-Sight
  Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Grau, Markus Plack, Patrick Haehn, Michael Weinmann, Matthias Hullin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality
that aims to recover objects or scene parts outside the field of view from
measurements of light that is indirectly scattered off a directly visible,
diffuse wall. Despite recent advances in acquisition and reconstruction
techniques, the well-posedness of the problem at large, and the recoverability
of objects and their shapes in particular, remains an open question. The
commonly employed Fermat path criterion is rather conservative with this
regard, as it classifies some surfaces as unrecoverable, although they
contribute to the signal.
  In this paper, we use a simpler necessary criterion for an opaque surface
patch to be recoverable. Such piece of surface must be directly visible from
some point on the wall, and it must occlude the space behind itself. Inspired
by recent advances in neural implicit representations, we devise a new
representation and reconstruction technique for NLoS scenes that unifies the
treatment of recoverability with the reconstruction itself. Our approach, which
we validate on various synthetic and experimental datasets, exhibits
interesting properties. Unlike memory-inefficient volumetric representations,
ours allows to infer adaptively tessellated surfaces from time-of-flight
measurements of moderate resolution. It can further recover features beyond the
Fermat path criterion, and it is robust to significant amounts of
self-occlusion. We believe that this is the first time that these properties
have been achieved in one system that, as an additional benefit, is trainable
and hence suited for data-driven approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic <span class="highlight-title">Review</span>-based Recommenders <span class="chip">SC21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostadin Cvejoski, Ramses J. Sanchez, Christian Bauckhage, Cesar Ojeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Just as user preferences change with time, item reviews also reflect those
same preference changes. In a nutshell, if one is to sequentially incorporate
review content knowledge into recommender systems, one is naturally led to
dynamical models of text. In the present work we leverage the known power of
reviews to enhance rating predictions in a way that (i) respects the causality
of review generation and (ii) includes, in a bidirectional fashion, the ability
of ratings to inform language review models and vice-versa, language
representations that help predict ratings end-to-end. Moreover, our
representations are time-interval aware and thus yield a continuous-time
representation of the dynamics. We provide experiments on real-world datasets
and show that our methodology is able to outperform several state-of-the-art
models. Source code for all models can be found at [1].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages, Published at International Data Science Conference 2021
  (iDSC21)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural
  Networks with Non-Zero Training Loss <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.11923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.11923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Chen, Takashi Matsubara, Takaharu Yaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many physical phenomena are described by Hamiltonian mechanics using an
energy function (the Hamiltonian). Recently, the Hamiltonian neural network,
which approximates the Hamiltonian as a neural network, and its extensions have
attracted much attention. This is a very powerful method, but its use in
theoretical studies remains limited. In this study, by combining the
statistical learning theory and Kolmogorov-Arnold-Moser (KAM) theory, we
provide a theoretical analysis of the behavior of Hamiltonian neural networks
when the learning error is not completely zero. A Hamiltonian neural network
with non-zero errors can be considered as a perturbation from the true
dynamics, and the perturbation theory of the Hamilton equation is widely known
as the KAM theory. To apply the KAM theory, we provide a generalization error
bound for Hamiltonian neural networks by deriving an estimate of the covering
number of the gradient of the multi-layer perceptron, which is the key
ingredient of the model. This error bound gives an $L^\infty$ bound on the
Hamiltonian that is required in the application of the KAM theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the thirty-sixth AAAI conference on artificial
  intelligence (AAAI-22) as an oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Basic Elements of Deep Learning Models for
  Spatial-Temporal Traffic Forecasting <span class="chip">AAAI-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07513v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07513v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyol Shin, Yoonjin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic forecasting plays a crucial role in intelligent transportation
systems. The spatial-temporal complexities in transportation networks make the
problem especially challenging. The recently suggested deep learning models
share basic elements such as graph convolution, graph attention, recurrent
units, and/or attention mechanism. In this study, we designed an in-depth
comparative study for four deep neural network models utilizing different basic
elements. For base models, one RNN-based model and one attention-based model
were chosen from previous literature. Then, the spatial feature extraction
layers in the models were substituted with graph convolution and graph
attention. To analyze the performance of each element in various environments,
we conducted experiments on four real-world datasets - highway speed, highway
flow, urban speed from a homogeneous road link network, and urban speed from a
heterogeneous road link network. The results demonstrate that the RNN-based
model and the attention-based model show a similar level of performance for
short-term prediction, and the attention-based model outperforms the RNN in
longer-term predictions. The choice of graph convolution and graph attention
makes a larger difference in the RNN-based models. Also, our modified version
of GMAN shows comparable performance with the original with less memory
consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 3 Tables, This paper is accepted for AAAI-22
  Workshop: AI for Transportation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving time dependent Fokker-Planck equations via temporal normalizing
  flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Feng, Li Zeng, Tao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose an adaptive learning approach based on temporal
normalizing flows for solving time-dependent Fokker-Planck (TFP) equations. It
is well known that solutions of such equations are probability density
functions, and thus our approach relies on modelling the target solutions with
the temporal normalizing flows. The temporal normalizing flow is then trained
based on the TFP loss function, without requiring any labeled data. Being a
machine learning scheme, the proposed approach is mesh-free and can be easily
applied to high dimensional problems. We present a variety of test problems to
show the effectiveness of the learning approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigation of Physics-Informed Deep Learning for the Prediction of
  Parametric, Three-Dimensional Flow Based on Boundary Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Heger, Markus Full, Daniel Hilger, Norbert Hosters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The placement of temperature sensitive and safety-critical components is
crucial in the automotive industry. It is therefore inevitable, even at the
design stage of new vehicles that these components are assessed for potential
safety issues. However, with increasing number of design proposals, risk
assessment quickly becomes expensive. We therefore present a parameterized
surrogate model for the prediction of three-dimensional flow fields in
aerothermal vehicle simulations. The proposed physics-informed neural network
(PINN) design is aimed at learning families of flow solutions according to a
geometric variation. In scope of this work, we could show that our
nondimensional, multivariate scheme can be efficiently trained to predict the
velocity and pressure distribution for different design scenarios and geometric
scales. The proposed algorithm is based on a parametric minibatch training
which enables the utilization of large datasets necessary for the
three-dimensional flow modeling. Further, we introduce a continuous resampling
algorithm that allows to operate on one static dataset. Every feature of our
methodology is tested individually and verified against conventional CFD
simulations. Finally, we apply our proposed method in context of an exemplary
real-world automotive application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reference to code and dataset are DOIs.The DOIs will be activated
  when article is reviewed. Until then please contact Philip Heger or Daniel
  Hilger if you wish for code and datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Control of SE(3) Hamiltonian Dynamics with Learned Disturbance
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai Duong, Nikolay Atanasov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive control is a critical component of reliable robot autonomy in
rapidly changing operational conditions. Adaptive control designs benefit from
a disturbance model, which is often unavailable in practice. This motivates the
use of machine learning techniques to learn disturbance features from training
data offline, which can subsequently be employed to compensate the disturbances
online. This paper develops geometric adaptive control with a learned
disturbance model for rigid-body systems, such as ground, aerial, and
underwater vehicles, that satisfy Hamilton's equations of motion over the
$SE(3)$ manifold. Our design consists of an \emph{offline disturbance model
identification stage}, using a Hamiltonian-based neural ordinary differential
equation (ODE) network trained from state-control trajectory data, and an
\emph{online adaptive control stage}, estimating and compensating the
disturbances based on geometric tracking errors. We demonstrate our adaptive
geometric controller in trajectory tracking simulations of fully-actuated
pendulum and under-actuated quadrotor systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://thaipduong.github.io/hamadapt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-Verse: Bidirectional Generation Between Image and Text <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11133v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11133v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.02446v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.02446v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice synthesis (SVS) systems are built to synthesize high-quality
and expressive singing voice, in which the acoustic model generates the
acoustic features (e.g., mel-spectrogram) given a music score. Previous singing
acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial
network (GAN) to reconstruct the acoustic features, while they suffer from
over-smoothing and unstable training issues respectively, which hinder the
naturalness of synthesized singing. In this work, we propose DiffSinger, an
acoustic model for SVS based on the diffusion probabilistic model. DiffSinger
is a parameterized Markov chain that iteratively converts the noise into
mel-spectrogram conditioned on the music score. By implicitly optimizing
variational bound, DiffSinger can be stably trained and generate realistic
outputs. To further improve the voice quality and speed up inference, we
introduce a shallow diffusion mechanism to make better use of the prior
knowledge learned by the simple loss. Specifically, DiffSinger starts
generation at a shallow step smaller than the total number of diffusion steps,
according to the intersection of the diffusion trajectories of the ground-truth
mel-spectrogram and the one predicted by a simple mel-spectrogram decoder.
Besides, we propose boundary prediction methods to locate the intersection and
determine the shallow step adaptively. The evaluations conducted on a Chinese
singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS
work. Extensional experiments also prove the generalization of our methods on
text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io.
Codes: https://github.com/MoonInTheRiver/DiffSinger. The old title of this
work: "Diffsinger: Diffusion acoustic model for singing voice synthesis".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SVS (DiffSinger), TTS (DiffSpeech), Shallow Diffusion Mechanism;
  Submitted to arxiv on 6 May 2021; Accepted by AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiayu Liang, Ying Gao, Shanrong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, many industries have applied classification algorithms to help them
solve problems in their business, like finance, medicine, manufacturing
industry and so on. However, in real-life scenarios, positive examples only
make up a small part of all instances and our datasets suffer from high
imbalance ratio which leads to poor performance of existing classification
models. To solve this problem, we come up with a bagging ensemble learning
framework based on an anomaly detection scoring system. We test out that our
ensemble learning model can dramatically improve performance of base estimators
(e.g. Decision Tree, Multilayer perceptron, KNN) and is more efficient than
other existing methods under a wide range of imbalance ratio, data scale and
data dimension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Splicing Approach to Best Subset of Groups Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.12576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.12576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhang Zhang, Junxian Zhu, Jin Zhu, Xueqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best subset of groups selection (BSGS) is the process of selecting a small
part of non-overlapping groups to achieve the best interpretability on the
response variable. It has attracted increasing attention and has far-reaching
applications in practice. However, due to the computational intractability of
BSGS in high-dimensional settings, developing efficient algorithms for solving
BSGS remains a research hotspot. In this paper,we propose a group-splicing
algorithm that iteratively detects the relevant groups and excludes the
irrelevant ones. Moreover, coupled with a novel group information criterion, we
develop an adaptive algorithm to determine the optimal model size. Under mild
conditions, it is certifiable that our algorithm can identify the optimal
subset of groups in polynomial time with high probability. Finally, we
demonstrate the efficiency and accuracy of our methods by comparing them with
several state-of-the-art algorithms on both synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Graph Neural Network Framework for Grid-Based Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Tang, Wennan Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir simulations are computationally expensive in the well control and
well placement optimization. Generally, numerous simulation runs (realizations)
are needed in order to achieve the optimal well locations. In this paper, we
propose a graph neural network (GNN) framework to build a surrogate
feed-forward model which replaces simulation runs to accelerate the
optimization process. Our GNN framework includes an encoder, a process, and a
decoder which takes input from the processed graph data designed and generated
from the simulation raw data. We train the GNN model with 6000 samples
(equivalent to 40 well configurations) with each containing the previous step
state variable and the next step state variable. We test the GNN model with
another 6000 samples and after model tuning, both one-step prediction and
rollout prediction achieve a close match with the simulation results. Our GNN
framework shows great potential in the application of well-related subsurface
optimization including oil and gas as well as carbon capture sequestration
(CCS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are conflict of interests and I need to modify the paper before
  resubmitting</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Cross-View Panorama Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songsong Wu, <span class="highlight-author">Hao Tan</span>g, Xiao-Yuan Jing, Haifeng Zhao, Jianjun Qian, Nicu Sebe, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of synthesizing a ground-view panorama
image conditioned on a top-view aerial image, which is a challenging problem
due to the large gap between the two image domains with different view-points.
Instead of learning cross-view mapping in a feedforward pass, we propose a
novel adversarial feedback GAN framework named PanoGAN with two key components:
an adversarial feedback module and a dual branch discrimination strategy.
First, the aerial image is fed into the generator to produce a target panorama
image and its associated segmentation map in favor of model training with
layout semantics. Second, the feature responses of the discriminator encoded by
our adversarial feedback module are fed back to the generator to refine the
intermediate representations, so that the generation performance is continually
improved through an iterative generation process. Third, to pursue
high-fidelity and semantic consistency of the generated panorama image, we
propose a pixel-segmentation alignment mechanism under the dual branch
discrimiantion strategy to facilitate cooperation between the generator and the
discriminator. Extensive experimental results on two challenging cross-view
image datasets show that PanoGAN enables high-quality panorama image generation
with more convincing details than state-of-the-art approaches. The source code
and trained models are available at \url{https://github.com/sswuai/PanoGAN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Learned Block-Based Image Compression with Block-Level Masked
  Convolutions and Asymptotic Closed Loop Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Kamisli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression research has achieved state-of-the-art compression
performance with auto-encoder based neural network architectures, where the
image is mapped via convolutional neural networks (CNN) into a latent
representation that is quantized and processed again with CNN to obtain the
reconstructed image. CNN operate on entire input images. On the other hand,
traditional state-of-the-art image and video compression methods process images
with a block-by-block processing approach for various reasons. Very recently,
work on learned image compression with block based approaches have also
appeared, which use the auto-encoder architecture on large blocks of the input
image and introduce additional neural networks that perform intra/spatial
prediction and deblocking/post-processing functions. This paper explores an
alternative learned block-based image compression approach in which neither an
explicit intra prediction neural network nor an explicit deblocking neural
network is used. A single auto-encoder neural network with block-level masked
convolutions is used and the block size is much smaller (8x8). By using
block-level masked convolutions, each block is processed using reconstructed
neighboring left and upper blocks both at the encoder and decoder. Hence, the
mutual information between adjacent blocks is exploited during compression and
each block is reconstructed using neighboring blocks, resolving the need for
explicit intra prediction and deblocking neural networks. Since the explored
system is a closed loop system, a special optimization procedure, the
asymptotic closed loop design, is used with standard stochastic gradient
descent based training. The experimental results indicate competitive image
compression performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FrameHopper: Selective Processing of Video Frames in Detection-driven
  Real-Time Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Adnan Arefeen, Sumaiya Tabassum Nimi, Md Yusuf Sarwar Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection-driven real-time video analytics require continuous detection of
objects contained in the video frames using deep learning models like YOLOV3,
EfficientDet. However, running these detectors on each and every frame in
resource-constrained edge devices is computationally intensive. By taking the
temporal correlation between consecutive video frames into account, we note
that detection outputs tend to be overlapping in successive frames. Elimination
of similar consecutive frames will lead to a negligible drop in performance
while offering significant performance benefits by reducing overall computation
and communication costs. The key technical questions are, therefore, (a) how to
identify which frames to be processed by the object detector, and (b) how many
successive frames can be skipped (called skip-length) once a frame is selected
to be processed. The overall goal of the process is to keep the error due to
skipping frames as small as possible. We introduce a novel error vs processing
rate optimization problem with respect to the object detection task that
balances between the error rate and the fraction of frames filtering.
Subsequently, we propose an off-line Reinforcement Learning (RL)-based
algorithm to determine these skip-lengths as a state-action policy of the RL
agent from a recorded video and then deploy the agent online for live video
streams. To this end, we develop FrameHopper, an edge-cloud collaborative video
analytics framework, that runs a lightweight trained RL agent on the camera and
passes filtered frames to the server where the object detection model runs for
a set of applications. We have tested our approach on a number of live videos
captured from real-life scenarios and show that FrameHopper processes only a
handful of frames but produces detection results closer to the oracle solution
and outperforms recent state-of-the-art solutions in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The 18th International Conference on Distributed
  Computing in Sensor Systems (DCOSS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making DeepFakes more spurious: evading deep face forgery detection via
  trace removal attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Liu, Huajie Chen, Tianqing Zhu, Jun Zhang, Wanlei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DeepFakes are raising significant social concerns. Although various DeepFake
detectors have been developed as forensic countermeasures, these detectors are
still vulnerable to attacks. Recently, a few attacks, principally adversarial
attacks, have succeeded in cloaking DeepFake images to evade detection.
However, these attacks have typical detector-specific designs, which require
prior knowledge about the detector, leading to poor transferability. Moreover,
these attacks only consider simple security scenarios. Less is known about how
effective they are in high-level scenarios where either the detectors or the
attacker's knowledge varies. In this paper, we solve the above challenges with
presenting a novel detector-agnostic trace removal attack for DeepFake
anti-forensics. Instead of investigating the detector side, our attack looks
into the original DeepFake creation pipeline, attempting to remove all
detectable natural DeepFake traces to render the fake images more "authentic".
To implement this attack, first, we perform a DeepFake trace discovery,
identifying three discernible traces. Then a trace removal network (TR-Net) is
proposed based on an adversarial learning framework involving one generator and
multiple discriminators. Each discriminator is responsible for one individual
trace representation to avoid cross-trace interference. These discriminators
are arranged in parallel, which prompts the generator to remove various traces
simultaneously. To evaluate the attack efficacy, we crafted heterogeneous
security scenarios where the detectors were embedded with different levels of
defense and the attackers' background knowledge of data varies. The
experimental results show that the proposed attack can significantly compromise
the detection accuracy of six state-of-the-art DeepFake detectors while causing
only a negligible loss in visual quality to the original DeepFake samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Speech Enhancement using <span class="highlight-title">Multimodal</span> Deep Convolutional
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1709.00944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1709.00944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus on addressing audio information only. In this work, inspired
by multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model. In the proposed AVDCNN SE model, audio
and visual data are first processed using individual CNNs, and then, fused into
a joint network to generate enhanced speech at the output layer. The AVDCNN
model is trained in an end-to-end manner, and parameters are jointly learned
through back-propagation. We evaluate enhanced speech using five objective
criteria. Results show that the AVDCNN yields notably better performance,
compared with an audio-only CNN-based SE model and two conventional SE
approaches, confirming the effectiveness of integrating visual information into
the SE process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is the same as arXiv:1703.10893v2. Apologies for the
  inconvenience</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-21T00:00:00Z">2022-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relevant CommonSense Subgraphs for "What if..." Procedural Reasoning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zheng, Parisa Kordjamshidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the challenge of learning causal reasoning over procedural text to
answer "What if..." questions when external commonsense knowledge is required.
We propose a novel multi-hop graph reasoning model to 1) efficiently extract a
commonsense subgraph with the most relevant information from a large knowledge
graph; 2) predict the causal answer by reasoning over the representations
obtained from the commonsense subgraph and the contextual interactions between
the questions and context. We evaluate our model on WIQA benchmark and achieve
state-of-the-art performance compared to the recent models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2022 findings short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistency Improves Chain of Thought Reasoning in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a simple ensemble strategy, self-consistency, that significantly
improves the reasoning accuracy of large language models. The idea is to sample
a diverse set of outputs from a language model and return the most consistent
answer in the set. Such ensembling method improves reasoning accuracy when
combined with chain of thought prompting. For arithmetic and commonsense
reasoning benchmarks we find that self-consistency yields significant accuracy
improvements in a variety of datasets, such as GSM8K (+10%), SVAMP (+14%),
MultiArith (+24%), CommonsenseQA (+5%) and ARC (easy +4%, challenge +5%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching language models to support answers with verified quotes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, Nat McAleese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models often answer factual questions correctly. But
users can't trust any given claim a model makes without fact-checking, because
language models can hallucinate convincing nonsense. In this work we use
reinforcement learning from human preferences (RLHP) to train "open-book" QA
models that generate answers whilst also citing specific evidence for their
claims, which aids in the appraisal of correctness. Supporting evidence is
drawn from multiple documents found via a search engine, or from a single
user-provided document. Our 280 billion parameter model, GopherCite, is able to
produce answers with high quality supporting evidence and abstain from
answering when unsure. We measure the performance of GopherCite by conducting
human evaluation of answers to questions in a subset of the NaturalQuestions
and ELI5 datasets. The model's response is found to be high-quality 80\% of the
time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
Abstaining from the third of questions for which it is most unsure improves
performance to 90\% and 80\% respectively, approaching human baselines.
However, analysis on the adversarial TruthfulQA dataset shows why citation is
only one part of an overall strategy for safety and trustworthiness: not all
claims supported by evidence are true.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainable Evaluation Metrics for Natural Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike classical lexical overlap metrics such as BLEU, most current
evaluation metrics (such as BERTScore or MoverScore) are based on black-box
language models such as BERT or XLM-R. They often achieve strong correlations
with human judgments, but recent research indicates that the lower-quality
classical metrics remain dominant, one of the potential reasons being that
their decision processes are transparent. To foster more widespread acceptance
of the novel high-quality metrics, explainability thus becomes crucial. In this
concept paper, we identify key properties and propose key goals of explainable
machine translation evaluation metrics. We also provide a synthesizing overview
over recent approaches for explainable machine translation metrics and discuss
how they relate to those goals and properties. Further, we conduct own novel
experiments, which (among others) find that current adversarial NLP techniques
are unsuitable for automatically identifying limitations of high-quality
black-box evaluation metrics, as they are not meaning-preserving. Finally, we
provide a vision of future approaches to explainable evaluation metrics and
their evaluation. We hope that our work can help catalyze and guide future
research on explainable evaluation metrics and, mediately, also contribute to
better and more transparent text generation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACS: A <span class="highlight-title">Dataset</span> for Physical Audiovisual CommonSense Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Yu, Peter Wu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, they should be able to reason about the
physical world by understanding the physical properties and affordances of
available objects, how they can be manipulated, and how they interact with
other physical objects. This research field of physical commonsense reasoning
is fundamentally a multi-sensory task since physical properties are manifested
through multiple modalities, two of them being vision and acoustics. Our paper
takes a step towards real-world physical commonsense reasoning by contributing
PACS: the first audiovisual benchmark annotated for physical commonsense
attributes. PACS contains a total of 13,400 question-answer pairs, involving
1,377 unique physical commonsense questions and 1,526 videos. Our dataset
provides new opportunities to advance the research field of physical reasoning
by bringing audio as a core component of this multimodal problem. Using PACS,
we evaluate multiple state-of-the-art models on this new challenging task.
While some models show promising results (70% accuracy), they all fall short of
human performance (95% accuracy). We conclude the paper by demonstrating the
importance of multimodal reasoning and providing possible avenues for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Clinical Coding: What, Why, and Where We Are? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Dong, Matúš Falis, William Whiteley, Beatrice Alex, Shaoxiong Ji, Jiaoyan Chen, Honghan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical coding is the task of transforming medical information in a
patient's health records into structured codes so that they can be used for
statistical analysis. This is a cognitive and time-consuming task that follows
a standard process in order to achieve a high level of consistency. Clinical
coding could potentially be supported by an automated system to improve the
efficiency and accuracy of the process. We introduce the idea of automated
clinical coding and summarise its challenges from the perspective of Artificial
Intelligence (AI) and Natural Language Processing (NLP), based on the
literature, our project experience over the past two and half years (late 2019
- early 2022), and discussions with clinical coding experts in Scotland and the
UK. Our research reveals the gaps between the current deep learning-based
approach applied to clinical coding and the need for explainability and
consistency in real-world practice. Knowledge-based methods that represent and
reason the standard, explainable process of a task may need to be incorporated
into deep learning-based methods for clinical coding. Automated clinical coding
is a promising task for AI, despite the technical and organisational
challenges. Coders are needed to be involved in the development process. There
is much to achieve to develop and deploy an AI-based automated system to
support coding in the next five years and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targeted Extraction of Temporal Facts from Textual Resources for
  Improved Temporal Question Answering over Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nithish Kannen, Udit Sharma, Sumit Neelam, Dinesh Khandelwal, Shajith Ikbal, Hima Karanam, L Venkata Subramaniam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Base Question Answering (KBQA) systems have the goal of answering
complex natural language questions by reasoning over relevant facts retrieved
from Knowledge Bases (KB). One of the major challenges faced by these systems
is their inability to retrieve all relevant facts due to factors such as
incomplete KB and entity/relation linking errors. In this paper, we address
this particular challenge for systems handling a specific category of questions
called temporal questions, where answer derivation involve reasoning over facts
asserting point/intervals of time for various events. We propose a novel
approach where a targeted temporal fact extraction technique is used to assist
KBQA whenever it fails to retrieve temporal facts from the KB. We use
$\lambda$-expressions of the questions to logically represent the component
facts and the reasoning steps needed to derive the answer. This allows us to
spot those facts that failed to get retrieved from the KB and generate textual
queries to extract them from the textual resources in an open-domain question
answering fashion. We evaluated our approach on a benchmark temporal question
answering dataset considering Wikidata and Wikipedia respectively as the KB and
textual resource. Experimental results show a significant $\sim$30\% relative
improvement in answer accuracy, demonstrating the effectiveness of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do We Answer Complex Questions: Discourse Structure of Long-form
  Answers <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Xu, Junyi Jessy Li, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form answers, consisting of multiple sentences, can provide nuanced and
comprehensive answers to a broader set of questions. To better understand this
complex and understudied task, we study the functional structure of long-form
answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our
main goal is to understand how humans organize information to craft complex
answers. We develop an ontology of six sentence-level functional roles for
long-form answers, and annotate 3.9k sentences in 640 answer paragraphs.
Different answer collection methods manifest in different discourse structures.
We further analyze model-generated answers -- finding that annotators agree
less with each other when annotating model-generated answers compared to
annotating human-written answers. Our annotated data enables training a strong
classifier that can be used for automatic analysis. We hope our work can
inspire future research on discourse-level modeling and evaluation of long-form
QA systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based HTR for Historical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Benjamin Ströbel, Simon Clematide, Martin Volk, Tobias Hodel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply the TrOCR framework to real-world, historical manuscripts and show
that TrOCR per se is a strong model, ideal for transfer learning. TrOCR has
been trained on English only, but it can adapt to other languages that use the
Latin alphabet fairly easily and with little training material. We compare
TrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such
systems. This finding is essential since Transkribus performs best when it has
access to baseline information, which is not needed at all to fine-tune TrOCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an abstract submitted and accepted at ComHum 2022 in
  Lausanne. We will be elaborating on these initial findings in the paper that
  we will submit after the conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word Order Does Matter (And Shuffled Language Models Know It) <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinit Ravishankar, Mostafa Abdou, Artur Kulmizev, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that language models pretrained and/or fine-tuned
on randomly permuted sentences exhibit competitive performance on GLUE, putting
into question the importance of word order information. Somewhat
counter-intuitively, some of these studies also report that position embeddings
appear to be crucial for models' good performance with shuffled text. We probe
these language models for word order information and investigate what position
embeddings learned from shuffled text encode, showing that these models retain
information pertaining to the original, naturalistic word order. We show this
is in part due to a subtlety in how shuffling is implemented in previous work
-- before rather than after subword segmentation. Surprisingly, we find even
Language models trained on text shuffled after subword segmentation retain some
semblance of information about word order because of the statistical
dependencies between sentence length and unigram probabilities. Finally, we
show that beyond GLUE, a variety of language understanding tasks do require
word order information, often to an extent that cannot be learned through
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ACL 2022; 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AraBART: a <span class="highlight-title">Pretrain</span>ed Arabic Sequence-to-Sequence Model for Abstractive
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Like most natural language understanding and generation tasks,
state-of-the-art models for summarization are transformer-based
sequence-to-sequence architectures that are pretrained on large corpora. While
most existing models focused on English, Arabic remained understudied. In this
paper we propose AraBART, the first Arabic model in which the encoder and the
decoder are pretrained end-to-end, based on BART. We show that AraBART achieves
the best performance on multiple abstractive summarization datasets,
outperforming strong baselines including a pretrained Arabic BERT-based model
and multilingual mBART and mT5 models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality Controlled Paraphrase Generation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elron Bandel, Ranit Aharonov, Michal Shmueli-Scheuer, Ilya Shnayderman, Noam Slonim, Liat Ein-Dor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Paraphrase generation has been widely used in various downstream tasks. Most
tasks benefit mainly from high quality paraphrases, namely those that are
semantically similar to, yet linguistically diverse from, the original
sentence. Generating high-quality paraphrases is challenging as it becomes
increasingly hard to preserve meaning as linguistic diversity increases. Recent
works achieve nice results by controlling specific aspects of the paraphrase,
such as its syntactic tree. However, they do not allow to directly control the
quality of the generated paraphrase, and suffer from low flexibility and
scalability. Here we propose $QCPG$, a quality-guided controlled paraphrase
generation model, that allows directly controlling the quality dimensions.
Furthermore, we suggest a method that given a sentence, identifies points in
the quality control space that are expected to yield optimal generated
paraphrases. We show that our method is able to generate paraphrases which
maintain the original meaning while achieving higher diversity than the
uncontrolled baseline. The models, the code, and the data can be found in
https://github.com/IBM/quality-controlled-paraphrase-generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General and Domain Adaptive Chinese Spelling Check with Error Consistent
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Lv, Ziqiang Cao, Lei Geng, Chunhui Ai, Xu Yan, Guohong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of label data is one of the significant bottlenecks for Chinese
Spelling Check (CSC). Existing researches use the method of automatic
generation by exploiting unlabeled data to expand the supervised corpus.
However, there is a big gap between the real input scenario and automatic
generated corpus. Thus, we develop a competitive general speller ECSpell which
adopts the Error Consistent masking strategy to create data for pretraining.
This error consistency masking strategy is used to specify the error types of
automatically generated sentences which is consistent with real scene. The
experimental result indicates our model outperforms previous state-of-the-art
models on the general benchmark. Moreover, spellers often work within a
particular domain in real life. Due to lots of uncommon domain terms,
experiments on our built domain specific datasets show that general models
perform terribly. Inspired by the common practice of input methods, we propose
to add an alterable user dictionary to handle the zero-shot domain adaption
problem. Specifically, we attach a User Dictionary guided inference module (UD)
to a general token classification based speller. Our experiments demonstrate
that ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other
baselines largely, even approaching the performance on the general benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ x-enVENT: A Corpus of Event Descriptions with Experiencer-specific
  Emotion and Appraisal Annotations <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrica Troiano, Laura Oberländer, Maximilian Wegge, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion classification is often formulated as the task to categorize texts
into a predefined set of emotion classes. So far, this task has been the
recognition of the emotion of writers and readers, as well as that of entities
mentioned in the text. We argue that a classification setup for emotion
analysis should be performed in an integrated manner, including the different
semantic roles that participate in an emotion episode. Based on appraisal
theories in psychology, which treat emotions as reactions to events, we compile
an English corpus of written event descriptions. The descriptions depict
emotion-eliciting circumstances, and they contain mentions of people who
responded emotionally. We annotate all experiencers, including the original
author, with the emotions they likely felt. In addition, we link them to the
event they found salient (which can be different for different experiencers in
a text) by annotating event properties, or appraisals (e.g., the perceived
event undesirability, the uncertainty of its outcome). Our analysis reveals
patterns in the co-occurrence of people's emotions in interaction. Hence, this
richly-annotated resource provides useful data to study emotions and event
evaluations from the perspective of different roles, and it enables the
development of experiencer-specific emotion and appraisal classification
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to LREC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document-Level Relation Extraction with Adaptive Focal Loss and
  Knowledge <span class="highlight-title">Distillation</span> <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Tan, Ruidan He, Lidong Bing, Hwee Tou Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level Relation Extraction (DocRE) is a more challenging task
compared to its sentence-level counterpart. It aims to extract relations from
multiple sentences at once. In this paper, we propose a semi-supervised
framework for DocRE with three novel components. Firstly, we use an axial
attention module for learning the interdependency among entity-pairs, which
improves the performance on two-hop relations. Secondly, we propose an adaptive
focal loss to tackle the class imbalance problem of DocRE. Lastly, we use
knowledge distillation to overcome the differences between human annotated data
and distantly supervised data. We conducted experiments on two DocRE datasets.
Our model consistently outperforms strong baselines and its performance exceeds
the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.
Our code and data will be released at https://github.com/tonytan48/KD-DocRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zoom Out and Observe: News Environment Perception for Fake News
  Detection <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding Wang, Yongchun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection is crucial for preventing the dissemination of
misinformation on social media. To differentiate fake news from real ones,
existing methods observe the language patterns of the news post and "zoom in"
to verify its content with knowledge sources or check its readers' replies.
However, these methods neglect the information in the external news environment
where a fake news post is created and disseminated. The news environment
represents recent mainstream media opinion and public attention, which is an
important inspiration of fake news fabrication because fake news is often
designed to ride the wave of popular events and catch public attention with
unexpected novel content for greater exposure and spread. To capture the
environmental signals of news posts, we "zoom out" to observe the news
environment and propose the News Environment Perception Framework (NEP). For
each post, we construct its macro and micro news environment from recent
mainstream news. Then we design a popularity-oriented and a novelty-oriented
module to perceive useful signals and further assist final prediction.
Experiments on our newly built datasets show that the NEP can efficiently
improve the performance of basic fake news detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Main Conference (Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrasing Techniques for Maritime QA system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Van Nguyen, Shirui Pan, Weiqing Wang, Reza Haffari, Yuan-Fang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been an increasing interest in incorporating Artificial
Intelligence (AI) into Defence and military systems to complement and augment
human intelligence and capabilities. However, much work still needs to be done
toward achieving an effective human-machine partnership. This work is aimed at
enhancing human-machine communications by developing a capability for
automatically translating human natural language into a machine-understandable
language (e.g., SQL queries). Techniques toward achieving this goal typically
involve building a semantic parser trained on a very large amount of
high-quality manually-annotated data. However, in many real-world Defence
scenarios, it is not feasible to obtain such a large amount of training data.
To the best of our knowledge, there are few works trying to explore the
possibility of training a semantic parser with limited manually-paraphrased
data, in other words, zero-shot. In this paper, we investigate how to exploit
paraphrasing methods for the automated generation of large-scale training
datasets (in the form of paraphrased utterances and their corresponding logical
forms in SQL format) and present our experimental results using real-world data
in the maritime domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Token Segmentation for High Token-Internal Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Brusilovsky, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenizing raw texts into word units is an essential pre-processing step for
critical tasks in the NLP pipeline such as tagging, parsing, named entity
recognition, and more. For most languages, this tokenization step
straightforward. However, for languages with high token-internal complexity,
further token-to-word segmentation is required. Previous canonical segmentation
studies were based on character-level frameworks, with no contextualised
representation involved. Contextualized vectors a la BERT show remarkable
results in many applications, but were not shown to improve performance on
linguistic segmentation per se. Here we propose a novel neural segmentation
model which combines the best of both worlds, contextualised token
representation and char-level decoding, which is particularly effective for
languages with high token-internal complexity and extreme morphological
ambiguity. Our model shows substantial improvements in segmentation accuracy on
Hebrew and Arabic compared to the state-of-the-art, and leads to further
improvements on downstream tasks such as Part-of-Speech Tagging, Dependency
Parsing and Named-Entity Recognition, over existing pipelines. When comparing
our segmentation-first pipeline with joint segmentation and labeling in the
same settings, we show that, contrary to pre-neural studies, the pipeline
performance is superior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TCM-SD: A Large <span class="highlight-title">Dataset</span> for Syndrome Differentiation in Traditional
  Chinese Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mucheng Ren, Heyan Huang, Yuxiang Zhou, Yuan Bu, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy
that has spread and been applied worldwide. The unique TCM diagnosis and
treatment system requires a comprehensive analysis of a patient's symptoms
hidden in the clinical record written in free text. Prior studies have shown
that this system can be informationized and intelligentized with the aid of
artificial intelligence (AI) technology, such as natural language processing
(NLP). However, existing datasets are not of sufficient quality nor quantity to
support the further development of data-driven AI technology in TCM. Therefore,
in this paper, we focus on the core task of the TCM diagnosis and treatment
system -- syndrome differentiation (SD) -- and we introduce the first public
large-scale dataset for SD, called TCM-SD. Our dataset contains 54,152
real-world clinical records covering 148 syndromes. Furthermore, we collect a
large-scale unlabelled textual corpus in the field of TCM and propose a
domain-specific pre-trained language model, called ZY-BERT. We conducted
experiments using deep neural networks to establish a strong performance
baseline, reveal various challenges in SD, and prove the potential of
domain-specific pre-trained language model. Our study and analysis reveal
opportunities for incorporating computer science and linguistics knowledge to
explore the empirical validity of TCM theories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, preprint,Work-In-Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Token Graph Modeling using a Novel Labeling Strategy for
  Structured Sentiment Analysis <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Shi, Fei Li, Jingye Li, Hao Fei, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art model for structured sentiment analysis casts the task
as a dependency parsing problem, which has some limitations: (1) The label
proportions for span prediction and span relation prediction are imbalanced.
(2) The span lengths of sentiment tuple components may be very large in this
task, which will further exacerbate the imbalance problem. (3) Two nodes in a
dependency graph cannot have multiple arcs, therefore some overlapped sentiment
tuples cannot be recognized. In this work, we propose nichetargeting solutions
for these issues. First, we introduce a novel labeling strategy, which contains
two sets of token pair labels, namely essential label set and whole label set.
The essential label set consists of the basic labels for this task, which are
relatively balanced and applied in the prediction layer. The whole label set
includes rich labels to help our model capture various token relations, which
are applied in the hidden layer to softly influence our model. Moreover, we
also propose an effective model to well collaborate with our labeling strategy,
which is equipped with the graph attention networks to iteratively refine token
representations, and the adaptive multi-label classifier to dynamically predict
multiple relations between token pairs. We perform extensive experiments on 5
benchmark datasets in four languages. Experimental results show that our model
outperforms previous SOTA models by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at the ACL 2022 Main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Slot Is Not Built in One Utterance: Spoken Language <span class="highlight-title">Dialog</span>s with
  Sub-Slots <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Zhang, Yuwei Hu, Yuchuan Wu, Jiaman Wu, Yongbin Li, Jian Sun, Caixia Yuan, Xiaojie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A slot value might be provided segment by segment over multiple-turn
interactions in a dialog, especially for some important information such as
phone numbers and names. It is a common phenomenon in daily life, but little
attention has been paid to it in previous work. To fill the gap, this paper
defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds
a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset
includes a total of 40K dialogs and 500K utterances from four different
domains: Chinese names, phone numbers, ID numbers and license plate numbers.
The data is well annotated with sub-slot values, slot values, dialog states and
actions. We find some new linguistic phenomena and interactive manners in SSTOD
which raise critical challenges of building dialog agents for the task. We test
three state-of-the-art dialog models on SSTOD and find they cannot handle the
task well on any of the four domains. We also investigate an improved model by
involving slot knowledge in a plug-in manner. More work should be done to meet
the new challenges raised from SSTOD which widely exists in real-life
applications. The dataset and code are publicly available via
https://github.com/shunjiu/SSTOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Match the Script, Adapt if Multilingual: Analyzing the Effect of
  Multilingual <span class="highlight-title">Pretrain</span>ing on Cross-lingual Transferability <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshinari Fujinuma, Jordan Boyd-Graber, Katharina Kann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained multilingual models enable zero-shot learning even for unseen
languages, and that performance can be further improved via adaptation prior to
finetuning. However, it is unclear how the number of pretraining languages
influences a model's zero-shot learning for languages unseen during
pretraining. To fill this gap, we ask the following research questions: (1) How
does the number of pretraining languages influence zero-shot performance on
unseen target languages? (2) Does the answer to that question change with model
adaptation? (3) Do the findings for our first question change if the languages
used for pretraining are all related? Our experiments on pretraining with
related languages indicate that choosing a diverse set of languages is crucial.
Without model adaptation, surprisingly, increasing the number of pretraining
languages yields better results up to adding related languages, after which
performance plateaus. In contrast, with model adaptation via continued
pretraining, pretraining on a larger number of languages often gives further
improvement, suggesting that model adaptation is crucial to exploit additional
pretraining languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> XTREME-S: Evaluating Cross-lingual Speech Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Conneau, Ankur Bapna, Yu Zhang, Min Ma, Patrick von Platen, Anton Lozhkov, Colin Cherry, Ye Jia, Clara Rivera, Mihir Kale, Daan Van Esch, Vera Axelrod, Simran Khanuja, Jonathan H. Clark, Orhan Firat, <span class="highlight-author">Sebastian Ruder</span>, Jason Riesa, Melvin Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual
speech representations in many languages. XTREME-S covers four task families:
speech recognition, classification, speech-to-text translation and retrieval.
Covering 102 languages from 10+ language families, 3 different domains and 4
task families, XTREME-S aims to simplify multilingual speech representation
evaluation, as well as catalyze research in "universal" speech representation
learning. This paper describes the new benchmark and establishes the first
speech-only and speech-text baselines using XLS-R and mSLAM on all downstream
tasks. We motivate the design choices and detail how to use the benchmark.
Datasets and fine-tuning scripts are made easily accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewang Zhang, Yibin Zheng, Xinhui Li, Li Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. Both quantitative and qualitative evaluation results demonstrate
the effectiveness of WeSinger in terms of accuracy and naturalness, and
WeSinger achieves state-of-the-art performance on the public corpus Opencpop.
Some synthesized singing samples are available
online\footnote{https://zzw922cn.github.io/wesinger}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to InterSpeech2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Programming Language Agnostic Mining of Code and Language Pairs with
  Sequence Labeling Based Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changran Hu, Akshara Reddi Methukupalli, Yutong Zhou, Chen Wu, Yubo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mining aligned natural language (NL) and programming language (PL) pairs is a
critical task to NL-PL understanding. Existing methods applied specialized
hand-crafted features or separately-trained models for each PL. However, they
usually suffered from low transferability across multiple PLs, especially for
niche PLs with less annotated data. Fortunately, a Stack Overflow answer post
is essentially a sequence of text and code blocks and its global textual
context can provide PL-agnostic supplementary information. In this paper, we
propose a Sequence Labeling based Question Answering (SLQA) method to mine
NL-PL pairs in a PL-agnostic manner. In particular, we propose to apply the BIO
tagging scheme instead of the conventional binary scheme to mine the code
solutions which are often composed of multiple blocks of a post. Experiments on
current single-PL single-block benchmarks and a manually-labeled cross-PL
multi-block benchmark prove the effectiveness and transferability of SLQA. We
further present a parallel NL-PL corpus named Lang2Code automatically mined
with SLQA, which contains about 1.4M pairs on 6 PLs. Under statistical analysis
and downstream evaluation, we demonstrate that Lang2Code is a large-scale
high-quality data resource for further NL-PL research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Academic Resource Text Level Multi-label Classification based on
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Yawen Li, Ang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical multi-label academic text classification (HMTC) is to assign
academic texts into a hierarchically structured labeling system. We propose an
attention-based hierarchical multi-label classification algorithm of academic
texts (AHMCA) by integrating features such as text, keywords, and hierarchical
structure, the academic documents are classified into the most relevant
categories. We utilize word2vec and BiLSTM to obtain embedding and latent
vector representations of text, keywords, and hierarchies. We use hierarchical
attention mechanism to capture the associations between keywords, label
hierarchies, and text word vectors to generate hierarchical-specific document
embedding vectors to replace the original text embeddings in HMCN-F. The
experimental results on the academic text dataset demonstrate the effectiveness
of the AHMCA algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long
  Document Summarization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyang Cao, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document structure is critical for efficient information consumption.
However, it is challenging to encode it efficiently into the modern Transformer
architecture. In this work, we present HIBRIDS, which injects Hierarchical
Biases foR Incorporating Document Structure into the calculation of attention
scores. We further present a new task, hierarchical question-summary
generation, for summarizing salient content in the source document into a
hierarchy of questions and summaries, where each follow-up question inquires
about the content of its parent question-summary pair. We also annotate a new
dataset with 6,153 question-summary hierarchies labeled on long government
reports. Experiment results show that our model produces better
question-summary hierarchies than comparisons on both hierarchy quality and
content coverage, a finding also echoed by human judges. Additionally, our
model improves the generation of long-form summaries from lengthy government
reports and Wikipedia articles, as measured by ROUGE scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intellectual Property Entity Recognition Method Based on <span class="highlight-title">Transformer</span>
  and Technological Word Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Wang, Junping Du, Yingxia Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patent texts contain a large amount of entity information. Through named
entity recognition, intellectual property entity information containing key
information can be extracted from it, helping researchers to understand the
patent content faster. Therefore, it is difficult for existing named entity
extraction methods to make full use of the semantic information at the word
level brought about by professional vocabulary changes. This paper proposes a
method for extracting intellectual property entities based on Transformer and
technical word information , and provides accurate word vector representation
in combination with the BERT language method. In the process of word vector
generation, the technical word information extracted by IDCNN is added to
improve the understanding of intellectual property entities Representation
ability. Finally, the Transformer encoder that introduces relative position
encoding is used to learn the deep semantic information of the text from the
sequence of word vectors, and realize entity label prediction. Experimental
results on public datasets and annotated patent datasets show that the method
improves the accuracy of entity recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span>ing-based Approach for Adversarial Example Generation and
  Robustness Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Yang, Pei Huang, Juan Cao, Jintao Li, Yun Lin, Jin Song Dong, Feifei Ma, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen the wide application of NLP models in crucial areas
such as finance, medical treatment, and news media, raising concerns of the
model robustness and vulnerabilities. In this paper, we propose a novel
prompt-based adversarial attack to compromise NLP models and robustness
enhancement technique. We first construct malicious prompts for each instance
and generate adversarial examples via mask-and-filling under the effect of a
malicious purpose. Our attack technique targets the inherent vulnerabilities of
NLP models, allowing us to generate samples even without interacting with the
victim NLP model, as long as it is based on pre-trained language models (PLMs).
Furthermore, we design a prompt-based adversarial training method to improve
the robustness of PLMs. As our training method does not actually generate
adversarial samples, it can be applied to large-scale training sets
efficiently. The experimental results show that our attack method can achieve a
high attack success rate with more diverse, fluent and natural adversarial
examples. In addition, our robustness enhancement method can significantly
improve the robustness of models to resist adversarial attacks. Our work
indicates that prompting paradigm has great potential in probing some
fundamental flaws of PLMs and fine-tuning them for downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Compression of Generative <span class="highlight-title">Pre-train</span>ed Language Models via Quantization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, <span class="highlight-author">Qun Liu</span>, Ping Luo, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Expert Guided Adversarial Augmentation For Improving
  Generalization in Named Entity Recognition <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Reich, Jiaao Chen, Aastha Agrawal, Yanzhe Zhang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) systems often demonstrate great performance on
in-distribution data, but perform poorly on examples drawn from a shifted
distribution. One way to evaluate the generalization ability of NER models is
to use adversarial examples, on which the specific variations associated with
named entities are rarely considered. To this end, we propose leveraging
expert-guided heuristics to change the entity tokens and their surrounding
contexts thereby altering their entity types as adversarial attacks. Using
expert-guided heuristics, we augmented the CoNLL 2003 test set and manually
annotated it to construct a high-quality challenging set. We found that
state-of-the-art NER systems trained on CoNLL 2003 training data drop
performance dramatically on our challenging set. By training on adversarial
augmented training examples and using mixup for regularization, we were able to
significantly improve the performance on the challenging set as well as improve
out-of-domain generalization which we evaluated by using OntoNotes data. We
have publicly released our dataset and code at
https://github.com/GT-SALT/Guided-Adversarial-Augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Language Model with Hypernym Class Prediction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Bai, Tong Wang, Alessandro Sordoni, Peng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-based language models (LMs) have been long devised to address context
sparsity in $n$-gram LMs. In this study, we revisit this approach in the
context of neural LMs. We hypothesize that class-based prediction leads to an
implicit context aggregation for similar words and thus can improve
generalization for rare words. We map words that have a common WordNet hypernym
to the same class and train large neural LMs by gradually annealing from
predicting the class to token prediction during training. Empirically, this
curriculum learning strategy consistently improves perplexity over various
large, highly-performant state-of-the-art Transformer-based models on two
datasets, WikiText-103 and Arxiv. Our analysis shows that the performance
improvement is achieved without sacrificing performance on rare words. Finally,
we document other attempts that failed to yield empirical gains, and discuss
future directions for the adoption of class-based LMs on a larger scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language modeling via stochastic processes <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rose E Wang, Esin Durmus, Noah Goodman, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models can generate high-quality short texts. However, they
often meander or are incoherent when generating longer texts. These issues
arise from the next-token-only language modeling objective. To address these
issues, we introduce Time Control (TC), a language model that implicitly plans
via a latent stochastic process. TC does this by learning a representation
which maps the dynamics of how text changes in a document to the dynamics of a
stochastic process of interest. Using this representation, the language model
can generate text by first implicitly generating a document plan via a
stochastic process, and then generating text that is consistent with this
latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a
variety of text domains, TC improves performance on text infilling and
discourse coherence. On long text generation settings, TC preserves the text
structure both in terms of ordering (up to +40% better) and text length
consistency (up to +17% better). Human evaluators also prefer TC's output 28.6%
more than the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR Oral 2022. Code:
  https://github.com/rosewang2008/language_modeling_via_stochastic_processes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information-theoretic Approach to <span class="highlight-title">Prompt</span> Engineering Without Ground
  Truth Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda, David Wingate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models derive substantial linguistic and factual
knowledge from the massive corpora on which they are trained, and prompt
engineering seeks to align these models to specific tasks. Unfortunately,
existing prompt engineering methods require significant amounts of labeled
data, access to model parameters, or both. We introduce a new method for
selecting prompt templates \textit{without labeled examples} and
\textit{without direct access to the model}. Specifically, over a set of
candidate templates, we choose the template that maximizes the mutual
information between the input and the corresponding model output. Across 8
datasets representing 7 distinct NLP tasks, we show that when a template has
high mutual information, it also has high accuracy on the task. On the largest
model, selecting prompts with our method gets 90\% of the way from the average
prompt accuracy to the best prompt accuracy and requires no ground truth
labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On The Robustness of Offensive Language Classifiers <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, Accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Speech Recognition Decoding via Layer Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Wullach, Shlomo E. Chazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently proposed speech recognition systems are designed to predict using
representations generated by their top layers, employing greedy decoding which
isolates each timestep from the rest of the sequence. Aiming for improved
performance, a beam search algorithm is frequently utilized and a language
model is incorporated to assist with ranking the top candidates. In this work,
we experiment with several speech recognition models and find that logits
predicted using the top layers may hamper beam search from achieving optimal
results. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield
highly confident predictions, and hypothesize that the predictions are based on
local information and may not take full advantage of the information encoded in
intermediate layers. To this end, we perform a layer analysis to reveal and
visualize how predictions evolve throughout the inference flow. We then propose
a prediction method that aggregates the top M layers, potentially leveraging
useful information encoded in intermediate layers and relaxing model
confidence. We showcase the effectiveness of our approach via beam search
decoding, conducting our experiments on Librispeech test and dev sets and
achieving WER, and CER reduction of up to 10% and 22%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Change that Matters in Discourse Parsing: Estimating the Impact of
  Domain Shift on Parser Error 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Atwell, Anthony Sicilia, Seong Jae Hwang, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discourse analysis allows us to attain inferences of a text document that
extend beyond the sentence-level. The current performance of discourse models
is very low on texts outside of the training distribution's coverage,
diminishing the practical utility of existing models. There is need for a
measure that can inform us to what extent our model generalizes from the
training to the test sample when these samples may be drawn from distinct
distributions. While this can be estimated via distribution shift, we argue
that this does not directly correlate with change in the observed error of a
classifier (i.e. error-gap). Thus, we propose to use a statistic from the
theoretical domain adaptation literature which can be directly tied to
error-gap. We study the bias of this statistic as an estimator of error-gap
both theoretically and through a large-scale empirical study of over 2400
experiments on 6 discourse datasets from domains including, but not limited to:
news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not
only motivate our proposal and help us to understand its limitations, but also
provide insight on the properties of discourse models and datasets which
improve performance in domain adaptation. For instance, we find that non-news
datasets are slightly easier to transfer to than news datasets when the
training and test sets are very different. Our code and an associated Python
package are available to allow practitioners to make more informed model and
dataset choices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Classification of Long Documents Using <span class="highlight-title">Transformer</span>s <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunji Hayley Park, Yogarshi Vyas, Kashif Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several methods have been proposed for classifying long textual documents
using Transformers. However, there is a lack of consensus on a benchmark to
enable a fair comparison among different approaches. In this paper, we provide
a comprehensive evaluation of the relative efficacy measured against various
baselines and diverse datasets -- both in terms of accuracy as well as time and
space overheads. Our datasets cover binary, multi-class, and multi-label
classification tasks and represent various ways information is organized in a
long text (e.g. information that is critical to making the classification
decision is at the beginning or towards the end of the document). Our results
show that more complex models often fail to outperform simple baselines and
yield inconsistent performance across datasets. These findings emphasize the
need for future studies to consider comprehensive baselines and datasets that
better represent the task of long document classification to develop robust
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022; 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DQ-BART: Efficient Sequence-to-Sequence Model via Joint <span class="highlight-title">Distillation</span> and
  Quantization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang, <span class="highlight-author">Dan Roth</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Inductive Bias of In-Context Learning: Rethinking <span class="highlight-title">Pretrain</span>ing
  Example Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose "kNN-Pretraining": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for pretraining
example design indicates new training schemes for self-improving
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base
  Question Answering <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.08678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.08678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, <span class="highlight-author">Caiming Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing KBQA approaches, despite achieving strong performance on i.i.d. test
data, often struggle in generalizing to questions involving unseen KB schema
items. Prior ranking-based approaches have shown some success in
generalization, but suffer from the coverage issue. We present RnG-KBQA, a
Rank-and-Generate approach for KBQA, which remedies the coverage issue with a
generation model while preserving a strong generalization capability. Our
approach first uses a contrastive ranker to rank a set of candidate logical
forms obtained by searching over the knowledge graph. It then introduces a
tailored generation model conditioned on the question and the top-ranked
candidates to compose the final logical form. We achieve new state-of-the-art
results on GrailQA and WebQSP datasets. In particular, our method surpasses the
prior state-of-the-art by a large margin on the GrailQA leaderboard. In
addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP
benchmark, even including the ones that use the oracle entity linking. The
experimental results demonstrate the effectiveness of the interplay between
ranking and generation, which leads to the superior performance of our proposed
approach across all settings with especially strong improvements in zero-shot
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Temporal Knowledge Embeddings with Contextualized Language
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Han, Ruotong Liao, Beiyan Liu, Yao Zhang, Zifeng Ding, Heinz Köppl, Hinrich Schütze, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emerging research effort to integrate structured and unstructured
knowledge, many approaches incorporate factual knowledge into pre-trained
language models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP
tasks. However, (1) they only consider static factual knowledge, but knowledge
graphs (KGs) also contain temporal facts or events indicating evolutionary
relationships among entities at different timestamps. (2) PLMs cannot be
directly applied to many KG tasks, such as temporal KG completion.
  In this paper, we focus on \textbf{e}nhancing temporal knowledge embeddings
with \textbf{co}ntextualized \textbf{la}nguage representations (ECOLA). We
align structured knowledge contained in temporal knowledge graphs with their
textual descriptions extracted from news articles and propose a novel
knowledge-text prediction task to inject the abundant information from
descriptions into temporal knowledge embeddings. ECOLA jointly optimizes the
knowledge-text prediction objective and the temporal knowledge embeddings,
which can simultaneously take full advantage of textual and knowledge
information. For training ECOLA, we introduce three temporal KG datasets with
aligned textual descriptions. Experimental results on the temporal knowledge
graph completion task show that ECOLA outperforms state-of-the-art temporal KG
models by a large margin. The proposed datasets can serve as new temporal KG
benchmarks and facilitate future research on structured and unstructured
knowledge integration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning When to Translate for Streaming Speech <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.07368v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.07368v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Dong, Yaoming Zhu, Mingxuan Wang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to find proper moments to generate partial sentence translation given a
streaming speech input? Existing approaches waiting-and-translating for a fixed
duration often break the acoustic units in speech, since the boundaries between
acoustic units in speech are not even. In this paper, we propose MoSST, a
simple yet effective method for translating streaming speech content. Given a
usually long speech sequence, we develop an efficient monotonic segmentation
module inside an encoder-decoder model to accumulate acoustic information
incrementally and detect proper speech unit boundaries for the input in speech
translation task. Experiments on multiple translation directions of the MuST-C
dataset show that MoSST outperforms existing methods and achieves the best
trade-off between translation quality (BLEU) and latency. Our code is available
at https://github.com/dqqcasia/mosst.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locating and Editing Factual Knowledge in <span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the mechanisms underlying factual knowledge recall in
autoregressive transformer language models. First, we develop a causal
intervention for identifying neuron activations capable of altering a model's
factual predictions. Within large GPT-style models, this reveals two distinct
sets of neurons that we hypothesize correspond to knowing an abstract fact and
saying a concrete word, respectively. This insight inspires the development of
ROME, a novel method for editing facts stored in model weights. For evaluation,
we assemble CounterFact, a dataset of over twenty thousand counterfactuals and
tools to facilitate sensitive measurements of knowledge editing. Using
CounterFact, we confirm the distinction between saying and knowing neurons, and
we find that ROME achieves state-of-the-art performance in knowledge editing
compared to other methods. An interactive demo notebook, full code
implementation, and the dataset are available at https://rome.baulab.info/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 22 figures. Code and data at https://rome.baulab.info/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models
  Robust with Little Cost <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper accepted by ACL main conference. 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> EdgeFormer: A Parameter-Efficient <span class="highlight-title">Transformer</span> for On-Device Seq2seq
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Ge, <span class="highlight-author">Furu Wei</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EdgeFormer -- a parameter-efficient Transformer of the
encoder-decoder architecture for on-device seq2seq generation, which is
customized under strict computation and memory constraints. EdgeFormer proposes
two novel principles for cost-effective parameterization and further enhance
the model with efficient layer adaptation. We conduct extensive experiments on
two practical on-device seq2seq tasks: Machine Translation and Grammatical
Error Correction, and show that EdgeFormer can effectively outperform previous
parameter-efficient Transformer baselines and achieve very competitive results
with knowledge distillation under both the computation and memory constraints.
Moreover, we release the pretrained EdgeFormer -- the first publicly available
pretrained model that can be easily fine-tuned for English seq2seq tasks with
strong results, largely facilitating on-device seq2seq generation in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CINS: Comprehensive Instruction for Few-shot Learning in <span class="highlight-title">Task-oriented</span>
  <span class="highlight-title">Dialog</span> Systems <span class="chip">AAAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04645v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04645v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Mi, Yitong Li, Yasheng Wang, Xin Jiang, <span class="highlight-author">Qun Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema (definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5) is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Five Psycholinguistic Characteristics for Better Interaction with Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.09692v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.09692v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanja Štajner, Seren Yenikent, Marc Franco-Salvador
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When two people pay attention to each other and are interested in what the
other has to say or write, they almost instantly adapt their writing/speaking
style to match the other. For a successful interaction with a user, chatbots
and dialogue systems should be able to do the same. We propose a framework
consisting of five psycholinguistic textual characteristics for better
human-computer interaction. We describe the annotation processes used for
collecting the data, and benchmark five binary classification tasks,
experimenting with different training sizes and model architectures. The best
architectures noticeably outperform several baselines and achieve
macro-averaged F$_1$-scores between 72\% and 96\% depending on the language and
the task. The proposed framework proved to be fairly easy to model for various
languages even with small amount of manually annotated data if right
architectures are used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Visual-<span class="highlight-title">Prompt</span> Temporal Answering Grounding in Medical
  Instructional Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06667v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06667v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Li, Yixuan Weng, Bin Sun, Shutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show that the proposed VPTSL
outperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a
large margin, which demonstrates the effectiveness of visual prompt and the
text span predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Just Rank: Rethinking Evaluation with Word and Sentence Similarities <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, C. -C. Jay Kuo, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word and sentence embeddings are useful feature representations in natural
language processing. However, intrinsic evaluation for embeddings lags far
behind, and there has been no significant update since the past decade. Word
and sentence similarity tasks have become the de facto evaluation method. It
leads models to overfit to such evaluations, negatively impacting embedding
models' development. This paper first points out the problems using semantic
similarity as the gold standard for word and sentence embedding evaluations.
Further, we propose a new intrinsic evaluation method called EvalRank, which
shows a much stronger correlation with downstream tasks. Extensive experiments
are conducted based on 60+ models and popular datasets to certify our
judgments. Finally, the practical evaluation toolkit is released for future
benchmarking purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Main Conference for ACL 2022. Code:
  https://github.com/BinWang28/EvalRank-Embedding-Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.10382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.10382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training machine learning models in a meaningful order, from the easy samples
to the hard ones, using curriculum learning can provide performance
improvements over the standard training approach based on random data
shuffling, without any additional computational costs. Curriculum learning
strategies have been successfully employed in all areas of machine learning, in
a wide range of tasks. However, the necessity of finding a way to rank the
samples from easy to hard, as well as the right pacing function for introducing
more difficult data can limit the usage of the curriculum approaches. In this
survey, we show how these limits have been tackled in the literature, and we
present different curriculum learning instantiations for various tasks in
machine learning. We construct a multi-perspective taxonomy of curriculum
learning approaches by hand, considering various classification criteria. We
further build a hierarchical tree of curriculum learning methods using an
agglomerative clustering algorithm, linking the discovered clusters with our
taxonomy. At the end, we provide some interesting directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented
  Structural Neural Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanning Gao, Lingfei Wu, Hongyun Zhang, Zhihua Wei, Po Hu, Fangli Xu, Bo Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Data Gap between Training and Inference for Unsupervised
  Neural Machine Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei He, Xing Wang, Rui Wang, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Back-translation is a critical component of Unsupervised Neural Machine
Translation (UNMT), which generates pseudo parallel data from target
monolingual data. A UNMT model is trained on the pseudo parallel data with
translated source, and translates natural source sentences in inference. The
source discrepancy between training and inference hinders the translation
performance of UNMT models. By carefully designing experiments, we identify two
representative characteristics of the data gap in source: (1) style gap (i.e.,
translated vs. natural text style) that leads to poor generalization
capability; (2) content gap that induces the model to produce hallucination
content biased towards the target language. To narrow the data gap, we propose
an online self-training approach, which simultaneously uses the pseudo parallel
data {natural source, translated target} to mimic the inference scenario.
Experimental results on several widely-used language pairs show that our
approach outperforms two strong baselines (XLM and MASS) by remedying the style
and content gaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in
  Practice <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Grivas, Nikolay Bogoychev, Adam Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifiers in natural language processing (NLP) often have a large number of
output classes. For example, neural language models (LMs) and machine
translation (MT) models both predict tokens from a vocabulary of thousands. The
Softmax output layer of these models typically receives as input a dense
feature representation, which has much lower dimensionality than the output. In
theory, the result is some words may be impossible to be predicted via argmax,
irrespective of input features, and empirically, there is evidence this happens
in small language models. In this paper we ask whether it can happen in
practical large language models and translation models. To do so, we develop
algorithms to detect such \emph{unargmaxable} tokens in public models. We find
that 13 out of 150 models do indeed have such tokens; however, they are very
infrequent and unlikely to impact model quality. We release our code so that
others can inspect their models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of conference paper accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Hierarchical Sketch Induction for Paraphrase Generation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Hosking, <span class="highlight-author">Hao Tan</span>g, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a generative model of paraphrase generation, that encourages
syntactic diversity by conditioning on an explicit syntactic sketch. We
introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE),
a method for learning decompositions of dense encodings as a sequence of
discrete latent variables that make iterative refinements of increasing
granularity. This hierarchy of codes is learned through end-to-end training,
and represents fine-to-coarse grained information about the input. We use
HRQ-VAE to encode the syntactic form of an input sentence as a path through the
hierarchy, allowing us to more easily predict syntactic sketches at test time.
Extensive experiments, including a human evaluation, confirm that HRQ-VAE
learns a hierarchical representation of the input space, and generates
paraphrases of higher quality than previous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Large Scale Substitution-based Word Sense Induction <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Eyal, Shoval Sadde, Hillel Taub-Tabib, <span class="highlight-author">Yoav Goldberg</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a word-sense induction method based on pre-trained masked language
models (MLMs), which can cheaply scale to large vocabularies and large corpora.
The result is a corpus which is sense-tagged according to a corpus-derived
sense inventory and where each sense is associated with indicative words.
Evaluation on English Wikipedia that was sense-tagged using our method shows
that both the induced senses, and the per-instance sense assignment, are of
high quality even compared to WSD methods, such as Babelfy. Furthermore, by
training a static word embeddings algorithm on the sense-tagged corpus, we
obtain high-quality static senseful embeddings. These outperform existing
senseful embeddings methods on the WiC dataset and on a new outlier detection
dataset we developed. The data driven nature of the algorithm allows to induce
corpora-specific senses, which may not appear in standard sense inventories, as
we demonstrate using a case study on the scientific domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphosyntactic Tagging with <span class="highlight-title">Pre-train</span>ed Language Models for Arabic and
  its Dialects <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Go Inoue, Salam Khalifa, Nizar Habash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present state-of-the-art results on morphosyntactic tagging across
different varieties of Arabic using fine-tuned pre-trained transformer language
models. Our models consistently outperform existing systems in Modern Standard
Arabic and all the Arabic dialects we study, achieving 2.6% absolute
improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8%
in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training
setups for fine-tuning pre-trained transformer language models, including
training data size, the use of external linguistic resources, and the use of
annotated data from other dialects in a low-resource scenario. Our results show
that strategic fine-tuning using datasets from other high-resource dialects is
beneficial for a low-resource dialect. Additionally, we show that high-quality
morphological analyzers as external linguistic resources are beneficial
especially in low-resource settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Linguistic Information For Logical Inference In <span class="highlight-title">Pre-train</span>ed
  Language Models <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeming Chen, Qiyue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in pre-trained language models has led to a surge of impressive
results on downstream tasks for natural language understanding. Recent work on
probing pre-trained language models uncovered a wide range of linguistic
properties encoded in their contextualized representations. However, it is
unclear whether they encode semantic knowledge that is crucial to symbolic
inference methods. We propose a methodology for probing linguistic information
for logical inference in pre-trained language model representations. Our
probing datasets cover a list of linguistic phenomena required by major
symbolic inference systems. We find that (i) pre-trained language models do
encode several types of linguistic information for inference, but there are
also some types of information that are weakly encoded, (ii) language models
can effectively learn missing linguistic information through fine-tuning.
Overall, our findings provide insights into which aspects of linguistic
information for logical inference do language models and their pre-training
procedures capture. Moreover, we have demonstrated language models' potential
as semantic and background knowledge bases for supporting symbolic inference
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Rethinking and Refining the Distinct Metric <span class="chip">ACL2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si<span class="highlight-author">yang Liu</span>, Sahand Sabour, Yinhe Zheng, Pei Ke, Xiaoyan Zhu, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, to be published at ACL2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Simultaneous Machine Translation with Mixture-of-Experts
  Wait-k Policy <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2021 (main conference). 12 pages, 7 figures, 4
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Structure Learning via Graph Neural Networks for Inductive
  Document Classification <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhua Piao, Sangseon Lee, Dohoon Lee, Sun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, graph neural networks (GNNs) have been widely used for document
classification. However, most existing methods are based on static word
co-occurrence graphs without sentence-level information, which poses three
challenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual
dependency. To address these challenges, we propose a novel GNN-based sparse
structure learning model for inductive document classification. Specifically, a
document-level graph is initially generated by a disjoint union of
sentence-level word co-occurrence graphs. Our model collects a set of trainable
edges connecting disjoint words between sentences and employs structure
learning to sparsely select edges with dynamic contextual dependencies. Graphs
with sparse structures can jointly exploit local and global contextual
information in documents through GNNs. For inductive learning, the refined
document graph is further fed into a general readout function for graph-level
classification and optimization in an end-to-end manner. Extensive experiments
on several real-world datasets demonstrate that the proposed model outperforms
most state-of-the-art results, and reveal the necessity to learn sparse
structures for each document.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking <span class="highlight-title">Self-Supervision</span> Objectives for Generalizable Coherence
  Modeling <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prathyusha Jwalapuram, Shafiq Joty, Xiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the claims of improved text generation quality across various
pre-trained neural models, we consider the coherence evaluation of machine
generated text to be one of the principal applications of coherence models that
needs to be investigated. Prior work in neural coherence modeling has primarily
focused on devising new architectures for solving the permuted document task.
We instead use a basic model architecture and show significant improvements
over state of the art within the same training regime. We then design a harder
self-supervision objective by increasing the ratio of negative samples within a
contrastive learning setup, and enhance the model further through automatic
hard negative mining coupled with a large global negative queue encoded by a
momentum encoder. We show empirically that increasing the density of negative
samples improves the basic model, and using a global negative queue further
improves and stabilizes the model while training with hard negative samples. We
evaluate the coherence model on task-independent test sets that resemble
real-world applications and show significant improvements in coherence
evaluations of downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> <span class="highlight-title">Bert</span>GCN: Transductive Text Classification by Combining GCN and <span class="highlight-title">BERT</span> <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.05727v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.05727v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han, Kun Kuang, <span class="highlight-author">Jiwei Li</span>, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose BertGCN, a model that combines large scale
pretraining and transductive learning for text classification. BertGCN
constructs a heterogeneous graph over the dataset and represents documents as
nodes using BERT representations. By jointly training the BERT and GCN modules
within BertGCN, the proposed model is able to leverage the advantages of both
worlds: large-scale pretraining which takes the advantage of the massive amount
of raw data and transductive learning which jointly learns representations for
both training data and unlabeled test data by propagating label influence
through graph convolution. Experiments show that BertGCN achieves SOTA
performances on a wide range of text classification datasets. Code is available
at https://github.com/ZeroRin/BertGCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of ACL 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chart-to-Text: A Large-Scale Benchmark for Chart Summarization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2022 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Automated Quality Evaluation Framework of Psychotherapy <span class="highlight-title">Conversation</span>s
  with Local Quality Estimates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.07922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.07922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohao Chen, Nikolaos Flemotomos, Karan Singla, Torrey A. Creed, David C. Atkins, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based computational approaches for assessing the quality of
psychotherapy are being developed to support quality assurance and clinical
training. However, due to the long durations of typical conversation based
therapy sessions, and due to limited annotated modeling resources,
computational methods largely rely on frequency-based lexical features or
dialogue acts to assess the overall session level characteristics. In this
work, we propose a hierarchical framework to automatically evaluate the quality
of transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the
richly dynamic nature of the spoken dialog within a talk therapy session, to
evaluate the overall session level quality, we propose to consider modeling it
as a function of local variations across the interaction. To implement that
empirically, we divide each psychotherapy session into conversation segments
and initialize the segment-level qualities with the session-level scores.
First, we produce segment embeddings by fine-tuning a BERT-based model, and
predict segment-level (local) quality scores. These embeddings are used as the
lower-level input to a Bidirectional LSTM-based neural network to predict the
session-level (global) quality estimates. In particular, we model the global
quality as a linear function of the local quality scores, which allows us to
update the segment-level quality estimates based on the session-level quality
prediction. These newly estimated segment-level scores benefit the BERT
fine-tuning process, which in turn results in better segment embeddings. We
evaluate the proposed framework on automatically derived transcriptions from
real-world CBT clinical recordings to predict session-level behavior codes. The
results indicate that our approach leads to improved evaluation accuracy for
most codes when used for both regression and classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Computer Speech & Language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trakhtenbrot's Theorem in Coq: Finite Model Theory through the
  Constructive Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.14445v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.14445v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Kirst, Dominique Larchey-Wendling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study finite first-order satisfiability (FSAT) in the constructive setting
of dependent type theory. Employing synthetic accounts of enumerability and
decidability, we give a full classification of FSAT depending on the
first-order signature of non-logical symbols. On the one hand, our development
focuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as
the signature contains an at least binary relation symbol. Our proof proceeds
by a many-one reduction chain starting from the Post correspondence problem. On
the other hand, we establish the decidability of FSAT for monadic first-order
logic, i.e. where the signature only contains at most unary function and
relation symbols, as well as the enumerability of FSAT for arbitrary enumerable
signatures. To showcase an application of Trakhtenbrot's theorem, we continue
our reduction chain with a many-one reduction from FSAT to separation logic.
All our results are mechanised in the framework of a growing Coq library of
synthetic undecidability proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, extended version of the IJCAR 2020 paper. arXiv admin note:
  substantial text overlap with arXiv:2004.07390</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Trade-offs of Domain Adaptation for Neural Language Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.10274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.10274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grangier, Dan Iter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work connects language model adaptation with concepts of machine
learning theory. We consider a training setup with a large out-of-domain set
and a small in-domain set. We derive how the benefit of training a model on
either set depends on the size of the sets and the distance between their
underlying distributions. We analyze how out-of-domain pre-training before
in-domain fine-tuning achieves better generalization than either solution
independently. Finally, we present how adaptation techniques based on data
selection, such as importance sampling, intelligent data selection and
influence functions, can be presented in a common framework which highlights
their similarity and also their subtle differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Annual Meeting of the Association for
  Computational Linguistics (ACL), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Ditch the Gold Standard: Re-evaluating <span class="highlight-title">Conversation</span>al Question Answering <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huihan Li, Tianyu Gao, Manan Goenka, <span class="highlight-author">Danqi Chen</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational question answering aims to provide natural-language answers to
users in information-seeking conversations. Existing conversational QA
benchmarks compare models with pre-collected human-human conversations, using
ground-truth answers provided in conversational history. It remains unclear
whether we can rely on this static evaluation for model development and whether
current systems can well generalize to real-world human-machine conversations.
In this work, we conduct the first large-scale human evaluation of
state-of-the-art conversational QA systems, where human evaluators converse
with models and judge the correctness of their answers. We find that the
distribution of human machine conversations differs drastically from that of
human-human conversations, and there is a disagreement between human and
gold-history evaluation in terms of model ranking. We further investigate how
to improve automatic evaluations, and propose a question rewriting mechanism
based on predicted history, which better correlates with human judgments.
Finally, we analyze the impact of various modeling strategies and discuss
future directions towards building better conversational question answering
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022; The dataset and code are available at
  https://github.com/princeton-nlp/EvalConvQA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Multi<span class="highlight-title">BERT</span>s: <span class="highlight-title">BERT</span> Reproductions for Robustness Analysis <span class="chip">ICLR'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.16163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.16163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Sellam, Steve Yadlowsky, Jason Wei, Naomi Saphra, Alexander D'Amour, Tal Linzen, Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das, Ian Tenney, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Experiments with pre-trained models such as BERT are often based on a single
checkpoint. While the conclusions drawn apply to the artifact tested in the
experiment (i.e., the particular instance of the model), it is not always clear
whether they hold for the more general procedure which includes the
architecture, training data, initialization scheme, and loss function. Recent
work has shown that repeating the pre-training process can lead to
substantially different performance, suggesting that an alternate strategy is
needed to make principled statements about procedures. To enable researchers to
draw more robust conclusions, we introduce the MultiBERTs, a set of 25
BERT-Base checkpoints, trained with similar hyper-parameters as the original
BERT model but differing in random weight initialization and shuffling of
training data. We also define the Multi-Bootstrap, a non-parametric bootstrap
method for statistical inference designed for settings where there are multiple
pre-trained models and limited test data. To illustrate our approach, we
present a case study of gender bias in coreference resolution, in which the
Multi-Bootstrap lets us measure effects that may not be detected with a single
checkpoint. We release our models and statistical library along with an
additional set of 140 intermediate checkpoints captured during pre-training to
facilitate research on learning dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR'22. Checkpoints and example analyses:
  http://goo.gle/multiberts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Direct speech-to-speech translation with discrete units <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.05604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.05604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ann Lee, Peng-Jen Chen, Changhan Wang, <span class="highlight-author">Jiatao Gu</span>, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a direct speech-to-speech translation (S2ST) model that translates
speech from one language to speech in another language without relying on
intermediate text generation. We tackle the problem by first applying a
self-supervised discrete speech encoder on the target speech and then training
a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the
discrete representations of the target speech. When target text transcripts are
available, we design a joint speech and text training framework that enables
the model to generate dual modality output (speech and text) simultaneously in
the same inference pass. Experiments on the Fisher Spanish-English dataset show
that the proposed framework yields improvement of 6.7 BLEU compared with a
baseline direct S2ST model that predicts spectrogram features. When trained
without any text transcripts, our model performance is comparable to models
that predict spectrograms and are trained with text supervision, showing the
potential of our system for translation between unwritten languages. Audio
samples are available at
https://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022 (long paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open Natural Language Processing Development Framework for EHR-based
  Clinical Research: A case demonstration using the National COVID Cohort
  Collaborative (N3C) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10780v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10780v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Liu, Andrew Wen, Liwei Wang, Huan He, Sunyang Fu, Robert Miller, Andrew Williams, Daniel Harris, Ramakanth Kavuluru, Mei Liu, Noor Abu-el-rub, Dalton Schutte, Rui Zhang, Masoud Rouhizadeh, John D. Osborne, Yongqun He, Umit Topaloglu, Stephanie S Hong, Joel H Saltz, Thomas Schaffter, Emily Pfaff, Christopher G. Chute, Tim Duong, Melissa A. Haendel, Rafael Fuentes, Peter Szolovits, Hua Xu, Hongfang Liu, National COVID Cohort Collaborative, Natural Language Processing,  Subgroup, National COVID Cohort Collaborative
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While we pay attention to the latest advances in clinical natural language
processing (NLP), we can notice some resistance in the clinical and
translational research community to adopt NLP models due to limited
transparency, interpretability, and usability. In this study, we proposed an
open natural language processing development framework. We evaluated it through
the implementation of NLP algorithms for the National COVID Cohort
Collaborative (N3C). Based on the interests in information extraction from
COVID-19 related clinical notes, our work includes 1) an open data annotation
process using COVID-19 signs and symptoms as the use case, 2) a
community-driven ruleset composing platform, and 3) a synthetic text data
generation workflow to generate texts for information extraction tasks without
involving human subjects. The corpora were derived from texts from three
different institutions (Mayo Clinic, University of Kentucky, University of
Minnesota). The gold standard annotations were tested with a single
institution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,
and 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,
respectively. The study as a consortium effort of the N3C NLP subgroup
demonstrates the feasibility of creating a federated NLP algorithm development
and benchmarking platform to enhance multi-institution clinical NLP study and
adoption. Although we use COVID-19 as a use case in this effort, our framework
is general enough to be applied to other domains of interest in clinical NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update on contents</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentive Temporal Pooling for Conformer-based Streaming Language
  Identification in Long-form Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12163v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12163v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Wang, Yang Yu, Jason Pelecanos, Yiling Huang, Ignacio Lopez Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel language identification system based on
conformer layers. We propose an attentive temporal pooling mechanism to allow
the model to carry information in long-form audio via a recurrent form, such
that the inference can be performed in a streaming fashion. Additionally, a
simple domain adaptation mechanism is introduced to allow adapting an existing
language identification model to a new domain where the prior language
distribution is different. We perform a comparative study of different model
topologies under different constraints of model size, and find that
conformer-base models outperform LSTM and transformer based models. Our
experiments also show that attentive temporal pooling and domain adaptation
significantly improve the model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Fast and Slow: Scene Decomposition via Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of segmenting scenes into constituent entities, i.e.
underlying objects and their parts. Current supervised visual detectors though
impressive within their training distribution, often fail to segment
out-of-distribution scenes into their constituent entities. Recent slot-centric
generative models break such dependence on supervision, by attempting to
segment scenes into entities unsupervised, by reconstructing pixels. However,
they have been restricted thus far to toy scenes as they suffer from a
reconstruction-segmentation trade-off: as the entity bottleneck gets wider,
reconstruction improves but then the segmentation collapses. We propose
GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue with two
ingredients: i) curriculum training in the form of primitives, often missing
from current generative models and, ii) test-time adaptation per scene through
gradient descent on the reconstruction objective, what we call slow inference,
missing from current feed-forward detectors. We show the proposed curriculum
suffices to break the reconstruction-segmentation trade-off, and slow inference
greatly improves segmentation in out-of-distribution scenes. We evaluate
GFS-Nets in 3D and 2D scene segmentation benchmarks of PartNet, CLEVR, Room
Diverse++, and show large ( 50%) performance improvements against SOTA
supervised feed-forward detectors and unsupervised object discovery methods
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://mihirp1998.github.io/project_pages/gfsnets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Model Prediction for Tracking <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Mayer, Martin Danelljan, Goutam Bhat, Matthieu Paul, Danda Pani Paudel, Fisher Yu, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization based tracking methods have been widely successful by
integrating a target model prediction module, providing effective global
reasoning by minimizing an objective function. While this inductive bias
integrates valuable domain knowledge, it limits the expressivity of the
tracking network. In this work, we therefore propose a tracker architecture
employing a Transformer-based model prediction module. Transformers capture
global relations with little inductive bias, allowing it to learn the
prediction of more powerful target models. We further extend the model
predictor to estimate a second set of weights that are applied for accurate
bounding box regression. The resulting tracker relies on training and on test
frame information in order to predict all weights transductively. We train the
proposed tracker end-to-end and validate its performance by conducting
comprehensive experiments on multiple tracking datasets. Our tracker sets a new
state of the art on three benchmarks, achieving an AUC of 68.5% on the
challenging LaSOT dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022. The code and trained models are available at
  https://github.com/visionml/pytracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Visual Tracking by Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Paul, Martin Danelljan, Christoph Mayer, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the target extent poses a fundamental challenge in visual object
tracking. Typically, trackers are box-centric and fully rely on a bounding box
to define the target in the scene. In practice, objects often have complex
shapes and are not aligned with the image axis. In these cases, bounding boxes
do not provide an accurate description of the target and often contain a
majority of background pixels. We propose a segmentation-centric tracking
pipeline that not only produces a highly accurate segmentation mask, but also
works internally with segmentation masks instead of bounding boxes. Thus, our
tracker is able to better learn a target representation that clearly
differentiates the target in the scene from background content. In order to
achieve the necessary robustness for the challenging tracking scenario, we
propose a separate instance localization component that is used to condition
the segmentation decoder when producing the output mask. We infer a bounding
box from the segmentation mask and validate our tracker on challenging tracking
datasets and achieve the new state of the art on LaSOT with a success AUC score
of 69.7%. Since fully evaluating the predicted masks on tracking datasets is
not possible due to the missing mask annotations, we further validate our
segmentation quality on two popular video object segmentation datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Discrimination for <span class="highlight-title">Self-Supervised</span> Learning on Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Liu, Mu Cai, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked autoencoding has achieved great success for self-supervised learning
in the image and language domains. However, mask based pretraining has yet to
show benefits for point cloud understanding, likely due to standard backbones
like PointNet being unable to properly handle the training versus testing
distribution mismatch introduced by masking during training. In this paper, we
bridge this gap by proposing a discriminative mask pretraining Transformer
framework, MaskPoint}, for point clouds. Our key idea is to represent the point
cloud as discrete occupancy values (1 if part of the point cloud; 0 if not),
and perform simple binary classification between masked object points and
sampled noise points as the proxy task. In this way, our approach is robust to
the point sampling variance in point clouds, and facilitates learning rich
representations. We evaluate our pretrained models across several downstream
tasks, including 3D shape classification, segmentation, and real-word object
detection, and demonstrate state-of-the-art results while achieving a
significant pretraining speedup (e.g., 4.1x on ScanNet) compared to the prior
state-of-the-art Transformer baseline. Code will be publicly available at
https://github.com/haotian-liu/MaskPoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPoseNet: Direct Differentiable Camera Pose Estimation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chethan M. Parameshwara, Gokul Hari, Cornelia Fermüller, Nitin J. Sanket, Yiannis Aloimonos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep neural network approaches for camera pose estimation rely on
scene structure for 3D motion estimation, but this decreases the robustness and
thereby makes cross-dataset generalization difficult. In contrast, classical
approaches to structure from motion estimate 3D motion utilizing optical flow
and then compute depth. Their accuracy, however, depends strongly on the
quality of the optical flow. To avoid this issue, direct methods have been
proposed, which separate 3D motion from depth estimation but compute 3D motion
using only image gradients in the form of normal flow. In this paper, we
introduce a network NFlowNet, for normal flow estimation which is used to
enforce robust and direct constraints. In particular, normal flow is used to
estimate relative camera pose based on the cheirality (depth positivity)
constraint. We achieve this by formulating the optimization problem as a
differentiable cheirality layer, which allows for end-to-end learning of camera
pose. We perform extensive qualitative and quantitative evaluation of the
proposed DiffPoseNet's sensitivity to noise and its generalization across
datasets. We compare our approach to existing state-of-the-art methods on
KITTI, TartanAir, and TUM-RGBD datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Class Conditional GANs with Channel Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqing He, Zhiyi Zhang, Jiapeng Zhu, Yujun Shen, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the mechanism of generative adversarial networks (GANs) helps
us better use GANs for downstream applications. Existing efforts mainly target
interpreting unconditional models, leaving it less explored how a conditional
GAN learns to render images regarding various categories. This work fills in
this gap by investigating how a class conditional generator unifies the
synthesis of multiple classes. For this purpose, we dive into the widely used
class-conditional batch normalization (CCBN), and observe that each feature
channel is activated at varying degrees given different categorical embeddings.
To describe such a phenomenon, we propose channel awareness, which
quantitatively characterizes how a single channel contributes to the final
synthesis. Extensive evaluations and analyses on the BigGAN model pre-trained
on ImageNet reveal that only a subset of channels is primarily responsible for
the generation of a particular category, similar categories (e.g., cat and dog)
usually get related to some same channels, and some channels turn out to share
information across all classes. For good measure, our algorithm enables several
novel applications with conditional GANs. Concretely, we achieve (1) versatile
image editing via simply altering a single channel and manage to (2)
harmoniously hybridize two different classes. We further verify that the
proposed channel awareness shows promising potential in (3) segmenting the
synthesized image and (4) evaluating the category-wise synthesis performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yingqinghe.github.io/interclassgan/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drive&Segment: Unsupervised Semantic Segmentation of Urban Scenes via
  <span class="highlight-title">Cross-modal</span> <span class="highlight-title">Distillation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonin Vobecky, David Hurych, Oriane Siméoni, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates learning pixel-wise semantic image segmentation in
urban scenes without any manual annotation, just from the raw non-curated data
collected by cars which, equipped with cameras and LiDAR sensors, drive around
a city. Our contributions are threefold. First, we propose a novel method for
cross-modal unsupervised learning of semantic image segmentation by leveraging
synchronized LiDAR and image data. The key ingredient of our method is the use
of an object proposal module that analyzes the LiDAR point cloud to obtain
proposals for spatially consistent objects. Second, we show that these 3D
object proposals can be aligned with the input images and reliably clustered
into semantically meaningful pseudo-classes. Finally, we develop a cross-modal
distillation approach that leverages image data partially annotated with the
resulting pseudo-classes to train a transformer-based model for image semantic
segmentation. We show the generalization capabilities of our method by testing
on four different testing datasets (Cityscapes, Dark Zurich, Nighttime Driving
and ACDC) without any finetuning, and demonstrate significant improvements
compared to the current state of the art on this problem. See project webpage
https://vobecant.github.io/DriveAndSegment/ for the code and more.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See project webpage https://vobecant.github.io/DriveAndSegment/ for
  the code and more</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operator Sketching for Deep Unrolling Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multispectral Satellite Data Classification using Soft Computing
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purbarag Pathak Choudhury, Ujjal Kr Dutta, Dhruba Kr Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A satellite image is a remotely sensed image data, where each pixel
represents a specific location on earth. The pixel value recorded is the
reflection radiation from the earth's surface at that location. Multispectral
images are those that capture image data at specific frequencies across the
electromagnetic spectrum as compared to Panchromatic images which are sensitive
to all wavelength of visible light. Because of the high resolution and high
dimensions of these images, they create difficulties for clustering techniques
to efficiently detect clusters of different sizes, shapes and densities as a
trade off for fast processing time. In this paper we propose a grid-density
based clustering technique for identification of objects. We also introduce an
approach to classify a satellite image data using a rule induction based
machine learning algorithm. The object identification and classification
methods have been validated using several synthetic and benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proc. of International Conference on Advances in Communication,
  Network, and Computing (CNC), 2014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Points Are Equal: Learning Highly Efficient Point-based
  Detectors for 3D LiDAR Point Clouds <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jianwei Wan, Yulan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of efficient object detection of 3D LiDAR point clouds.
To reduce the memory and computational cost, existing point-based pipelines
usually adopt task-agnostic random sampling or farthest point sampling to
progressively downsample input point clouds, despite the fact that not all
points are equally important to the task of object detection. In particular,
the foreground points are inherently more important than background points for
object detectors. Motivated by this, we propose a highly-efficient single-stage
point-based 3D detector in this paper, termed IA-SSD. The key of our approach
is to exploit two learnable, task-oriented, instance-aware downsampling
strategies to hierarchically select the foreground points belonging to objects
of interest. Additionally, we also introduce a contextual centroid perception
module to further estimate precise instance centers. Finally, we build our
IA-SSD following the encoder-only architecture for efficiency. Extensive
experiments conducted on several large-scale detection benchmarks demonstrate
the competitive performance of our IA-SSD. Thanks to the low memory footprint
and a high degree of parallelism, it achieves a superior speed of 80+
frames-per-second on the KITTI dataset with a single RTX2080Ti GPU. The code is
available at \url{https://github.com/yifanzhang713/IA-SSD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2022, code avaliable at: https://github.com/yifanzhang713/IA-SSD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span> of Disentanglement Approaches for Medical Applications -- Towards
  Solving the Gordian Knot of Generative Models in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jana Fragemann, Lynton Ardizzone, Jan Egger, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are commonly used for medical purposes such as image
generation, segmentation, or classification. Besides this, they are often
criticized as black boxes as their decision process is often not human
interpretable. Encouraging the latent representation of a generative model to
be disentangled offers new perspectives of control and interpretability.
Understanding the data generation process could help to create artificial
medical data sets without violating patient privacy, synthesizing different
data modalities, or discovering data generating characteristics. These
characteristics might unravel novel relationships that can be related to
genetic traits or patient outcomes. In this paper, we give a comprehensive
overview of popular generative models, like Generative Adversarial Networks
(GANs), Variational Autoencoders (VAEs), and Flow-based Models. Furthermore, we
summarize the different notions of disentanglement, review approaches to
disentangle latent space representations and metrics to evaluate the degree of
disentanglement. After introducing the theoretical frameworks, we give an
overview of recent medical applications and discuss the impact and importance
of disentanglement approaches for medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACS: A <span class="highlight-title">Dataset</span> for Physical Audiovisual CommonSense Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Yu, Peter Wu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, they should be able to reason about the
physical world by understanding the physical properties and affordances of
available objects, how they can be manipulated, and how they interact with
other physical objects. This research field of physical commonsense reasoning
is fundamentally a multi-sensory task since physical properties are manifested
through multiple modalities, two of them being vision and acoustics. Our paper
takes a step towards real-world physical commonsense reasoning by contributing
PACS: the first audiovisual benchmark annotated for physical commonsense
attributes. PACS contains a total of 13,400 question-answer pairs, involving
1,377 unique physical commonsense questions and 1,526 videos. Our dataset
provides new opportunities to advance the research field of physical reasoning
by bringing audio as a core component of this multimodal problem. Using PACS,
we evaluate multiple state-of-the-art models on this new challenging task.
While some models show promising results (70% accuracy), they all fall short of
human performance (95% accuracy). We conclude the paper by demonstrating the
importance of multimodal reasoning and providing possible avenues for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static
  Models by Fitting Feature-level Space-time Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR 2022 (Source Code:
  https://github.com/jx-zhong-for-academic-purpose/Kinet )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Expression Analysis Using Decomposed Multiscale Spatiotemporal
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wheidima Carneiro de Melo, Eric Granger, Miguel Bordallo Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based analysis of facial expressions has been increasingly applied to
infer health states of individuals, such as depression and pain. Among the
existing approaches, deep learning models composed of structures for multiscale
spatiotemporal processing have shown strong potential for encoding facial
dynamics. However, such models have high computational complexity, making for a
difficult deployment of these solutions. To address this issue, we introduce a
new technique to decompose the extraction of multiscale spatiotemporal
features. Particularly, a building block structure called Decomposed Multiscale
Spatiotemporal Network (DMSN) is presented along with three variants: DMSN-A,
DMSN-B, and DMSN-C blocks. The DMSN-A block generates multiscale
representations by analyzing spatiotemporal features at multiple temporal
ranges, while the DMSN-B block analyzes spatiotemporal features at multiple
ranges, and the DMSN-C block analyzes spatiotemporal features at multiple
spatial sizes. Using these variants, we design our DMSN architecture which has
the ability to explore a variety of multiscale spatiotemporal features,
favoring the adaptation to different facial behaviors. Our extensive
experiments on challenging datasets show that the DMSN-C block is effective for
depression detection, whereas the DMSN-A block is efficient for pain
estimation. Results also indicate that our DMSN architecture provides a
cost-effective solution for expressions that range from fewer facial variations
over time, as in depression detection, to greater variations, as in pain
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-fidelity GAN Inversion with Padding Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyan Bai, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, Yujun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverting a Generative Adversarial Network (GAN) facilitates a wide range of
image editing tasks using pre-trained generators. Existing methods typically
employ the latent space of GANs as the inversion space yet observe the
insufficient recovery of spatial details. In this work, we propose to involve
the padding space of the generator to complement the latent space with spatial
information. Concretely, we replace the constant padding (e.g., usually zeros)
used in convolution layers with some instance-aware coefficients. In this way,
the inductive bias assumed in the pre-trained model can be appropriately
adapted to fit each individual image. Through learning a carefully designed
encoder, we manage to improve the inversion quality both qualitatively and
quantitatively, outperforming existing alternatives. We then demonstrate that
such a space extension barely affects the native GAN manifold, hence we can
still reuse the prior knowledge learned by GANs for various downstream
applications. Beyond the editing tasks explored in prior arts, our approach
allows a more flexible image manipulation, such as the separate control of face
contour and facial details, and enables a novel editing manner where users can
customize their own manipulations highly efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ezioby.github.io/padinv/; Code:
  https://github.com/EzioBy/padinv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP meets GamePhysics: Towards bug identification in gameplay videos
  using zero-shot transfer learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Taesiri, Finlay Macklon, Cor-Paul Bezemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gameplay videos contain rich information about how players interact with the
game and how the game responds. Sharing gameplay videos on social media
platforms, such as Reddit, has become a common practice for many players.
Often, players will share gameplay videos that showcase video game bugs. Such
gameplay videos are software artifacts that can be utilized for game testing,
as they provide insight for bug analysis. Although large repositories of
gameplay videos exist, parsing and mining them in an effective and structured
fashion has still remained a big challenge. In this paper, we propose a search
method that accepts any English text query as input to retrieve relevant videos
from large repositories of gameplay videos. Our approach does not rely on any
external information (such as video metadata); it works solely based on the
content of the video. By leveraging the zero-shot transfer capabilities of the
Contrastive Language-Image Pre-Training (CLIP) model, our approach does not
require any data labeling or training. To evaluate our approach, we present the
$\texttt{GamePhysics}$ dataset consisting of 26,954 videos from 1,873 games,
that were collected from the GamePhysics section on the Reddit website. Our
approach shows promising results in our extensive analysis of simple queries,
compound queries, and bug queries, indicating that our approach is useful for
object and event detection in gameplay videos. An example application of our
approach is as a gameplay video search engine to aid in reproducing video game
bugs. Please visit the following link for the code and the data:
$\href{https://asgaardlab.github.io/CLIPxGamePhysics/}{\text{asgaardlab.github.io/CLIPxGamePhysics/}}$
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MSR 2022 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersFormer: 3D Lane Detection via Perspective <span class="highlight-title">Transformer</span> and the
  OpenLane Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for 3D lane detection have been recently proposed to address the
issue of inaccurate lane layouts in many autonomous driving scenarios
(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to
their simple designs of the spatial transformation between front view and
bird's eye view (BEV) and the lack of a realistic dataset. Towards these
issues, we present PersFormer: an end-to-end monocular 3D lane detector with a
novel Transformer-based spatial feature transformation module. Our model
generates BEV features by attending to related front-view local regions with
camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor
design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing
the feature consistency and sharing the benefits of multi-task learning.
Moreover, we release one of the first large-scale real-world 3D lane datasets,
which is called OpenLane, with high-quality annotation and scenario diversity.
OpenLane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane
categories, along with scene tags and the closed-in-path object annotations to
encourage the development of lane detection and more industrial-related
autonomous driving methods. We show that PersFormer significantly outperforms
competitive baselines in the 3D lane detection task on our new OpenLane dataset
as well as Apollo 3D Lane Synthetic dataset, and is also on par with
state-of-the-art algorithms in the 2D task on OpenLane. The project page is
available at https://github.com/OpenPerceptionX/OpenLane.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixFormer: End-to-End Tracking with Iterative Mixed Attention <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Cui, Jiang Cheng, Limin Wang, Gangshan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking often uses a multi-stage pipeline of feature extraction, target
information integration, and bounding box estimation. To simplify this pipeline
and unify the process of feature extraction and target information integration,
we present a compact tracking framework, termed as {\em MixFormer}, built upon
transformers. Our core design is to utilize the flexibility of attention
operations, and propose a Mixed Attention Module (MAM) for simultaneous feature
extraction and target information integration. This synchronous modeling scheme
allows to extract target-specific discriminative features and perform extensive
communication between target and search area. Based on MAM, we build our
MixFormer tracking framework simply by stacking multiple MAMs with progressive
patch embedding and placing a localization head on top. In addition, to handle
multiple target templates during online tracking, we devise an asymmetric
attention scheme in MAM to reduce computational cost, and propose an effective
score prediction module to select high-quality templates. Our MixFormer sets a
new state-of-the-art performance on five tracking benchmarks, including LaSOT,
TrackingNet, VOT2020, GOT-10k, and UAV123. In particular, our MixFormer-L
achieves NP score of 79.9 on LaSOT, 88.9 on TrackingNet and EAO of 0.555 on
VOT2020. We also perform in-depth ablation studies to demonstrate the
effectiveness of simultaneous feature extraction and information integration.
Code and trained models are publicly available at
\href{https://github.com/MCG-NJU/MixFormer}{https://github.com/MCG-NJU/MixFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Classification on Accelerated Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilkay Sikdokur, Inci Baytas, Arda Yurdakul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For image classification problems, various neural network models are commonly
used due to their success in yielding high accuracies. Convolutional Neural
Network (CNN) is one of the most frequently used deep learning methods for
image classification applications. It may produce extraordinarily accurate
results with regard to its complexity. However, the more complex the model is
the longer it takes to train. In this paper, an acceleration design that uses
the power of FPGA is given for a basic CNN model which consists of one
convolutional layer and one fully connected layer for the training phase of the
fully connected layer. Nonetheless, inference phase is also accelerated
automatically due to the fact that training phase includes inference. In this
design, the convolutional layer is calculated by the host computer and the
fully connected layer is calculated by an FPGA board. It should be noted that
the training of convolutional layer is not taken into account in this design
and is left for future research. The results are quite encouraging as this FPGA
design tops the performance of some of the state-of-the-art deep learning
platforms such as Tensorflow on the host computer approximately 2 times in both
training and inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Basarim 2020 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTBF-33: A multi-temporal building footprint <span class="highlight-title">dataset</span> for 33 counties in
  the United States (1900-2015) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes H. Uhl, Stefan Leyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite abundant data on the spatial distribution of contemporary human
settlements, historical data on the long-term evolution of human settlements at
fine spatial and temporal granularity is scarce, limiting our quantitative
understanding of long-term changes of built-up areas. This is because commonly
used mapping methods (e.g., image classification) and suitable data sources
(i.e., aerial imagery, multi-spectral remote sensing data, LiDAR) have only
been available in recent decades. However, there are alternative data sources
such as cadastral records that are digitally available, containing relevant
information such as building age information, allowing for an approximate,
digital reconstruction of past building distributions. We conducted a
non-exhaustive search of open and publicly available data resources from
administrative institutions in the United States and gathered, integrated, and
harmonized cadastral parcel data, tax assessment data, and building footprint
data for 33 counties, wherever building footprint geometries and building
construction year information was available. The result of this effort is a
unique dataset which we call the Multi-Temporal Building Footprint Dataset for
33 U.S. Counties (MTBF-33). MTBF-33 contains over 6.2 million building
footprints including their construction year, and can be used to derive
retrospective depictions of built-up areas from 1900 to 2015, at fine spatial
and temporal grain and can be used for data validation purposes, or to train
statistical learning approaches aiming to extract historical information on
human settlements from remote sensing data, historical maps, or similar data
sources. MTBF-33 is available at http://doi.org/10.17632/w33vbvjtdy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Siamese Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwei Zhang, Jiangmiao Pang, Kai Chen, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Dense Siamese Network (DenseSiam), a simple unsupervised
learning framework for dense prediction tasks. It learns visual representations
by maximizing the similarity between two views of one image with two types of
consistency, i.e., pixel consistency and region consistency. Concretely,
DenseSiam first maximizes the pixel level spatial consistency according to the
exact location correspondence in the overlapped area. It also extracts a batch
of region embeddings that correspond to some sub-regions in the overlapped area
to be contrasted for region consistency. In contrast to previous methods that
require negative pixel pairs, momentum encoders, or heuristic masks, DenseSiam
benefits from the simple Siamese network and optimizes the consistency of
different granularities. It also proves that the simple location correspondence
and interacted region embeddings are effective enough to learn the similarity.
We apply DenseSiam on ImageNet and obtain competitive improvements on various
downstream tasks. We also show that only with some extra task-specific losses,
the simple framework can directly conduct dense prediction tasks. On an
existing unsupervised semantic segmentation benchmark, it surpasses
state-of-the-art segmentation methods by 2.1 mIoU with 28% training costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Enriched Illuminants for Cross and Single Sensor Color
  Constancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Cun, Zhendong Wang, Chi-Man Pun, Jianzhuang Liu, Wengang Zhou, Xu Jia, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color constancy aims to restore the constant colors of a scene under
different illuminants. However, due to the existence of camera spectral
sensitivity, the network trained on a certain sensor, cannot work well on
others. Also, since the training datasets are collected in certain
environments, the diversity of illuminants is limited for complex real world
prediction. In this paper, we tackle these problems via two aspects. First, we
propose cross-sensor self-supervised training to train the network. In detail,
we consider both the general sRGB images and the white-balanced RAW images from
current available datasets as the white-balanced agents. Then, we train the
network by randomly sampling the artificial illuminants in a sensor-independent
manner for scene relighting and supervision. Second, we analyze a previous
cascaded framework and present a more compact and accurate model by sharing the
backbone parameters with learning attention specifically. Experiments show that
our cross-sensor model and single-sensor model outperform other
state-of-the-art methods by a large margin on cross and single sensor
evaluations, respectively, with only 16% parameters of the previous best model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo Neural Vernier Caliper <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichao Li, Zechun Liu, Zhiqiang Shen, Kwang-Ting Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new object-centric framework for learning-based stereo 3D object
detection. Previous studies build scene-centric representations that do not
consider the significant variation among outdoor instances and thus lack the
flexibility and functionalities that an instance-level model can offer. We
build such an instance-level model by formulating and tackling a local update
problem, i.e., how to predict a refined update given an initial 3D cuboid
guess. We demonstrate how solving this problem can complement scene-centric
approaches in (i) building a coarse-to-fine multi-resolution system, (ii)
performing model-agnostic object location refinement, and (iii) conducting
stereo 3D tracking-by-detection. Extensive experiments demonstrate the
effectiveness of our approach, which achieves state-of-the-art performance on
the KITTI benchmark. Code and pre-trained models are available at
https://github.com/Nicholasli1995/SNVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Skeleton-based Action Recognition with Continual Spatio-Temporal
  Graph Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Hedegaard, Negar Heidari, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based reasoning over skeleton data has emerged as a promising approach
for human action recognition. However, the application of prior graph-based
methods, which predominantly employ whole temporal sequences as their input, to
the setting of online inference entails considerable computational redundancy.
In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph
Convolutional Neural Network as a Continual Inference Network, which can
perform step-by-step predictions in time without repeat frame processing. To
evaluate our method, we create a continual version of ST-GCN, CoST-GCN,
alongside two derived methods with different self-attention mechanisms, CoAGCN
and CoS-TR. We investigate weight transfer strategies and architectural
modifications for inference acceleration, and perform experiments on the NTU
RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar
predictive accuracy, we observe up to 109x reduction in time complexity,
on-hardware accelerations of 26x, and reductions in maximum allocated memory of
52% during online inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based HTR for Historical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Benjamin Ströbel, Simon Clematide, Martin Volk, Tobias Hodel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply the TrOCR framework to real-world, historical manuscripts and show
that TrOCR per se is a strong model, ideal for transfer learning. TrOCR has
been trained on English only, but it can adapt to other languages that use the
Latin alphabet fairly easily and with little training material. We compare
TrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such
systems. This finding is essential since Transkribus performs best when it has
access to baseline information, which is not needed at all to fine-tune TrOCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an abstract submitted and accepted at ComHum 2022 in
  Lausanne. We will be elaborating on these initial findings in the paper that
  we will submit after the conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational ergonomics for task delegation in Human-Robot
  Collaboration: spatiotemporal adaptation of the robot to the human through
  contactless gesture recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brenda Elizabeth Olivas-Padilla, Dimitris Papanagiotou, Gavriela Senteri, Sotiris Manitsaris, Alina Glushkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high prevalence of work-related musculoskeletal disorders (WMSDs) could
be addressed by optimizing Human-Robot Collaboration (HRC) frameworks for
manufacturing applications. In this context, this paper proposes two hypotheses
for ergonomically effective task delegation and HRC. The first hypothesis
states that it is possible to quantify ergonomically professional tasks using
motion data from a reduced set of sensors. Then, the most dangerous tasks can
be delegated to a collaborative robot. The second hypothesis is that by
including gesture recognition and spatial adaptation, the ergonomics of an HRC
scenario can be improved by avoiding needless motions that could expose
operators to ergonomic risks and by lowering the physical effort required of
operators. An HRC scenario for a television manufacturing process is optimized
to test both hypotheses. For the ergonomic evaluation, motion primitives with
known ergonomic risks were modeled for their detection in professional tasks
and to estimate a risk score based on the European Assembly Worksheet (EAWS). A
Deep Learning gesture recognition module trained with egocentric television
assembly data was used to complement the collaboration between the human
operator and the robot. Additionally, a skeleton-tracking algorithm provided
the robot with information about the operator's pose, allowing it to spatially
adapt its motion to the operator's anthropometrics. Three experiments were
conducted to determine the effect of gesture recognition and spatial adaptation
on the operator's range of motion. The rate of spatial adaptation was used as a
key performance indicator (KPI), and a new KPI for measuring the reduction in
the operator's motion is presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Underwater Light Field Retention : Neural Rendering for Underwater
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Ye, Sixiang Chen, Yun Liu, Erkang Chen, Yi Ye, Yuche Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater Image Rendering aims to generate a true-to-life underwater image
from a given clean one, which could be applied to various practical
applications such as underwater image enhancement, camera filter, and virtual
gaming. We explore two less-touched but challenging problems in underwater
image rendering, namely, i) how to render diverse underwater scenes by a single
neural network? ii) how to adaptively learn the underwater light fields from
natural exemplars, \textit{i,e.}, realistic underwater images? To this end, we
propose a neural rendering method for underwater imaging, dubbed UWNR
(Underwater Neural Rendering). Specifically, UWNR is a data-driven neural
network that implicitly learns the natural degenerated model from authentic
underwater images, avoiding introducing erroneous biases by hand-craft imaging
models.
  Compared with existing underwater image generation methods, UWNR utilizes the
natural light field to simulate the main characteristics of the underwater
scene. Thus, it is able to synthesize a wide variety of underwater images from
one clean image with various realistic underwater images.
  Extensive experiments demonstrate that our approach achieves better visual
effects and quantitative metrics over previous methods. Moreover, we adopt UWNR
to build an open Large Neural Rendering Underwater Dataset containing various
types of water quality, dubbed LNRUD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Road Layout Parsing with Graph Auto-Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Lu, Gijs Dubbelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming for higher-level scene understanding, this work presents a neural
network approach that takes a road-layout map in bird's eye view as input, and
predicts a human-interpretable graph that represents the road's topological
layout. Our approach elevates the understanding of road layouts from pixel
level to the level of graphs. To achieve this goal, an image-graph-image
auto-encoder is utilized. The network is designed to learn to regress the graph
representation at its auto-encoder bottleneck. This learning is self-supervised
by an image reconstruction loss, without needing any external manual
annotations. We create a synthetic dataset containing common road layout
patterns and use it for training of the auto-encoder in addition to the
real-world Argoverse dataset. By using this additional synthetic dataset, which
conceptually captures human knowledge of road layouts and makes this available
to the network for training, we are able to stabilize and further improve the
performance of topological road layout understanding on the real-world
Argoverse dataset. The evaluation shows that our approach exhibits comparable
performance to a strong fully-supervised baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoDTR: Monocular 3D Object Detection with Depth-Aware <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D object detection is an important yet challenging task in
autonomous driving. Some existing methods leverage depth information from an
off-the-shelf depth estimator to assist 3D detection, but suffer from the
additional computational burden and achieve limited performance caused by
inaccurate depth priors. To alleviate this, we propose MonoDTR, a novel
end-to-end depth-aware transformer network for monocular 3D object detection.
It mainly consists of two components: (1) the Depth-Aware Feature Enhancement
(DFE) module that implicitly learns depth-aware features with auxiliary
supervision without requiring extra computation, and (2) the Depth-Aware
Transformer (DTR) module that globally integrates context- and depth-aware
features. Moreover, different from conventional pixel-wise positional
encodings, we introduce a novel depth positional encoding (DPE) to inject depth
positional hints into transformers. Our proposed depth-aware modules can be
easily plugged into existing image-only monocular 3D object detectors to
improve the performance. Extensive experiments on the KITTI dataset demonstrate
that our approach outperforms previous state-of-the-art monocular-based methods
and achieves real-time detection. Code is available at
https://github.com/kuanchihhuang/MonoDTR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving anatomical plausibility in medical image segmentation via
  hybrid graph neural networks: applications to chest x-ray analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolás Gaggion, Lucas Mansilla, Candelaria Mosquera, Diego H. Milone, Enzo Ferrante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical segmentation is a fundamental task in medical image computing,
generally tackled with fully convolutional neural networks which produce dense
segmentation masks. These models are often trained with loss functions such as
cross-entropy or Dice, which assume pixels to be independent of each other,
thus ignoring topological errors and anatomical inconsistencies. We address
this limitation by moving from pixel-level to graph representations, which
allow to naturally incorporate anatomical constraints by construction. To this
end, we introduce HybridGNet, an encoder-decoder neural architecture that
leverages standard convolutions for image feature encoding and graph
convolutional neural networks (GCNNs) to decode plausible representations of
anatomical structures. We also propose a novel image-to-graph skip connection
layer which allows localized features to flow from standard convolutional
blocks to GCNN blocks, and show that it improves segmentation accuracy. The
proposed architecture is extensively evaluated in a variety of domain shift and
image occlusion scenarios, and audited considering different types of
demographic domain shift. Our comprehensive experimental setup compares
HybridGNet with other landmark and pixel-based models for anatomical
segmentation in chest x-ray images, and shows that it produces anatomically
plausible results in challenging scenarios where other models tend to fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code at https://github.com/ngaggion/HybridGNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards <span class="highlight-title">Self-Supervised</span> Gaze Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arya Farkhondeh, Cristina Palmero, Simone Scardapane, Sergio Escalera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent joint embedding-based self-supervised methods have surpassed standard
supervised approaches on various image recognition tasks such as image
classification. These self-supervised methods aim at maximizing agreement
between features extracted from two differently transformed views of the same
image, which results in learning an invariant representation with respect to
appearance and geometric image transformations. However, the effectiveness of
these approaches remains unclear in the context of gaze estimation, a
structured regression task that requires equivariance under geometric
transformations (e.g., rotations, horizontal flip). In this work, we propose
SwAT, an equivariant version of the online clustering-based self-supervised
approach SwAV, to learn more informative representations for gaze estimation.
We identify the most effective image transformations for self-supervised
pretraining and demonstrate that SwAT, with ResNet-50 and supported with
uncurated unlabeled face images, outperforms state-of-the-art gaze estimation
methods and supervised baselines in various experiments. In particular, we
achieve up to 57% and 25% improvements in cross-dataset and within-dataset
evaluation tasks on existing benchmarks (ETH-XGaze, Gaze360, and MPIIFaceGaze).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge
  Modality Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Buchner, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online 3D multi-object tracking (MOT) has witnessed significant research
interest in recent years, largely driven by demand from the autonomous systems
community. However, 3D offline MOT is relatively less explored. Labeling 3D
trajectory scene data at a large scale while not relying on high-cost human
experts is still an open research question. In this work, we propose Batch3DMOT
that follows the tracking-by-detection paradigm and represents real-world
scenes as directed, acyclic, and category-disjoint tracking graphs that are
attributed using various modalities such as camera, LiDAR, and radar. We
present a multi-modal graph neural network that uses a cross-edge attention
mechanism mitigating modality intermittence, which translates into sparsity in
the graph domain. Additionally, we present attention-weighted convolutions over
frame-wise k-NN neighborhoods as suitable means to allow information exchange
across disconnected graph components. We evaluate our approach using various
sensor modalities and model configurations on the challenging nuScenes and
KITTI datasets. Extensive experiments demonstrate that our proposed approach
yields an overall improvement of 2.8% in the AMOTA score on nuScenes thereby
setting a new benchmark for 3D tracking methods and successfully enhances false
positive filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Occlusion-Aware Coarse-to-Fine Depth Map for <span class="highlight-title">Self-supervised</span>
  Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng<span class="highlight-author">ming Zhou</span>, Qiulei Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation, aiming to learn scene depths from
single images in a self-supervised manner, has received much attention
recently. In spite of recent efforts in this field, how to learn accurate scene
depths and alleviate the negative influence of occlusions for self-supervised
depth estimation, still remains an open problem. Addressing this problem, we
firstly empirically analyze the effects of both the continuous and discrete
depth constraints which are widely used in the training process of many
existing works. Then inspired by the above empirical analysis, we propose a
novel network to learn an Occlusion-aware Coarse-to-Fine Depth map for
self-supervised monocular depth estimation, called OCFD-Net. Given an arbitrary
training set of stereo image pairs, the proposed OCFD-Net does not only employ
a discrete depth constraint for learning a coarse-level depth map, but also
employ a continuous depth constraint for learning a scene depth residual,
resulting in a fine-level depth map. In addition, an occlusion-aware module is
designed under the proposed OCFD-Net, which is able to improve the capability
of the learnt fine-level depth map for handling occlusions. Extensive
experimental results on the public KITTI and Make3D datasets demonstrate that
the proposed method outperforms 20 existing state-of-the-art methods in most
cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth Completion using Geometry-Aware Embedding <span class="chip">ICRA22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenchao Du, Hu Chen, Hongyu Yang, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting internal spatial geometric constraints of sparse LiDARs is
beneficial to depth completion, however, has been not explored well. This paper
proposes an efficient method to learn geometry-aware embedding, which encodes
the local and global geometric structure information from 3D points, e.g.,
scene layout, object's sizes and shapes, to guide dense depth estimation.
Specifically, we utilize the dynamic graph representation to model generalized
geometric relationship from irregular point clouds in a flexible and efficient
manner. Further, we joint this embedding and corresponded RGB appearance
information to infer missing depths of the scene with well structure-preserved
details. The key to our method is to integrate implicit 3D geometric
representation into a 2D learning architecture, which leads to a better
trade-off between the performance and efficiency. Extensive experiments
demonstrate that the proposed method outperforms previous works and could
reconstruct fine depths with crisp boundaries in regions that are over-smoothed
by them. The ablation study gives more insights into our method that could
achieve significant gains with a simple design, while having better
generalization capability and stability. The code is available at
https://github.com/Wenchao-Du/GAENet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acceptted by ICRA22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Multivariate Gaussian Mixture for Efficient Neural Image
  Compression <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosu Zhu, Jingkuan Song, Lianli Gao, Feng Zheng, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling latent variables with priors and hyperpriors is an essential problem
in variational image compression. Formally, trade-off between rate and
distortion is handled well if priors and hyperpriors precisely describe latent
variables. Current practices only adopt univariate priors and process each
variable individually. However, we find inter-correlations and
intra-correlations exist when observing latent variables in a vectorized
perspective. These findings reveal visual redundancies to improve
rate-distortion performance and parallel processing ability to speed up
compression. This encourages us to propose a novel vectorized prior.
Specifically, a multivariate Gaussian mixture is proposed with means and
covariances to be estimated. Then, a novel probabilistic vector quantization is
utilized to effectively approximate means, and remaining covariances are
further induced to a unified mixture and solved by cascaded estimation without
context models involved. Furthermore, codebooks involved in quantization are
extended to multi-codebooks for complexity reduction, which formulates an
efficient compression procedure. Extensive experiments on benchmark datasets
against state-of-the-art indicate our model has better rate-distortion
performance and an impressive $3.18\times$ compression speed up, giving us the
ability to perform real-time, high-quality variational image compression in
practice. Our source code is publicly available at
\url{https://github.com/xiaosu-zhu/McQuic}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Domain Generalized Stereo Matching Networks from a Feature
  Consistency Perspective <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhang, Xiang Wang, Xiao Bai, Chen Wang, Lei Huang, Yimin Chen, Lin Gu, Jun Zhou, Tatsuya Harada, Edwin R. Hancock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent stereo matching networks achieving impressive performance
given sufficient training data, they suffer from domain shifts and generalize
poorly to unseen domains. We argue that maintaining feature consistency between
matching pixels is a vital factor for promoting the generalization capability
of stereo matching networks, which has not been adequately considered. Here we
address this issue by proposing a simple pixel-wise contrastive learning across
the viewpoints. The stereo contrastive feature loss function explicitly
constrains the consistency between learned features of matching pixel pairs
which are observations of the same 3D points. A stereo selective whitening loss
is further introduced to better preserve the stereo feature consistency across
domains, which decorrelates stereo features from stereo viewpoint-specific
style information. Counter-intuitively, the generalization of feature
consistency between two viewpoints in the same scene translates to the
generalization of stereo matching performance to unseen domains. Our method is
generic in nature as it can be easily embedded into existing stereo networks
and does not require access to the samples in the target domain. When trained
on synthetic data and generalized to four real-world testing sets, our method
achieves superior performance over several state-of-the-art networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELIC: Efficient Learned Image Compression with Unevenly Grouped
  Space-Channel Contextual Adaptive Coding <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, learned image compression techniques have achieved remarkable
performance, even surpassing the best manually designed lossy image coders.
They are promising to be large-scale adopted. For the sake of practicality, a
thorough investigation of the architecture design of learned image compression,
regarding both compression performance and running speed, is essential. In this
paper, we first propose uneven channel-conditional adaptive coding, motivated
by the observation of energy compaction in learned image compression. Combining
the proposed uneven grouping model with existing context models, we obtain a
spatial-channel contextual adaptive model to improve the coding performance
without damage to running speed. Then we study the structure of the main
transform and propose an efficient model, ELIC, to achieve state-of-the-art
speed and compression ability. With superior performance, the proposed model
also supports extremely fast preview decoding and progressive decoding, which
makes the coming application of learning-based image compression more
promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Remote Photoplethysmography with Temporal Derivative Modules
  and Time-Shift Invariant Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joaquim Comas, Adria Ruiz, Federico Sukno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the video facial PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> RGB-Depth Fusion GAN for Indoor Depth Completion <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Wang, Mingyuan Wang, Zhengping Che, Zhiyuan Xu, Xiuquan Qiao, Mengshi Qi, Feifei Feng, <span class="highlight-author">Jian Tang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The raw depth image captured by the indoor depth sensor usually has an
extensive range of missing depth values due to inherent limitations such as the
inability to perceive transparent objects and limited distance range. The
incomplete depth map burdens many downstream vision tasks, and a rising number
of depth completion methods have been proposed to alleviate this issue. While
most existing methods can generate accurate dense depth maps from sparse and
uniformly sampled depth maps, they are not suitable for complementing the large
contiguous regions of missing depth values, which is common and critical. In
this paper, we design a novel two-branch end-to-end fusion network, which takes
a pair of RGB and incomplete depth images as input to predict a dense and
completed depth map. The first branch employs an encoder-decoder structure to
regress the local dense depth values from the raw depth map, with the help of
local guidance information extracted from the RGB image. In the other branch,
we propose an RGB-depth fusion GAN to transfer the RGB image to the
fine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN
to propagate the features across the two branches, and we append a confidence
fusion head to fuse the two outputs of the branches for the final depth map.
Extensive experiments on NYU-Depth V2 and SUN RGB-D demonstrate that our
proposed method clearly improves the depth completion performance, especially
in a more realistic setting of indoor environments with the help of the pseudo
depth map.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boost <span class="highlight-title">Test</span>-Time Performance with Closed-Loop Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Guanghui Xu, Haokun Li, Junzhou Huang, Yaowei Wang, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional deep models predict a test sample with a single forward
propagation, which, however, may not be sufficient for predicting
hard-classified samples. On the contrary, we human beings may need to carefully
check the sample many times before making a final decision. During the recheck
process, one may refine/adjust the prediction by referring to related samples.
Motivated by this, we propose to predict those hard-classified test samples in
a looped manner to boost the model performance. However, this idea may pose a
critical challenge: how to construct looped inference, so that the original
erroneous predictions on these hard test samples can be corrected with little
additional effort. To address this, we propose a general Closed-Loop Inference
(CLI) method. Specifically, we first devise a filtering criterion to identify
those hard-classified test samples that need additional inference loops. For
each hard sample, we construct an additional auxiliary learning task based on
its original top-$K$ predictions to calibrate the model, and then use the
calibrated model to obtain the final prediction. Promising results on ImageNet
(in-distribution test samples) and ImageNet-C (out-of-distribution test
samples) demonstrate the effectiveness of CLI in improving the performance of
any pre-trained model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-modal</span> learning for predicting the genotype of glioma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Wei, Xi Chen, Lei Zhu, Lipei Zhang, Carola-Bibiane Schönlieb, Stephen J. Price, Chao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The isocitrate dehydrogenase (IDH) gene mutation is an essential biomarker
for the diagnosis and prognosis of glioma. It is promising to better predict
glioma genotype by integrating focal tumor image and geometric features with
brain network features derived from MRI. Convolutions neural networks show
reasonable performance in predicting IDH mutation, which, however, cannot learn
from non-Euclidean data, e.g., geometric and network data. In this study, we
propose a multi-modal learning framework using three separate encoders to
extract features of focal tumor image, tumor geometrics and global brain
networks. To mitigate the limited availability of diffusion MRI, we develop a
self-supervised approach to generate brain networks from anatomical
multi-sequence MRI. Moreover, to extract tumor-related features from the brain
network, we design a hierarchical attention module for the brain network
encoder. Further, we design a bi-level multi-modal contrastive loss to align
the multi-modal features and tackle the domain gap at the focal tumor and
global brain. Finally, we propose a weighted population graph to integrate the
multi-modal features for genotype prediction. Experimental results on the
testing set show that the proposed model outperforms the baseline deep learning
models. The ablation experiments validate the performance of different
components of the framework. The visualized interpretation corresponds to
clinical knowledge with further validation. In conclusion, the proposed
learning framework provides a novel approach for predicting the genotype of
glioma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Vision <span class="highlight-title">Transformer</span>s: Combining Improvements in Metric
  Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pretrained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://donydchen.github.io/sem2nerf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARM: Any-Time Super-Resolution Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohong Chen, Mingbao Lin, Kekai Sheng, Mengdan Zhang, Peixian Chen, Ke Li, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an Any-time super-Resolution Method (ARM) to tackle the
over-parameterized single image super-resolution (SISR) models. Our ARM is
motivated by three observations: (1) The performance of different image patches
varies with SISR networks of different sizes. (2) There is a tradeoff between
computation overhead and performance of the reconstructed image. (3) Given an
input image, its edge information can be an effective option to estimate its
PSNR. Subsequently, we train an ARM supernet containing SISR subnets of
different sizes to deal with image patches of various complexity. To that
effect, we construct an Edge-to-PSNR lookup table that maps the edge score of
an image patch to the PSNR performance for each subnet, together with a set of
computation costs for the subnets. In the inference, the image patches are
individually distributed to different subnets for a better
computation-performance tradeoff. Moreover, each SISR subnet shares weights of
the ARM supernet, thus no extra parameters are introduced. The setting of
multiple subnets can well adapt the computational cost of SISR model to the
dynamically available hardware resources, allowing the SISR task to be in
service at any time. Extensive experiments on resolution datasets of different
sizes with popular SISR networks as backbones verify the effectiveness and the
versatility of our ARM. The source code is available at
\url{https://github.com/chenbong/ARM-Net}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnoViT: Unsupervised Anomaly Detection and Localization with Vision
  <span class="highlight-title">Transformer</span>-based Encoder-Decoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunseung Lee, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image anomaly detection problems aim to determine whether an image is
abnormal, and to detect anomalous areas. These methods are actively used in
various fields such as manufacturing, medical care, and intelligent
information. Encoder-decoder structures have been widely used in the field of
anomaly detection because they can easily learn normal patterns in an
unsupervised learning environment and calculate a score to identify
abnormalities through a reconstruction error indicating the difference between
input and reconstructed images. Therefore, current image anomaly detection
methods have commonly used convolutional encoder-decoders to extract normal
information through the local features of images. However, they are limited in
that only local features of the image can be utilized when constructing a
normal representation owing to the characteristics of convolution operations
using a filter of fixed size. Therefore, we propose a vision transformer-based
encoder-decoder model, named AnoViT, designed to reflect normal information by
additionally learning the global relationship between image patches, which is
capable of both image anomaly detection and localization. The proposed approach
constructs a feature map that maintains the existing location information of
individual patches by using the embeddings of all patches passed through
multiple self-attention layers. The proposed AnoViT model performed better than
the convolution-based model on three benchmark datasets. In MVTecAD, which is a
representative benchmark dataset for anomaly localization, it showed improved
results on 10 out of 15 classes compared with the baseline. Furthermore, the
proposed method showed good performance regardless of the class and type of the
anomalous area when localization results were evaluated qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViM: Out-Of-Distribution with Virtual-logit Matching <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqi Wang, Zhizhong Li, Litong Feng, Wayne Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing Out-Of-Distribution (OOD) detection algorithms depend on
single input source: the feature, the logit, or the softmax probability.
However, the immense diversity of the OOD examples makes such methods fragile.
There are OOD samples that are easy to identify in the feature space while hard
to distinguish in the logit space and vice versa. Motivated by this
observation, we propose a novel OOD scoring method named Virtual-logit Matching
(ViM), which combines the class-agnostic score from feature space and the
In-Distribution (ID) class-dependent logits. Specifically, an additional logit
representing the virtual OOD class is generated from the residual of the
feature against the principal space, and then matched with the original logits
by a constant scaling. The probability of this virtual logit after softmax is
the indicator of OOD-ness. To facilitate the evaluation of large-scale OOD
detection in academia, we create a new OOD dataset for ImageNet-1K, which is
human-annotated and is 8.8x the size of existing datasets. We conducted
extensive experiments, including CNNs and vision transformers, to demonstrate
the effectiveness of the proposed ViM score. In particular, using the BiT-S
model, our method gets an average AUROC 90.91% on four difficult OOD
benchmarks, which is 4% ahead of the best baseline. Code and dataset are
available at https://github.com/haoqiwang/vim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Longitudinal <span class="highlight-title">Self-Supervision</span> for COVID-19 Pathology Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Czempiel, Coco Rogers, Matthias Keicher, Magdalini Paschali, Rickmer Braren, Egon Burian, Marcus Makowski, Nassir Navab, Thomas Wendler, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying COVID-19 infection over time is an important task to manage the
hospitalization of patients during a global pandemic. Recently, deep
learning-based approaches have been proposed to help radiologists automatically
quantify COVID-19 pathologies on longitudinal CT scans. However, the learning
process of deep learning methods demands extensive training data to learn the
complex characteristics of infected regions over longitudinal scans. It is
challenging to collect a large-scale dataset, especially for longitudinal
training. In this study, we want to address this problem by proposing a new
self-supervised learning method to effectively train longitudinal networks for
the quantification of COVID-19 infections. For this purpose, longitudinal
self-supervision schemes are explored on clinical longitudinal COVID-19 CT
scans. Experimental results show that the proposed method is effective, helping
the model better exploit the semantics of longitudinal data and improve two
COVID-19 quantification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScalableViT: Rethinking the Context-oriented Generalization of Vision
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng Xiao, Min Zheng, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vanilla self-attention mechanism inherently relies on pre-defined and
steadfast computational dimensions. Such inflexibility restricts it from
possessing context-oriented generalization that can bring more contextual cues
and global representations. To mitigate this issue, we propose a Scalable
Self-Attention (SSA) mechanism that leverages two scaling factors to release
dimensions of query, key, and value matrix while unbinding them with the input.
This scalability fetches context-oriented generalization and enhances object
sensitivity, which pushes the whole network into a more effective trade-off
state between accuracy and cost. Furthermore, we propose an Interactive
Window-based Self-Attention (IWSA), which establishes interaction between
non-overlapping regions by re-merging independent value tokens and aggregating
spatial information from adjacent windows. By stacking the SSA and IWSA
alternately, the Scalable Vision Transformer (ScalableViT) achieves
state-of-the-art performance in general-purpose vision tasks. For example,
ScalableViT-S outperforms Twins-SVT-S by 1.4% and Swin-T by 1.8% on ImageNet-1K
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization by Mutual-Information Regularization with
  <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbum Cha, Kyungjae Lee, Sungrae Park, Sanghyuk Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to learn a generalized model to an unseen
target domain using only limited source domains. Previous attempts to DG fail
to learn domain-invariant representations only from the source domains due to
the significant domain shifts between training and test domains. Instead, we
re-formulate the DG objective using mutual information with the oracle model, a
model generalized to any possible domain. We derive a tractable variational
lower bound via approximating the oracle model by a pre-trained model, called
Mutual Information Regularization with Oracle (MIRO). Our extensive experiments
show that MIRO significantly improves the out-of-distribution performance.
Furthermore, our scaling experiments show that the larger the scale of the
pre-trained model, the greater the performance improvement of MIRO. Source code
is available at https://github.com/kakaobrain/miro.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifications of Skull Fractures using CT Scan Images via CNN with
  Lazy Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Moniruzzaman Emon, Tareque Rahman Ornob, Moqsadur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification of skull fracture is a challenging task for both radiologists
and researchers. Skull fractures result in broken pieces of bone, which can cut
into the brain and cause bleeding and other injury types. So it is vital to
detect and classify the fracture very early. In real world, often fractures
occur at multiple sites. This makes it harder to detect the fracture type where
many fracture types might summarize a skull fracture. Unfortunately, manual
detection of skull fracture and the classification process is time-consuming,
threatening a patient's life. Because of the emergence of deep learning, this
process could be automated. Convolutional Neural Networks (CNNs) are the most
widely used deep learning models for image categorization because they deliver
high accuracy and outstanding outcomes compared to other models. We propose a
new model called SkullNetV1 comprising a novel CNN by taking advantage of CNN
for feature extraction and lazy learning approach which acts as a classifier
for classification of skull fractures from brain CT images to classify five
fracture types. Our suggested model achieved a subset accuracy of 88%, an F1
score of 93%, the Area Under the Curve (AUC) of 0.89 to 0.98, a Hamming score
of 92% and a Hamming loss of 0.04 for this seven-class multi-labeled
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GroupTransNet: Group <span class="highlight-title">Transformer</span> Network for RGB-D Salient Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Fang, Jinshao Zhu, Xiuli Shao, Hongpeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient object detection on RGB-D images is an active topic in computer
vision. Although the existing methods have achieved appreciable performance,
there are still some challenges. The locality of convolutional neural network
requires that the model has a sufficiently deep global receptive field, which
always leads to the loss of local details. To address the challenge, we propose
a novel Group Transformer Network (GroupTransNet) for RGB-D salient object
detection. This method is good at learning the long-range dependencies of cross
layer features to promote more perfect feature expression. At the beginning,
the features of the slightly higher classes of the middle three levels and the
latter three levels are soft grouped to absorb the advantages of the high-level
features. The input features are repeatedly purified and enhanced by the
attention mechanism to purify the cross modal features of color modal and depth
modal. The features of the intermediate process are first fused by the features
of different layers, and then processed by several transformers in multiple
groups, which not only makes the size of the features of each scale unified and
interrelated, but also achieves the effect of sharing the weight of the
features within the group. The output features in different groups complete the
clustering staggered by two owing to the level difference, and combine with the
low-level features. Extensive experiments demonstrate that GroupTransNet
outperforms the comparison models and achieves the new state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive and Cascaded Compressive Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Qiu, Tao Yue, Xuemei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene-dependent adaptive compressive sensing (CS) has been a long pursuing
goal which has huge potential in significantly improving the performance of CS.
However, without accessing to the ground truth image, how to design the
scene-dependent adaptive strategy is still an open-problem and the improvement
in sampling efficiency is still quite limited. In this paper, a restricted
isometry property (RIP) condition based error clamping is proposed, which could
directly predict the reconstruction error, i.e. the difference between the
currently-stage reconstructed image and the ground truth image, and adaptively
allocate samples to different regions at the successive sampling stage.
Furthermore, we propose a cascaded feature fusion reconstruction network that
could efficiently utilize the information derived from different adaptive
sampling stages. The effectiveness of the proposed adaptive and cascaded CS
method is demonstrated with extensive quantitative and qualitative results,
compared with the state-of-the-art CS algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delving into the Estimation Shift of Batch Normalization in a Network <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Yi Zhou, Tian Wang, Jie Luo, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch normalization (BN) is a milestone technique in deep learning. It
normalizes the activation using mini-batch statistics during training but the
estimated population statistics during inference. This paper focuses on
investigating the estimation of population statistics. We define the estimation
shift magnitude of BN to quantitatively measure the difference between its
estimated population statistics and expected ones. Our primary observation is
that the estimation shift can be accumulated due to the stack of BN in a
network, which has detriment effects for the test performance. We further find
a batch-free normalization (BFN) can block such an accumulation of estimation
shift. These observations motivate our design of XBNBlock that replace one BN
with BFN in the bottleneck block of residual-style networks. Experiments on the
ImageNet and COCO benchmarks show that XBNBlock consistently improves the
performance of different architectures, including ResNet and ResNeXt, by a
significant margin and seems to be more robust to distribution shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022. The Code is available at:
  https://github.com/huangleiBuaa/XBNBlock</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K-space and Image Domain Collaborative Energy based Model for Parallel
  MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongjiang Tu, Chen Jiang, Yu Guan, Shanshan Wang, Jijun Liu, Qiegen Liu, Dong Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decreasing magnetic resonance (MR) image acquisition times can potentially
make MR examinations more accessible. Prior arts including the deep learning
models have been devoted to solving the problem of long MRI imaging time.
Recently, deep generative models have exhibited great potentials in algorithm
robustness and usage flexibility. Nevertheless, no existing such schemes that
can be learned or employed directly to the k-space measurement. Furthermore,
how do the deep generative models work well in hybrid domain is also worth to
be investigated. In this work, by taking advantage of the deep en-ergy-based
models, we propose a k-space and image domain collaborative generative model to
comprehensively estimate the MR data from under-sampled measurement.
Experimental comparisons with the state-of-the-arts demonstrated that the
proposed hybrid method has less error in reconstruction and is more stable
under different acceleration factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slice Imputation: Intermediate Slice Interpolation for Anisotropic 3D
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaotao Wu, Jia Wei, Jiabing Wang, Rui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel frame-interpolation-based method for slice imputation to
improve segmentation accuracy for anisotropic 3D medical images, in which the
number of slices and their corresponding segmentation labels can be increased
between two consecutive slices in anisotropic 3D medical volumes. Unlike
previous inter-slice imputation methods, which only focus on the smoothness in
the axial direction, this study aims to improve the smoothness of the
interpolated 3D medical volumes in all three directions: axial, sagittal, and
coronal. The proposed multitask inter-slice imputation method, in particular,
incorporates a smoothness loss function to evaluate the smoothness of the
interpolated 3D medical volumes in the through-plane direction (sagittal and
coronal). It not only improves the resolution of the interpolated 3D medical
volumes in the through-plane direction but also transforms them into isotropic
representations, which leads to better segmentation performances. Experiments
on whole tumor segmentation in the brain, liver tumor segmentation, and
prostate segmentation indicate that our method outperforms the competing slice
imputation methods on both computed tomography and magnetic resonance images
volumes in most cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upsampling Autoencoder for <span class="highlight-title">Self-Supervised</span> Point Cloud Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Zhang, Jian Shi, Xuan Deng, Zizhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer-aided design (CAD) community, the point cloud data is pervasively
applied in reverse engineering, where the point cloud analysis plays an
important role. While a large number of supervised learning methods have been
proposed to handle the unordered point clouds and demonstrated their remarkable
success, their performance and applicability are limited to the costly data
annotation. In this work, we propose a novel self-supervised pretraining model
for point cloud learning without human annotations, which relies solely on
upsampling operation to perform feature learning of point cloud in an effective
manner. The key premise of our approach is that upsampling operation encourages
the network to capture both high-level semantic information and low-level
geometric information of the point cloud, thus the downstream tasks such as
classification and segmentation will benefit from the pre-trained model.
Specifically, our method first conducts the random subsampling from the input
point cloud at a low proportion e.g., 12.5%. Then, we feed them into an
encoder-decoder architecture, where an encoder is devised to operate only on
the subsampled points, along with a upsampling decoder is adopted to
reconstruct the original point cloud based on the learned features. Finally, we
design a novel joint loss function which enforces the upsampled points to be
similar with the original point cloud and uniformly distributed on the
underlying shape surface. By adopting the pre-trained encoder weights as
initialisation of models for downstream tasks, we find that our UAE outperforms
previous state-of-the-art methods in shape classification, part segmentation
and point cloud upsampling tasks. Code will be made publicly available upon
acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Mixup for Data-efficient Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Liu, Siyuan Li, Ge Wang, Cheng Tan, Lirong Wu, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixup is an efficient data augmentation approach that improves the
generalization of neural networks by smoothing the decision boundary with mixed
data. Recently, dynamic mixup methods improve previous static policies (e.g.,
linear interpolation) by maximizing discriminative regions or maintaining the
salient objects in mixed samples. We notice that The mixed samples from dynamic
policies are more separable than the static ones while preventing models from
overfitting. Inspired by this finding, we first argue that there exists an
over-smoothing issue in the mixup objective, which focuses on regression the
mixing ratio instead of identifying discriminative features. We are therefore
prompted to propose a decoupled mixup (DM) loss that can adaptively mine
discriminative features without losing smoothness. DM enables static mixup
methods to achieve comparable performance with dynamic methods while avoiding
heavy computational overhead. This also leads to an interesting objective
design problem for mixup training that we need to focus not only on smoothing
the decision boundaries but also on identifying discriminative features.
Extensive experiments on supervised and semi-supervised learning benchmarks
across seven classification datasets validate the effectiveness of DM by
equipping with various mixup methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first preprint version, 21 pages. The source code is available at
  https://github.com/Westlake-AI/openmixup</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAutoDet: Efficient Architecture Search for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxing Wang, Jiale Lin, Junchi Yan, Juanping Zhao, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training CNN for detection is time-consuming due to the large dataset and
complex network modules, making it hard to search architectures on detection
datasets directly, which usually requires vast search costs (usually tens and
even hundreds of GPU-days). In contrast, this paper introduces an efficient
framework, named EAutoDet, that can discover practical backbone and FPN
architectures for object detection in 1.4 GPU-days. Specifically, we construct
a supernet for both backbone and FPN modules and adopt the differentiable
method. To reduce the GPU memory requirement and computational cost, we propose
a kernel reusing technique by sharing the weights of candidate operations on
one edge and consolidating them into one convolution. A dynamic channel
refinement strategy is also introduced to search channel numbers. Extensive
experiments show significant efficacy and efficiency of our method. In
particular, the discovered architectures surpass state-of-the-art object
detection NAS methods and achieve 40.1 mAP with 120 FPS and 49.2 mAP with 41.3
FPS on COCO test-dev set. We also transfer the discovered architectures to
rotation detection task, which achieve 77.05 mAP$_{\text{50}}$ on DOTA-v1.0
test set with 21.1M parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Liang, Tiancai Wang, Xiangyu Zhang, Jian Sun, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megviiresearch/TEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Segmentation with Active Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneesh Rangnekar, Christopher Kanan, Matthew Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using deep learning, we now have the ability to create exceptionally good
semantic segmentation systems; however, collecting the prerequisite pixel-wise
annotations for training images remains expensive and time-consuming.
Therefore, it would be ideal to minimize the number of human annotations needed
when creating a new dataset. Here, we address this problem by proposing a novel
algorithm that combines active learning and semi-supervised learning. Active
learning is an approach for identifying the best unlabeled samples to annotate.
While there has been work on active learning for segmentation, most methods
require annotating all pixel objects in each image, rather than only the most
informative regions. We argue that this is inefficient. Instead, our active
learning approach aims to minimize the number of annotations per-image. Our
method is enriched with semi-supervised learning, where we use pseudo labels
generated with a teacher-student framework to identify image regions that help
disambiguate confused classes. We also integrate mechanisms that enable better
performance on imbalanced label distributions, which have not been studied
previously for active learning in semantic segmentation. In experiments on the
CamVid and CityScapes datasets, our method obtains over 95% of the network's
performance on the full-training set using less than 19% of the training data,
whereas the previous state of the art required 40% of the training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSRRTracker: Dynamic Search Region Refinement for Attention-based
  Siamese Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JiaXu Wan, Hong Zhang, Jin Zhang, Yuan Ding, Yifan Yang, Yan Li, Xuliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many multi-object tracking (MOT) methods follow the framework of "tracking by
detection", which associates the target objects-of-interest based on the
detection results. However, due to the separate models for detection and
association, the tracking results are not optimal.Moreover, the speed is
limited by some cumbersome association methods to achieve high tracking
performance. In this work, we propose an end-to-end MOT method, with a Gaussian
filter-inspired dynamic search region refinement module to dynamically filter
and refine the search region by considering both the template information from
the past frames and the detection results from the current frame with little
computational burden, and a lightweight attention-based tracking head to
achieve the effective fine-grained instance association. Extensive experiments
and ablation study on MOT17 and MOT20 datasets demonstrate that our method can
achieve the state-of-the-art performance with reasonable speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation
  with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Liu, Yunhe Gao, Qilong Zhangli, Zhennan Yan, Mu Zhou, Dimitris Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining information from multi-view images is crucial to improve the
performance and robustness of automated methods for disease diagnosis. However,
due to the non-alignment characteristics of multi-view images, building
correlation and data fusion across views largely remain an open problem. In
this study, we present TransFusion, a Transformer-based architecture to merge
divergent multi-view imaging information using convolutional layers and
powerful attention mechanisms. In particular, the Divergent Fusion Attention
(DiFA) module is proposed for rich cross-view context modeling and semantic
dependency mining, addressing the critical issue of capturing long-range
correlations between unaligned data from different image views. We further
propose the Multi-Scale Attention (MSA) to collect global correspondence of
multi-scale feature representations. We evaluate TransFusion on the
Multi-Disease, Multi-View \& Multi-Center Right Ventricular Segmentation in
Cardiac MRI (M\&Ms-2) challenge cohort. TransFusion demonstrates leading
performance against the state-of-the-art methods and opens up new perspectives
for multi-view imaging integration towards robust medical image segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intermediate-level Attack Framework on The Basis of Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Guo, Qizhang Li, Wangmeng Zuo, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper substantially extends our work published at ECCV, in which an
intermediate-level attack was proposed to improve the transferability of some
baseline adversarial examples. We advocate to establish a direct linear mapping
from the intermediate-level discrepancies (between adversarial features and
benign features) to classification prediction loss of the adversarial example.
In this paper, we delve deep into the core components of such a framework by
performing comprehensive studies and extensive experiments. We show that 1) a
variety of linear regression models can all be considered in order to establish
the mapping, 2) the magnitude of the finally obtained intermediate-level
discrepancy is linearly correlated with adversarial transferability, 3) further
boost of the performance can be achieved by performing multiple runs of the
baseline attack with random initialization. By leveraging these findings, we
achieve new state-of-the-arts on transfer-based $\ell_\infty$ and $\ell_2$
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LocATe: End-to-end Localization of Actions in 3D with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankai Sun, <span class="highlight-author">Bolei Zhou</span>, Michael J. Black, Arjun Chandrasekaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding a person's behavior from their 3D motion is a fundamental
problem in computer vision with many applications. An important component of
this problem is 3D Temporal Action Localization (3D-TAL), which involves
recognizing what actions a person is performing, and when. State-of-the-art
3D-TAL methods employ a two-stage approach in which the action span detection
task and the action recognition task are implemented as a cascade. This
approach, however, limits the possibility of error-correction. In contrast, we
propose LocATe, an end-to-end approach that jointly localizes and recognizes
actions in a 3D sequence. Further, unlike existing autoregressive models that
focus on modeling the local context in a sequence, LocATe's transformer model
is capable of capturing long-term correlations between actions in a sequence.
Unlike transformer-based object-detection and classification models which
consider image or patch features as input, the input in 3D-TAL is a long
sequence of highly correlated frames. To handle the high-dimensional input, we
implement an effective input representation, and overcome the diffuse attention
across long time horizons by introducing sparse attention in the model. LocATe
outperforms previous approaches on the existing PKU-MMD 3D-TAL benchmark
(mAP=93.2%). Finally, we argue that benchmark datasets are most useful where
there is clear room for performance improvement. To that end, we introduce a
new, challenging, and more realistic benchmark dataset, BABEL-TAL-20 (BT20),
where the performance of state-of-the-art methods is significantly worse. The
dataset and code for the method will be available for research purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceMap: Towards Unsupervised Face Clustering via Map Equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Yu, Yifan Yang, Aibo Wang, Ling Xing, Hanling Yi, Guangming Lu, Xiaoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face clustering is an essential task in computer vision due to the explosion
of related applications such as augmented reality or photo album management.
The main challenge of this task lies in the imperfectness of similarities among
image feature representations. Given an existing feature extraction model, it
is still an unresolved problem that how can the inherent characteristics of
similarities of unlabelled images be leveraged to improve the clustering
performance. Motivated by answering the question, we develop an effective
unsupervised method, named as FaceMap, by formulating face clustering as a
process of non-overlapping community detection, and minimizing the entropy of
information flows on a network of images. The entropy is denoted by the map
equation and its minimum represents the least description of paths among images
in expectation. Inspired by observations on the ranked transition probabilities
in the affinity graph constructed from facial images, we develop an outlier
detection strategy to adaptively adjust transition probabilities among images.
Experiments with ablation studies demonstrate that FaceMap significantly
outperforms existing methods and achieves new state-of-the-arts on three
popular large-scale datasets for face clustering, e.g., an absolute improvement
of more than $10\%$ and $4\%$ comparing with prior unsupervised and supervised
methods respectively in terms of average of Pairwise F-score. Our code is
publicly available on github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Makes RAFT Better Than PWC-Net? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deqing Sun, Charles Herrmann, Fitsum Reda, Michael Rubinstein, David Fleet, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How important are training details and datasets to recent optical flow models
like RAFT? And do they generalize? To explore these questions, rather than
develop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and
RAFT, with a common set of modern training techniques and datasets, and observe
significant performance gains, demonstrating the importance and generality of
these training details. Our newly trained PWC-Net and IRR-PWC models show
surprisingly large improvements, up to 30% versus original published results on
Sintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on
KITTI 2015 while being 3x faster during inference. Our newly trained RAFT
achieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all
published optical flow methods at the time of writing. Our results demonstrate
the benefits of separating the contributions of models, training techniques and
datasets when analyzing performance gains of optical flow methods. Our source
code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular Vision-based Prediction of Cut-in Maneuvers with LSTM Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yagiz Nalcakan, Yalin Bastanlar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced driver assistance and automated driving systems should be capable of
predicting and avoiding dangerous situations. This study proposes a method to
predict potentially dangerous cut-in maneuvers happening in the ego lane. We
follow a computer vision-based approach that only employs a single in-vehicle
RGB camera, and we classify the target vehicle's maneuver based on the recent
video frames. Our algorithm consists of a CNN-based vehicle detection and
tracking step and an LSTM-based maneuver classification step. It is more
computationally efficient than other vision-based methods since it exploits a
small number of features for the classification step rather than feeding CNNs
with RGB frames. We evaluated our approach on a publicly available driving
dataset and a lane change detection dataset. We obtained 0.9585 accuracy with
side-aware two-class (cut-in vs. lane-pass) classification models. Experiment
results also reveal that our approach outperforms state-of-the-art approaches
when used for lane change detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Compression of Generative <span class="highlight-title">Pre-train</span>ed Language Models via Quantization <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, <span class="highlight-author">Qun Liu</span>, Ping Luo, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HP-Capsule: Unsupervised Face Part Discovery by Hierarchical Parsing
  Capsule Network <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Xiangyu Zhu, Xiaomei Zhang, Zidu Wang, Zhaoxiang Zhang, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capsule networks are designed to present the objects by a set of parts and
their relationships, which provide an insight into the procedure of visual
perception. Although recent works have shown the success of capsule networks on
simple objects like digits, the human faces with homologous structures, which
are suitable for capsules to describe, have not been explored. In this paper,
we propose a Hierarchical Parsing Capsule Network (HP-Capsule) for unsupervised
face subpart-part discovery. When browsing large-scale face images without
labels, the network first encodes the frequently observed patterns with a set
of explainable subpart capsules. Then, the subpart capsules are assembled into
part-level capsules through a Transformer-based Parsing Module (TPM) to learn
the compositional relations between them. During training, as the face
hierarchy is progressively built and refined, the part capsules adaptively
encode the face parts with semantic consistency. HP-Capsule extends the
application of capsule networks from digits to human faces and takes a step
forward to show how the neural networks understand homologous objects without
human intervention. Besides, HP-Capsule gives unsupervised face segmentation
results by the covered regions of part capsules, enabling qualitative and
quantitative evaluation. Experiments on BP4D and Multi-PIE datasets show the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier Disentangled Space-Time Attention for Aerial Video Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divya Kothandaraman, Tianrui Guan, Xijun Wang, Sean Hu, Ming Lin, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm, Fourier Activity Recognition (FAR), for UAV video
activity recognition. Our formulation uses a novel Fourier object
disentanglement method to innately separate out the human agent (which is
typically small) from the background. Our disentanglement technique operates in
the frequency domain to characterize the extent of temporal change of spatial
pixels, and exploits convolution-multiplication properties of Fourier transform
to map this representation to the corresponding object-background entangled
features obtained from the network. To encapsulate contextual information and
long-range space-time dependencies, we present a novel Fourier Attention
algorithm, which emulates the benefits of self-attention by modeling the
weighted outer product in the frequency domain. Our Fourier attention
formulation uses much fewer computations than self-attention. We have evaluated
our approach on multiple UAV datasets including UAV Human RGB, UAV Human Night,
Drone Action, and NEC Drone. We demonstrate a relative improvement of 8.02% -
38.69% in top-1 accuracy and up to 3 times faster over prior works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hidden Convexity of Wasserstein GANs: Interpretable Generative Models
  with Closed-Form Solutions <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.05680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.05680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) are commonly used for modeling complex
distributions of data. Both the generators and discriminators of GANs are often
modeled by neural networks, posing a non-transparent optimization problem which
is non-convex and non-concave over the generator and discriminator,
respectively. Such networks are often heuristically optimized with gradient
descent-ascent (GDA), but it is unclear whether the optimization problem
contains any saddle points, or whether heuristic methods can find them in
practice. In this work, we analyze the training of Wasserstein GANs with
two-layer neural network discriminators through the lens of convex duality, and
for a variety of generators expose the conditions under which Wasserstein GANs
can be solved exactly with convex optimization approaches, or can be
represented as convex-concave games. Using this convex duality interpretation,
we further demonstrate the impact of different activation functions of the
discriminator. Our observations are verified with numerical results
demonstrating the power of the convex interpretation, with applications in
progressive training of convex architectures corresponding to linear generators
and quadratic-activation discriminators for CelebA image generation. The code
for our experiments is available at https://github.com/ardasahiner/ProCoGAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as paper in ICLR 2022. First two authors contributed
  equally to this work; 34 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Efficient <span class="highlight-title">Transformer</span>-Based Image <span class="highlight-title">Pre-train</span>ing for Low-Level Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Li, Xin Lu, Shengju Qian, Jiangbo Lu, Xiangyu Zhang, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training has marked numerous state of the arts in high-level computer
vision, while few attempts have ever been made to investigate how pre-training
acts in image processing systems. In this paper, we tailor transformer-based
pre-training regimes that boost various low-level tasks. To comprehensively
diagnose the influence of pre-training, we design a whole set of principled
evaluation tools that uncover its effects on internal representations. The
observations demonstrate that pre-training plays strikingly different roles in
low-level tasks. For example, pre-training introduces more local information to
higher layers in super-resolution (SR), yielding significant performance gains,
while pre-training hardly affects internal feature representations in
denoising, resulting in limited gains. Further, we explore different methods of
pre-training, revealing that multi-related-task pre-training is more effective
and data-efficient than other alternatives. Finally, we extend our study to
varying data scales and model sizes, as well as comparisons between
transformers and CNNs-based architectures. Based on the study, we successfully
develop state-of-the-art models for multiple low-level tasks. Code is released
at https://github.com/fenglinglwb/EDT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Built Environment Features for Planning Research with
  Computer Vision: A <span class="highlight-title">Review</span> and Discussion of State-of-the-Art Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqing Li, Hao Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is an extended abstract for a presentation at The 17th International
Conference on CUPUM - Computational Urban Planning and Urban Management in June
2021. This study presents an interdisciplinary synthesis of the
state-of-the-art approaches in computer vision technologies to extract built
environment features that could improve the robustness of empirical research in
planning. We discussed the findings from the review of studies in both planning
and computer science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CUPUM 2021 (The 17th International Conference on Computational Urban
  Planning and Urban Management)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimCVD: Simple <span class="highlight-title">Contrastive</span> Voxel-Wise Representation <span class="highlight-title">Distillation</span> for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06227v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06227v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, most existing
learning-based approaches usually suffer from limited manually annotated
medical data, which poses a major practical problem for accurate and robust
medical image segmentation. In addition, most existing semi-supervised
approaches are usually not robust compared with the supervised counterparts,
and also lack explicit modeling of geometric structure and semantic
information, both of which limit the segmentation accuracy. In this work, we
present SimCVD, a simple contrastive distillation framework that significantly
advances state-of-the-art voxel-wise representation learning. We first describe
an unsupervised training strategy, which takes two views of an input volume and
predicts their signed distance maps of object boundaries in a contrastive
objective, with only two independent dropout as mask. This simple approach
works surprisingly well, performing on the same level as previous fully
supervised methods with much less labeled data. We hypothesize that dropout can
be viewed as a minimal form of data augmentation and makes the network robust
to representation collapse. Then, we propose to perform structural distillation
by distilling pair-wise similarities. We evaluate SimCVD on two popular
datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT
dataset. The results on the LA dataset demonstrate that, in two types of
labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of
90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to
previous best results. Our method can be trained in an end-to-end fashion,
showing the promise of utilizing SimCVD as a general framework for downstream
tasks, such as medical image synthesis, enhancement, and registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Medical Imaging (IEEE-TMI) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Data-Efficient Detection <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Jing Zhang, Yang Cao, Yongliang Shen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection Transformers have achieved competitive performance on the
sample-rich COCO dataset. However, we show most of them suffer from significant
performance drops on small-size datasets, like Cityscapes. In other words, the
detection transformers are generally data-hungry. To tackle this problem, we
empirically analyze the factors that affect data efficiency, through a
step-by-step transition from a data-efficient RCNN variant to the
representative DETR. The empirical results suggest that sparse feature sampling
from local image areas holds the key. Based on this observation, we alleviate
the data-hungry issue of existing detection transformers by simply alternating
how key and value sequences are constructed in the cross-attention layer, with
minimum modifications to the original models. Besides, we introduce a simple
yet effective label augmentation method to provide richer supervision and
improve data efficiency. Experiments show that our method can be readily
applied to different detection transformers and improve their performance on
both small-size and sample-rich datasets. Code will be made publicly available
at \url{https://github.com/encounter1997/DE-DETRs}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/encounter1997/DE-DETRs and
  https://github.com/encounter1997/DE-CondDETR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention based Memory video portrait matting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufeng Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed a novel trimap free video matting method based on the attention
mechanism. By the nature of the problem, most existing approaches use either
multiple computational expansive modules or complex algorithms to exploit
temporal information fully. We designed a temporal aggregation module to
compute the temporal coherence between the current frame and its two previous
frames.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip
  Connections and Hybrid Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo R. Corral-Soto, Mrigank Rochan, Yannis Y. He, Shubhra Aich, <span class="highlight-author">Yang Liu</span>, Liu Bingbing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we address the challenging problem of domain adaptation in
LiDAR semantic segmentation. We consider the setting where we have a
fully-labeled data set from source domain and a target domain with a few
labeled and many unlabeled examples. We propose a domain adaption framework
that mitigates the issue of domain shift and produces appealing performance on
the target domain. To this end, we develop a GAN-based image-to-image
translation engine that has generators with alternating connections, and couple
it with a state-of-the-art LiDAR semantic segmentation network. Our framework
is hybrid in nature in the sense that our model learning is composed of
self-supervision, semi-supervision and unsupervised learning. Extensive
experiments on benchmark LiDAR semantic segmentation data sets demonstrate that
our method achieves superior performance in comparison to strong baselines and
prior arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1) Introduced Fig 1, 2) Simplified Fig. 2 diagram, 3) Fixed typos in
  losses, 4) Introduced Fig. 3, 5) Updated evaluation results, included
  evaluation on SemanticPOSS, 6) Introduced Table 3 - effects on covariance
  matrix and mean, 7) Updated Fig. 5, 8) Added more references. Improved
  writing in general, especially the motivation and description of each element
  and contribution from the method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moving Towards Centers: Re-ranking with Attention and Memory for
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.01447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.01447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Zhou, Yi Wang, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Re-ranking utilizes contextual information to optimize the initial ranking
list of person or vehicle re-identification (re-ID), which boosts the retrieval
performance at post-processing steps. This paper proposes a re-ranking network
to predict the correlations between the probe and top-ranked neighbor samples.
Specifically, all the feature embeddings of query and gallery images are
expanded and enhanced by a linear combination of their neighbors, with the
correlation prediction serving as discriminative combination weights. The
combination process is equivalent to moving independent embeddings toward the
identity centers, improving cluster compactness. For correlation prediction, we
first aggregate the contextual information for probe's k-nearest neighbors via
the Transformer encoder. Then, we distill and refine the probe-related features
into the Contextual Memory cell via attention mechanism. Like humans that
retrieve images by not only considering probe images but also memorizing the
retrieved ones, the Contextual Memory produces multi-view descriptions for each
instance. Finally, the neighbors are reconstructed with features fetched from
the Contextual Memory, and a binary classifier predicts their correlations with
the probe. Experiments on six widely-used person and vehicle re-ID benchmarks
demonstrate the effectiveness of the proposed method. Especially, our method
surpasses the state-of-the-art re-ranking approaches on large-scale datasets by
a significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP
improvements on VERI-Wild, MSMT17, and VehicleID datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages. Accepted for Publication at IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElasticFace: Elastic Margin Loss for Deep Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09416v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09416v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadi Boutros, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning discriminative face features plays a major role in building
high-performing face recognition models. The recent state-of-the-art face
recognition solutions proposed to incorporate a fixed penalty margin on
commonly used classification loss function, softmax loss, in the normalized
hypersphere to increase the discriminative power of face recognition models, by
minimizing the intra-class variation and maximizing the inter-class variation.
Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the
geodesic distance between and within the different identities can be equally
learned using a fixed penalty margin. However, such a learning objective is not
realistic for real data with inconsistent inter-and intra-class variation,
which might limit the discriminative and generalizability of the face
recognition model. In this paper, we relax the fixed penalty margin constrain
by proposing elastic penalty margin loss (ElasticFace) that allows flexibility
in the push for class separability. The main idea is to utilize random margin
values drawn from a normal distribution in each training iteration. This aims
at giving the decision boundary chances to extract and retract to allow space
for flexible class separability learning. We demonstrate the superiority of our
ElasticFace loss over ArcFace and CosFace losses, using the same geometric
transformation, on a large set of mainstream benchmarks. From a wider
perspective, our ElasticFace has advanced the state-of-the-art face recognition
performance on seven out of nine mainstream benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motron: <span class="highlight-title">Multimodal</span> Probabilistic Human Motion Forecasting <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Marco Pavone, Markus Ryll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Part Discovery from <span class="highlight-title">Contrastive</span> Reconstruction <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.06349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.06349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Choudhury, Iro Laina, Christian Rupprecht, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of self-supervised visual representation learning is to learn
strong, transferable image representations, with the majority of research
focusing on object or scene level. On the other hand, representation learning
at part level has received significantly less attention. In this paper, we
propose an unsupervised approach to object part discovery and segmentation and
make three contributions. First, we construct a proxy task through a set of
objectives that encourages the model to learn a meaningful decomposition of the
image into its parts. Secondly, prior work argues for reconstructing or
clustering pre-computed features as a proxy to parts; we show empirically that
this alone is unlikely to find meaningful parts; mainly because of their low
resolution and the tendency of classification networks to spatially smear out
information. We suggest that image reconstruction at the level of pixels can
alleviate this problem, acting as a complementary cue. Lastly, we show that the
standard evaluation based on keypoint regression does not correlate well with
segmentation quality and thus introduce different metrics, NMI and ARI, that
better characterize the decomposition of objects into parts. Our method yields
semantic parts which are consistent across fine-grained but visually distinct
categories, outperforming the state of the art on three benchmark datasets.
Code is available at the project page:
https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021. Project page:
  https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Federated Learning with Local Regularization and
  Sparsification <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anda Cheng, Peisong Wang, Xi Sheryl Zhang, Jian Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-level differential privacy (DP) provides certifiable privacy guarantees
to the information that is specific to any user's data in federated learning.
Existing methods that ensure user-level DP come at the cost of severe accuracy
decrease. In this paper, we study the cause of model performance degradation in
federated learning under user-level DP guarantee. We find the key to solving
this issue is to naturally restrict the norm of local updates before executing
operations that guarantee DP. To this end, we propose two techniques, Bounded
Local Update Regularization and Local Update Sparsification, to increase model
quality without sacrificing privacy. We provide theoretical analysis on the
convergence of our framework and give rigorous privacy guarantees. Extensive
experiments show that our framework significantly improves the privacy-utility
trade-off over the state-of-the-arts for federated learning with user-level DP
guarantee.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UVCGAN: UNet Vision <span class="highlight-title">Transformer</span> cycle-consistent GAN for unpaired
  image-to-image translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitrii Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shinjae Yoo, Meifeng Lin, Brett Viren, Yihui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation has broad applications in art, design, and
scientific simulations. The original CycleGAN model emphasizes one-to-one
mapping via a cycle-consistent loss, while more recent works promote
one-to-many mapping to boost the diversity of the translated images. With
scientific simulation and one-to-one needs in mind, this work examines if
equipping CycleGAN with a vision transformer (ViT) and employing advanced
generative adversarial network (GAN) training techniques can achieve better
performance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is
compared with previous best-performing models on open benchmark image-to-image
translation datasets, Selfie2Anime and CelebA. UVCGAN performs better and
retains a strong correlation between the original and translated images. An
accompanying ablation study shows that the gradient penalty and BERT-like
pre-training also contribute to the improvement.~To promote reproducibility and
open science, the source code, hyperparameter configurations, and pre-trained
model will be made available at: https://github.com/LS4GAN/uvcgan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Visual-<span class="highlight-title">Prompt</span> Temporal Answering Grounding in Medical
  Instructional Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06667v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06667v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Li, Yixuan Weng, Bin Sun, Shutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show that the proposed VPTSL
outperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a
large margin, which demonstrates the effectiveness of visual prompt and the
text span predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.10382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.10382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training machine learning models in a meaningful order, from the easy samples
to the hard ones, using curriculum learning can provide performance
improvements over the standard training approach based on random data
shuffling, without any additional computational costs. Curriculum learning
strategies have been successfully employed in all areas of machine learning, in
a wide range of tasks. However, the necessity of finding a way to rank the
samples from easy to hard, as well as the right pacing function for introducing
more difficult data can limit the usage of the curriculum approaches. In this
survey, we show how these limits have been tackled in the literature, and we
present different curriculum learning instantiations for various tasks in
machine learning. We construct a multi-perspective taxonomy of curriculum
learning approaches by hand, considering various classification criteria. We
further build a hierarchical tree of curriculum learning methods using an
agglomerative clustering algorithm, linking the discovered clusters with our
taxonomy. At the end, we provide some interesting directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask-guided Spectral-wise <span class="highlight-title">Transformer</span> for Efficient Hyperspectral Image
  Reconstruction <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) reconstruction aims to recover the 3D
spatial-spectral signal from a 2D measurement in the coded aperture snapshot
spectral imaging (CASSI) system. The HSI representations are highly similar and
correlated across the spectral dimension. Modeling the inter-spectra
interactions is beneficial for HSI reconstruction. However, existing CNN-based
methods show limitations in capturing spectral-wise similarity and long-range
dependencies. Besides, the HSI information is modulated by a coded aperture
(physical mask) in CASSI. Nonetheless, current algorithms have not fully
explored the guidance effect of the mask for HSI restoration. In this paper, we
propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI
reconstruction. Specifically, we present a Spectral-wise Multi-head
Self-Attention (S-MSA) that treats each spectral feature as a token and
calculates self-attention along the spectral dimension. In addition, we
customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to
spatial regions with high-fidelity spectral representations. Extensive
experiments show that our MST significantly outperforms state-of-the-art (SOTA)
methods on simulation and real HSI datasets while requiring dramatically
cheaper computational and memory costs. Code and pre-trained models are
available at https://github.com/caiyuanhao1998/MST/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022; The first Transformer-based method for snapshot
  compressive imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive
  Pseudo Labeling and Informative Active Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Zhang, Lei Zhu, James Hallinan, Andrew Makmur, Shengyu Zhang, Qingpeng Cai, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel semi-supervised learning (SSL) framework
named BoostMIS that combines adaptive pseudo labeling and informative active
annotation to unleash the potential of medical image SSL models: (1) BoostMIS
can adaptively leverage the cluster assumption and consistency regularization
of the unlabeled data according to the current learning status. This strategy
can adaptively generate one-hot "hard" labels converted from task model
predictions for better task model training. (2) For the unselected unlabeled
images with low confidence, we introduce an Active learning (AL) algorithm to
find the informative samples as the annotation candidates by exploiting virtual
adversarial perturbation and model's density-aware entropy. These informative
candidates are subsequently fed into the next training cycle for better SSL
label propagation. Notably, the adaptive pseudo-labeling and informative active
annotation form a learning closed-loop that are mutually collaborative to boost
medical image SSL. To verify the effectiveness of the proposed method, we
collected a metastatic epidural spinal cord compression (MESCC) dataset that
aims to optimize MESCC diagnosis and classification for improved specialist
referral and treatment. We conducted an extensive experimental study of
BoostMIS on MESCC and another public dataset COVIDx. The experimental results
verify our framework's effectiveness and generalisability for different medical
image datasets with a significant improvement over various state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Atlas Generative Models and Geodesic Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.00264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.00264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Stolberg-Larsen, Stefan Sommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative neural networks have a well recognized ability to estimate
underlying manifold structure of high dimensional data. However, if a single
latent space is used, it is not possible to faithfully represent a manifold
with topology different from Euclidean space. In this work we define the
general class of Atlas Generative Models (AGMs), models with hybrid
discrete-continuous latent space that estimate an atlas on the underlying data
manifold together with a partition of unity on the data space. We identify
existing examples of models from various popular generative paradigms that fit
into this class. Due to the atlas interpretation, ideas from non-linear latent
space analysis and statistics, e.g. geodesic interpolation, which has
previously only been investigated for models with simply connected latent
spaces, may be extended to the entire class of AGMs in a natural way. We
exemplify this by generalizing an algorithm for graph based geodesic
interpolation to the setting of AGMs, and verify its performance
experimentally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pyramid Fusion <span class="highlight-title">Transformer</span> for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Qin, Jianbo Liu, Xiaolin Zhang, Maoqing Tian, Aojun Zhou, Shuai Yi, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently proposed MaskFormer gives a refreshed perspective on the task of
semantic segmentation: it shifts from the popular pixel-level classification
paradigm to a mask-level classification method. In essence, it generates paired
probabilities and masks corresponding to category segments and combines them
during inference for the segmentation maps. In our study, we find that per-mask
classification decoder on top of a single-scale feature is not effective enough
to extract reliable probability or mask. To mine for rich semantic information
across the feature pyramid, we propose a transformer-based Pyramid Fusion
Transformer (PFT) for per-mask approach semantic segmentation with multi-scale
features. The proposed transformer decoder performs cross-attention between the
learnable queries and each spatial feature from the feature pyramid in parallel
and uses cross-scale inter-query attention to exchange complimentary
information. We achieve competitive performance on three widely used semantic
segmentation datasets. In particular, on ADE20K validation set, our result with
Swin-B backbone surpasses that of MaskFormer's with a much larger Swin-L
backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU
and 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale
56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on
the dataset. Extensive experiments on three widely used semantic segmentation
datasets verify the effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoNeRF: Generalizing NeRF with Geometry Priors <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Johari, Yann Lepoittevin, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GeoNeRF, a generalizable photorealistic novel view synthesis
method based on neural radiance fields. Our approach consists of two main
stages: a geometry reasoner and a renderer. To render a novel view, the
geometry reasoner first constructs cascaded cost volumes for each nearby source
view. Then, using a Transformer-based attention mechanism and the cascaded cost
volumes, the renderer infers geometry and appearance, and renders detailed
images via classical volume rendering techniques. This architecture, in
particular, allows sophisticated occlusion reasoning, gathering information
from consistent source views. Moreover, our method can easily be fine-tuned on
a single scene, and renders competitive results with per-scene optimized neural
rendering methods with a fraction of computational cost. Experiments show that
GeoNeRF outperforms state-of-the-art generalizable neural rendering models on
various synthetic and real datasets. Lastly, with a slight modification to the
geometry reasoner, we also propose an alternative model that adapts to RGBD
images. This model directly exploits the depth information often available
thanks to depth sensors. The implementation code is available at
https://www.idiap.ch/paper/geonerf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseUNets with feedback non-local attention for the segmentation of
  specular microscopy images of the corneal endothelium with guttae 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan P. Vigueras-Guillén, Jeroen van Rooij, Bart T. H. van Dooren, Hans G. Lemij, Esma Islamaj, Lucas J. van Vliet, Koenraad A. Vermeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To estimate the corneal endothelial parameters from specular microscopy
images depicting cornea guttata (Fuchs dystrophy), we propose a new deep
learning methodology that includes a novel attention mechanism named feedback
non-local attention (fNLA). Our approach first infers the cell edges, then
selects the cells that are well detected, and finally applies a postprocessing
method to correct mistakes and provide the binary segmentation from which the
corneal parameters are estimated (cell density [ECD], coefficient of variation
[CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired
with a Topcon SP-1P microscope, 500 of which contained guttae. Manual
segmentation was performed in all images. We compared the results of different
networks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with
fNLA provided the best performance, with a mean absolute error of 23.16
[cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6
times smaller than the error obtained by Topcon's built-in software. Our
approach handled the cells affected by guttae remarkably well, detecting cell
edges occluded by small guttae while discarding areas covered by large guttae.
Overall, the proposed method obtained accurate estimations in extremely
challenging specular images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 2 tables. Code:
  https://github.com/jpviguerasguillen/feedback-non-local-attention-fNLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseCLIP: Language-Guided Dense Prediction with Context-Aware <span class="highlight-title">Prompt</span>ing <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress has shown that large-scale pre-training using contrastive
image-text pairs can be a promising alternative for high-quality visual
representation learning from natural language supervision. Benefiting from a
broader source of supervision, this new paradigm exhibits impressive
transferability to downstream classification tasks and datasets. However, the
problem of transferring the knowledge learned from image-text pairs to more
complex dense prediction tasks has barely been visited. In this work, we
present a new framework for dense prediction by implicitly and explicitly
leveraging the pre-trained knowledge from CLIP. Specifically, we convert the
original image-text matching problem in CLIP to a pixel-text matching problem
and use the pixel-text score maps to guide the learning of dense prediction
models. By further using the contextual information from the image to prompt
the language model, we are able to facilitate our model to better exploit the
pre-trained knowledge. Our method is model-agnostic, which can be applied to
arbitrary dense prediction systems and various pre-trained visual backbones
including both CLIP models and ImageNet pre-trained models. Extensive
experiments demonstrate the superior performance of our methods on semantic
segmentation, object detection, and instance segmentation tasks. Code is
available at https://github.com/raoyongming/DenseCLIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022. Project page:
  https://denseclip.ivg-research.xyz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Autoencoder Training Performance for Hyperspectral Unmixing
  with Network Reinitialisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.13748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.13748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Książek, Przemysław Głomb, Michał Romaszewski, Michał Cholewa, Bartosz Grabowski, Krisztián Búza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks, in particular autoencoders, are one of the most promising
solutions for unmixing hyperspectral data, i.e. reconstructing the spectra of
observed substances (endmembers) and their relative mixing fractions
(abundances), which is needed for effective hyperspectral analysis and
classification. However, as we show in this paper, the training of autoencoders
for unmixing is highly dependent on weights initialisation; some sets of
weights lead to degenerate or low-performance solutions, introducing negative
bias in the expected performance. In this work, we experimentally investigate
autoencoders stability as well as network reinitialisation methods based on
coefficients of neurons' dead activations. We demonstrate that the proposed
techniques have a positive effect on autoencoder training in terms of
reconstruction, abundances and endmembers errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Continual Learning on Class Incremental Blurry Task Configuration
  with Anytime Inference <span class="chip">ICLR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseo Koh, Dahyun Kim, Jung-Woo Ha, Jonghyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite rapid advances in continual learning, a large body of research is
devoted to improving performance in the existing setups. While a handful of
work do propose new continual learning setups, they still lack practicality in
certain aspects. For better practicality, we first propose a novel continual
learning setup that is online, task-free, class-incremental, of blurry task
boundaries and subject to inference queries at any moment. We additionally
propose a new metric to better measure the performance of the continual
learning methods subject to inference queries at any moment. To address the
challenging setup and evaluation protocol, we propose an effective method that
employs a new memory management scheme and novel learning techniques. Our
empirical validation demonstrates that the proposed method outperforms prior
arts by large margins. Code and data splits are available at
https://github.com/naver-ai/i-Blurry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ICLR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedFR: Joint Optimization Federated Framework for Generic and
  Personalized Face Recognition <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Ting Liu, Chien-Yi Wang, Shao-Yi Chien, Shang-Hong Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art deep learning based face recognition (FR) models
require a large number of face identities for central training. However, due to
the growing privacy awareness, it is prohibited to access the face images on
user devices to continually improve face recognition models. Federated Learning
(FL) is a technique to address the privacy issue, which can collaboratively
optimize the model without sharing the data between clients. In this work, we
propose a FL based framework called FedFR to improve the generic face
representation in a privacy-aware manner. Besides, the framework jointly
optimizes personalized models for the corresponding clients via the proposed
Decoupled Feature Customization module. The client-specific personalized model
can serve the need of optimized face recognition experience for registered
identities at the local device. To the best of our knowledge, we are the first
to explore the personalized face recognition in FL setup. The proposed
framework is validated to be superior to previous approaches on several generic
and personalized face recognition benchmarks with diverse FL scenarios. The
source codes and our proposed personalized FR benchmark under FL setup are
available at https://github.com/jackie840129/FedFR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by AAAI 2022 Conference on Artificial
  Intelligence and selected as an oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> <span class="highlight-title">Cross-modal</span> Map Learning for <span class="highlight-title">Vision and Language</span> Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Georgakis, Karl Schmeckpeper, Karan Wanchoo, Soham Dan, Eleni Miltsakaki, <span class="highlight-author">Dan Roth</span>, Kostas Daniilidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of Vision-and-Language Navigation (VLN). The majority
of current methods for VLN are trained end-to-end using either unstructured
memory such as LSTM, or using cross-modal attention over the egocentric
observations of the agent. In contrast to other works, our key insight is that
the association between language and vision is stronger when it occurs in
explicit spatial representations. In this work, we propose a cross-modal map
learning model for vision-and-language navigation that first learns to predict
the top-down semantics on an egocentric map for both observed and unobserved
regions, and then predicts a path towards the goal as a set of waypoints. In
both cases, the prediction is informed by the language through cross-modal
attention mechanisms. We experimentally test the basic hypothesis that
language-driven navigation can be solved given a map, and then show competitive
results on the full VLN-CE benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TO-Scene: A Large-scale <span class="highlight-title">Dataset</span> for Understanding 3D Tabletop Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian Xu, Pei Chen, Haolin Liu, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many basic indoor activities such as eating or writing are always conducted
upon different tabletops (e.g., coffee tables, writing desks). It is
indispensable to understanding tabletop scenes in 3D indoor scene parsing
applications. Unfortunately, it is hard to meet this demand by directly
deploying data-driven algorithms, since 3D tabletop scenes are rarely available
in current datasets. To remedy this defect, we introduce TO-Scene, a
large-scale dataset focusing on tabletop scenes, which contains 20,740 scenes
with three variants. To acquire the data, we design an efficient and scalable
framework, where a crowdsourcing UI is developed to transfer CAD objects onto
tables from ScanNet, then the output tabletop scenes are simulated into real
scans and annotated automatically.
  Further, a tabletop-aware learning strategy is proposed for better perceiving
the small-sized tabletop instances. Notably, we also provide a real scanned
test set TO-Real to verify the practical value of TO-Scene. Experiments show
that the algorithms trained on TO-Scene indeed work on the realistic test data,
and our proposed tabletop-aware learning strategy greatly improves the
state-of-the-art results on both 3D semantic segmentation and object detection
tasks. TO-Scene and TO-Real, plus Web UI, will all be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Memory Learning for Fine-Grained Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youming Deng, Yansheng Li, Yongjun Zhang, Xiang Xiang, Jian Wang, Jingdong Chen, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the
dataset due to the crowd-sourced labeling, and the long-tail problem is also
pronounced. Given this tricky situation, many existing SGG methods treat the
predicates equally and learn the model under the supervision of
mixed-granularity predicates in one stage, leading to relatively coarse
predictions. In order to alleviate the negative impact of the suboptimum
mixed-granularity annotation and long-tail effect problems, this paper proposes
a novel Hierarchical Memory Learning (HML) framework to learn the model from
simple to complex, which is similar to the human beings' hierarchical memory
learning process. After the autonomous partition of coarse and fine predicates,
the model is first trained on the coarse predicates and then learns the fine
predicates. In order to realize this hierarchical learning pattern, this paper,
for the first time, formulates the HML framework using the new Concept
Reconstruction (CR) and Model Reconstruction (MR) constraints. It is worth
noticing that the HML framework can be taken as one general optimization
strategy to improve various SGG models, and significant improvement can be
achieved on the SGG benchmark (i.e., Visual Genome).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud
  Completion <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03530v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03530v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud is an important 3D representation for capturing real world 3D
objects. However, real-scanned 3D point clouds are often incomplete, and it is
important to recover complete point clouds for downstream applications. Most
existing point cloud completion methods use Chamfer Distance (CD) loss for
training. The CD loss estimates correspondences between two point clouds by
searching nearest neighbors, which does not capture the overall point density
distribution on the generated shape, and therefore likely leads to non-uniform
point cloud generation. To tackle this problem, we propose a novel Point
Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of
a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The
CGNet uses a conditional generative model called the denoising diffusion
probabilistic model (DDPM) to generate a coarse completion conditioned on the
partial observation. DDPM establishes a one-to-one pointwise mapping between
the generated point cloud and the uniform ground truth, and then optimizes the
mean squared error loss to realize uniform generation. The RFNet refines the
coarse output of the CGNet and further improves quality of the completed point
cloud. Furthermore, we develop a novel dual-path architecture for both
networks. The architecture can (1) effectively and efficiently extract
multi-level features from partially observed point clouds to guide completion,
and (2) accurately manipulate spatial locations of 3D points to obtain smooth
surfaces and sharp details. Extensive experimental results on various benchmark
datasets show that our PDR paradigm outperforms previous state-of-the-art
methods for point cloud completion. Remarkably, with the help of the RFNet, we
can accelerate the iterative generation process of the DDPM by up to 50 times
without much performance drop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022. Code is released at
  https://github.com/ZhaoyangLyu/Point_Diffusion_Refinement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiscale Convolutional <span class="highlight-title">Transformer</span> with Center Mask <span class="highlight-title">Pretrain</span>ing for
  Hyperspectral Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04771v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04771v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Jia, Yifan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images (HSI) not only have a broad macroscopic field of view
but also contain rich spectral information, and the types of surface objects
can be identified through spectral information, which is one of the main
applications in hyperspectral image related research.In recent years, more and
more deep learning methods have been proposed, among which convolutional neural
networks (CNN) are the most influential. However, CNN-based methods are
difficult to capture long-range dependencies, and also require a large amount
of labeled data for model training.Besides, most of the self-supervised
training methods in the field of HSI classification are based on the
reconstruction of input samples, and it is difficult to achieve effective use
of unlabeled samples. To address the shortcomings of CNN networks, we propose a
noval multi-scale convolutional embedding module for HSI to realize effective
extraction of spatial-spectral information, which can be better combined with
Transformer network.In order to make more efficient use of unlabeled data, we
propose a new self-supervised pretask. Similar to Mask autoencoder, but our
pre-training method only masks the corresponding token of the central pixel in
the encoder, and inputs the remaining token into the decoder to reconstruct the
spectral information of the central pixel.Such a pretask can better model the
relationship between the central feature and the domain feature, and obtain
more stable training results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 26 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math
  Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhong, Jheng-Hong Yang, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent success of dense retrieval methods based on bi-encoders, a
number of studies have applied this approach to various interesting downstream
retrieval tasks with good efficiency and in-domain effectiveness. Recently, we
have also seen the presence of dense retrieval models in Math Information
Retrieval (MIR) tasks, but the most effective systems remain "classic"
retrieval methods that consider rich structure features. In this work, we try
to combine the best of both worlds: a well-defined structure search method for
effective formula search and bi-encoder dense retrieval models to capture
contextual similarities in mathematical documents. Specifically, we have
evaluated two representative bi-encoder models (ColBERT and DPR) for
token-level and passage-level dense retrieval on recent MIR tasks. To our best
knowledge, this is the first time a DPR model has been evaluated in the MIR
domain. Our result shows that bi-encoder models are complementary to existing
structure search methods, and we are able to advance the state of the art on a
recent MIR dataset. We have made our model checkpoints and source code publicly
available for the reproduction of our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IoT Data Discovery: Routing Table and Summarization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Tran, Son Nguyen, I-Ling Yen, Farokh Bastani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the IoT data discovery problem in very large and
growing scale networks. Through analysis, examples, and experimental studies,
we show the importance of peer-to-peer, unstructured routing for IoT data
discovery and point out the space efficiency issue that has been overlooked in
keyword-based routing algorithms in unstructured networks. Specifically, as the
first in the field, this paper investigates routing table designs and various
compression techniques to support effective and space-efficient IoT data
discovery routing. Novel summarization algorithms, including alphabetical,
hash, and meaning-based summarization and their corresponding coding schemes,
are proposed. We also consider routing table design to support summarization
without degrading lookup efficiency for discovery query routing. The issue of
potentially misleading routing due to summarization is also investigated.
Subsequently, we analyze the strategy of when to summarize to balance the
tradeoff between the routing table compression rate and the chance of causing
misleading routing. For the experimental study, we have collected 100K IoT data
streams from various IoT databases as the input dataset. Experimental results
show that our summarization solution can reduce the routing table size by 20 to
30 folds with a 2-5% increase in latency compared with similar peer-to-peer
discovery routing algorithms without summarization. Also, our approach
outperforms DHT-based approaches by 2 to 6 folds in terms of latency and
traffic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 21 figures, 1 table, 3 algorithms. arXiv admin note:
  substantial text overlap with arXiv:2107.09558</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction Algorithm for Heat Demand of Science and Technology Topics
  Based on Time Convolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cui Haiyan, Li Yawen, Xu Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the rapid development of deep learning, big data analysis
technology is not only widely used in the field of natural language processing,
but also more mature in the field of numerical prediction. It is of great
significance for the subject heat prediction and analysis of science and
technology demand data. How to apply theme features to accurately predict the
theme heat of science and technology demand is the core to solve this problem.
In this paper, a prediction method of subject heat of science and technology
demand based on time convolution network (TCN) is proposed to obtain the
subject feature representation of science and technology demand. Time series
prediction is carried out based on TCN network and self attention mechanism,
which increases the accuracy of subject heat prediction of science and
technology demand data Experiments show that the prediction accuracy of this
algorithm is better than other time series prediction methods on the real
science and technology demand datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIANES: A DEI Audit Toolkit for News Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Shang, Zhiyuan Peng, Qiming Yuan, Sabiq Khan, Lauren Xie, Yi Fang, Subramaniam Vincent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Professional news media organizations have always touted the importance that
they give to multiple perspectives. However, in practice the traditional
approach to all-sides has favored people in the dominant culture. Hence it has
come under ethical critique under the new norms of diversity, equity, and
inclusion (DEI). When DEI is applied to journalism, it goes beyond conventional
notions of impartiality and bias and instead democratizes the journalistic
practice of sourcing -- who is quoted or interviewed, who is not, how often,
from which demographic group, gender, and so forth. There is currently no
real-time or on-demand tool in the hands of reporters to analyze the persons
they quote. In this paper, we present DIANES, a DEI Audit Toolkit for News
Sources. It consists of a natural language processing pipeline on the backend
to extract quotes, speakers, titles, and organizations from news articles in
real time. On the frontend, DIANES offers the WordPress plugins, a Web monitor,
and a DEI annotation API service, to help news media monitor their own quoting
patterns and push themselves towards DEI norms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Similarity Computing for Scientific Academic Conferences fused
  with domain features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyu Yu, Yawen Li, Ang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming at the problem that the current general-purpose semantic text
similarity calculation methods are difficult to use the semantic information of
scientific academic conference data, a semantic similarity calculation
algorithm for scientific academic conferences by fusion with domain features is
proposed. First, the domain feature information of the conference is obtained
through entity recognition and keyword extraction, and it is input into the
BERT network as a feature and the conference information. The structure of the
Siamese network is used to solve the anisotropy problem of BERT. The output of
the network is pooled and normalized, and finally the cosine similarity is used
to calculate the similarity between the two sessions. Experimental results show
that the SBFD algorithm has achieved good results on different data sets, and
the Spearman correlation coefficient has a certain improvement compared with
the comparison algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web Page Content Extraction Based on Multi-feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Yu, Junping Du, Yingxia Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Internet technology, people have more and more
access to a variety of web page resources. At the same time, the current rapid
development of deep learning technology is often inseparable from the huge
amount of Web data resources. On the other hand, NLP is also an important part
of data processing technology, such as web page data extraction. At present,
the extraction technology of web page text mainly uses a single heuristic
function or strategy, and most of them need to determine the threshold
manually. With the rapid growth of the number and types of web resources, there
are still problems to be solved when using a single strategy to extract the
text information of different pages. This paper proposes a web page text
extraction algorithm based on multi-feature fusion. According to the text
information characteristics of web resources, DOM nodes are used as the
extraction unit to design multiple statistical features, and high-order
features are designed according to heuristic strategies. This method
establishes a small neural network, takes multiple features of DOM nodes as
input, predicts whether the nodes contain text information, makes full use of
different statistical information and extraction strategies, and adapts to more
types of pages. Experimental results show that this method has a good ability
of web page text extraction and avoids the problem of manually determining the
threshold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-adaptive Asymmetric Deep <span class="highlight-title">Cross-modal</span> Hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.00197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.00197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengling Li, Tong Wang, Lei Zhu, Zheng Zhang, Xinhua Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised cross-modal hashing aims to embed the semantic correlations of
heterogeneous modality data into the binary hash codes with discriminative
semantic labels. Because of its advantages on retrieval and storage efficiency,
it is widely used for solving efficient cross-modal retrieval. However,
existing researches equally handle the different tasks of cross-modal
retrieval, and simply learn the same couple of hash functions in a symmetric
way for them. Under such circumstance, the uniqueness of different cross-modal
retrieval tasks are ignored and sub-optimal performance may be brought.
Motivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal
Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash
functions for two sub-retrieval tasks via simultaneous modality representation
and asymmetric hash learning. Unlike previous cross-modal hashing approaches,
our learning framework jointly optimizes semantic preserving that transforms
deep features of multimedia data into binary hash codes, and the semantic
regression which directly regresses query modality representation to explicit
label. With our model, the binary codes can effectively preserve semantic
correlations across different modalities, meanwhile, adaptively capture the
query semantics. The superiority of TA-ADCMH is proved on two standard datasets
from many aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open Natural Language Processing Development Framework for EHR-based
  Clinical Research: A case demonstration using the National COVID Cohort
  Collaborative (N3C) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10780v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10780v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Liu, Andrew Wen, Liwei Wang, Huan He, Sunyang Fu, Robert Miller, Andrew Williams, Daniel Harris, Ramakanth Kavuluru, Mei Liu, Noor Abu-el-rub, Dalton Schutte, Rui Zhang, Masoud Rouhizadeh, John D. Osborne, Yongqun He, Umit Topaloglu, Stephanie S Hong, Joel H Saltz, Thomas Schaffter, Emily Pfaff, Christopher G. Chute, Tim Duong, Melissa A. Haendel, Rafael Fuentes, Peter Szolovits, Hua Xu, Hongfang Liu, National COVID Cohort Collaborative, Natural Language Processing,  Subgroup, National COVID Cohort Collaborative
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While we pay attention to the latest advances in clinical natural language
processing (NLP), we can notice some resistance in the clinical and
translational research community to adopt NLP models due to limited
transparency, interpretability, and usability. In this study, we proposed an
open natural language processing development framework. We evaluated it through
the implementation of NLP algorithms for the National COVID Cohort
Collaborative (N3C). Based on the interests in information extraction from
COVID-19 related clinical notes, our work includes 1) an open data annotation
process using COVID-19 signs and symptoms as the use case, 2) a
community-driven ruleset composing platform, and 3) a synthetic text data
generation workflow to generate texts for information extraction tasks without
involving human subjects. The corpora were derived from texts from three
different institutions (Mayo Clinic, University of Kentucky, University of
Minnesota). The gold standard annotations were tested with a single
institution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,
and 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,
respectively. The study as a consortium effort of the N3C NLP subgroup
demonstrates the feasibility of creating a federated NLP algorithm development
and benchmarking platform to enhance multi-institution clinical NLP study and
adoption. Although we use COVID-19 as a use case in this effort, our framework
is general enough to be applied to other domains of interest in clinical NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update on contents</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Fast and Slow: Scene Decomposition via Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of segmenting scenes into constituent entities, i.e.
underlying objects and their parts. Current supervised visual detectors though
impressive within their training distribution, often fail to segment
out-of-distribution scenes into their constituent entities. Recent slot-centric
generative models break such dependence on supervision, by attempting to
segment scenes into entities unsupervised, by reconstructing pixels. However,
they have been restricted thus far to toy scenes as they suffer from a
reconstruction-segmentation trade-off: as the entity bottleneck gets wider,
reconstruction improves but then the segmentation collapses. We propose
GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue with two
ingredients: i) curriculum training in the form of primitives, often missing
from current generative models and, ii) test-time adaptation per scene through
gradient descent on the reconstruction objective, what we call slow inference,
missing from current feed-forward detectors. We show the proposed curriculum
suffices to break the reconstruction-segmentation trade-off, and slow inference
greatly improves segmentation in out-of-distribution scenes. We evaluate
GFS-Nets in 3D and 2D scene segmentation benchmarks of PartNet, CLEVR, Room
Diverse++, and show large ( 50%) performance improvements against SOTA
supervised feed-forward detectors and unsupervised object discovery methods
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://mihirp1998.github.io/project_pages/gfsnets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinqin Yang, Zi Wang, Kunyuan Guo, Congbo Cai, Xiaobo Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has innovated the field of computational imaging. One of its
bottlenecks is unavailable or insufficient training data. This article reviews
an emerging paradigm, imaging physics-based data synthesis (IPADS), that can
provide huge training data in biomedical magnetic resonance without or with few
real data. Following the physical law of magnetic resonance, IPADS generates
signals from differential equations or analytical solution models, making the
learning more scalable, explainable, and better protecting privacy. Key
components of IPADS learning, including signal generation models, basic deep
learning network structures, enhanced data generation, and learning methods are
discussed. Great potentials of IPADS have been demonstrated by representative
applications in fast imaging, ultrafast signal reconstruction and accurate
parameter quantification. Finally, open questions and future work have been
discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One After Another: Learning Incremental Skills for a Changing World <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nur Muhammad Shafiullah, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward-free, unsupervised discovery of skills is an attractive alternative to
the bottleneck of hand-designing rewards in environments where task supervision
is scarce or expensive. However, current skill pre-training methods, like many
RL techniques, make a fundamental assumption - stationary environments during
training. Traditional methods learn all their skills simultaneously, which
makes it difficult for them to both quickly adapt to changes in the
environment, and to not forget earlier skills after such adaptation. On the
other hand, in an evolving or expanding environment, skill learning must be
able to adapt fast to new environment situations while not forgetting
previously learned skills. These two conditions make it difficult for classic
skill discovery to do well in an evolving environment. In this work, we propose
a new framework for skill discovery, where skills are learned one after another
in an incremental fashion. This framework allows newly learned skills to adapt
to new environment or agent dynamics, while the fixed old skills ensure the
agent doesn't forget a learned skill. We demonstrate experimentally that in
both evolving and static environments, incremental skills significantly
outperform current state-of-the-art skill discovery methods on both skill
quality and the ability to solve downstream tasks. Videos for learned skills
and code are made public on https://notmahi.github.io/disk
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in The International Conference on Learning
  Representations (ICLR) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Force-matching Coarse-Graining without Forces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Köhler, Yaoyi Chen, Andreas Krämer, Cecilia Clementi, Frank Noé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coarse-grained (CG) molecular simulations have become a standard tool to
study molecular processes on time-~and length-scales inaccessible to all-atom
simulations. Learning CG force fields from all-atom data has mainly relied on
force-matching and relative entropy minimization. Force-matching is
straightforward to implement but requires the forces on the CG particles to be
saved during all-atom simulation, and because these instantaneous forces depend
on all degrees of freedom, they provide a very noisy signal that makes training
the CG force field data inefficient. Relative entropy minimization does not
require forces to be saved and is more data-efficient, but requires the CG
model to be re-simulated during the iterative training procedure, which can
make the training procedure extremely costly or lead to failure to converge.
Here we present \emph{flow-matching}, a new training method for CG force fields
that combines the advantages of force-matching and relative entropy
minimization by leveraging normalizing flows, a generative deep learning
method. Flow-matching first trains a normalizing flow to represent the CG
probability density by using relative entropy minimization without suffering
from the re-simulation problem because flows can directly sample from the
equilibrium distribution they represent. Subsequently, the forces of the flow
are used to train a CG force field by matching the coarse-grained forces
directly, which is a much easier problem than traditional force-matching as it
does not suffer from the noise problem. Besides not requiring forces,
flow-matching also outperforms classical force-matching by an order of
magnitude in terms of data efficiency and produces CG models that can capture
the folding and unfolding of small proteins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operator Sketching for Deep Unrolling Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching language models to support answers with verified quotes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, Nat McAleese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models often answer factual questions correctly. But
users can't trust any given claim a model makes without fact-checking, because
language models can hallucinate convincing nonsense. In this work we use
reinforcement learning from human preferences (RLHP) to train "open-book" QA
models that generate answers whilst also citing specific evidence for their
claims, which aids in the appraisal of correctness. Supporting evidence is
drawn from multiple documents found via a search engine, or from a single
user-provided document. Our 280 billion parameter model, GopherCite, is able to
produce answers with high quality supporting evidence and abstain from
answering when unsure. We measure the performance of GopherCite by conducting
human evaluation of answers to questions in a subset of the NaturalQuestions
and ELI5 datasets. The model's response is found to be high-quality 80\% of the
time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
Abstaining from the third of questions for which it is most unsure improves
performance to 90\% and 80\% respectively, approaching human baselines.
However, analysis on the adversarial TruthfulQA dataset shows why citation is
only one part of an overall strategy for safety and trustworthiness: not all
claims supported by evidence are true.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multispectral Satellite Data Classification using Soft Computing
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purbarag Pathak Choudhury, Ujjal Kr Dutta, Dhruba Kr Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A satellite image is a remotely sensed image data, where each pixel
represents a specific location on earth. The pixel value recorded is the
reflection radiation from the earth's surface at that location. Multispectral
images are those that capture image data at specific frequencies across the
electromagnetic spectrum as compared to Panchromatic images which are sensitive
to all wavelength of visible light. Because of the high resolution and high
dimensions of these images, they create difficulties for clustering techniques
to efficiently detect clusters of different sizes, shapes and densities as a
trade off for fast processing time. In this paper we propose a grid-density
based clustering technique for identification of objects. We also introduce an
approach to classify a satellite image data using a rule induction based
machine learning algorithm. The object identification and classification
methods have been validated using several synthetic and benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proc. of International Conference on Advances in Communication,
  Network, and Computing (CNC), 2014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we integrate spatial verification methods into neural-network loss
  functions for atmospheric science? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Lagerquist, Imme Ebert-Uphoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, much work in atmospheric science has focused on spatial
verification (SV) methods for gridded prediction, which overcome serious
disadvantages of pixelwise verification. However, neural networks (NN) in
atmospheric science are almost always trained to optimize pixelwise loss
functions, even when ultimately assessed with SV methods. This establishes a
disconnect between model verification during vs. after training. To address
this issue, we develop spatially enhanced loss functions (SELF) and demonstrate
their use for a real-world problem: predicting the occurrence of thunderstorms
(henceforth, "convection") with NNs. In each SELF we use either a neighbourhood
filter, which highlights convection at scales larger than a threshold, or a
spectral filter (employing Fourier or wavelet decomposition), which is more
flexible and highlights convection at scales between two thresholds. We use
these filters to spatially enhance common verification scores, such as the
Brier score. We train each NN with a different SELF and compare their
performance at many scales of convection, from discrete storm cells to tropical
cyclones. Among our many findings are that (a) for a low (high) risk threshold,
the ideal SELF focuses on small (large) scales; (b) models trained with a
pixelwise loss function perform surprisingly well; (c) however, models trained
with a spectral filter produce better-calibrated probabilities than a pixelwise
model. We provide a general guide to using SELFs, including technical
challenges and the final Python code, as well as demonstrating their use for
the convection problem. To our knowledge this is the most in-depth guide to
SELFs in the geosciences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages + 6 pages of supplemental material, 15 figures, 3 tables,
  submitted to Artificial Intelligence for Earth Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span> of Disentanglement Approaches for Medical Applications -- Towards
  Solving the Gordian Knot of Generative Models in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jana Fragemann, Lynton Ardizzone, Jan Egger, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are commonly used for medical purposes such as image
generation, segmentation, or classification. Besides this, they are often
criticized as black boxes as their decision process is often not human
interpretable. Encouraging the latent representation of a generative model to
be disentangled offers new perspectives of control and interpretability.
Understanding the data generation process could help to create artificial
medical data sets without violating patient privacy, synthesizing different
data modalities, or discovering data generating characteristics. These
characteristics might unravel novel relationships that can be related to
genetic traits or patient outcomes. In this paper, we give a comprehensive
overview of popular generative models, like Generative Adversarial Networks
(GANs), Variational Autoencoders (VAEs), and Flow-based Models. Furthermore, we
summarize the different notions of disentanglement, review approaches to
disentangle latent space representations and metrics to evaluate the degree of
disentanglement. After introducing the theoretical frameworks, we give an
overview of recent medical applications and discuss the impact and importance
of disentanglement approaches for medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainable Evaluation Metrics for Natural Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike classical lexical overlap metrics such as BLEU, most current
evaluation metrics (such as BERTScore or MoverScore) are based on black-box
language models such as BERT or XLM-R. They often achieve strong correlations
with human judgments, but recent research indicates that the lower-quality
classical metrics remain dominant, one of the potential reasons being that
their decision processes are transparent. To foster more widespread acceptance
of the novel high-quality metrics, explainability thus becomes crucial. In this
concept paper, we identify key properties and propose key goals of explainable
machine translation evaluation metrics. We also provide a synthesizing overview
over recent approaches for explainable machine translation metrics and discuss
how they relate to those goals and properties. Further, we conduct own novel
experiments, which (among others) find that current adversarial NLP techniques
are unsuitable for automatically identifying limitations of high-quality
black-box evaluation metrics, as they are not meaning-preserving. Finally, we
provide a vision of future approaches to explainable evaluation metrics and
their evaluation. We hope that our work can help catalyze and guide future
research on explainable evaluation metrics and, mediately, also contribute to
better and more transparent text generation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACS: A <span class="highlight-title">Dataset</span> for Physical Audiovisual CommonSense Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Yu, Peter Wu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, they should be able to reason about the
physical world by understanding the physical properties and affordances of
available objects, how they can be manipulated, and how they interact with
other physical objects. This research field of physical commonsense reasoning
is fundamentally a multi-sensory task since physical properties are manifested
through multiple modalities, two of them being vision and acoustics. Our paper
takes a step towards real-world physical commonsense reasoning by contributing
PACS: the first audiovisual benchmark annotated for physical commonsense
attributes. PACS contains a total of 13,400 question-answer pairs, involving
1,377 unique physical commonsense questions and 1,526 videos. Our dataset
provides new opportunities to advance the research field of physical reasoning
by bringing audio as a core component of this multimodal problem. Using PACS,
we evaluate multiple state-of-the-art models on this new challenging task.
While some models show promising results (70% accuracy), they all fall short of
human performance (95% accuracy). We conclude the paper by demonstrating the
importance of multimodal reasoning and providing possible avenues for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static
  Models by Fitting Feature-level Space-time Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR 2022 (Source Code:
  https://github.com/jx-zhong-for-academic-purpose/Kinet )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGAN: Federated Generative Adversarial Networks for Anomaly Detection in
  Network Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankha Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last two decades, a lot of work has been done in improving network
security, particularly in intrusion detection systems (IDS) and anomaly
detection. Machine learning solutions have also been employed in IDSs to detect
known and plausible attacks in incoming traffic. Parameters such as packet
contents, sender IP and sender port, connection duration, etc. have been
previously used to train these machine learning models to learn to
differentiate genuine traffic from malicious ones. Generative Adversarial
Networks (GANs) have been significantly successful in detecting such anomalies,
mostly attributed to the adversarial training of the generator and
discriminator in an attempt to bypass each other and in turn increase their own
power and accuracy. However, in large networks having a wide variety of traffic
at possibly different regions of the network and susceptible to a large number
of potential attacks, training these GANs for a particular kind of anomaly may
make it oblivious to other anomalies and attacks. In addition, the dataset
required to train these models has to be made centrally available and publicly
accessible, posing the obvious question of privacy of the communications of the
respective participants of the network. The solution proposed in this work aims
at tackling the above two issues by using GANs in a federated architecture in
networks of such scale and capacity. In such a setting, different users of the
network will be able to train and customize a centrally available adversarial
model according to their own frequently faced conditions. Simultaneously, the
member users of the network will also able to gain from the experiences of the
other users in the network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Counterfactual Explanations for Anomaly Detection in Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deborah Sulem, Michele Donini, Muhammad Bilal Zafar, Francois-Xavier Aubet, Jan Gasthaus, Tim Januschowski, Sanjiv Das, Krishnaram Kenthapadi, Cedric Archambeau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven methods that detect anomalies in times series data are ubiquitous
in practice, but they are in general unable to provide helpful explanations for
the predictions they make. In this work we propose a model-agnostic algorithm
that generates counterfactual ensemble explanations for time series anomaly
detection models. Our method generates a set of diverse counterfactual
examples, i.e, multiple perturbed versions of the original time series that are
not considered anomalous by the detection model. Since the magnitude of the
perturbations is limited, these counterfactuals represent an ensemble of inputs
similar to the original time series that the model would deem normal. Our
algorithm is applicable to any differentiable anomaly detection model. We
investigate the value of our method on univariate and multivariate real-world
datasets and two deep-learning-based anomaly detection models, under several
explainability criteria previously proposed in other data domains such as
Validity, Plausibility, Closeness and Diversity. We show that our algorithm can
produce ensembles of counterfactual examples that satisfy these criteria and
thanks to a novel type of visualisation, can convey a richer interpretation of
a model's internal mechanism than existing methods. Moreover, we design a
sparse variant of our method to improve the interpretability of counterfactual
explanations for high-dimensional time series anomalies. In this setting, our
explanation is localised on only a few dimensions and can therefore be
communicated more efficiently to the model's user.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ovid: A Machine Learning Approach for Automated Vandalism Detection in
  OpenStreetMap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Tempelmeier, Elena Demidova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenStreetMap is a unique source of openly available worldwide map data,
increasingly adopted in real-world applications. Vandalism detection in
OpenStreetMap is critical and remarkably challenging due to the large scale of
the dataset, the sheer number of contributors, various vandalism forms, and the
lack of annotated data to train machine learning algorithms. This paper
presents Ovid - a novel machine learning method for vandalism detection in
OpenStreetMap. Ovid relies on a neural network architecture that adopts a
multi-head attention mechanism to effectively summarize information indicating
vandalism from OpenStreetMap changesets. To facilitate automated vandalism
detection, we introduce a set of original features that capture changeset,
user, and edit information. Our evaluation results on real-world vandalism data
demonstrate that the proposed Ovid method outperforms the baselines by 4.7
percentage points in F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2201.10406</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming Oscillations in Quantization-Aware Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training neural networks with simulated quantization, we observe that
quantized weights can, rather unexpectedly, oscillate between two grid-points.
The importance of this effect and its impact on quantization-aware training are
not well-understood or investigated in literature. In this paper, we delve
deeper into the phenomenon of weight oscillations and show that it can lead to
a significant accuracy degradation due to wrongly estimated batch-normalization
statistics during inference and increased noise during training. These effects
are particularly pronounced in low-bit ($\leq$ 4-bits) quantization of
efficient networks with depth-wise separable layers, such as MobileNets and
EfficientNets. In our analysis we investigate several previously proposed
quantization-aware training (QAT) algorithms and show that most of these are
unable to overcome oscillations. Finally, we propose two novel QAT algorithms
to overcome oscillations during training: oscillation dampening and iterative
weight freezing. We demonstrate that our algorithms achieve state-of-the-art
accuracy for low-bit (3 & 4 bits) weight and activation quantization of
efficient architectures, such as MobileNetV2, MobileNetV3, and EfficentNet-lite
on ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Telling Stories from Computational Notebooks: AI-Assisted Presentation
  Slides Creation for Presenting Data Science Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengbo Zheng, Dakuo Wang, April Yi Wang, Xiaojuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating presentation slides is a critical but time-consuming task for data
scientists. While researchers have proposed many AI techniques to lift data
scientists' burden on data preparation and model selection, few have targeted
the presentation creation task. Based on the needs identified from a formative
study, this paper presents NB2Slides, an AI system that facilitates users to
compose presentations of their data science work. NB2Slides uses deep learning
methods as well as example-based prompts to generate slides from computational
notebooks, and take users' input (e.g., audience background) to structure the
slides. NB2Slides also provides an interactive visualization that links the
slides with the notebook to help users further edit the slides. A follow-up
user evaluation with 12 data scientists shows that participants believed
NB2Slides can improve efficiency and reduces the complexity of creating slides.
Yet, participants questioned the future of full automation and suggested a
human-AI collaboration paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHI'2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Learning for Cyberattack Detection in Blockchain Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tran Viet Khoa, Do Hai Son, Dinh Thai Hoang, Nguyen Linh Trung, Tran Thi Thuy Quynh, Diep N. Nguyen, Nguyen Viet Ha, Eryk Dutkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article aims to study intrusion attacks and then develop a novel
cyberattack detection framework for blockchain networks. Specifically, we first
design and implement a blockchain network in our laboratory. This blockchain
network will serve two purposes, i.e., generate the real traffic data
(including both normal data and attack data) for our learning models and
implement real-time experiments to evaluate the performance of our proposed
intrusion detection framework. To the best of our knowledge, this is the first
dataset that is synthesized in a laboratory for cyberattacks in a blockchain
network. We then propose a novel collaborative learning model that allows
efficient deployment in the blockchain network to detect attacks. The main idea
of the proposed learning model is to enable blockchain nodes to actively
collect data, share the knowledge learned from its data, and then exchange the
knowledge with other blockchain nodes in the network. In this way, we can not
only leverage the knowledge from all the nodes in the network but also do not
need to gather all raw data for training at a centralized node like
conventional centralized learning solutions. Such a framework can also avoid
the risk of exposing local data's privacy as well as the excessive network
overhead/congestion. Both intensive simulations and real-time experiments
clearly show that our proposed collaborative learning-based intrusion detection
framework can achieve an accuracy of up to 97.7% in detecting attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Concept Drift to Model Degradation: An <span class="highlight-title">Overview</span> on
  Performance-Aware Drift Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Firas Bayram, Bestoun S. Ahmed, Andreas Kassler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamicity of real-world systems poses a significant challenge to
deployed predictive machine learning (ML) models. Changes in the system on
which the ML model has been trained may lead to performance degradation during
the system's life cycle. Recent advances that study non-stationary environments
have mainly focused on identifying and addressing such changes caused by a
phenomenon called concept drift. Different terms have been used in the
literature to refer to the same type of concept drift and the same term for
various types. This lack of unified terminology is set out to create confusion
on distinguishing between different concept drift variants. In this paper, we
start by grouping concept drift types by their mathematical definitions and
survey the different terms used in the literature to build a consolidated
taxonomy of the field. We also review and classify performance-based concept
drift detection methods proposed in the last decade. These methods utilize the
predictive model's performance degradation to signal substantial changes in the
systems. The classification is outlined in a hierarchical diagram to provide an
orderly navigation between the methods. We present a comprehensive analysis of
the main attributes and strategies for tracking and evaluating the model's
performance in the predictive system. The paper concludes by discussing open
research challenges and possible research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Revenue Maximization and Demand Learning in Airline Revenue
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Gatti Pinheiro, Michael Defoin-Platel, Jean-Charles Regin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correctly estimating how demand respond to prices is fundamental for airlines
willing to optimize their pricing policy. Under some conditions, these
policies, while aiming at maximizing short term revenue, can present too little
price variation which may decrease the overall quality of future demand
forecasting. This problem, known as earning while learning problem, is not
exclusive to airlines, and it has been investigated by academia and industry in
recent years. One of the most promising methods presented in literature
combines the revenue maximization and the demand model quality into one single
objective function. This method has shown great success in simulation studies
and real life benchmarks. Nevertheless, this work needs to be adapted to
certain constraints that arise in the airline revenue management (RM), such as
the need to control the prices of several active flights of a leg
simultaneously. In this paper, we adjust this method to airline RM while
assuming unconstrained capacity. Then, we show that our new algorithm
efficiently performs price experimentation in order to generate more revenue
over long horizons than classical methods that seek to maximize revenue only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Duration Modeling for End-to-End Text-to-Speech <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bac Nguyen, Fabien Cardinaux, Stefan Uhlich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel text-to-speech (TTS) models have recently enabled fast and
highly-natural speech synthesis. However, such models typically require
external alignment models, which are not necessarily optimized for the decoder
as they are not jointly trained. In this paper, we propose a differentiable
duration method for learning monotonic alignments between input and output
sequences. Our method is based on a soft-duration mechanism that optimizes a
stochastic process in expectation. Using this differentiable duration method, a
direct text to waveform TTS model is introduced to produce raw audio as output
instead of performing neural vocoding. Our model learns to perform
high-fidelity speech synthesis through a combination of adversarial training
and matching the total ground-truth duration. Experimental results show that
our model obtains competitive results while enjoying a much simpler training
pipeline. Audio samples are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A new perspective on probabilistic image modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Gepperth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new
probabilistic approach for image modeling capable of density estimation,
sampling and tractable inference. DCGMM instances exhibit a CNN-like layered
structure, in which the principal building blocks are convolutional Gaussian
Mixture (cGMM) layers. A key innovation w.r.t. related models like sum-product
networks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer
optimizes an independent loss function and therefore has an independent
probabilistic interpretation. This modular approach permits intervening
transformation layers to harness the full spectrum of (potentially
non-invertible) mappings available to CNNs, e.g., max-pooling or
half-convolutions. DCGMM sampling and inference are realized by a deep chain of
hierarchical priors, where a sample generated by a given cGMM layer defines the
parameters of sampling in the next-lower cGMM layer. For sampling through
non-invertible transformation layers, we introduce a new gradient-based
sharpening technique that exploits redundancy (overlap) in, e.g.,
half-convolutions. DCGMMs can be trained end-to-end by SGD from random initial
conditions, much like CNNs. We show that DCGMMs compare favorably to several
recent PC and SPN models in terms of inference, classification and sampling,
the latter particularly for challenging datasets such as SVHN. We provide a
public TF2 implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Skeleton-based Action Recognition with Continual Spatio-Temporal
  Graph Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Hedegaard, Negar Heidari, Alexandros Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based reasoning over skeleton data has emerged as a promising approach
for human action recognition. However, the application of prior graph-based
methods, which predominantly employ whole temporal sequences as their input, to
the setting of online inference entails considerable computational redundancy.
In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph
Convolutional Neural Network as a Continual Inference Network, which can
perform step-by-step predictions in time without repeat frame processing. To
evaluate our method, we create a continual version of ST-GCN, CoST-GCN,
alongside two derived methods with different self-attention mechanisms, CoAGCN
and CoS-TR. We investigate weight transfer strategies and architectural
modifications for inference acceleration, and perform experiments on the NTU
RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar
predictive accuracy, we observe up to 109x reduction in time complexity,
on-hardware accelerations of 26x, and reductions in maximum allocated memory of
52% during online inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Chmiel, Itay Hubara, Ron Banner, Daniel Soudry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, fine-grained N:M sparsity reduces the data footprint and
bandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by
skipping computation of zero values. So far, it was only used to prune weights.
We examine how this method can be used also for activations and their gradients
(i.e., "neural gradients"). To this end, we first establish tensor-level
optimality criteria. Previous works aimed to minimize the mean-square-error
(MSE) of each pruned block. We show that while minimization of the MSE works
fine for pruning the activations, it catastrophically fails for the neural
gradients. Instead, we show that optimal pruning of the neural gradients
requires an unbiased minimum-variance pruning mask. We design such specialized
masks, and find that in most cases, 1:2 sparsity is sufficient for training,
and 2:4 sparsity is usually enough when this is not the case. Further, we
suggest combining several such methods together in order to speed up training
even more. A reference implementation is supplied in
https://github.com/brianchmiel/Act-and-Grad-structured-sparsity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical autoregressive neural networks for statistical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Białas, Piotr Korcyl, Tomasz Stebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It was recently proposed that neural networks could be used to approximate
many-dimensional probability distributions that appear e.g. in lattice field
theories or statistical mechanics. Subsequently they can be used as variational
approximators to asses extensive properties of statistical systems, like free
energy, and also as neural samplers used in Monte Carlo simulations. The
practical application of this approach is unfortunately limited by its
unfavorable scaling both of the numerical cost required for training, and the
memory requirements with the system size. This is due to the fact that the
original proposition involved a neural network of width which scaled with the
total number of degrees of freedom, e.g. $L^2$ in case of a two dimensional
$L\times L$ lattice. In this work we propose a hierarchical association of
physical degrees of freedom, for instance spins, to neurons which replaces it
with the scaling with the linear extent $L$ of the system. We demonstrate our
approach on the two-dimensional Ising model by simulating lattices of various
sizes up to $128 \times 128$ spins, with time benchmarks reaching lattices of
size $512 \times 512$. We observe that our proposal improves the quality of
neural network training, i.e. the approximated probability distribution is
closer to the target that could be previously achieved. As a consequence, the
variational free energy reaches a value closer to its theoretical expectation
and, if applied in a Markov Chain Monte Carlo algorithm, the resulting
autocorrelation time is smaller. Finally, the replacement of a single neural
network by a hierarchy of smaller networks considerably reduces the memory
requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks
  with Boundary Node Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wan, Youjie Li, Ang Li, Nam Sung Kim, Yingyan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art
method for graph-based learning tasks. However, training GCNs at scale is still
challenging, hindering both the exploration of more sophisticated GCN
architectures and their applications to real-world large graphs. While it might
be natural to consider graph partition and distributed training for tackling
this challenge, this direction has only been slightly scratched the surface in
the previous works due to the limitations of existing designs. In this work, we
first analyze why distributed GCN training is ineffective and identify the
underlying cause to be the excessive number of boundary nodes of each
partitioned subgraph, which easily explodes the memory and communication costs
for GCN training. Furthermore, we propose a simple yet effective method dubbed
BNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and
scalable distributed GCN training. Experiments and ablation studies
consistently validate the effectiveness of BNS-GCN, e.g., boosting the
throughput by up to 16.2x and reducing the memory usage by up to 58%, while
maintaining a full-graph accuracy. Furthermore, both theoretical and empirical
analysis show that BNS-GCN enjoys a better convergence than existing
sampling-based methods. We believe that our BNS-GCN has opened up a new
paradigm for enabling GCN training at scale. The code is available at
https://github.com/RICE-EIC/BNS-GCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GCF: Generalized Causal Forest for Heterogeneous Treatment Effect
  Estimation in Online Marketplace 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wan, Chen Zheng, Zhonggen Sun, Mengfan Xu, Xiaoqing Yang, Hongtu Zhu, Jiecheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uplift modeling is a rapidly growing approach that utilizes machine learning
and causal inference methods to estimate the heterogeneous treatment effects.
It has been widely adopted and applied to online marketplaces to assist
large-scale decision-making in recent years. The existing popular methods, like
forest-based modeling, either work only for discrete treatments or make
partially linear or parametric assumptions that may suffer from model
misspecification. To alleviate these problems, we extend causal forest (CF)
with non-parametric dose-response functions (DRFs) that can be estimated
locally using a kernel-based doubly robust estimator. Moreover, we propose a
distance-based splitting criterion in the functional space of conditional DRFs
to capture the heterogeneity for the continuous treatments. We call the
proposed algorithm generalized causal forest (GCF) as it generalizes the use
case of CF to a much broader setup. We show the effectiveness of GCF by
comparing it to popular uplift modeling models on both synthetic and real-world
datasets. We implement GCF in Spark and successfully deploy it into DiDi's
real-time pricing system. Online A/B testing results further validate the
superiority of GCF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Local Convergence Theory for the Stochastic Gradient Descent Method in
  Non-Convex Optimization With Non-isolated Local Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehee Ko, Xiantao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-convex loss functions arise frequently in modern machine learning, and
for the theoretical analysis of stochastic optimization methods, the presence
of non-isolated minima presents a unique challenge that has remained
under-explored. In this paper, we study the local convergence of the stochastic
gradient descent method to non-isolated global minima. Under mild assumptions,
we estimate the probability for the iterations to stay near the minima by
adopting the notion of stochastic stability. After establishing such stability,
we present the lower bound complexity in terms of various error criteria for a
given error tolerance $\epsilon$ and a failure probability $\gamma$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Trajectories for Highway Driving with Offline Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branka Mirchevska, Moritz Werling, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing an autonomous vehicle that is able to output feasible, smooth
and efficient trajectories is a long-standing challenge. Several approaches
have been considered, roughly falling under two categories: rule-based and
learning-based approaches. The rule-based approaches, while guaranteeing safety
and feasibility, fall short when it comes to long-term planning and
generalization. The learning-based approaches are able to account for long-term
planning and generalization to unseen situations, but may fail to achieve
smoothness, safety and the feasibility which rule-based approaches ensure.
Hence, combining the two approaches is an evident step towards yielding the
best compromise out of both. We propose a Reinforcement Learning-based
approach, which learns target trajectory parameters for fully autonomous
driving on highways. The trained agent outputs continuous trajectory parameters
based on which a feasible polynomial-based trajectory is generated and
executed. We compare the performance of our agent against four other highway
driving agents. The experiments are conducted in the Sumo simulator, taking
into consideration various realistic, dynamically changing highway scenarios,
including surrounding vehicles with different driver behaviors. We demonstrate
that our offline trained agent, with randomly collected data, learns to drive
smoothly, achieving velocities as close as possible to the desired velocity,
while outperforming the other agents. Code, training data and details available
at: https://nrgit.informatik.uni-freiburg. de/branka.mirchevska/offline-rl-tp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge
  Modality Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Buchner, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online 3D multi-object tracking (MOT) has witnessed significant research
interest in recent years, largely driven by demand from the autonomous systems
community. However, 3D offline MOT is relatively less explored. Labeling 3D
trajectory scene data at a large scale while not relying on high-cost human
experts is still an open research question. In this work, we propose Batch3DMOT
that follows the tracking-by-detection paradigm and represents real-world
scenes as directed, acyclic, and category-disjoint tracking graphs that are
attributed using various modalities such as camera, LiDAR, and radar. We
present a multi-modal graph neural network that uses a cross-edge attention
mechanism mitigating modality intermittence, which translates into sparsity in
the graph domain. Additionally, we present attention-weighted convolutions over
frame-wise k-NN neighborhoods as suitable means to allow information exchange
across disconnected graph components. We evaluate our approach using various
sensor modalities and model configurations on the challenging nuScenes and
KITTI datasets. Extensive experiments demonstrate that our proposed approach
yields an overall improvement of 2.8% in the AMOTA score on nuScenes thereby
setting a new benchmark for 3D tracking methods and successfully enhances false
positive filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyMLOps: Operational Challenges for Widespread Edge AI Adoption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Leroux, Pieter Simoens, Meelis Lootus, Kartik Kathore, Akshay Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying machine learning applications on edge devices can bring clear
benefits such as improved reliability, latency and privacy but it also
introduces its own set of challenges. Most works focus on the limited
computational resources of edge platforms but this is not the only bottleneck
standing in the way of widespread adoption. In this paper we list several other
challenges that a TinyML practitioner might need to consider when
operationalizing an application on edge devices. We focus on tasks such as
monitoring and managing the application, common functionality for a MLOps
platform, and show how they are complicated by the distributed nature of edge
deployment. We also discuss issues that are unique to edge applications such as
protecting a model's intellectual property and verifying its integrity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th Workshop on Parallel AI and Systems for the Edge (PAISE2022)
  paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Imitation Learning from Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgiy Pshikhachev, Dmitry Ivanov, Vladimir Egorov, Aleksei Shpilman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the numerous breakthroughs achieved with Reinforcement Learning (RL),
solving environments with sparse rewards remains a challenging task that
requires sophisticated exploration. Learning from Demonstrations (LfD) remedies
this issue by guiding the agent's exploration towards states experienced by an
expert. Naturally, the benefits of this approach hinge on the quality of
demonstrations, which are rarely optimal in realistic scenarios. Modern LfD
algorithms require meticulous tuning of hyperparameters that control the
influence of demonstrations and, as we show in the paper, struggle with
learning from suboptimal demonstrations. To address these issues, we extend
Self-Imitation Learning (SIL), a recent RL algorithm that exploits the agent's
past good experience, to the LfD setup by initializing its replay buffer with
demonstrations. We denote our algorithm as SIL from Demonstrations (SILfD). We
empirically show that SILfD can learn from demonstrations that are noisy or far
from optimal and can automatically adjust the influence of demonstrations
throughout the training without additional hyperparameters or handcrafted
schedules. We also find SILfD superior to the existing state-of-the-art LfD
algorithms in sparse environments, especially when demonstrations are highly
suboptimal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Algorithms for Extreme Bandits <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorian Baudry, Yoan Russac, Emilie Kaufmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we contribute to the Extreme Bandit problem, a variant of
Multi-Armed Bandits in which the learner seeks to collect the largest possible
reward. We first study the concentration of the maximum of i.i.d random
variables under mild assumptions on the tail of the rewards distributions. This
analysis motivates the introduction of Quantile of Maxima (QoMax). The
properties of QoMax are sufficient to build an Explore-Then-Commit (ETC)
strategy, QoMax-ETC, achieving strong asymptotic guarantees despite its
simplicity. We then propose and analyze a more adaptive, anytime algorithm,
QoMax-SDA, which combines QoMax with a subsampling method recently introduced
by Baudry et al. (2021). Both algorithms are more efficient than existing
approaches in two aspects (1) they lead to better empirical performance (2)
they enjoy a significant reduction of the memory and time complexities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 25 th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-class Label Noise Learning via Loss Decomposition and Centroid
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Ding, Tao Zhou, Chuang Zhang, Yijing Luo, Juan Tang, Chen Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, many large-scale datasets often contain inaccurate
labels, i.e., noisy labels, which may confuse model training and lead to
performance degradation. To overcome this issue, Label Noise Learning (LNL) has
recently attracted much attention, and various methods have been proposed to
design an unbiased risk estimator to the noise-free dataset to combat such
label noise. Among them, a trend of works based on Loss Decomposition and
Centroid Estimation (LDCE) has shown very promising performance. However,
existing LNL methods based on LDCE are only designed for binary classification,
and they are not directly extendable to multi-class situations. In this paper,
we propose a novel multi-class robust learning method for LDCE, which is termed
"MC-LDCE". Specifically, we decompose the commonly adopted loss (e.g., mean
squared loss) function into a label-dependent part and a label-independent
part, in which only the former is influenced by label noise. Further, by
defining a new form of data centroid, we transform the recovery problem of a
label-dependent part to a centroid estimation problem. Finally, by critically
examining the mathematical expectation of clean data centroid given the
observed noisy set, the centroid can be estimated which helps to build an
unbiased risk estimator for multi-class learning. The proposed MC-LDCE method
is general and applicable to different types (i.e., linear and nonlinear) of
classification models. The experimental results on five public datasets
demonstrate the superiority of the proposed MC-LDCE against other
representative LNL methods in tackling multi-class label noise problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> RGB-Depth Fusion GAN for Indoor Depth Completion <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Wang, Mingyuan Wang, Zhengping Che, Zhiyuan Xu, Xiuquan Qiao, Mengshi Qi, Feifei Feng, <span class="highlight-author">Jian Tang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The raw depth image captured by the indoor depth sensor usually has an
extensive range of missing depth values due to inherent limitations such as the
inability to perceive transparent objects and limited distance range. The
incomplete depth map burdens many downstream vision tasks, and a rising number
of depth completion methods have been proposed to alleviate this issue. While
most existing methods can generate accurate dense depth maps from sparse and
uniformly sampled depth maps, they are not suitable for complementing the large
contiguous regions of missing depth values, which is common and critical. In
this paper, we design a novel two-branch end-to-end fusion network, which takes
a pair of RGB and incomplete depth images as input to predict a dense and
completed depth map. The first branch employs an encoder-decoder structure to
regress the local dense depth values from the raw depth map, with the help of
local guidance information extracted from the RGB image. In the other branch,
we propose an RGB-depth fusion GAN to transfer the RGB image to the
fine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN
to propagate the features across the two branches, and we append a confidence
fusion head to fuse the two outputs of the branches for the final depth map.
Extensive experiments on NYU-Depth V2 and SUN RGB-D demonstrate that our
proposed method clearly improves the depth completion performance, especially
in a more realistic setting of indoor environments with the help of the pseudo
depth map.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lean Evolutionary Reinforcement Learning by Multitasking with Importance
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Zhang, Abhishek Gupta, Zefeng Chen, Yew-Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies have shown evolution strategies (ES) to be a promising approach for
reinforcement learning (RL) with deep neural networks. However, the issue of
high sample complexity persists in applications of ES to deep RL. In this
paper, we address the shortcoming of today's methods via a novel
neuroevolutionary multitasking (NuEMT) algorithm, designed to transfer
information from a set of auxiliary tasks (of short episode length) to the
target (full length) RL task at hand. The artificially generated auxiliary
tasks allow an agent to update and quickly evaluate policies on shorter time
horizons. The evolved skills are then transferred to guide the longer and
harder task towards an optimal policy. We demonstrate that the NuEMT algorithm
achieves data-lean evolutionary RL, reducing expensive agent-environment
interaction data requirements. Our key algorithmic contribution in this setting
is to introduce, for the first time, a multitask information transfer mechanism
based on the statistical importance sampling technique. In addition, an
adaptive resource allocation strategy is utilized to assign computational
resources to auxiliary tasks based on their gleaned usefulness. Experiments on
a range of continuous control tasks from the OpenAI Gym confirm that our
proposed algorithm is efficient compared to recent ES baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-class versus One-class classifier in spontaneous speech analysis
  oriented to Alzheimer Disease diagnosis <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. López-de-Ipiña, Marcos Faundez-Zanuy, Jordi Solé-Casals, Fernando Zelarin, Pilar Calvo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of medical developments require the ability to identify samples that are
anomalous with respect to a target group or control group, in the sense they
could belong to a new, previously unseen class or are not class data. In this
case when there are not enough data to train two-class One-class classification
appear like an available solution. On the other hand non-linear approaches
could give very useful information. The aim of our project is to contribute to
earlier diagnosis of AD and better estimates of its severity by using automatic
analysis performed through new biomarkers extracted from speech signal. The
methods selected in this case are speech biomarkers oriented to Spontaneous
Speech and Emotional Response Analysis. In this approach One-class classifiers
and two-class classifiers are analyzed. The use of information about outlier
and Fractal Dimension features improves the system performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, published in International Conference on NONLINEAR SPEECH
  PROCESSING, NOLISP 2015 jointly organized with the 25th Italian Workshop on
  Neural Networks, WIRN 2015, held at May 2015, Vietri sul Mare, Salerno, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Vision <span class="highlight-title">Transformer</span>s: Combining Improvements in Metric
  Learning <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Features as Markers of Parkinson's Disease: The Issue of
  Clinical Interpretability <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiri Mekyska, Zdenek Smekal, Zoltan Galaz, Zdenek Mzourek, Irena Rektorova, Marcos Faundez-Zanuy, Karmele Lopez-De-Ipina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Up to 90% of patients with Parkinson's disease (PD) suffer from hypokinetic
dysathria (HD) which is also manifested in the field of phonation. Clinical
signs of HD like monoloudness, monopitch or hoarse voice are usually quantified
by conventional clinical interpretable features (jitter, shimmer,
harmonic-to-noise ratio, etc.). This paper provides large and robust insight
into perceptual analysis of 5 Czech vowels of 84 PD patients and proves that
despite the clinical inexplicability the perceptual features outperform the
conventional ones, especially in terms of discrimination power (classification
accuracy ACC = 92 %, sensitivity SEN = 93 %, specificity SPE = 92 %) and
partial correlation with clinical scores like UPDRS (Unified Parkinson's
disease rating scale), MMSE (Mini-mental state examination) or FOG (Freezing of
gait questionnaire), where p < 0.0001.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, published in International Conference on NONLINEAR SPEECH
  PROCESSING, NOLISP 2015 jointly organized with the 25th Italian Workshop on
  Neural Networks, WIRN 2015, held at May 2015, Vietri sul Mare, Salerno, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc R. Schlichting, Stefan Notter, Walter Fichter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning-based path planning for multi-agent systems of varying
size constitutes a research topic with increasing significance as progress in
domains such as urban air mobility and autonomous aerial vehicles continues.
Reinforcement learning with continuous state and action spaces is used to train
a policy network that accommodates desirable path planning behaviors and can be
used for time-critical applications. A Long Short-Term Memory module is
proposed to encode an unspecified number of states for a varying, indefinite
number of agents. The described training strategies and policy architecture
lead to a guidance that scales to an infinite number of agents and unlimited
physical dimensions, although training takes place at a smaller scale. The
guidance is implemented on a low-cost, off-the-shelf onboard computer. The
feasibility of the proposed approach is validated by presenting flight test
results of up to four drones, autonomously navigating collision-free in a
real-world environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For associated source code, see
  https://github.com/MarcSchlichting/LSTMSpatialEncoding , For associated video
  of flight test, see https://schlichting.page.link/lstm_flight_test , 17
  pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnoViT: Unsupervised Anomaly Detection and Localization with Vision
  <span class="highlight-title">Transformer</span>-based Encoder-Decoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunseung Lee, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image anomaly detection problems aim to determine whether an image is
abnormal, and to detect anomalous areas. These methods are actively used in
various fields such as manufacturing, medical care, and intelligent
information. Encoder-decoder structures have been widely used in the field of
anomaly detection because they can easily learn normal patterns in an
unsupervised learning environment and calculate a score to identify
abnormalities through a reconstruction error indicating the difference between
input and reconstructed images. Therefore, current image anomaly detection
methods have commonly used convolutional encoder-decoders to extract normal
information through the local features of images. However, they are limited in
that only local features of the image can be utilized when constructing a
normal representation owing to the characteristics of convolution operations
using a filter of fixed size. Therefore, we propose a vision transformer-based
encoder-decoder model, named AnoViT, designed to reflect normal information by
additionally learning the global relationship between image patches, which is
capable of both image anomaly detection and localization. The proposed approach
constructs a feature map that maintains the existing location information of
individual patches by using the embeddings of all patches passed through
multiple self-attention layers. The proposed AnoViT model performed better than
the convolution-based model on three benchmark datasets. In MVTecAD, which is a
representative benchmark dataset for anomaly localization, it showed improved
results on 10 out of 15 classes compared with the baseline. Furthermore, the
proposed method showed good performance regardless of the class and type of the
anomalous area when localization results were evaluated qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Wireless Communications: From Theory to
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Shen, Jun Zhang, S. H. Song, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based approaches have been developed to solve challenging
problems in wireless communications, leading to promising results. Early
attempts adopted neural network architectures inherited from applications such
as computer vision. They often require huge amounts of training samples (i.e.,
poor generalization), and yield poor performance in large-scale networks (i.e.,
poor scalability). To resolve these issues, graph neural networks (GNNs) have
been recently adopted, as they can effectively exploit the domain knowledge,
i.e., the graph topology in wireless communication problems. GNN-based methods
can achieve near-optimal performance in large-scale networks and generalize
well under different system settings, but the theoretical underpinnings and
design guidelines remain elusive, which may hinder their practical
implementations. This paper endeavors to fill both the theoretical and
practical gaps. For theoretical guarantees, we prove that GNNs achieve
near-optimal performance in wireless networks with much fewer training samples
than traditional neural architectures. Specifically, to solve an optimization
problem on an $n$-node graph (where the nodes may represent users, base
stations, or antennas), GNNs' generalization error and required number of
training samples are $\mathcal{O}(n)$ and $\mathcal{O}(n^2)$ times lower than
the unstructured multi-layer perceptrons. For design guidelines, we propose a
unified framework that is applicable to general design problems in wireless
networks, which includes graph modeling, neural architecture design, and
theory-guided performance enhancement. Extensive simulations, which cover a
variety of important problems and network settings, verify our theory and
effectiveness of the proposed design framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization by Mutual-Information Regularization with
  <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbum Cha, Kyungjae Lee, Sungrae Park, Sanghyuk Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to learn a generalized model to an unseen
target domain using only limited source domains. Previous attempts to DG fail
to learn domain-invariant representations only from the source domains due to
the significant domain shifts between training and test domains. Instead, we
re-formulate the DG objective using mutual information with the oracle model, a
model generalized to any possible domain. We derive a tractable variational
lower bound via approximating the oracle model by a pre-trained model, called
Mutual Information Regularization with Oracle (MIRO). Our extensive experiments
show that MIRO significantly improves the out-of-distribution performance.
Furthermore, our scaling experiments show that the larger the scale of the
pre-trained model, the greater the performance improvement of MIRO. Source code
is available at https://github.com/kakaobrain/miro.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifications of Skull Fractures using CT Scan Images via CNN with
  Lazy Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Moniruzzaman Emon, Tareque Rahman Ornob, Moqsadur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification of skull fracture is a challenging task for both radiologists
and researchers. Skull fractures result in broken pieces of bone, which can cut
into the brain and cause bleeding and other injury types. So it is vital to
detect and classify the fracture very early. In real world, often fractures
occur at multiple sites. This makes it harder to detect the fracture type where
many fracture types might summarize a skull fracture. Unfortunately, manual
detection of skull fracture and the classification process is time-consuming,
threatening a patient's life. Because of the emergence of deep learning, this
process could be automated. Convolutional Neural Networks (CNNs) are the most
widely used deep learning models for image categorization because they deliver
high accuracy and outstanding outcomes compared to other models. We propose a
new model called SkullNetV1 comprising a novel CNN by taking advantage of CNN
for feature extraction and lazy learning approach which acts as a classifier
for classification of skull fractures from brain CT images to classify five
fracture types. Our suggested model achieved a subset accuracy of 88%, an F1
score of 93%, the Area Under the Curve (AUC) of 0.89 to 0.98, a Hamming score
of 92% and a Hamming loss of 0.04 for this seven-class multi-labeled
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delving into the Estimation Shift of Batch Normalization in a Network <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Yi Zhou, Tian Wang, Jie Luo, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch normalization (BN) is a milestone technique in deep learning. It
normalizes the activation using mini-batch statistics during training but the
estimated population statistics during inference. This paper focuses on
investigating the estimation of population statistics. We define the estimation
shift magnitude of BN to quantitatively measure the difference between its
estimated population statistics and expected ones. Our primary observation is
that the estimation shift can be accumulated due to the stack of BN in a
network, which has detriment effects for the test performance. We further find
a batch-free normalization (BFN) can block such an accumulation of estimation
shift. These observations motivate our design of XBNBlock that replace one BN
with BFN in the bottleneck block of residual-style networks. Experiments on the
ImageNet and COCO benchmarks show that XBNBlock consistently improves the
performance of different architectures, including ResNet and ResNeXt, by a
significant margin and seems to be more robust to distribution shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022. The Code is available at:
  https://github.com/huangleiBuaa/XBNBlock</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiayu Liang, Ying Gao, Shanrong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, many industries have applied classification algorithms to help them
solve problems in their business, like finance, medicine, manufacturing
industry and so on. However, in real-life scenarios, positive examples only
make up a small part of all instances and our datasets suffer from high
imbalance ratio which leads to poor performance of existing classification
models. To solve this problem, we come up with a bagging ensemble learning
framework based on an anomaly detection scoring system. We test out that our
ensemble learning model can dramatically improve performance of base estimators
(e.g. Decision Tree, Multilayer perceptron, KNN) and is more efficient than
other existing methods under a wide range of imbalance ratio, data scale and
data dimension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Mixup for Data-efficient Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Liu, Siyuan Li, Ge Wang, Cheng Tan, Lirong Wu, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixup is an efficient data augmentation approach that improves the
generalization of neural networks by smoothing the decision boundary with mixed
data. Recently, dynamic mixup methods improve previous static policies (e.g.,
linear interpolation) by maximizing discriminative regions or maintaining the
salient objects in mixed samples. We notice that The mixed samples from dynamic
policies are more separable than the static ones while preventing models from
overfitting. Inspired by this finding, we first argue that there exists an
over-smoothing issue in the mixup objective, which focuses on regression the
mixing ratio instead of identifying discriminative features. We are therefore
prompted to propose a decoupled mixup (DM) loss that can adaptively mine
discriminative features without losing smoothness. DM enables static mixup
methods to achieve comparable performance with dynamic methods while avoiding
heavy computational overhead. This also leads to an interesting objective
design problem for mixup training that we need to focus not only on smoothing
the decision boundaries but also on identifying discriminative features.
Extensive experiments on supervised and semi-supervised learning benchmarks
across seven classification datasets validate the effectiveness of DM by
equipping with various mixup methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first preprint version, 21 pages. The source code is available at
  https://github.com/Westlake-AI/openmixup</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Inductive Bias of In-Context Learning: Rethinking <span class="highlight-title">Pretrain</span>ing
  Example Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose "kNN-Pretraining": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for pretraining
example design indicates new training schemes for self-improving
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Altruistic Behaviours in Reinforcement Learning without
  External Rewards <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.09598v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.09598v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Franzmeyer, Mateusz Malinowski, João F. Henriques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can artificial agents learn to assist others in achieving their goals without
knowing what those goals are? Generic reinforcement learning agents could be
trained to behave altruistically towards others by rewarding them for
altruistic behaviour, i.e., rewarding them for benefiting other agents in a
given situation. Such an approach assumes that other agents' goals are known so
that the altruistic agent can cooperate in achieving those goals. However,
explicit knowledge of other agents' goals is often difficult to acquire. In the
case of human agents, their goals and preferences may be difficult to express
fully; they might be ambiguous or even contradictory. Thus, it is beneficial to
develop agents that do not depend on external supervision and learn altruistic
behaviour in a task-agnostic manner. We propose to act altruistically towards
other agents by giving them more choice and allowing them to achieve their
goals better. Some concrete examples include opening a door for others or
safeguarding them to pursue their objectives without interference. We formalize
this concept and propose an altruistic agent that learns to increase the
choices another agent has by preferring to maximize the number of states that
the other agent can reach in its future. We evaluate our approach in three
different multi-agent environments where another agent's success depends on
altruistic behaviour. Finally, we show that our unsupervised agents can perform
comparably to agents explicitly trained to work cooperatively, in some cases
even outperforming them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 Spotlight Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Does Kohn-Sham Regularizer Work for Weakly Correlated Systems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14846v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14846v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhupalee Kalita, Ryan Pederson, Jielun Chen, Li Li, Kieron Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kohn-Sham regularizer (KSR) is a differentiable machine learning approach to
finding the exchange-correlation functional in Kohn-Sham density functional
theory (DFT) that works for strongly correlated systems. Here we test KSR for
weak correlation. We propose spin-adapted KSR (sKSR) with trainable local,
semilocal, and nonlocal approximations found by minimizing density and total
energy loss. We assess the atoms-to-molecules generalizability by training on
one-dimensional (1D) H, He, Li, Be, Be$^{++}$ and testing on 1D hydrogen
chains, LiH, BeH$_2$, and helium hydride complexes. The generalization error
from our semilocal approximation is comparable to other differentiable
approaches, but our nonlocal functional outperforms any existing machine
learning functionals, predicting ground-state energies of test systems with a
mean absolute error of 2.7 milli-Hartrees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Enough: Searching for Sufficient Measures of Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvodeep Majumder, Joymallya Chakraborty, Gina R. Bai, Kathryn T. Stolee, Tim Menzies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing machine learning software for ethical bias has become a pressing
current concern. In response, recent research has proposed a plethora of new
fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360
toolkit. This raises the question: How can any fairness tool satisfy such a
diverse range of goals? While we cannot completely simplify the task of
fairness testing, we can certainly reduce the problem. This paper shows that
many of those fairness metrics effectively measure the same thing. Based on
experiments using seven real-world datasets, we find that (a) 26 classification
metrics can be clustered into seven groups, and (b) four dataset metrics can be
clustered into three groups. Further, each reduced set may actually predict
different things. Hence, it is no longer necessary (or even possible) to
satisfy all fairness metrics. In summary, to simplify the fairness testing
problem, we recommend the following steps: (1)~determine what type of fairness
is desirable (and we offer a handful of such types); then (2) lookup those
types in our clusters; then (3) just test for one item per cluster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 tables and 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hidden Convexity of Wasserstein GANs: Interpretable Generative Models
  with Closed-Form Solutions <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.05680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.05680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) are commonly used for modeling complex
distributions of data. Both the generators and discriminators of GANs are often
modeled by neural networks, posing a non-transparent optimization problem which
is non-convex and non-concave over the generator and discriminator,
respectively. Such networks are often heuristically optimized with gradient
descent-ascent (GDA), but it is unclear whether the optimization problem
contains any saddle points, or whether heuristic methods can find them in
practice. In this work, we analyze the training of Wasserstein GANs with
two-layer neural network discriminators through the lens of convex duality, and
for a variety of generators expose the conditions under which Wasserstein GANs
can be solved exactly with convex optimization approaches, or can be
represented as convex-concave games. Using this convex duality interpretation,
we further demonstrate the impact of different activation functions of the
discriminator. Our observations are verified with numerical results
demonstrating the power of the convex interpretation, with applications in
progressive training of convex architectures corresponding to linear generators
and quadratic-activation discriminators for CelebA image generation. The code
for our experiments is available at https://github.com/ardasahiner/ProCoGAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as paper in ICLR 2022. First two authors contributed
  equally to this work; 34 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Temporal Knowledge Embeddings with Contextualized Language
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Han, Ruotong Liao, Beiyan Liu, Yao Zhang, Zifeng Ding, Heinz Köppl, Hinrich Schütze, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emerging research effort to integrate structured and unstructured
knowledge, many approaches incorporate factual knowledge into pre-trained
language models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP
tasks. However, (1) they only consider static factual knowledge, but knowledge
graphs (KGs) also contain temporal facts or events indicating evolutionary
relationships among entities at different timestamps. (2) PLMs cannot be
directly applied to many KG tasks, such as temporal KG completion.
  In this paper, we focus on \textbf{e}nhancing temporal knowledge embeddings
with \textbf{co}ntextualized \textbf{la}nguage representations (ECOLA). We
align structured knowledge contained in temporal knowledge graphs with their
textual descriptions extracted from news articles and propose a novel
knowledge-text prediction task to inject the abundant information from
descriptions into temporal knowledge embeddings. ECOLA jointly optimizes the
knowledge-text prediction objective and the temporal knowledge embeddings,
which can simultaneously take full advantage of textual and knowledge
information. For training ECOLA, we introduce three temporal KG datasets with
aligned textual descriptions. Experimental results on the temporal knowledge
graph completion task show that ECOLA outperforms state-of-the-art temporal KG
models by a large margin. The proposed datasets can serve as new temporal KG
benchmarks and facilitate future research on structured and unstructured
knowledge integration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plumber: Diagnosing and Removing Performance Bottlenecks in Machine
  Learning Data Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kuchnik, Ana Klimovic, Jiri Simsa, Virginia Smith, George Amvrosiadis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Input pipelines, which ingest and transform input data, are an essential part
of training Machine Learning (ML) models. However, it is challenging to
implement efficient input pipelines, as it requires reasoning about
parallelism, asynchrony, and variability in fine-grained profiling information.
Our analysis of over two million ML jobs in Google datacenters reveals that a
significant fraction of model training jobs could benefit from faster input
data pipelines. At the same time, our analysis indicates that most jobs do not
saturate host hardware, pointing in the direction of software-based
bottlenecks. Motivated by these findings, we propose Plumber, a tool for
finding bottlenecks in ML input pipelines. Plumber uses an extensible and
interpretable operational analysis analytical model to automatically tune
parallelism, prefetching, and caching under host resource constraints. Across
five representative ML pipelines, Plumber obtains speedups of up to 47x for
misconfigured pipelines. By automating caching, Plumber obtains end-to-end
speedups of over 50% compared to state-of-the-art tuners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Batch Normalization in ReLU Networks: Equivalent Convex
  Optimization Models and Implicit Regularization <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.01499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.01499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch Normalization (BN) is a commonly used technique to accelerate and
stabilize training of deep neural networks. Despite its empirical success, a
full theoretical understanding of BN is yet to be developed. In this work, we
analyze BN through the lens of convex optimization. We introduce an analytic
framework based on convex duality to obtain exact convex representations of
weight-decay regularized ReLU networks with BN, which can be trained in
polynomial-time. Our analyses also show that optimal layer weights can be
obtained as simple closed-form formulas in the high-dimensional and/or
overparameterized regimes. Furthermore, we find that Gradient Descent provides
an algorithmic bias effect on the standard non-convex BN network, and we design
an approach to explicitly encode this implicit regularization into the convex
objective. Experiments with CIFAR image classification highlight the
effectiveness of this explicit regularization for mimicking and substantially
improving the performance of standard BN networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022. First two authors contributed equally to this
  work; 36 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimCVD: Simple <span class="highlight-title">Contrastive</span> Voxel-Wise Representation <span class="highlight-title">Distillation</span> for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.06227v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.06227v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, most existing
learning-based approaches usually suffer from limited manually annotated
medical data, which poses a major practical problem for accurate and robust
medical image segmentation. In addition, most existing semi-supervised
approaches are usually not robust compared with the supervised counterparts,
and also lack explicit modeling of geometric structure and semantic
information, both of which limit the segmentation accuracy. In this work, we
present SimCVD, a simple contrastive distillation framework that significantly
advances state-of-the-art voxel-wise representation learning. We first describe
an unsupervised training strategy, which takes two views of an input volume and
predicts their signed distance maps of object boundaries in a contrastive
objective, with only two independent dropout as mask. This simple approach
works surprisingly well, performing on the same level as previous fully
supervised methods with much less labeled data. We hypothesize that dropout can
be viewed as a minimal form of data augmentation and makes the network robust
to representation collapse. Then, we propose to perform structural distillation
by distilling pair-wise similarities. We evaluate SimCVD on two popular
datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT
dataset. The results on the LA dataset demonstrate that, in two types of
labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of
90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to
previous best results. Our method can be trained in an end-to-end fashion,
showing the promise of utilizing SimCVD as a general framework for downstream
tasks, such as medical image synthesis, enhancement, and registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Medical Imaging (IEEE-TMI) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundamental limits for rank-one matrix estimation with groupwise
  heteroskedasticity <span class="chip">AISTATS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.11950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.11950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua K. Behne, Galen Reeves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank matrix recovery problems involving high-dimensional and
heterogeneous data appear in applications throughout statistics and machine
learning. The contribution of this paper is to establish the fundamental limits
of recovery for a broad class of these problems. In particular, we study the
problem of estimating a rank-one matrix from Gaussian observations where
different blocks of the matrix are observed under different noise levels. In
the setting where the number of blocks is fixed while the number of variables
tends to infinity, we prove asymptotically exact formulas for the minimum
mean-squared error in estimating both the matrix and underlying factors. These
results are based on a novel reduction from the low-rank matrix tensor product
model (with homogeneous noise) to a rank-one model with heteroskedastic noise.
  As an application of our main result, we show that recently proposed methods
based on applying principal component analysis (PCA) to weighted combinations
of the data are optimal in some settings but sub-optimal in others. We also
provide numerical results comparing our asymptotic formulas with the
performance of methods based on weighted PCA, gradient descent, and approximate
message passing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 4 figures. To appear in: The 25th International Conference
  on Artificial Intelligence and Statistics (AISTATS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention based Memory video portrait matting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufeng Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed a novel trimap free video matting method based on the attention
mechanism. By the nature of the problem, most existing approaches use either
multiple computational expansive modules or complex algorithms to exploit
temporal information fully. We designed a temporal aggregation module to
compute the temporal coherence between the current frame and its two previous
frames.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization of Neural Combinatorial Solvers Through the Lens of
  Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Geisler, Johanna Sommer, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end (geometric) deep learning has seen first successes in
approximating the solution of combinatorial optimization problems. However,
generating data in the realm of NP-hard/-complete tasks brings practical and
theoretical challenges, resulting in evaluation protocols that are too
optimistic. Specifically, most datasets only capture a simpler subproblem and
likely suffer from spurious features. We investigate these effects by studying
adversarial robustness - a local generalization property - to reveal hard,
model-specific instances and spurious features. For this purpose, we derive
perturbation models for SAT and TSP. Unlike in other applications, where
perturbation models are designed around subjective notions of imperceptibility,
our perturbation models are efficient and sound, allowing us to determine the
true label of perturbed samples without a solver. Surprisingly, with such
perturbations, a sufficiently expressive neural solver does not suffer from the
limitations of the accuracy-robustness trade-off common in supervised learning.
Although such robust solvers exist, we show empirically that the assessed
neural solvers do not generalize well w.r.t. small perturbations of the problem
instance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid determination of protein resonance assignments and
  three-dimensional structures from raw NMR spectra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Klukowski, Roland Riek, Peter Güntert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclear Magnetic Resonance (NMR) spectroscopy is one of the major techniques
in structural biology with over 11,800 protein structures deposited in the
Protein Data Bank. NMR can elucidate structures and dynamics of small and
medium size proteins in solution, living cells, and solids, but has been
limited by the tedious data analysis process. It typically requires weeks or
months of manual work of a trained expert to turn NMR measurements into a
protein structure. Automation of this process is an open problem, formulated in
the field over 30 years ago. Here, we present a solution to this challenge that
enables the completely automated analysis of protein NMR data within hours
after completing the measurements. Our machine learning-based method, ARTINA,
uses as input only NMR spectra and the protein sequence, and delivers signal
positions, resonance assignments, and structures strictly without any human
intervention. Tested on a 100-protein benchmark comprising 1329
multidimensional NMR spectra, ARTINA demonstrated its ability to solve
structures with 1.44 {\AA} median RMSD to the PDB reference and to identify
91.36% correct NMR resonance assignments. ARTINA can be used by non-experts,
reducing the effort for a protein assignment or structure determination by NMR
essentially to the preparation of the sample and the spectra measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locating and Editing Factual Knowledge in <span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the mechanisms underlying factual knowledge recall in
autoregressive transformer language models. First, we develop a causal
intervention for identifying neuron activations capable of altering a model's
factual predictions. Within large GPT-style models, this reveals two distinct
sets of neurons that we hypothesize correspond to knowing an abstract fact and
saying a concrete word, respectively. This insight inspires the development of
ROME, a novel method for editing facts stored in model weights. For evaluation,
we assemble CounterFact, a dataset of over twenty thousand counterfactuals and
tools to facilitate sensitive measurements of knowledge editing. Using
CounterFact, we confirm the distinction between saying and knowing neurons, and
we find that ROME achieves state-of-the-art performance in knowledge editing
compared to other methods. An interactive demo notebook, full code
implementation, and the dataset are available at https://rome.baulab.info/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 22 figures. Code and data at https://rome.baulab.info/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directed Weight Neural Networks for Protein Structure Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahan Li, Shitong Luo, Congyue Deng, Chaoran Cheng, Jiaqi Guan, Leonidas Guibas, Jian Peng, Jianzhu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A protein performs biological functions by folding to a particular 3D
structure. To accurately model the protein structures, both the overall
geometric topology and local fine-grained relations between amino acids (e.g.
side-chain torsion angles and inter-amino-acid orientations) should be
carefully considered. In this work, we propose the Directed Weight Neural
Network for better capturing geometric relations among different amino acids.
Extending a single weight from a scalar to a 3D directed vector, our new
framework supports a rich set of geometric operations on both classical and
SO(3)--representation features, on top of which we construct a perceptron unit
for processing amino-acid information. In addition, we introduce an equivariant
message passing paradigm on proteins for plugging the directed weight
perceptrons into existing Graph Neural Networks, showing superior versatility
in maintaining SO(3)-equivariance at the global scale. Experiments show that
our network has remarkably better expressiveness in representing geometric
relations in comparison to classical neural networks and the (globally)
equivariant networks. It also achieves state-of-the-art performance on various
computational biology applications related to protein 3D structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Reinforcement Learning with Linear Function
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the wide adoption of reinforcement learning (RL) in real-world
personalized services, where users' sensitive and private information needs to
be protected, we study regret minimization in finite-horizon Markov decision
processes (MDPs) under the constraints of differential privacy (DP). Compared
to existing private RL algorithms that work only on tabular finite-state,
finite-actions MDPs, we take the first step towards privacy-preserving learning
in MDPs with large state and action spaces. Specifically, we consider MDPs with
linear function approximation (in particular linear mixture MDPs) under the
notion of joint differential privacy (JDP), where the RL agent is responsible
for protecting users' sensitive data. We design two private RL algorithms that
are based on value iteration and policy optimization, respectively, and show
that they enjoy sub-linear regret performance while guaranteeing privacy
protection. Moreover, the regret bounds are independent of the number of
states, and scale at most logarithmically with the number of actions, making
the algorithms suitable for privacy protection in nowadays large-scale
personalized services. Our results are achieved via a general procedure for
learning in linear mixture MDPs under changing regularizers, which not only
generalizes previous results for non-private learning, but also serves as a
building block for general private reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Sigmetrics 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip
  Connections and Hybrid Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo R. Corral-Soto, Mrigank Rochan, Yannis Y. He, Shubhra Aich, <span class="highlight-author">Yang Liu</span>, Liu Bingbing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we address the challenging problem of domain adaptation in
LiDAR semantic segmentation. We consider the setting where we have a
fully-labeled data set from source domain and a target domain with a few
labeled and many unlabeled examples. We propose a domain adaption framework
that mitigates the issue of domain shift and produces appealing performance on
the target domain. To this end, we develop a GAN-based image-to-image
translation engine that has generators with alternating connections, and couple
it with a state-of-the-art LiDAR semantic segmentation network. Our framework
is hybrid in nature in the sense that our model learning is composed of
self-supervision, semi-supervision and unsupervised learning. Extensive
experiments on benchmark LiDAR semantic segmentation data sets demonstrate that
our method achieves superior performance in comparison to strong baselines and
prior arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1) Introduced Fig 1, 2) Simplified Fig. 2 diagram, 3) Fixed typos in
  losses, 4) Introduced Fig. 3, 5) Updated evaluation results, included
  evaluation on SemanticPOSS, 6) Introduced Table 3 - effects on covariance
  matrix and mean, 7) Updated Fig. 5, 8) Added more references. Improved
  writing in general, especially the motivation and description of each element
  and contribution from the method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Learning for Stabilizing Label Selection in Speech
  Separation with Mapping-based Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Gao, Yue Gu, Ivan Marsic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech separation has been studied in time domain because of lower latency
and higher performance compared to time-frequency domain. The masking-based
method has been mostly used in time domain, and the other common method
(mapping-based) has been inadequately studied. We investigate the use of the
mapping-based method in the time domain and show that it can perform better on
a large training set than the masking-based method. We also investigate the
frequent label-switching problem in permutation invariant training (PIT), which
results in suboptimal training because the labels selected by PIT differ across
training epochs. Our experiment results showed that PIT works well in a shallow
separation model, and the label switching occurs for a deeper model. We
inferred that layer decoupling may be the reason for the frequent label
switching. Therefore, we propose a training strategy based on progressive
learning. This approach significantly reduced inconsistent label assignment
without added computational complexity or training corpus. By combining this
training strategy with the mapping-based method, we significantly improved the
separation performance compared to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motron: <span class="highlight-title">Multimodal</span> Probabilistic Human Motion Forecasting <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Marco Pavone, Markus Ryll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A systematic approach to random data augmentation on graph neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.04314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.04314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Billy Joe Franks, Markus Anders, Marius Kloft, Pascal Schweitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random data augmentations (RDAs) are state of the art regarding practical
graph neural networks that are provably universal. There is great diversity
regarding terminology, methodology, benchmarks, and evaluation metrics used
among existing RDAs. Not only does this make it increasingly difficult for
practitioners to decide which technique to apply to a given problem, but it
also stands in the way of systematic improvements. We propose a new
comprehensive framework that captures all previous RDA techniques. On the
theoretical side, among other results, we formally prove that under natural
conditions all instantiations of our framework are universal. On the practical
side, we develop a method to systematically and automatically train RDAs. This
in turn enables us to impartially and objectively compare all existing RDAs.
New RDAs naturally emerge from our approach, and our experiments demonstrate
that they improve the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stochastic Halpern Iteration with Variance Reduction for Stochastic
  Monotone Inclusion Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Cai, Chaobing Song, Cristóbal Guzmán, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study stochastic monotone inclusion problems, which widely appear in
machine learning applications, including robust regression and adversarial
learning. We propose novel variants of stochastic Halpern iteration with
recursive variance reduction. In the cocoercive -- and more generally
Lipschitz-monotone -- setup, our algorithm attains $\epsilon$ norm of the
operator with $\mathcal{O}(\frac{1}{\epsilon^3})$ stochastic operator
evaluations, which significantly improves over state of the art
$\mathcal{O}(\frac{1}{\epsilon^4})$ stochastic operator evaluations required
for existing monotone inclusion solvers applied to the same problem classes. We
further show how to couple one of the proposed variants of stochastic Halpern
iteration with a scheduled restart scheme to solve stochastic monotone
inclusion problems with ${\mathcal{O}}(\frac{\log(1/\epsilon)}{\epsilon^2})$
stochastic operator evaluations under additional sharpness or strong
monotonicity assumptions. Finally, we argue via reductions between different
problem classes that our stochastic oracle complexity bounds are tight up to
logarithmic factors in terms of their $\epsilon$-dependence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Part Discovery from <span class="highlight-title">Contrastive</span> Reconstruction <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.06349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.06349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Choudhury, Iro Laina, Christian Rupprecht, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of self-supervised visual representation learning is to learn
strong, transferable image representations, with the majority of research
focusing on object or scene level. On the other hand, representation learning
at part level has received significantly less attention. In this paper, we
propose an unsupervised approach to object part discovery and segmentation and
make three contributions. First, we construct a proxy task through a set of
objectives that encourages the model to learn a meaningful decomposition of the
image into its parts. Secondly, prior work argues for reconstructing or
clustering pre-computed features as a proxy to parts; we show empirically that
this alone is unlikely to find meaningful parts; mainly because of their low
resolution and the tendency of classification networks to spatially smear out
information. We suggest that image reconstruction at the level of pixels can
alleviate this problem, acting as a complementary cue. Lastly, we show that the
standard evaluation based on keypoint regression does not correlate well with
segmentation quality and thus introduce different metrics, NMI and ARI, that
better characterize the decomposition of objects into parts. Our method yields
semantic parts which are consistent across fine-grained but visually distinct
categories, outperforming the state of the art on three benchmark datasets.
Code is available at the project page:
https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021. Project page:
  https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Methods for Stabilizing Models across Large Samples of Projects (with
  case studies on Predicting Defect and Project Health) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.04250v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.04250v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvodeep Majumder, Tianpei Xia, Rahul Krishna, Tim Menzies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite decades of research, SE lacks widely accepted models (that offer
precise quantitative stable predictions) about what factors most influence
software quality. This paper provides a promising result showing such stable
models can be generated using a new transfer learning framework called
"STABILIZER". Given a tree of recursively clustered projects (using project
meta-data), STABILIZER promotes a model upwards if it performs best in the
lower clusters (stopping when the promoted model performs worse than the models
seen at a lower level).
  The number of models found by STABILIZER is minimal: one for defect
prediction (756 projects) and less than a dozen for project health (1628
projects). Hence, via STABILIZER, it is possible to find a few projects which
can be used for transfer learning and make conclusions that hold across
hundreds of projects at a time. Further, the models produced in this manner
offer predictions that perform as well or better than the prior
state-of-the-art.
  To the best of our knowledge, STABILIZER is order of magnitude faster than
the prior state-of-the-art transfer learners which seek to find conclusion
stability, and these case studies are the largest demonstration of the
generalizability of quantitative predictions of project quality yet reported in
the SE literature.
  In order to support open science, all our scripts and data are online at
https://github.com/Anonymous633671/STABILIZER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CINS: Comprehensive Instruction for Few-shot Learning in <span class="highlight-title">Task-oriented</span>
  <span class="highlight-title">Dialog</span> Systems <span class="chip">AAAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04645v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04645v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Mi, Yitong Li, Yasheng Wang, Xin Jiang, <span class="highlight-author">Qun Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema (definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5) is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Federated Learning with Local Regularization and
  Sparsification <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anda Cheng, Peisong Wang, Xi Sheryl Zhang, Jian Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-level differential privacy (DP) provides certifiable privacy guarantees
to the information that is specific to any user's data in federated learning.
Existing methods that ensure user-level DP come at the cost of severe accuracy
decrease. In this paper, we study the cause of model performance degradation in
federated learning under user-level DP guarantee. We find the key to solving
this issue is to naturally restrict the norm of local updates before executing
operations that guarantee DP. To this end, we propose two techniques, Bounded
Local Update Regularization and Local Update Sparsification, to increase model
quality without sacrificing privacy. We provide theoretical analysis on the
convergence of our framework and give rigorous privacy guarantees. Extensive
experiments show that our framework significantly improves the privacy-utility
trade-off over the state-of-the-arts for federated learning with user-level DP
guarantee.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Acoustic Echo Cancellation for Keyword Spotting and
  Device-Directed Speech Detection <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10639v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10639v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuele Cornell, Thomas Balestri, Thibaud Sénéchal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many speech-enabled human-machine interaction scenarios, user speech can
overlap with the device playback audio. In these instances, the performance of
tasks such as keyword-spotting (KWS) and device-directed speech detection (DDD)
can degrade significantly. To address this problem, we propose an implicit
acoustic echo cancellation (iAEC) framework where a neural network is trained
to exploit the additional information from a reference microphone channel to
learn to ignore the interfering signal and improve detection performance. We
study this framework for the tasks of KWS and DDD on, respectively, an
augmented version of Google Speech Commands v2 and a real-world Alexa device
dataset. Notably, we show a 56% reduction in false-reject rate for the DDD task
during device playback conditions. We also show comparable or superior
performance over a strong end-to-end neural echo cancellation + KWS baseline
for the KWS task with an order of magnitude less computational requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.10382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.10382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training machine learning models in a meaningful order, from the easy samples
to the hard ones, using curriculum learning can provide performance
improvements over the standard training approach based on random data
shuffling, without any additional computational costs. Curriculum learning
strategies have been successfully employed in all areas of machine learning, in
a wide range of tasks. However, the necessity of finding a way to rank the
samples from easy to hard, as well as the right pacing function for introducing
more difficult data can limit the usage of the curriculum approaches. In this
survey, we show how these limits have been tackled in the literature, and we
present different curriculum learning instantiations for various tasks in
machine learning. We construct a multi-perspective taxonomy of curriculum
learning approaches by hand, considering various classification criteria. We
further build a hierarchical tree of curriculum learning methods using an
agglomerative clustering algorithm, linking the discovered clusters with our
taxonomy. At the end, we provide some interesting directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matérn Gaussian processes on Riemannian manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.10160v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.10160v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Peter Deisenroth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes are an effective model class for learning unknown
functions, particularly in settings where accurately representing predictive
uncertainty is of key importance. Motivated by applications in the physical
sciences, the widely-used Mat\'ern class of Gaussian processes has recently
been generalized to model functions whose domains are Riemannian manifolds, by
re-expressing said processes as solutions of stochastic partial differential
equations. In this work, we propose techniques for computing the kernels of
these processes on compact Riemannian manifolds via spectral theory of the
Laplace-Beltrami operator in a fully constructive manner, thereby allowing them
to be trained via standard scalable techniques such as inducing point methods.
We also extend the generalization from the Mat\'ern to the widely-used squared
exponential Gaussian process. By allowing Riemannian Mat\'ern Gaussian
processes to be trained using well-understood techniques, our work enables
their use in mini-batch, online, and non-conjugate settings, and makes them
more accessible to machine learning practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Seizure Detection using EEG: A Comprehensive Comparison of
  Recent Approaches under a Realistic Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwanhyung Lee, Hyewon Jeong, Seyun Kim, Donghwa Yang, Hoon-Chul Kang, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) is an important diagnostic test that physicians
use to record brain activity and detect seizures by monitoring the signals.
There have been several attempts to detect seizures and abnormalities in EEG
signals with modern deep learning models to reduce the clinical burden.
However, they cannot be fairly compared against each other as they were tested
in distinct experimental settings. Also, some of them are not trained in
real-time seizure detection tasks, making it hard for on-device applications.
Therefore in this work, for the first time, we extensively compare multiple
state-of-the-art models and signal feature extractors in a real-time seizure
detection framework suitable for real-world application, using various
evaluation metrics including a new one we propose to evaluate more practical
aspects of seizure detection models. Our code is available at
https://github.com/AITRICS/EEG_real_time_seizure_detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Real-Time Seizure Detection with EEG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven discoveries of Bäcklund transforms and soliton evolution
  equations via deep neural network learning schemes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.09489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.09489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhou, Li Wang, Weifang Weng, Zhenya Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a deep neural network learning scheme to learn the B\"acklund
transforms (BTs) of soliton evolution equations and an enhanced deep learning
scheme for data-driven soliton equation discovery based on the known BTs,
respectively. The first scheme takes advantage of some solution (or soliton
equation) information to study the data-driven BT of sine-Gordon equation, and
complex and real Miura transforms between the defocusing (focusing) mKdV
equation and KdV equation, as well as the data-driven mKdV equation discovery
via the Miura transforms. The second deep learning scheme uses the
explicit/implicit BTs generating the higher-order solitons to train the
data-driven discovery of mKdV and sine-Gordon equations, in which the
high-order solution informations are more powerful for the enhanced leaning
soliton equations with higher accurates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics Informed Neural Networks for Control Oriented Thermal Modeling
  of Buildings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gargya Gokhale, Bert Claessens, Chris Develder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a data-driven modeling approach for developing
control-oriented thermal models of buildings. These models are developed with
the objective of reducing energy consumption costs while controlling the indoor
temperature of the building within required comfort limits. To combine the
interpretability of white/gray box physics models and the expressive power of
neural networks, we propose a physics informed neural network approach for this
modeling task. Along with measured data and building parameters, we encode the
neural networks with the underlying physics that governs the thermal behavior
of these buildings. Thus, realizing a model that is guided by physics, aids in
modeling the temporal evolution of room temperature and power consumption as
well as the hidden state, i.e., the temperature of building thermal mass for
subsequent time steps. The main research contributions of this work are: (1) we
propose two variants of physics informed neural network architectures for the
task of control-oriented thermal modeling of buildings, (2) we show that
training these architectures is data-efficient, requiring less training data
compared to conventional, non-physics informed neural networks, and (3) we show
that these architectures achieve more accurate predictions than conventional
neural networks for longer prediction horizons. We test the prediction
performance of the proposed architectures using simulated and real-word data to
demonstrate (2) and (3) and show that the proposed physics informed neural
network architectures can be used for this control-oriented modeling problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Which Model to Trust: Assessing the Influence of Models on the
  Performance of Reinforcement Learning Algorithms for Continuous Control Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Arcieri, David Wölfle, Eleni Chatzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for algorithms able to solve Reinforcement Learning (RL) problems
with few trials has motivated the advent of model-based RL methods. The
reported performance of model-based algorithms has dramatically increased
within recent years. However, it is not clear how much of the recent progress
is due to improved algorithms or due to improved models. While different
modeling options are available to choose from when applying a model-based
approach, the distinguishing traits and particular strengths of different
models are not clear. The main contribution of this work lies precisely in
assessing the model influence on the performance of RL algorithms. A set of
commonly adopted models is established for the purpose of model comparison.
These include Neural Networks (NNs), ensembles of NNs, two different
approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the
Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is
evaluated on a suite of continuous control benchmarking tasks. Our results
reveal that significant differences in model performance do exist. The Concrete
Dropout NN reports persistently superior performance. We summarize these
differences for the benefit of the modeler and suggest that the model choice is
tailored to the standards required by each specific application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Material Parameters and Hydrodynamics of Soft Robotic Fish via
  Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Z. Zhang, Yu Zhang, Pingchuan Ma, Elvis Nava, Tao Du, Philip Arm, Wojciech Matusik, Robert K. Katzschmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high dimensionality of soft mechanisms and the complex physics of
fluid-structure interactions render the sim2real gap for soft robots
particularly challenging. Our framework allows high fidelity prediction of
dynamic behavior for composite bi-morph bending structures in real hardware to
millimeter-accuracy. We address this gap with our differentiable simulation
tool by learning the material parameters and hydrodynamics of our robots. We
demonstrate an experimentally-verified, fast optimization pipeline for learning
the material parameters and hydrodynamics from quasi-static and dynamic data
via differentiable simulation. Our method identifies physically plausible
Young's moduli for various soft silicone elastomers and stiff acetal copolymers
used in creation of our three different fish robot designs and is compatible
with varying internal geometry of the actuators, such as number of air
chambers. For these robots we provide a differentiable and more robust estimate
of the thrust force than analytical models and we successfully predict
deformation to millimeter accuracy in dynamic experiments under various
actuation signals. Although we focus on a specific application for underwater
soft robots, our framework is applicable to any pneumatically actuated soft
mechanism. This work presents a prototypical hardware and simulation problem
solved using our framework that can be extended straightforwardly to higher
dimensional parameter inference, learning control policies, and computational
design enabled by its differentiability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive
  Pseudo Labeling and Informative Active Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Zhang, Lei Zhu, James Hallinan, Andrew Makmur, Shengyu Zhang, Qingpeng Cai, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel semi-supervised learning (SSL) framework
named BoostMIS that combines adaptive pseudo labeling and informative active
annotation to unleash the potential of medical image SSL models: (1) BoostMIS
can adaptively leverage the cluster assumption and consistency regularization
of the unlabeled data according to the current learning status. This strategy
can adaptively generate one-hot "hard" labels converted from task model
predictions for better task model training. (2) For the unselected unlabeled
images with low confidence, we introduce an Active learning (AL) algorithm to
find the informative samples as the annotation candidates by exploiting virtual
adversarial perturbation and model's density-aware entropy. These informative
candidates are subsequently fed into the next training cycle for better SSL
label propagation. Notably, the adaptive pseudo-labeling and informative active
annotation form a learning closed-loop that are mutually collaborative to boost
medical image SSL. To verify the effectiveness of the proposed method, we
collected a metastatic epidural spinal cord compression (MESCC) dataset that
aims to optimize MESCC diagnosis and classification for improved specialist
referral and treatment. We conducted an extensive experimental study of
BoostMIS on MESCC and another public dataset COVIDx. The experimental results
verify our framework's effectiveness and generalisability for different medical
image datasets with a significant improvement over various state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phase Collapse in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florentin Guth, John Zarka, Stéphane Mallat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional classifiers linearly separate image classes and improve
accuracy as depth increases. They progressively reduce the spatial dimension
whereas the number of channels grows with depth. Spatial variability is
therefore transformed into variability along channels. A fundamental challenge
is to understand the role of non-linearities together with convolutional
filters in this transformation. ReLUs with biases are often interpreted as
thresholding operators that improve discrimination through sparsity. This paper
demonstrates that it is a different mechanism called phase collapse which
eliminates spatial variability while linearly separating classes. We show that
collapsing the phases of complex wavelet coefficients is sufficient to reach
the classification accuracy of ResNets of similar depths. However, replacing
the phase collapses with thresholding operators that enforce sparsity
considerably degrades the performance. We explain these numerical results by
showing that the iteration of phase collapses progressively improves separation
of classes, as opposed to thresholding non-linearities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Atlas Generative Models and Geodesic Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.00264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.00264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Stolberg-Larsen, Stefan Sommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative neural networks have a well recognized ability to estimate
underlying manifold structure of high dimensional data. However, if a single
latent space is used, it is not possible to faithfully represent a manifold
with topology different from Euclidean space. In this work we define the
general class of Atlas Generative Models (AGMs), models with hybrid
discrete-continuous latent space that estimate an atlas on the underlying data
manifold together with a partition of unity on the data space. We identify
existing examples of models from various popular generative paradigms that fit
into this class. Due to the atlas interpretation, ideas from non-linear latent
space analysis and statistics, e.g. geodesic interpolation, which has
previously only been investigated for models with simply connected latent
spaces, may be extended to the entire class of AGMs in a natural way. We
exemplify this by generalizing an algorithm for graph based geodesic
interpolation to the setting of AGMs, and verify its performance
experimentally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in
  Practice <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Grivas, Nikolay Bogoychev, Adam Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifiers in natural language processing (NLP) often have a large number of
output classes. For example, neural language models (LMs) and machine
translation (MT) models both predict tokens from a vocabulary of thousands. The
Softmax output layer of these models typically receives as input a dense
feature representation, which has much lower dimensionality than the output. In
theory, the result is some words may be impossible to be predicted via argmax,
irrespective of input features, and empirically, there is evidence this happens
in small language models. In this paper we ask whether it can happen in
practical large language models and translation models. To do so, we develop
algorithms to detect such \emph{unargmaxable} tokens in public models. We find
that 13 out of 150 models do indeed have such tokens; however, they are very
infrequent and unlikely to impact model quality. We release our code so that
others can inspect their models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of conference paper accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Usefulness of the Fit-on-the-<span class="highlight-title">Test</span> View on Evaluating Calibration
  of Classifiers <span class="chip">ECML-PKDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Kängsepp, Kaspar Valk, Meelis Kull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Every uncalibrated classifier has a corresponding true calibration map that
calibrates its confidence. Deviations of this idealistic map from the identity
map reveal miscalibration. Such calibration errors can be reduced with many
post-hoc calibration methods which fit some family of calibration maps on a
validation dataset. In contrast, evaluation of calibration with the expected
calibration error (ECE) on the test set does not explicitly involve fitting.
However, as we demonstrate, ECE can still be viewed as if fitting a family of
functions on the test data. This motivates the fit-on-the-test view on
evaluation: first, approximate a calibration map on the test data, and second,
quantify its distance from the identity. Exploiting this view allows us to
unlock missed opportunities: (1) use the plethora of post-hoc calibration
methods for evaluating calibration; (2) tune the number of bins in ECE with
cross-validation. Furthermore, we introduce: (3) benchmarking on pseudo-real
data where the true calibration map can be estimated very precisely; and (4)
novel calibration and evaluation methods using new calibration map families PL
and PL3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML-PKDD journal track. Update 1: removed Statements and Declaration
  section, added a line about source code to Experiments section, fixed a
  couple of typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confident Off-Policy Evaluation and Selection through Self-Normalized
  Importance Weighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.10460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.10460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilja Kuzborskij, Claire Vernade, András György, Csaba Szepesvári
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider off-policy evaluation in the contextual bandit setting for the
purpose of obtaining a robust off-policy selection strategy, where the
selection strategy is evaluated based on the value of the chosen policy in a
set of proposal (target) policies. We propose a new method to compute a lower
bound on the value of an arbitrary target policy given some logged data in
contextual bandits for a desired coverage. The lower bound is built around the
so-called Self-normalized Importance Weighting (SN) estimator. It combines the
use of a semi-empirical Efron-Stein tail inequality to control the
concentration and a new multiplicative (rather than additive) control of the
bias. The new approach is evaluated on a number of synthetic and real datasets
and is found to be superior to its main competitors, both in terms of tightness
of the confidence intervals and the quality of the policies chosen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Neural Tangent Kernel Perspective of Infinite Tree Ensembles <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuichi Kanoh, Mahito Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In practical situations, the tree ensemble is one of the most popular models
along with neural networks. A soft tree is a variant of a decision tree.
Instead of using a greedy method for searching splitting rules, the soft tree
is trained using a gradient method in which the entire splitting operation is
formulated in a differentiable form. Although ensembles of such soft trees have
been used increasingly in recent years, little theoretical work has been done
to understand their behavior. By considering an ensemble of infinite soft
trees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK),
which provides new insights into the behavior of the infinite ensemble of soft
trees. Using the TNTK, we theoretically identify several non-trivial
properties, such as global convergence of the training, the equivalence of the
oblivious tree structure, and the degeneracy of the TNTK induced by the
deepening of the trees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Consistency of Max-Margin Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.15069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.15069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Nowak-Vila, Alessandro Rudi, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The foundational concept of Max-Margin in machine learning is ill-posed for
output spaces with more than two labels such as in structured prediction. In
this paper, we show that the Max-Margin loss can only be consistent to the
classification task under highly restrictive assumptions on the discrete loss
measuring the error between outputs. These conditions are satisfied by
distances defined in tree graphs, for which we prove consistency, thus being
the first losses shown to be consistent for Max-Margin beyond the binary
setting. We finally address these limitations by correcting the concept of
Max-Margin and introducing the Restricted-Max-Margin, where the maximization of
the loss-augmented scores is maintained, but performed over a subset of the
original domain. The resulting loss is also a generalization of the binary
support vector machine and it is consistent under milder conditions on the
discrete loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Thompson Sampling strategies for support-aware CVaR bandits <span class="chip">ICML 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.05754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.05754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorian Baudry, Romain Gautron, Emilie Kaufmann, Odalric-Ambryn Maillard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study a multi-arm bandit problem in which the quality of
each arm is measured by the Conditional Value at Risk (CVaR) at some level
alpha of the reward distribution. While existing works in this setting mainly
focus on Upper Confidence Bound algorithms, we introduce a new Thompson
Sampling approach for CVaR bandits on bounded rewards that is flexible enough
to solve a variety of problems grounded on physical resources. Building on a
recent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded
rewards and M-CVTS for multinomial distributions. On the theoretical side, we
provide a non-trivial extension of their analysis that enables to theoretically
bound their CVaR regret minimization performance. Strikingly, our results show
that these strategies are the first to provably achieve asymptotic optimality
in CVaR bandits, matching the corresponding asymptotic lower bounds for this
setting. Further, we illustrate empirically the benefit of Thompson Sampling
approaches both in a realistic environment simulating a use-case in agriculture
and on various synthetic examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Thirty-eighth International Conference on Machine
  Learning (ICML 2021). In this version we refine Lemma 2 and correct its proof
  (does not change the main theorems)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Connections between Numerical Algorithms for PDEs and Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.14742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.14742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Alt, Karl Schrader, Matthias Augustin, Pascal Peter, Joachim Weickert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate numerous structural connections between numerical algorithms
for partial differential equations (PDEs) and neural architectures. Our goal is
to transfer the rich set of mathematical foundations from the world of PDEs to
neural networks. Besides structural insights we provide concrete examples and
experimental evaluations of the resulting architectures. Using the example of
generalised nonlinear diffusion in 1D, we consider explicit schemes,
acceleration strategies thereof, implicit schemes, and multigrid approaches. We
connect these concepts to residual networks, recurrent neural networks, and
U-net architectures. Our findings inspire a symmetric residual network design
with provable stability guarantees and justify the effectiveness of skip
connections in neural networks from a numerical perspective. Moreover, we
present U-net architectures that implement multigrid techniques for learning
efficient solutions of partial differential equation models, and motivate
uncommon design choices such as trainable nonmonotone activation functions.
Experimental evaluations show that the proposed architectures save half of the
trainable parameters and can thus outperform standard ones with the same model
complexity. Our considerations serve as a basis for explaining the success of
popular neural architectures and provide a blueprint for developing new
mathematically well-founded neural building blocks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amortised inference of fractional Brownian motion with linear
  computational complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Verdier, François Laurent, Christian Vestergaard, Jean-Baptiste Masson, Alhassan Cassé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simulation-based, amortised Bayesian inference scheme to infer
the parameters of random walks. Our approach learns the posterior distribution
of the walks' parameters with a likelihood-free method. In the first step a
graph neural network is trained on simulated data to learn optimized
low-dimensional summary statistics of the random walk. In the second step an
invertible neural network generates the posterior distribution of the
parameters from the learnt summary statistics using variational inference. We
apply our method to infer the parameters of the fractional Brownian motion
model from single trajectories. The computational complexity of the amortized
inference procedure scales linearly with trajectory length, and its precision
scales similarly to the Cram{\'e}r-Rao bound over a wide range of lengths. The
approach is robust to positional noise, and generalizes well to trajectories
longer than those seen during training. Finally, we adapt this scheme to show
that a finite decorrelation time in the environment can furthermore be inferred
from individual trajectories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret analysis of the Piyavskii-Shu<span class="highlight-title">bert</span> algorithm for global Lipschitz
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.02390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.02390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Bouttier, Tommaso Cesari, Mélanie Ducoffe, Sébastien Gerchinovitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of maximizing a non-concave Lipschitz multivariate
function over a compact domain by sequentially querying its (possibly
perturbed) values. We study a natural algorithm designed originally by
Piyavskii and Shubert in 1972, for which we prove new bounds on the number of
evaluations of the function needed to reach or certify a given optimization
accuracy. Our analysis uses a bandit-optimization viewpoint and solves an open
problem from Hansen et al.\ (1991) by bounding the number of evaluations to
certify a given accuracy with a near-optimal sum of packing numbers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Unsupervised Anomaly Detection with Score-Guided Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyuan Huang, Baohua Zhang, Guoqiang Hu, Longyuan Li, Yanyan Xu, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection plays a crucial role in various real-world applications,
including healthcare and finance systems. Owing to the limited number of
anomaly labels in these complex systems, unsupervised anomaly detection methods
have attracted great attention in recent years. Two major challenges faced by
the existing unsupervised methods are: (i) distinguishing between normal and
abnormal data in the transition field, where normal and abnormal data are
highly mixed together; (ii) defining an effective metric to maximize the gap
between normal and abnormal data in a hypothesis space, which is built by a
representation learner. To that end, this work proposes a novel scoring network
with a score-guided regularization to learn and enlarge the anomaly score
disparities between normal and abnormal data. With such score-guided strategy,
the representation learner can gradually learn more informative representation
during the model training stage, especially for the samples in the transition
field. We next propose a score-guided autoencoder (SG-AE), incorporating the
scoring network into an autoencoder framework for anomaly detection, as well as
other three state-of-the-art models, to further demonstrate the effectiveness
and transferability of the design. Extensive experiments on both synthetic and
real-world datasets demonstrate the state-of-the-art performance of these
score-guided models (SGMs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseUNets with feedback non-local attention for the segmentation of
  specular microscopy images of the corneal endothelium with guttae 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan P. Vigueras-Guillén, Jeroen van Rooij, Bart T. H. van Dooren, Hans G. Lemij, Esma Islamaj, Lucas J. van Vliet, Koenraad A. Vermeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To estimate the corneal endothelial parameters from specular microscopy
images depicting cornea guttata (Fuchs dystrophy), we propose a new deep
learning methodology that includes a novel attention mechanism named feedback
non-local attention (fNLA). Our approach first infers the cell edges, then
selects the cells that are well detected, and finally applies a postprocessing
method to correct mistakes and provide the binary segmentation from which the
corneal parameters are estimated (cell density [ECD], coefficient of variation
[CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired
with a Topcon SP-1P microscope, 500 of which contained guttae. Manual
segmentation was performed in all images. We compared the results of different
networks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with
fNLA provided the best performance, with a mean absolute error of 23.16
[cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6
times smaller than the error obtained by Topcon's built-in software. Our
approach handled the cells affected by guttae remarkably well, detecting cell
edges occluded by small guttae while discarding areas covered by large guttae.
Overall, the proposed method obtained accurate estimations in extremely
challenging specular images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 2 tables. Code:
  https://github.com/jpviguerasguillen/feedback-non-local-attention-fNLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-adaptive Asymmetric Deep <span class="highlight-title">Cross-modal</span> Hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.00197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.00197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengling Li, Tong Wang, Lei Zhu, Zheng Zhang, Xinhua Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised cross-modal hashing aims to embed the semantic correlations of
heterogeneous modality data into the binary hash codes with discriminative
semantic labels. Because of its advantages on retrieval and storage efficiency,
it is widely used for solving efficient cross-modal retrieval. However,
existing researches equally handle the different tasks of cross-modal
retrieval, and simply learn the same couple of hash functions in a symmetric
way for them. Under such circumstance, the uniqueness of different cross-modal
retrieval tasks are ignored and sub-optimal performance may be brought.
Motivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal
Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash
functions for two sub-retrieval tasks via simultaneous modality representation
and asymmetric hash learning. Unlike previous cross-modal hashing approaches,
our learning framework jointly optimizes semantic preserving that transforms
deep features of multimedia data into binary hash codes, and the semantic
regression which directly regresses query modality representation to explicit
label. With our model, the binary codes can effectively preserve semantic
correlations across different modalities, meanwhile, adaptively capture the
query semantics. The superiority of TA-ADCMH is proved on two standard datasets
from many aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseCLIP: Language-Guided Dense Prediction with Context-Aware <span class="highlight-title">Prompt</span>ing <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress has shown that large-scale pre-training using contrastive
image-text pairs can be a promising alternative for high-quality visual
representation learning from natural language supervision. Benefiting from a
broader source of supervision, this new paradigm exhibits impressive
transferability to downstream classification tasks and datasets. However, the
problem of transferring the knowledge learned from image-text pairs to more
complex dense prediction tasks has barely been visited. In this work, we
present a new framework for dense prediction by implicitly and explicitly
leveraging the pre-trained knowledge from CLIP. Specifically, we convert the
original image-text matching problem in CLIP to a pixel-text matching problem
and use the pixel-text score maps to guide the learning of dense prediction
models. By further using the contextual information from the image to prompt
the language model, we are able to facilitate our model to better exploit the
pre-trained knowledge. Our method is model-agnostic, which can be applied to
arbitrary dense prediction systems and various pre-trained visual backbones
including both CLIP models and ImageNet pre-trained models. Extensive
experiments demonstrate the superior performance of our methods on semantic
segmentation, object detection, and instance segmentation tasks. Code is
available at https://github.com/raoyongming/DenseCLIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022. Project page:
  https://denseclip.ivg-research.xyz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Autoencoder Training Performance for Hyperspectral Unmixing
  with Network Reinitialisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.13748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.13748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Książek, Przemysław Głomb, Michał Romaszewski, Michał Cholewa, Bartosz Grabowski, Krisztián Búza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks, in particular autoencoders, are one of the most promising
solutions for unmixing hyperspectral data, i.e. reconstructing the spectra of
observed substances (endmembers) and their relative mixing fractions
(abundances), which is needed for effective hyperspectral analysis and
classification. However, as we show in this paper, the training of autoencoders
for unmixing is highly dependent on weights initialisation; some sets of
weights lead to degenerate or low-performance solutions, introducing negative
bias in the expected performance. In this work, we experimentally investigate
autoencoders stability as well as network reinitialisation methods based on
coefficients of neurons' dead activations. We demonstrate that the proposed
techniques have a positive effect on autoencoder training in terms of
reconstruction, abundances and endmembers errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Continual Learning on Class Incremental Blurry Task Configuration
  with Anytime Inference <span class="chip">ICLR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseo Koh, Dahyun Kim, Jung-Woo Ha, Jonghyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite rapid advances in continual learning, a large body of research is
devoted to improving performance in the existing setups. While a handful of
work do propose new continual learning setups, they still lack practicality in
certain aspects. For better practicality, we first propose a novel continual
learning setup that is online, task-free, class-incremental, of blurry task
boundaries and subject to inference queries at any moment. We additionally
propose a new metric to better measure the performance of the continual
learning methods subject to inference queries at any moment. To address the
challenging setup and evaluation protocol, we propose an effective method that
employs a new memory management scheme and novel learning techniques. Our
empirical validation demonstrates that the proposed method outperforms prior
arts by large margins. Code and data splits are available at
https://github.com/naver-ai/i-Blurry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ICLR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust and Resource-Efficient Data-Free Knowledge <span class="highlight-title">Distillation</span> by
  Generative Pseudo Replay <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuluhan Binici, Shivam Aggarwal, Nam Trung Pham, Karianto Leman, Tulika Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-Free Knowledge Distillation (KD) allows knowledge transfer from a
trained neural network (teacher) to a more compact one (student) in the absence
of original training data. Existing works use a validation set to monitor the
accuracy of the student over real data and report the highest performance
throughout the entire process. However, validation data may not be available at
distillation time either, making it infeasible to record the student snapshot
that achieved the peak accuracy. Therefore, a practical data-free KD method
should be robust and ideally provide monotonically increasing student accuracy
during distillation. This is challenging because the student experiences
knowledge degradation due to the distribution shift of the synthetic data. A
straightforward approach to overcome this issue is to store and rehearse the
generated samples periodically, which increases the memory footprint and
creates privacy concerns. We propose to model the distribution of the
previously observed synthetic samples with a generative network. In particular,
we design a Variational Autoencoder (VAE) with a training objective that is
customized to learn the synthetic data representations optimally. The student
is rehearsed by the generative pseudo replay technique, with samples produced
by the VAE. Hence knowledge degradation can be prevented without storing any
samples. Experiments on image classification benchmarks show that our method
optimizes the expected value of the distilled model accuracy while eliminating
the large memory overhead incurred by the sample-storing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Thirty-Sixth AAAI Conference on Artificial
  Intelligence (AAAI-22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Linguistic Information For Logical Inference In <span class="highlight-title">Pre-train</span>ed
  Language Models <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeming Chen, Qiyue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in pre-trained language models has led to a surge of impressive
results on downstream tasks for natural language understanding. Recent work on
probing pre-trained language models uncovered a wide range of linguistic
properties encoded in their contextualized representations. However, it is
unclear whether they encode semantic knowledge that is crucial to symbolic
inference methods. We propose a methodology for probing linguistic information
for logical inference in pre-trained language model representations. Our
probing datasets cover a list of linguistic phenomena required by major
symbolic inference systems. We find that (i) pre-trained language models do
encode several types of linguistic information for inference, but there are
also some types of information that are weakly encoded, (ii) language models
can effectively learn missing linguistic information through fine-tuning.
Overall, our findings provide insights into which aspects of linguistic
information for logical inference do language models and their pre-training
procedures capture. Moreover, we have demonstrated language models' potential
as semantic and background knowledge bases for supporting symbolic inference
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACS: A <span class="highlight-title">Dataset</span> for Physical Audiovisual CommonSense Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Yu, Peter Wu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, they should be able to reason about the
physical world by understanding the physical properties and affordances of
available objects, how they can be manipulated, and how they interact with
other physical objects. This research field of physical commonsense reasoning
is fundamentally a multi-sensory task since physical properties are manifested
through multiple modalities, two of them being vision and acoustics. Our paper
takes a step towards real-world physical commonsense reasoning by contributing
PACS: the first audiovisual benchmark annotated for physical commonsense
attributes. PACS contains a total of 13,400 question-answer pairs, involving
1,377 unique physical commonsense questions and 1,526 videos. Our dataset
provides new opportunities to advance the research field of physical reasoning
by bringing audio as a core component of this multimodal problem. Using PACS,
we evaluate multiple state-of-the-art models on this new challenging task.
While some models show promising results (70% accuracy), they all fall short of
human performance (95% accuracy). We conclude the paper by demonstrating the
importance of multimodal reasoning and providing possible avenues for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static
  Models by Fitting Feature-level Space-time Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing Wang, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at CVPR 2022 (Source Code:
  https://github.com/jx-zhong-for-academic-purpose/Kinet )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theme <span class="highlight-title">Transformer</span>: Symbolic Music Generation with Theme-Conditioned
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.04093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.04093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Jen Shih, Shih-Lun Wu, Frank Zalkow, Meinard Müller, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based Transformer models have been increasingly employed for
automatic music generation. To condition the generation process of such a model
with a user-specified sequence, a popular approach is to take that conditioning
sequence as a priming sequence and ask a Transformer decoder to generate a
continuation. However, this prompt-based conditioning cannot guarantee that the
conditioning sequence would develop or even simply repeat itself in the
generated continuation. In this paper, we propose an alternative conditioning
approach, called theme-based conditioning, that explicitly trains the
Transformer to treat the conditioning sequence as a thematic material that has
to manifest itself multiple times in its generation result. This is achieved
with two main technical contributions. First, we propose a deep learning-based
approach that uses contrastive representation learning and clustering to
automatically retrieve thematic materials from music pieces in the training
data. Second, we propose a novel gated parallel attention module to be used in
a sequence-to-sequence (seq2seq) encoder/decoder architecture to more
effectively account for a given conditioning thematic material in the
generation process of the Transformer decoder. We report on objective and
subjective evaluations of variants of the proposed Theme Transformer and the
conventional prompt-based baseline, showing that our best model can generate,
to some extent, polyphonic pop piano music with repetition and plausible
variations of a given condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published at IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-20T00:00:00Z">2022-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Gender Bias in Machine Translation through Adversarial
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eve Fleisig, Christiane Fellbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation and other NLP systems often contain significant biases
regarding sensitive attributes, such as gender or race, that worsen system
performance and perpetuate harmful stereotypes. Recent preliminary research
suggests that adversarial learning can be used as part of a model-agnostic bias
mitigation method that requires no data modifications. However, adapting this
strategy for machine translation and other modern NLP domains requires (1)
restructuring training objectives in the context of fine-tuning pretrained
large language models and (2) developing measures for gender or other protected
variables for tasks in which these attributes must be deduced from the data
itself.
  We present an adversarial learning framework that addresses these challenges
to mitigate gender bias in seq2seq machine translation. Our framework improves
the disparity in translation quality for sentences with male vs. female
entities by 86% for English-German translation and 91% for English-French
translation, with minimal effect on translation quality. The results suggest
that adversarial learning is a promising technique for mitigating gender bias
in machine translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Stance to Concern: Adaptation of Propositional Analysis to New
  Tasks and Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brodie Mather, Bonnie J Dorr, Adam Dalton, William de Beaumont, Owen Rambow, Sonja M. Schmer-Galunder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a generalized paradigm for adaptation of propositional analysis
(predicate-argument pairs) to new tasks and domains. We leverage an analogy
between stances (belief-driven sentiment) and concerns (topical issues with
moral dimensions/endorsements) to produce an explanatory representation. A key
contribution is the combination of semi-automatic resource building for
extraction of domain-dependent concern types (with 2-4 hours of human labor per
domain) and an entirely automatic procedure for extraction of
domain-independent moral dimensions and endorsement values. Prudent (automatic)
selection of terms from propositional structures for lexical expansion (via
semantic similarity) produces new moral dimension lexicons at three levels of
granularity beyond a strong baseline lexicon. We develop a ground truth (GT)
based on expert annotators and compare our concern detection output to GT, to
yield 231% improvement in recall over baseline, with only a 10% loss in
precision. F1 yields 66% improvement over baseline and 97.8% of human
performance. Our lexically based approach yields large savings over approaches
that employ costly human labor and model building. We provide to the community
a newly expanded moral dimension/value lexicon, annotation guidelines, and GT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of the Association for Computational
  Linguistics, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Sequence Generation with Adaptive Compositional Modules <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhe Zhang, Xuezhi Wang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is essential for real-world deployment when there is a
need to quickly adapt the model to new tasks without forgetting knowledge of
old tasks. Existing work on continual sequence generation either always reuses
existing parameters to learn new tasks, which is vulnerable to catastrophic
forgetting on dissimilar tasks, or blindly adds new parameters for every new
task, which could prevent knowledge sharing between similar tasks. To get the
best of both worlds, in this work, we propose continual sequence generation
with adaptive compositional modules to adaptively add modules in transformer
architectures and compose both old and new modules for new tasks. We also
incorporate pseudo experience replay to facilitate knowledge transfer in those
shared modules. Experiment results on various sequences of generation tasks
show that our framework can adaptively add modules or reuse modules based on
task similarity, outperforming state-of-the-art baselines in terms of both
performance and parameter efficiency. We make our code public at
https://github.com/GT-SALT/Adaptive-Compositional-Modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Unsupervised User Embedding via Medical Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolei Huang, Franck Dernoncourt, Mark Dredze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical notes in Electronic Health Records (EHR) present rich documented
information of patients to inference phenotype for disease diagnosis and study
patient characteristics for cohort selection. Unsupervised user embedding aims
to encode patients into fixed-length vectors without human supervisions.
Medical concepts extracted from the clinical notes contain rich connections
between patients and their clinical categories. However, existing unsupervised
approaches of user embeddings from clinical notes do not explicitly incorporate
medical concepts. In this study, we propose a concept-aware unsupervised user
embedding that jointly leverages text documents and medical concepts from two
clinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both
extrinsic and intrinsic tasks, including phenotype classification, in-hospital
mortality prediction, patient retrieval, and patient relatedness. Experiments
on the two clinical corpora show our approach exceeds unsupervised baselines,
and incorporating medical concepts can significantly improve the baseline
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ACM CHIL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibration of Machine Reading Systems at Scale <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shehzaad Dhuliawala, Leonard Adolphs, Rajarshi Das, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In typical machine learning systems, an estimate of the probability of the
prediction is used to assess the system's confidence in the prediction. This
confidence measure is usually uncalibrated; i.e.\ the system's confidence in
the prediction does not match the true probability of the predicted output. In
this paper, we present an investigation into calibrating open setting machine
reading systems such as open-domain question answering and claim verification
systems. We show that calibrating such complex systems which contain discrete
retrieval and deep reading components is challenging and current calibration
techniques fail to scale to these settings. We propose simple extensions to
existing calibration approaches that allows us to adapt them to these settings.
Our experimental results reveal that the approach works well, and can be useful
to selectively predict answers when question answering systems are posed with
unanswerable or out-of-the-training distribution questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immersive Text Game and Personality Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanshui Li, Yifan Bai, Jiaxuan Lu, Kexin Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We designed and built a game called \textit{Immersive Text Game}, which
allows the player to choose a story and a character, and interact with other
characters in the story in an immersive manner of dialogues. The game is based
on several latest models, including text generation language model, information
extraction model, commonsense reasoning model, and psychology evaluation model.
In the past, similar text games usually let players choose from limited actions
instead of answering on their own, and not every time what characters said are
determined by the player. Through the combination of these models and elaborate
game mechanics and modes, the player will find some novel experiences as driven
through the storyline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Reasoning over Long Stories -- Assessing Systematic
  Generalisation in Neural Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanshui Li, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary neural networks have achieved a series of developments and
successes in many aspects; however, when exposed to data outside the training
distribution, they may fail to predict correct answers. In this work, we were
concerned about this generalisation issue and thus analysed a broad set of
models systematically and robustly over long stories. Related experiments were
conducted based on the CLUTRR, which is a diagnostic benchmark suite that can
analyse generalisation of natural language understanding (NLU) systems by
training over small story graphs and testing on larger ones. In order to handle
the multi-relational story graph, we consider two classes of neural models:
"E-GNN", the graph-based models that can process graph-structured data and
consider the edge attributes simultaneously; and "L-Graph", the sequence-based
models which can process linearized version of the graphs. We performed an
extensive empirical evaluation, and we found that the modified recurrent neural
network yield surprisingly accurate results across every systematic
generalisation tasks which outperform the modified graph neural network, while
the latter produced more robust models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Large-Scale Interpretable Knowledge Graph Reasoning for <span class="highlight-title">Dialogue</span>
  Systems <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Tuan, Sajjad Beygi, Maryam Fazel-Zarandi, Qiaozi Gao, Alessandra Cervone, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users interacting with voice assistants today need to phrase their requests
in a very specific manner to elicit an appropriate response. This limits the
user experience, and is partly due to the lack of reasoning capabilities of
dialogue platforms and the hand-crafted rules that require extensive labor. One
possible way to improve user experience and relieve the manual efforts of
designers is to build an end-to-end dialogue system that can do reasoning
itself while perceiving user's utterances. In this work, we propose a novel
method to incorporate the knowledge reasoning capability into dialogue systems
in a more scalable and generalizable manner. Our proposed method allows a
single transformer model to directly walk on a large-scale knowledge graph to
generate responses. To the best of our knowledge, this is the first work to
have transformer models generate responses by reasoning over differentiable
knowledge graphs. We investigate the reasoning abilities of the proposed method
on both task-oriented and domain-specific chit-chat dialogues. Empirical
results show that this method can effectively and efficiently incorporate a
knowledge graph into a dialogue system with fully-interpretable reasoning
paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to the Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster & Tune: Boost Cold Start Performance in Text Classification <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eyal Shnarch, Ariel Gera, Alon Halfon, Lena Dankin, Leshem Choshen, Ranit Aharonov, Noam Slonim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, a text classification task often begins with a cold
start, when labeled data is scarce. In such cases, the common practice of
fine-tuning pre-trained models, such as BERT, for a target classification task,
is prone to produce poor performance. We suggest a method to boost the
performance of such models by adding an intermediate unsupervised
classification task, between the pre-training and fine-tuning phases. As such
an intermediate task, we perform clustering and train the pre-trained model on
predicting the cluster labels. We test this hypothesis on various data sets,
and show that this additional classification phase can significantly improve
performance, mainly for topical classification tasks, when the number of
labeled instances available for fine-tuning is only a couple of dozen to a few
hundred.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures; To be published in ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Batch Sizes Improve Training of Low-Resource Neural MT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Àlex R. Atrio, Andrei Popescu-Belis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the role of an essential hyper-parameter that governs the training
of Transformers for neural machine translation in a low-resource setting: the
batch size. Using theoretical insights and experimental evidence, we argue
against the widespread belief that batch size should be set as large as allowed
by the memory of the GPUs. We show that in a low-resource setting, a smaller
batch size leads to higher scores in a shorter training time, and argue that
this is due to better regularization of the gradients during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in 18th International Conference on Natural Language
  Processing (ICON 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who will share Fake-News on Twitter? Psycholinguistic cues in online
  post histories discriminate Between actors in the misinformation ecosystem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Verena Schoenmueller, Simon J. Blanchard, Gita V. Johar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of misinformation or fake-news is a global concern that undermines
progress on issues such as protecting democracy and public health. Past
research aiming to combat its spread has largely focused on identifying its
semantic content and media outlets publishing such news. In contrast, we aim to
identify individuals who are more likely to share fake-news by studying the
language of actors in the fake-news ecosystem (such as fake-news sharers,
fact-check sharers and random twitter users), and creating a linguistic profile
of them. Fake-news sharers and fact-check sharers use significantly more
high-arousal negative emotions in their language, but fake-news sharers express
more existentially-based needs than other actors. Incorporating
psycholinguistic cues as inferred from their tweets into a model of
socio-demographic predictors considerably improves classification accuracy of
fake-news sharers. The finding that fake-news sharers differ in important ways
from other actors in the fake-news ecosystem (such as in their existential
needs), but are also similar to them in other ways (such as in their anger
levels), highlights the importance of studying the entire fake-news ecosystem
to increase accuracy in identification and prediction. Our approach can help
mitigate fake-news sharing by enabling platforms to pre-emptively screen
potential fake-news sharers' posts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 6 figures and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Neural-Symbolic Approach to Natural Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liu, Zihao Wang, Yuan Lin, Hang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks, empowered by pre-trained language models, have achieved
remarkable results in natural language understanding (NLU) tasks. However,
their performances can deteriorate drastically when logical reasoning is needed
in the process. This is because, ideally, NLU needs to depend on not only
analogical reasoning, which deep neural networks are good at, but also logical
reasoning. According to the dual-process theory, analogical reasoning and
logical reasoning are respectively carried out by System 1 and System 2 in the
human brain. Inspired by the theory, we present a novel framework for NLU
called Neural-Symbolic Processor (NSP), which performs analogical reasoning
based on neural processing and performs logical reasoning based on both neural
and symbolic processing. As a case study, we conduct experiments on two NLU
tasks, question answering (QA) and natural language inference (NLI), when
numerical reasoning (a type of logical reasoning) is necessary. The
experimental results show that our method significantly outperforms
state-of-the-art methods in both tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Instance Query Network for Named Entity Recognition <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Shen, Xiaobin Wang, Zeqi Tan, Guangwei Xu, Pengjun Xie, Fei Huang, Weiming Lu, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is a fundamental task in natural language
processing. Recent works treat named entity recognition as a reading
comprehension task, constructing type-specific queries manually to extract
entities. This paradigm suffers from three issues. First, type-specific queries
can only extract one type of entities per inference, which is inefficient.
Second, the extraction for different types of entities is isolated, ignoring
the dependencies between them. Third, query construction relies on external
knowledge and is difficult to apply to realistic scenarios with hundreds of
entity types. To deal with them, we propose Parallel Instance Query Network
(PIQN), which sets up global and learnable instance queries to extract entities
from a sentence in a parallel manner. Each instance query predicts one entity,
and by feeding all instance queries simultaneously, we can query all entities
in parallel. Instead of being constructed from external knowledge, instance
queries can learn their different query semantics during training. For training
the model, we treat label assignment as a one-to-many Linear Assignment Problem
(LAP) and dynamically assign gold entities to instance queries with minimal
assignment cost. Experiments on both nested and flat NER datasets demonstrate
that our proposed method outperforms previous state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022, camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Hierarchical Inductive Transfer for Continual <span class="highlight-title">Dialogue</span> Learning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Feng, Xuancheng Ren, Kan Li, <span class="highlight-author">Xu Sun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained models have achieved excellent performance on the dialogue task.
However, for the continual increase of online chit-chat scenarios, directly
fine-tuning these models for each of the new tasks not only explodes the
capacity of the dialogue system on the embedded devices but also causes
knowledge forgetting on pre-trained models and knowledge interference among
diverse dialogue tasks. In this work, we propose a hierarchical inductive
transfer framework to learn and deploy the dialogue skills continually and
efficiently. First, we introduce the adapter module into pre-trained models for
learning new dialogue tasks. As the only trainable module, it is beneficial for
the dialogue system on the embedded devices to acquire new dialogue skills with
negligible additional parameters. Then, for alleviating knowledge interference
between tasks yet benefiting the regularization between them, we further design
hierarchical inductive transfer that enables new tasks to use general knowledge
in the base adapter without being misled by diverse knowledge in task-specific
adapters. Empirical evaluation and analysis indicate that our framework obtains
comparable performance under deployment-friendly model capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entailment Relation Aware Paraphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilasha Sancheti, Balaji Vasan Srinivasan, Rachel Rudinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new task of entailment relation aware paraphrase generation
which aims at generating a paraphrase conforming to a given entailment relation
(e.g. equivalent, forward entailing, or reverse entailing) with respect to a
given input. We propose a reinforcement learning-based weakly-supervised
paraphrasing system, ERAP, that can be trained using existing paraphrase and
natural language inference (NLI) corpora without an explicit task-specific
corpus. A combination of automated and human evaluations show that ERAP
generates paraphrases conforming to the specified entailment relation and are
of good quality as compared to the baselines and uncontrolled paraphrasing
systems. Using ERAP for augmenting training data for downstream textual
entailment task improves performance over an uncontrolled paraphrasing system,
and introduces fewer training artifacts, indicating the benefit of explicit
control during paraphrasing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEIM: An effective deep encoding and interaction model for sentence
  matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Jiang, Yahui Zhao, Rongyi Cui, Zhenguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language sentence matching is the task of comparing two sentences and
identifying the relationship between them.It has a wide range of applications
in natural language processing tasks such as reading comprehension, question
and answer systems. The main approach is to compute the interaction between
text representations and sentence pairs through an attention mechanism, which
can extract the semantic information between sentence pairs well. However,this
kind of method can not gain satisfactory results when dealing with complex
semantic features. To solve this problem, we propose a sentence matching method
based on deep encoding and interaction to extract deep semantic information. In
the encoder layer,we refer to the information of another sentence in the
process of encoding a single sentence, and later use a heuristic algorithm to
fuse the information. In the interaction layer, we use a bidirectional
attention mechanism and a self-attention mechanism to obtain deep semantic
information.Finally, we perform a pooling operation and input it to the MLP for
classification. we evaluate our model on three tasks: recognizing textual
entailment, paraphrase recognition, and answer selection. We conducted
experiments on the SNLI and SciTail datasets for the recognizing textual
entailment task, the Quora dataset for the paraphrase recognition task, and the
WikiQA dataset for the answer selection task. The experimental results show
that the proposed algorithm can effectively extract deep semantic features that
verify the effectiveness of the algorithm on sentence matching tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Structuring Real-World Data at Scale: Deep Learning for
  Extracting Key Oncology Information from Clinical Text with Patient-Level
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Preston, Mu Wei, Rajesh Rao, Robert Tinn, Naoto Usuyama, Michael Lucas, Roshanthi Weerasinghe, Soohee Lee, Brian Piening, Paul Tittel, Naveen Valluri, Tristan Naumann, Carlo Bifulco, Hoifung Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: The majority of detailed patient information in real-world data
(RWD) is only consistently available in free-text clinical documents. Manual
curation is expensive and time-consuming. Developing natural language
processing (NLP) methods for structuring RWD is thus essential for scaling
real-world evidence generation.
  Materials and Methods: Traditional rule-based systems are vulnerable to the
prevalent linguistic variations and ambiguities in clinical text, and prior
applications of machine-learning methods typically require sentence-level or
report-level labeled examples that are hard to produce at scale. We propose
leveraging patient-level supervision from medical registries, which are often
readily available and capture key patient information, for general RWD
applications. To combat the lack of sentence-level or report-level annotations,
we explore advanced deep-learning methods by combining domain-specific
pretraining, recurrent neural networks, and hierarchical attention.
  Results: We conduct an extensive study on 135,107 patients from the cancer
registry of a large integrated delivery network (IDN) comprising healthcare
systems in five western US states. Our deep learning methods attain test AUROC
of 94-99% for key tumor attributes and comparable performance on held-out data
from separate health systems and states.
  Discussion and Conclusion: Ablation results demonstrate clear superiority of
these advanced deep-learning methods over prior approaches. Error analysis
shows that our NLP system sometimes even corrects errors in registrar labels.
We also conduct a preliminary investigation in accelerating registry curation
and general RWD structuring via assisted curation for over 1.2 million cancer
patients in this healthcare network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretability of Fine-grained Classification of Sadness and
  Depression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiasa Singha Roy, Priyam Basu, Aman Priyanshu, Rakshit Naidu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While sadness is a human emotion that people experience at certain times
throughout their lives, inflicting them with emotional disappointment and pain,
depression is a longer term mental illness which impairs social, occupational,
and other vital regions of functioning making it a much more serious issue and
needs to be catered to at the earliest. NLP techniques can be utilized for the
detection and subsequent diagnosis of these emotions. Most of the open sourced
data on the web deal with sadness as a part of depression, as an emotion even
though the difference in severity of both is huge. Thus, we create our own
novel dataset illustrating the difference between the two. In this paper, we
aim to highlight the difference between the two and highlight how interpretable
our models are to distinctly label sadness and depression. Due to the sensitive
nature of such information, privacy measures need to be taken for handling and
training of such data. Hence, we also explore the effect of Federated Learning
(FL) on contextualised language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ g2pW: A Conditional Weighted Softmax <span class="highlight-title">BERT</span> for Polyphone Disambiguation
  in Mandarin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chang Chen, Yu-Chuan Chang, Yen-Cheng Chang, Yi-Ren Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have benefited from this
problem because of pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by the strategies, we
proposed a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments showed that learning a soft-weighting function for the candidate
phonemes benefits performance. Besides, our g2pW does not require extra
pre-trained POS tagging models while using POS tags as auxiliary features since
we train the POS tagging model simultaneously with the unified encoder. The
experiments show that our g2pW outperforms existing methods on the public
dataset. All codes, model weights, and a user-friendly package are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STEMM: Self-learning with Speech-text Manifold Mixup for Speech
  Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingkai Fang, Rong Ye, Lei Li, Yang Feng, Mingxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to learn a better speech representation for end-to-end speech-to-text
translation (ST) with limited labeled data? Existing techniques often attempt
to transfer powerful machine translation (MT) capabilities to ST, but neglect
the representation discrepancy across modalities. In this paper, we propose the
Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.
Specifically, we mix up the representation sequences of different modalities,
and take both unimodal speech sequences and multimodal mixed sequences as input
to the translation model in parallel, and regularize their output predictions
with a self-learning framework. Experiments on MuST-C speech translation
benchmark and further analysis show that our method effectively alleviates the
cross-modal representation discrepancy, and achieves significant improvements
over a strong baseline on eight translation directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaOnce: A Metaverse Framework Based on Multi-scene Relations and
  Entity-relation-event Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing metaverse systems lack rich relation types between entities and
events. The challenge is that there is no portable framework to introduce rich
concepts, relations, events into the metaverse. This paper introduces a new
metaverse framework, MetaOnce. This framework proposes to build multi-scene
graphs. This framework not only describes rich relations in a single scene but
also combines multiple scene graphs into a complete graph for more
comprehensive analysis and inference. Prior social network systems mainly
describe friend relations. They ignore the effect of entity-relation-event
games on the metaverse system and existing rule constraints. We propose a rule
controller and impose constraints on the relations that allow the framework to
behave in a compliant manner. We build a metaverse system to test the features
of the framework, and experimental results show that our framework can build a
multi-scene metaverse with memory and rule constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How does the <span class="highlight-title">pre-train</span>ing objective affect what large language models
  learn about linguistic properties? <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Alajrami, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several pre-training objectives, such as masked language modeling (MLM), have
been proposed to pre-train language models (e.g. BERT) with the aim of learning
better language representations. However, to the best of our knowledge, no
previous work so far has investigated how different pre-training objectives
affect what BERT learns about linguistics properties. We hypothesize that
linguistically motivated objectives such as MLM should help BERT to acquire
better linguistic knowledge compared to other non-linguistically motivated
objectives that are not intuitive or hard for humans to guess the association
between the input and the label to be predicted. To this end, we pre-train BERT
with two linguistically motivated objectives and three non-linguistically
motivated ones. We then probe for linguistic characteristics encoded in the
representation of the resulting models. We find strong evidence that there are
only small differences in probing performance between the representations
learned by the two different types of objectives. These surprising results
question the dominant narrative of linguistically informed pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.08256v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.08256v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance and pervasiveness of emotions in our lives makes affective
computing a tremendously important and vibrant line of work. Systems for
automatic emotion recognition (AER) and sentiment analysis can be facilitators
of enormous progress (e.g., in improving public health and commerce) but also
enablers of great harm (e.g., for suppressing dissidents and manipulating
voters). Thus, it is imperative that the affective computing community actively
engage with the ethical ramifications of their creations. In this paper, I have
synthesized and organized information from AI Ethics and Emotion Recognition
literature to present fifty ethical considerations relevant to AER. Notably,
the sheet fleshes out assumptions hidden in how AER is commonly framed, and in
the choices often made regarding the data, method, and evaluation. Special
attention is paid to the implications of AER on privacy and social groups.
Along the way, key recommendations are made for responsible AER. The objective
of the sheet is to facilitate and encourage more thoughtfulness on why to
automate, how to automate, and how to judge success well before the building of
AER systems. Additionally, the sheet acts as a useful introductory document on
emotion recognition (complementing survey articles).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear in Computational Linguistics, June 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRILLsson: Distilled Universal Paralinguistic Speech Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Subhashini Venugopalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervision have dramatically improved the quality of
speech representations. However, deployment of state-of-the-art embedding
models on devices has been restricted due to their limited public availability
and large resource footprint. Our work addresses these issues by publicly
releasing a collection of paralinguistic speech models that are small and near
state-of-the-art performance. Our approach is based on knowledge distillation,
and our models are distilled on public data only. We explore different
architectures and thoroughly evaluate our models on the Non-Semantic Speech
(NOSS) benchmark. Our largest distilled model is less than 15% the size of the
original model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7
tasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)
and achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the
open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model
outperforms the open source Wav2Vec 2.0 on both emotion recognition tasks
despite being 7% the size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A new approach to calculating <span class="highlight-title">BERT</span>Score for automatic assessment of
  translation quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. A. Vetrov, E. A. Gorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of the applicability of the BERTScore metric was conducted to
translation quality assessment at the sentence level for English -> Russian
direction. Experiments were performed with a pre-trained Multilingual BERT as
well as with a pair of Monolingual BERT models. To align monolingual
embeddings, an orthogonal transformation based on anchor tokens was used. It
was demonstrated that such transformation helps to prevent mismatching issue
and shown that this approach gives better results than using embeddings of the
Multilingual model. To improve the token matching process it is proposed to
combine all incomplete WorkPiece tokens into meaningful words and use simple
averaging of corresponding vectors and to calculate BERTScore based on anchor
tokens only. Such modifications allowed us to achieve a better correlation of
the model predictions with human judgments. In addition to evaluating machine
translation, several versions of human translation were evaluated as well, the
problems of this approach were listed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Uncertainty-based Query Strategies for Active Learning with
  <span class="highlight-title">Transformer</span>s <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.05687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.05687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Schröder, Andreas Niekler, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning is the iterative construction of a classification model
through targeted labeling, enabling significant labeling cost savings. As most
research on active learning has been carried out before transformer-based
language models ("transformers") became popular, despite its practical
importance, comparably few papers have investigated how transformers can be
combined with active learning to date. This can be attributed to the fact that
using state-of-the-art query strategies for transformers induces a prohibitive
runtime overhead, which effectively nullifies, or even outweighs the desired
cost savings. For this reason, we revisit uncertainty-based query strategies,
which had been largely outperformed before, but are particularly suited in the
context of fine-tuning transformers. In an extensive evaluation, we connect
transformers to experiments from previous research, assessing their performance
on five widely used text classification benchmarks. For active learning with
transformers, several other uncertainty-based approaches outperform the
well-known prediction entropy query strategy, thereby challenging its status as
most popular uncertainty baseline in active learning for text classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Think Before You Speak: Explicitly Generating Implicit Commonsense
  Knowledge for Response Generation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, <span class="highlight-author">Yang Liu</span>, Dilek Hakkani-Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit knowledge, such as common sense, is key to fluid human
conversations. Current neural response generation (RG) models are trained to
generate responses directly, omitting unstated implicit knowledge. In this
paper, we present Think-Before-Speaking (TBS), a generative approach to first
externalize implicit commonsense knowledge (think) and use this knowledge to
generate responses (speak). We expect that externalizing implicit knowledge
allows more efficient learning, produces more informative responses, and
enables more explainable models. We analyze different choices to collect
knowledge-aligned dialogues, represent implicit knowledge, and transition
between knowledge and dialogues. Empirical results show TBS models outperform
end-to-end and knowledge-augmented RG baselines on most automatic metrics and
generate more informative, specific, and commonsense-following responses, as
evaluated by human annotators. TBS also generates knowledge that makes sense
and is relevant to the dialogue around 85\% of the time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022 main conference. 16 pages, 9 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple-Source Domain Adaptation via Coordinated Domain Encoders and
  Paired Classifiers <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel multiple-source unsupervised model for text classification
under domain shift. Our model exploits the update rates in document
representations to dynamically integrate domain encoders. It also employs a
probabilistic heuristic to infer the error rate in the target domain in order
to pair source classifiers. Our heuristic exploits data transformation cost and
the classifier accuracy in the target feature space. We have used real world
scenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We
also used pretrained multi-layer transformers as the document encoder in the
experiments to demonstrate whether the improvement achieved by domain
adaptation models can be delivered by out-of-the-box language model
pretraining. The experiments testify that our model is the top performing
approach in this setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> P-Tuning v2: <span class="highlight-title">Prompt</span> Tuning Can Be Comparable to Fine-tuning Universally
  Across Scales and Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07602v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07602v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, <span class="highlight-author">Zhilin Yang</span>, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning, which only tunes continuous prompts with a frozen language
model, substantially reduces per-task storage and memory usage at training.
However, in the context of NLU, prior work reveals that prompt tuning does not
perform well for normal-sized pretrained models. We also find that existing
methods of prompt tuning cannot handle hard sequence labeling tasks, indicating
a lack of universality. We present a novel empirical finding that properly
optimized prompt tuning can be universally effective across a wide range of
model scales and NLU tasks. It matches the performance of finetuning while
having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an
implementation of Deep Prompt Tuning \cite{li2021prefix,qin2021learning}
optimized and adapted for NLU. Given the universality and simplicity of
P-Tuning v2, we believe it can serve as an alternative to finetuning and a
strong baseline for future research.Our code and data are released at
https://github.com/THUDM/P-tuning-v2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 60th Annual Meeting of the Association of
  Computational Linguistics, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Parsing in <span class="highlight-title">Task-Oriented</span> <span class="highlight-title">Dialog</span> with Recursive Insertion-based
  Encoder <span class="chip">AAAI-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elman Mansimov, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for
semantic parsing in task-oriented dialog. Our model consists of an encoder
network that incrementally builds the semantic parse tree by predicting the
non-terminal label and its positions in the linearized tree. At the generation
time, the model constructs the semantic parse tree by recursively inserting the
predicted non-terminal labels at the predicted positions until termination.
RINE achieves state-of-the-art exact match accuracy on low- and high-resource
versions of the conversational semantic parsing benchmark TOP (Gupta et al.,
2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and
transition-based parsers. We also show that our model design is applicable to
nested named entity recognition task, where it performs on par with
state-of-the-art approach designed for that task. Finally, we demonstrate that
our approach is 2-3.5 times faster than the sequence-to-sequence model at
inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ (Un)solving Morphological Inflection: Lemma Overlap Artificially
  Inflates Models' Performance <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.05682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.05682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Goldman, David Guriel, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of Morphology, Inflection is a fundamental and important task
that gained a lot of traction in recent years, mostly via SIGMORPHON's
shared-tasks. With average accuracy above 0.9 over the scores of all languages,
the task is considered mostly solved using relatively generic neural seq2seq
models, even with little data provided. In this work, we propose to re-evaluate
morphological inflection models by employing harder train-test splits that will
challenge the generalization capacity of the models. In particular, as opposed
to the na{\"i}ve split-by-form, we propose a split-by-lemma method to challenge
the performance on existing benchmarks. Our experiments with the three
top-ranked systems on the SIGMORPHON's 2020 shared-task show that the
lemma-split presents an average drop of 30 percentage points in macro-average
for the 90 languages included. The effect is most significant for low-resourced
languages with a drop as high as 95 points, but even high-resourced languages
lose about 10 points on average. Our results clearly show that generalizing
inflection to unseen lemmas is far from being solved, presenting a simple yet
effective means to promote more sophisticated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphological Reinflection with Multiple Arguments: An Extended
  Annotation schema and a Georgian Case Study <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Guriel, Omer Goldman, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, a flurry of morphological datasets had emerged, most notably
UniMorph, a multi-lingual repository of inflection tables. However, the flat
structure of the current morphological annotation schema makes the treatment of
some languages quirky, if not impossible, specifically in cases of polypersonal
agreement, where verbs agree with multiple arguments using true affixes. In
this paper, we propose to address this phenomenon by expanding the UniMorph
annotation schema to a hierarchical feature structure that naturally
accommodates complex argument marking. We apply this extended schema to one
such language, Georgian, and provide a human-verified, accurate and balanced
morphological dataset for Georgian verbs. The dataset has 4 times more tables
and 6 times more verb forms compared to the existing UniMorph dataset, covering
all possible variants of argument marking, demonstrating the adequacy of our
proposed scheme. Experiments with a standard reinflection model show that
generalization is easy when the data is split at the form level, but extremely
hard when splitting along lemma lines. Expanding the other languages in
UniMorph to this schema is expected to improve both the coverage, consistency
and interpretability of this benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Domain, Content-based, <span class="highlight-title">Multi-modal</span> Fact-checking of Out-of-Context
  Images via Online Resources <span class="chip">CVPR'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00061v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00061v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Abdelnabi, Rakibul Hasan, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation is now a major problem due to its potential high risks to our
core democratic and societal values and orders. Out-of-context misinformation
is one of the easiest and effective ways used by adversaries to spread viral
false stories. In this threat, a real image is re-purposed to support other
narratives by misrepresenting its context and/or elements. The internet is
being used as the go-to way to verify information using different sources and
modalities. Our goal is an inspectable method that automates this
time-consuming and reasoning-intensive process by fact-checking the
image-caption pairing using Web evidence. To integrate evidence and cues from
both modalities, we introduce the concept of 'multi-modal cycle-consistency
check'; starting from the image/caption, we gather textual/visual evidence,
which will be compared against the other paired caption/image, respectively.
Moreover, we propose a novel architecture, Consistency-Checking Network (CCN),
that mimics the layered human reasoning across the same and different
modalities: the caption vs. textual evidence, the image vs. visual evidence,
and the image vs. caption. Our work offers the first step and benchmark for
open-domain, content-based, multi-modal fact-checking, and significantly
outperforms previous baselines that did not leverage external evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Universal Conditional Masked Language <span class="highlight-title">Pre-train</span>ing for Neural Machine
  Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Li, Liangyou Li, Meng Zhang, Minghao Wu, <span class="highlight-author">Qun Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching & masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2022 Main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Automatic Speech Recognition: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.04897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.04897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanan Aldarmaki, Asad Ullah, Nazar Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) systems can be trained to achieve
remarkable performance given large amounts of manually transcribed speech, but
large labeled data sets can be difficult or expensive to acquire for all
languages of interest. In this paper, we review the research literature to
identify models and ideas that could lead to fully unsupervised ASR, including
unsupervised segmentation of the speech signal, unsupervised mapping from
speech segments to text, and semi-supervised models with nominal amounts of
labeled examples. The objective of the study is to identify the limitations of
what can be learned from speech data alone and to understand the minimum
requirements for speech recognition. Identifying these limitations would help
optimize the resources and efforts in ASR development for low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages + 10 pages of references, 3 figures. Speech Communication
  (2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Conformer Based Acoustic Model for Robust Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Yang, Peidong Wang, DeLiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses robust automatic speech recognition (ASR) by introducing
a Conformer-based acoustic model. The proposed model builds on a
state-of-the-art recognition system using a bi-directional long short-term
memory (BLSTM) model with utterance-wise dropout and iterative speaker
adaptation, but employs a Conformer encoder instead of the BLSTM network. The
Conformer encoder uses a convolution-augmented attention mechanism for acoustic
modeling. The proposed system is evaluated on the monaural ASR task of the
CHiME-4 corpus. Coupled with utterance-wise normalization and speaker
adaptation, our model achieves $6.25\%$ word error rate, which outperforms the
previous best system by $8.4\%$ relatively. In addition, the proposed
Conformer-based model is $18.3\%$ smaller in model size and reduces total
training time by $79.6\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SalKG: Learning From Knowledge Graph Explanations for Commonsense
  Reasoning <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.08793v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.08793v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Chan, Jiashu Xu, Boyuan Long, Soumya Sanyal, Tanishq Gupta, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting pre-trained language models with knowledge graphs (KGs) has
achieved success on various commonsense reasoning tasks. However, for a given
task instance, the KG, or certain parts of the KG, may not be useful. Although
KG-augmented models often use attention to focus on specific KG components, the
KG is still always used, and the attention mechanism is never explicitly taught
which KG components should be used. Meanwhile, saliency methods can measure how
much a KG feature (e.g., graph, node, path) influences the model to make the
correct prediction, thus explaining which KG features are useful. This paper
explores how saliency explanations can be used to improve KG-augmented models'
performance. First, we propose to create coarse (Is the KG useful?) and fine
(Which nodes/paths in the KG are useful?) saliency explanations. Second, to
motivate saliency-based supervision, we analyze oracle KG-augmented models
which directly use saliency explanations as extra inputs for guiding their
attention. Third, we propose SalKG, a framework for KG-augmented models to
learn from coarse and/or fine saliency explanations. Given saliency
explanations created from a task's training set, SalKG jointly trains the model
to predict the explanations, then solve the task by attending to KG features
highlighted by the predicted explanations. On three commonsense QA benchmarks
(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can
yield considerable performance gains -- up to 2.76% absolute improvement on
CSQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dict-<span class="highlight-title">BERT</span>: Enhancing Language Model <span class="highlight-title">Pre-train</span>ing with Dictionary <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) aim to learn universal language
representations by conducting self-supervised training tasks on large-scale
corpora. Since PLMs capture word semantics in different contexts, the quality
of word representations highly depends on word frequency, which usually follows
a heavy-tailed distributions in the pre-training corpus. Therefore, the
embeddings of rare words on the tail are usually poorly optimized. In this
work, we focus on enhancing language model pre-training by leveraging
definitions of the rare words in dictionaries (e.g., Wiktionary). To
incorporate a rare word definition as a part of input, we fetch its definition
from the dictionary and append it to the end of the input text sequence. In
addition to training with the masked language modeling objective, we propose
two novel self-supervised pre-training tasks on word and sentence-level
alignment between input text sequence and rare word definitions to enhance
language modeling representation with dictionary. We evaluate the proposed
Dict-BERT model on the language understanding benchmark GLUE and eight
specialized domain benchmark datasets. Extensive experiments demonstrate that
Dict-BERT can significantly improve the understanding of rare words and boost
model performance on various NLP downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Internal Language Model Adaptation with Text-Only Data for End-to-End
  Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05354v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05354v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong Meng, Yashesh Gaur, Naoyuki Kanda, Jinyu Li, Xie Chen, Yu Wu, Yifan Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-only adaptation of an end-to-end (E2E) model remains a challenging task
for automatic speech recognition (ASR). Language model (LM) fusion-based
approaches require an additional external LM during inference, significantly
increasing the computation cost. To overcome this, we propose an internal LM
adaptation (ILMA) of the E2E model using text-only data. Trained with
audio-transcript pairs, an E2E model implicitly learns an internal LM that
characterizes the token sequence probability which is approximated by the E2E
model output after zeroing out the encoder contribution. During ILMA, we
fine-tune the internal LM, i.e., the E2E components excluding the encoder, to
minimize a cross-entropy loss. To make ILMA effective, it is essential to train
the E2E model with an internal LM loss besides the standard E2E loss.
Furthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler
divergence between the output distributions of the adapted and unadapted
internal LMs. ILMA is the most effective when we update only the last linear
layer of the joint network. ILMA enables a fast text-only adaptation of the E2E
model without increasing the run-time computational cost. Experimented with
30K-hour trained transformer transducer models, ILMA achieves up to 34.9%
relative word error rate reduction from the unadapted baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, v4: adding shallow fusion and fast text-only adaptation
  results as baseline, submitted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit User Manipulation in Reinforcement Learning Based Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Sparr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are highly prevalent in the modern world due to their
value to both users and platforms and services that employ them. Generally,
they can improve the user experience and help to increase satisfaction, but
they do not come without risks. One such risk is that of their effect on users
and their ability to play an active role in shaping user preferences. This risk
is more significant for reinforcement learning based recommender systems. These
are capable of learning for instance, how recommended content shown to a user
today may tamper that user's preference for other content recommended in the
future. Reinforcement learning based recommendation systems can thus implicitly
learn to influence users if that means maximizing clicks, engagement, or
consumption. On social news and media platforms, in particular, this type of
behavior is cause for alarm. Social media undoubtedly plays a role in public
opinion and has been shown to be a contributing factor to increased political
polarization. Recommender systems on such platforms, therefore, have great
potential to influence users in undesirable ways. However, it may also be
possible for this form of manipulation to be used intentionally. With
advancements in political opinion dynamics modeling and larger collections of
user data, explicit user manipulation in which the beliefs and opinions of
users are tailored towards a certain end emerges as a significant concern in
reinforcement learning based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Multi-behavior <span class="highlight-title">Contrastive Learning</span> in Recommendation <span class="chip">DASFAA 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Wu, Ruobing Xie, Yongchun Zhu, Xiang Ao, Xin Chen, Xu Zhang, Fuzhen Zhuang, Leyu Lin, Qing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-behavior recommendation (MBR) aims to jointly consider multiple
behaviors to improve the target behavior's performance. We argue that MBR
models should: (1) model the coarse-grained commonalities between different
behaviors of a user, (2) consider both individual sequence view and global
graph view in multi-behavior modeling, and (3) capture the fine-grained
differences between multiple behaviors of a user. In this work, we propose a
novel Multi-behavior Multi-view Contrastive Learning Recommendation (MMCLR)
framework, including three new CL tasks to solve the above challenges,
respectively. The multi-behavior CL aims to make different user single-behavior
representations of the same user in each view to be similar. The multi-view CL
attempts to bridge the gap between a user's sequence-view and graph-view
representations. The behavior distinction CL focuses on modeling fine-grained
differences of different behaviors. In experiments, we conduct extensive
evaluations and ablation tests to verify the effectiveness of MMCLR and various
CL tasks on two real-world datasets, achieving SOTA performance over existing
baselines. Our code will be available on
\url{https://github.com/wyqing20/MMCLR}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DASFAA 2022 Main Conference Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZOOMER: Boosting Retrieval on Web-scale Graphs by Regions of Interest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuezihan Jiang, Yu Cheng, Hanyu Zhao, Wentao Zhang, Xupeng Miao, Yu He, Liang Wang, Zhi Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ZOOMER, a system deployed at Taobao, the largest e-commerce
platform in China, for training and serving GNN-based recommendations over
web-scale graphs. ZOOMER is designed for tackling two challenges presented by
the massive user data at Taobao: low training/serving efficiency due to the
huge scale of the graphs, and low recommendation quality due to the information
overload which distracts the recommendation model from specific user
intentions. ZOOMER achieves this by introducing a key concept, Region of
Interests (ROI) in GNNs for recommendations, i.e., a neighborhood region in the
graph with significant relevance to a strong user intention. ZOOMER narrows the
focus from the whole graph and "zooms in" on the more relevant ROIs, thereby
reducing the training/serving cost and mitigating the information overload at
the same time. With carefully designed mechanisms, ZOOMER identifies the
interest expressed by each recommendation request, constructs an ROI subgraph
by sampling with respect to the interest, and guides the GNN to reweigh
different parts of the ROI towards the interest by a multi-level attention
module. Deployed as a large-scale distributed system, ZOOMER supports graphs
with billions of nodes for training and thousands of requests per second for
serving. ZOOMER achieves up to 14x speedup when downsizing sampling scales with
comparable (even better) AUC performance than baseline methods. Besides, both
the offline evaluation and online A/B test demonstrate the effectiveness of
ZOOMER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Publication and collaboration anomalies in academic papers originating
  from a paper mill: evidence from a Russia-based paper mill 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.13322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.13322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Abalkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study attempts to detect papers originating from the Russia-based paper
mill International publisher LLC. A total of 1009 offers published during
2019-2021 on the 123mi.ru website were analysed. The study allowed us to
identify at least 434 papers that are potentially linked to the paper mill
including one preprint, a duplication paper and 15 republications of papers
erroneously published in hijacked journals. Evidence of suspicious provenance
from the paper mill is provided: matches in title, number of coauthorship
slots, year of publication, country of the journal, country of a coauthorship
slot and similarities of abstracts. These problematic papers are coauthored by
scholars associated with at least 39 countries and submitted both to predatory
and reputable journals. This study also demonstrates collaboration anomalies
and the phenomenon of suspicious collaboration in questionable papers and
examines the predictors of the Russia-based paper mill. The value of
coauthorship slots offered by International Publisher LLC in 2019-2021 is
estimated at $6.5 million. Since the study analysed a particular paper mill, it
is likely that the number of papers with forged authorship is much higher.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double-Scale <span class="highlight-title">Self-Supervised</span> Hypergraph Learning for Group
  Recommendation <span class="chip">CIKM 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prevalence of social media, there has recently been a proliferation
of recommenders that shift their focus from individual modeling to group
recommendation. Since the group preference is a mixture of various
predilections from group members, the fundamental challenge of group
recommendation is to model the correlations among members. Existing methods
mostly adopt heuristic or attention-based preference aggregation strategies to
synthesize group preferences. However, these models mainly focus on the
pairwise connections of users and ignore the complex high-order interactions
within and beyond groups. Besides, group recommendation suffers seriously from
the problem of data sparsity due to severely sparse group-item interactions. In
this paper, we propose a self-supervised hypergraph learning framework for
group recommendation to achieve two goals: (1) capturing the intra- and
inter-group interactions among users; (2) alleviating the data sparsity issue
with the raw data itself. Technically, for (1), a hierarchical hypergraph
convolutional network based on the user- and group-level hypergraphs is
developed to model the complex tuplewise correlations among users within and
beyond groups. For (2), we design a double-scale node dropout strategy to
create self-supervision signals that can regularize user representations with
different granularities against the sparsity issue. The experimental analysis
on multiple benchmark datasets demonstrates the superiority of the proposed
model and also elucidates the rationality of the hypergraph modeling and the
double-scale self-supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, CIKM 2021</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic <span class="highlight-title">Review</span> on Affective Computing: Emotion Models, Databases,
  and Recent Advances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Wang, Wei Song, Wei Tao, Antonio Liotta, Dawei Yang, Xinlei Li, Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affective computing plays a key role in human-computer interactions,
entertainment, teaching, safe driving, and multimedia integration. Major
breakthroughs have been made recently in the areas of affective computing
(i.e., emotion recognition and sentiment analysis). Affective computing is
realized based on unimodal or multimodal data, primarily consisting of physical
information (e.g., textual, audio, and visual data) and physiological signals
(e.g., EEG and ECG signals). Physical-based affect recognition caters to more
researchers due to multiple public databases. However, it is hard to reveal
one's inner emotion hidden purposely from facial expressions, audio tones, body
gestures, etc. Physiological signals can generate more precise and reliable
emotional results; yet, the difficulty in acquiring physiological signals also
hinders their practical application. Thus, the fusion of physical information
and physiological signals can provide useful features of emotional states and
lead to higher accuracy. Instead of focusing on one specific field of affective
analysis, we systematically review recent advances in the affective computing,
and taxonomize unimodal affect recognition as well as multimodal affective
analysis. Firstly, we introduce two typical emotion models followed by commonly
used databases for affective computing. Next, we survey and taxonomize
state-of-the-art unimodal affect recognition and multimodal affective analysis
in terms of their detailed architectures and performances. Finally, we discuss
some important aspects on affective computing and their applications and
conclude this review with an indication of the most promising future
directions, such as the establishment of baseline dataset, fusion strategies
for multimodal affective analysis, and unsupervised learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-19T00:00:00Z">2022-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> On Robust Prefix-Tuning for Text Classification <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghan Yang, <span class="highlight-author">Yang Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, prefix-tuning has gained increasing attention as a
parameter-efficient finetuning method for large-scale pretrained language
models. The method keeps the pretrained models fixed and only updates the
prefix token parameters for each downstream task. Despite being lightweight and
modular, prefix-tuning still lacks robustness to textual adversarial attacks.
However, most currently developed defense techniques necessitate auxiliary
model update and storage, which inevitably hamper the modularity and low
storage of prefix-tuning. In this work, we propose a robust prefix-tuning
framework that preserves the efficiency and modularity of prefix-tuning. The
core idea of our framework is leveraging the layerwise activations of the
language model by correctly-classified training data as the standard for
additional prefix finetuning. During the test phase, an extra batch-level
prefix is tuned for each batch and added to the original prefix for robustness
enhancement. Extensive experiments on three text classification benchmarks show
that our framework substantially improves robustness over several strong
baselines against five textual attacks of different types while maintaining
comparable accuracy on clean texts. We also interpret our robust prefix-tuning
framework from the optimal control perspective and pose several directions for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2022. We release the code at
  https://github.com/minicheshire/Robust-Prefix-Tuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Online Behaviour of the Algerian Abusers in Social Media Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kheireddine Abainia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connecting to social media networks becomes a daily task for the majority of
people around the world, and the amount of shared information is growing
exponentially. Thus, controlling the way in which people communicate is
necessary, in order to protect them from disorientation, conflicts,
aggressions, etc. In this paper, we conduct a statistical study on the
cyber-bullying and the abusive content in social media (i.e. Facebook), where
we try to spot the online behaviour of the abusers in the Algerian community.
More specifically, we have involved 200 Facebook users from different regions
among 600 to carry out this study. The aim of this investigation is to aid
automatic systems of abuse detection to take decision by incorporating the
online activity. Abuse detection systems require a large amount of data to
perform better on such kind of texts (i.e. unstructured and informal texts),
and this is due to the lack of standard orthography, where there are various
Algerian dialects and languages spoken.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BigDML 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> AI Autonomy: Self-Initiation, Adaptation and Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Bing Liu</span>, Sahisnu Mazumder, Eric Robertson, Scott Grigsby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties,
detecting novelties, characterizing them, accommodating or adapting to them,
and gathering ground-truth training data and incrementally learning the
unknowns/novelties are critical to making the AI agent more and more
knowledgeable and powerful over time. The key challenge is how to automate the
process so that it is carried out continually on the agent's own initiative and
through its own interactions with humans, other agents and the environment just
like human on-the-job learning. This paper proposes a framework (called SOLA)
for this learning paradigm to promote the research of building autonomous and
continual learning enabled AI agents. To show feasibility, an implemented agent
is also described.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2110.11385</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Domain Representative Keywords Selection: A Probabilistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang, Yunyao Li, Lucian Popa, <span class="highlight-author">ChengXiang Zhai</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a probabilistic approach to select a subset of a \textit{target
domain representative keywords} from a candidate set, contrasting with a
context domain. Such a task is crucial for many downstream tasks in natural
language processing. To contrast the target domain and the context domain, we
adapt the \textit{two-component mixture model} concept to generate a
distribution of candidate keywords. It provides more importance to the
\textit{distinctive} keywords of the target domain than common keywords
contrasting with the context domain. To support the \textit{representativeness}
of the selected keywords towards the target domain, we introduce an
\textit{optimization algorithm} for selecting the subset from the generated
candidate distribution. We have shown that the optimization algorithm can be
efficiently implemented with a near-optimal approximation guarantee. Finally,
extensive experiments on multiple domains demonstrate the superiority of our
approach over other baselines for the tasks of keyword summary generation and
trending keywords selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Learning for Online Update of Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Kim, Hwanjun Song, Yooju Shin, Dongmin Park, Kijung Shin, Jae-Gil Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online recommender systems should be always aligned with users' current
interest to accurately suggest items that each user would like. Since user
interest usually evolves over time, the update strategy should be flexible to
quickly catch users' current interest from continuously generated new user-item
interactions. Existing update strategies focus either on the importance of each
user-item interaction or the learning rate for each recommender parameter, but
such one-directional flexibility is insufficient to adapt to varying
relationships between interactions and parameters. In this paper, we propose
MeLON, a meta-learning based novel online recommender update strategy that
supports two-directional flexibility. It is featured with an adaptive learning
rate for each parameter-interaction pair for inducing a recommender to quickly
learn users' up-to-date interest. The procedure of MeLON is optimized following
a meta-learning approach: it learns how a recommender learns to generate the
optimal learning rates for future updates. Specifically, MeLON first enriches
the meaning of each interaction based on previous interactions and identifies
the role of each parameter for the interaction; and then combines these two
pieces of information to generate an adaptive learning rate. Theoretical
analysis and extensive evaluation on three real-world online recommender
datasets validate the effectiveness of MeLON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Robust Collaborative Targeted Learning for Recommendation on Data
  Missing Not at Random 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wu, Haoxuan Li, Yan Lyu, Xiao-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommender systems, the feedback data received is always missing not at
random (MNAR), which poses challenges for accurate rating prediction. To
address this issue, many recent studies have been conducted on the doubly
robust (DR) method and its variants to reduce bias. However, theoretical
analysis shows that the DR method has a relatively large variance, while that
of the error imputation-based (EIB) method is smaller. In this paper, we
propose {\bf DR-TMLE} that effectively captures the merits of both EIB and DR,
by leveraging the targeted maximum likelihood estimation (TMLE) technique.
DR-TMLE first obtains an initial EIB estimator and then updates the error
imputation model along with the bias-reduced direction. Furthermore, we propose
a novel RCT-free collaborative targeted learning algorithm for DR-TMLE, called
{\bf DR-TMLE-TL}, which updates the propensity model adaptively to reduce the
bias of imputed errors. Both theoretical analysis and experiments demonstrate
the advantages of the proposed methods compared with existing debiasing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DuReader_retrieval: A Large-scale Chinese Benchmark for Passage
  Retrieval from Web Search Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, <span class="highlight-author">Haifeng Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present DuReader_retrieval, a large-scale Chinese dataset
for passage retrieval. DuReader_retrieval contains more than 90K queries and
over 8M unique passages from Baidu search. To ensure the quality of our
benchmark and address the shortcomings in other existing datasets, we (1)
reduce the false negatives in development and testing sets by pooling the
results from multiple retrievers with human annotations, (2) and remove the
semantically similar questions between training with development and testing
sets. We further introduce two extra out-of-domain testing sets for
benchmarking the domain generalization capability. Our experiment results
demonstrate that DuReader_retrieval is challenging and there is still plenty of
room for the community to improve, e.g. the generalization across domains,
salient phrase and syntax mismatch between query and paragraph and robustness.
DuReader_retrieval will be publicly available at
https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-03-18T00:00:00Z">2022-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating Bandit Learning from User Feedback for Extractive Question
  Answering <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Gao, Eunsol Choi, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study learning from user feedback for extractive question answering by
simulating feedback using supervised data. We cast the problem as contextual
bandit learning, and analyze the characteristics of several learning scenarios
with focus on reducing data annotation. We show that systems initially trained
on a small number of examples can dramatically improve given feedback from
users on model-predicted answers, and that one can use existing datasets to
deploy systems in new domains without any annotation, but instead improving the
system on-the-fly via user feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RELIC: Retrieving Evidence for Literary Claims <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Thai, Yapei Chang, Kalpesh Krishna, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 camera ready (19 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offensive Language Detection in Under-resourced Algerian Dialectal
  Arabic Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oussama Boucherit, Kheireddine Abainia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of detecting the offensive and abusive
content in Facebook comments, where we focus on the Algerian dialectal Arabic
which is one of under-resourced languages. The latter has a variety of dialects
mixed with different languages (i.e. Berber, French and English). In addition,
we deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due
to the scarcity of works on the same language, we have built a new corpus
regrouping more than 8.7k texts manually annotated as normal, abusive and
offensive. We have conducted a series of experiments using the state-of-the-art
classifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.
The results showed acceptable performances, but the problem requires further
investigation on linguistic features to increase the identification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BigDML 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges and Strategies in Cross-Cultural NLP <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various efforts in the Natural Language Processing (NLP) community have been
made to accommodate linguistic diversity and serve speakers of many different
languages. However, it is important to acknowledge that speakers and the
content they produce and require, vary not just by language, but also by
culture. Although language and culture are tightly linked, there are important
differences. Analogous to cross-lingual and multilingual NLP, cross-cultural
and multicultural NLP considers these differences in order to better serve
users of NLP systems. We propose a principled framework to frame these efforts,
and survey existing and potential strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 - Theme track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Report from the NSF Future Directions Workshop on Automatic Evaluation
  of <span class="highlight-title">Dialog</span>: Research Directions and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikib Mehri, Jinho Choi, Luis Fernando D'Haro, Jan Deriu, Maxine Eskenazi, Milica Gasic, Kallirroi Georgila, Dilek Hakkani-Tur, Zekang Li, Verena Rieser, Samira Shaikh, David Traum, Yi-Ting Yeh, <span class="highlight-author">Zhou Yu</span>, Yizhe Zhang, Chen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Report from the NSF AED Workshop (http://dialrc.org/AED/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaMEL: Case Marker Extraction without Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Weissweiler, Valentin Hofmann, Masoud Jalili Sabet, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CaMEL (Case Marker Extraction without Labels), a novel and
challenging task in computational morphology that is especially relevant for
low-resource languages. We propose a first model for CaMEL that uses a
massively multilingual corpus to extract case markers in 83 languages based
only on a noun phrase chunker and an alignment system. To evaluate CaMEL, we
automatically construct a silver standard from UniMorph. The case markers
extracted by our model can be used to detect and visualise similarities and
differences between the case systems of different languages as well as to
annotate fine-grained deep cases in languages in which they are not overtly
marked.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FORCE: A Framework of Rule-Based <span class="highlight-title">Conversation</span>al Recommender System <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Quan, Ze Wei, Qiang Gan, Jingqi Yao, Jingyi Lu, Yuchen Dong, Yiming Liu, Yi Zeng, Chao Zhang, Yongzhi Li, Huang Hu, Yingying He, Yang Yang, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 (Demonstration Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Text <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Pre-train</span>ing for Medical Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjin Park, Seongsu Bae, Jiho Kim, Tackeun Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the volume of Electronic Health Records (EHR) sharply grows, there has
been emerging interest in learning the representation of EHR for healthcare
applications. Representation learning of EHR requires appropriate modeling of
the two dominant modalities in EHR: structured data and unstructured text. In
this paper, we present MedGTX, a pre-trained model for multi-modal
representation learning of the structured and textual EHR data. MedGTX uses a
novel graph encoder to exploit the graphical nature of structured EHR data, and
a text encoder to handle unstructured text, and a cross-modal encoder to learn
a joint representation space. We pre-train our model through four proxy tasks
on MIMIC-III, an open-source EHR data, and evaluate our model on two clinical
benchmarks and three novel downstream tasks which tackle real-world problems in
EHR data. The results consistently show the effectiveness of pre-training the
model for joint representation of both structured and unstructured information
from EHR. Given the promising performance of MedGTX, we believe this work opens
a new door to jointly understanding the two fundamental modalities of EHR data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of the Conference on Health, Inference, and
  Learning (CHIL 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossAligner & Co: Zero-Shot Transfer Methods for <span class="highlight-title">Task-Oriented</span>
  Cross-lingual Natural Language Understanding <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Gritta, Ruoyu Hu, Ignacio Iacobacci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented personal assistants enable people to interact with a host of
devices and services using natural language. One of the challenges of making
neural dialogue systems available to more users is the lack of training data
for all but a few languages. Zero-shot methods try to solve this issue by
acquiring task knowledge in a high-resource language such as English with the
aim of transferring it to the low-resource language(s). To this end, we
introduce CrossAligner, the principal method of a variety of effective
approaches for zero-shot cross-lingual transfer based on learning alignment
from unlabelled parallel data. We present a quantitative analysis of individual
methods as well as their weighted combinations, several of which exceed
state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test
sets and three benchmark multilingual datasets. A detailed qualitative error
analysis of the best methods shows that our fine-tuned language models can
zero-shot transfer the task knowledge better than anticipated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper (multilingual track) to appear at ACL (Findings) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIOS: An Algorithmically Generated Biomedical Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Yu, Zheng Yuan, Jun Xia, Shengxuan Luo, Huaiyuan Ying, Sihang Zeng, Jingyi Ren, Hongyi Yuan, Zhengyun Zhao, Yucong Lin, Keming Lu, Jing Wang, Yutao Xie, Heung-Yeung Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for
biomedical and healthcare big data and artificial intelligence (AI),
facilitating natural language processing, model development, and data exchange.
For many decades, these knowledge graphs have been built via expert curation,
which can no longer catch up with the speed of today's AI development, and a
transition to algorithmically generated BioMedKGs is necessary. In this work,
we introduce the Biomedical Informatics Ontology System (BIOS), the first large
scale publicly available BioMedKG that is fully generated by machine learning
algorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in
two languages, and 7.3 million relation triplets. We introduce the methodology
for developing BIOS, which covers curation of raw biomedical terms,
computationally identifying synonymous terms and aggregating them to create
concept nodes, semantic type classification of the concepts, relation
identification, and biomedical machine translation. We provide statistics about
the current content of BIOS and perform preliminary assessment for term
quality, synonym grouping, and relation extraction. Results suggest that
machine learning-based BioMedKG development is a totally viable solution for
replacing traditional expert curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Lithuanian grammatical error correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyone wants to write beautiful and correct text, yet the lack of language
skills, experience, or hasty typing can result in errors. By employing the
recent advances in transformer architectures, we construct a grammatical error
correction model for Lithuanian, the language rich in archaic features. We
compare subword and byte-level approaches and share our best trained model,
achieving F$_{0.5}$=0.92, and accompanying code, in an online open-source
repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-based Generative Approach towards Multi-Hierarchical Medical
  <span class="highlight-title">Dialogue</span> State Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Liu, Tong Ruan, Haofen Wang, Huanhuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The medical dialogue system is a promising application that can provide great
convenience for patients. The dialogue state tracking (DST) module in the
medical dialogue system which interprets utterances into the machine-readable
structure for downstream tasks is particularly challenging. Firstly, the states
need to be able to represent compound entities such as symptoms with their body
part or diseases with degrees of severity to provide enough information for
decision support. Secondly, these named entities in the utterance might be
discontinuous and scattered across sentences and speakers. These also make it
difficult to annotate a large corpus which is essential for most methods.
Therefore, we first define a multi-hierarchical state structure. We annotate
and publish a medical dialogue dataset in Chinese. To the best of our
knowledge, there are no publicly available ones before. Then we propose a
Prompt-based Generative Approach which can generate slot values with
multi-hierarchies incrementally using a top-down approach. A dialogue style
prompt is also supplemented to utilize the large unlabeled dialogue corpus to
alleviate the data scarcity problem. The experiments show that our approach
outperforms other DST methods and is rather effective in the scenario with
little data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake News Detection Using Majority Voting Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dharmaraj R. Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the evolution of the Web and social network platforms it becomes very
easy to disseminate the information. Peoples are creating and sharing more
information than ever before, which may be misleading, misinformation or fake
information. Fake news detection is a crucial and challenging task due to the
unstructured nature of the available information. In the recent years,
researchers have provided significant solutions to tackle with the problem of
fake news detection, but due to its nature there are still many open issues. In
this paper, we have proposed majority voting approach to detect fake news
articles. We have used different textual properties of fake and real news. We
have used publicly available fake news dataset, comprising of 20,800 news
articles among which 10,387 are real and 10,413 are fake news labeled as binary
0 and 1. For the evaluation of our approach, we have used commonly used machine
learning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random
Forest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the
aforementioned classifiers, we built a multi-model fake news detection system
using Majority Voting technique to achieve the more accurate results. The
experimental results show that, our proposed approach achieved accuracy of
96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation
confirms that, Majority Voting technique achieved more acceptable results as
compare to individual learning technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Multilingual Language Models Capture Differing Moral Norms? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindřich Libovický, Alexander Fraser, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massively multilingual sentence representations are trained on large corpora
of uncurated data, with a very imbalanced proportion of languages included in
the training. This may cause the models to grasp cultural values including
moral judgments from the high-resource languages and impose them on the
low-resource languages. The lack of data in certain languages can also lead to
developing random and thus potentially harmful beliefs. Both these issues can
negatively influence zero-shot cross-lingual model transfer and potentially
lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these
issues by comparing different models in different languages, (2) develop
methods for improving undesirable properties of the models. Our initial
experiments using the multilingual model XLM-R show that indeed multilingual
LMs capture moral norms, even with potentially higher human-agreement than
monolingual ones. However, it is not yet clear to what extent these moral norms
differ between languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCoT: Sense Clustering over Time: a tool for the analysis of lexical
  change 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Haase, Saba Anwar, Seid Muhie Yimam, Alexander Friedrich, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Sense Clustering over Time (SCoT), a novel network-based tool for
analysing lexical change. SCoT represents the meanings of a word as clusters of
similar words. It visualises their formation, change, and demise. There are two
main approaches to the exploration of dynamic networks: the discrete one
compares a series of clustered graphs from separate points in time. The
continuous one analyses the changes of one dynamic network over a time-span.
SCoT offers a new hybrid solution. First, it aggregates time-stamped documents
into intervals and calculates one sense graph per discrete interval. Then, it
merges the static graphs to a new type of dynamic semantic neighbourhood graph
over time. The resulting sense clusters offer uniquely detailed insights into
lexical change over continuous intervals with model transparency and
provenance. SCoT has been successfully used in a European study on the changing
meaning of `crisis'.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update of https://aclanthology.org/2021.eacl-demos.23/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias
  in Speech Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, Marco Turchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender bias is largely recognized as a problematic phenomenon affecting
language technologies, with recent studies underscoring that it might surface
differently across languages. However, most of current evaluation practices
adopt a word-level focus on a narrow set of occupational nouns under synthetic
conditions. Such protocols overlook key features of grammatical gender
languages, which are characterized by morphosyntactic chains of gender
agreement, marked on a variety of lexical items and parts-of-speech (POS). To
overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE
corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS
and agreement chains), and explore to what extent different lexical categories
and agreement phenomena are impacted by gender skews. Focusing on speech
translation, we conduct a multifaceted evaluation on three language directions
(English-French/Italian/Spanish), with models trained on varying amounts of
data and different word segmentation techniques. By shedding light on model
behaviours, gender bias, and its detection at several levels of granularity,
our findings emphasize the value of dedicated analyses beyond aggregated
overall results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaVocoder: Adaptive Vocoder for Custom Voice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yuan, Yongbing Feng, Mingming Ye, Cheng Tuo, Minghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Custom voice is to construct a personal speech synthesis system by adapting
the source speech synthesis model to the target model through the target few
recordings. The solution to constructing a custom voice is to combine an
adaptive acoustic model with a robust vocoder. However, training a robust
vocoder usually requires a multi-speaker dataset, which should include various
age groups and various timbres, so that the trained vocoder can be used for
unseen speakers. Collecting such a multi-speaker dataset is difficult, and the
dataset distribution always has a mismatch with the distribution of the target
speaker dataset. This paper proposes an adaptive vocoder for custom voice from
another novel perspective to solve the above problems. The adaptive vocoder
mainly uses a cross-domain consistency loss to solve the overfitting problem
encountered by the GAN-based neural vocoder in the transfer learning of
few-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.
First, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,
respectively. Then, fine-tune it on the internal dataset VXI-children with few
adaptation data. The empirical results show that a high-quality custom voice
system can be built by combining a adaptive acoustic model with a adaptive
vocoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>my paper was submitted to Insterspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are You Ro<span class="highlight-title">bert</span> or Ro<span class="highlight-title">BERT</span>a? Deceiving Online Authorship Attribution
  Models Using Neural Text Generators <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keenan Jones, Jason R. C. Nurse, Shujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a rise in the development of powerful pre-trained
natural language models, including GPT-2, Grover, and XLM. These models have
shown state-of-the-art capabilities towards a variety of different NLP tasks,
including question answering, content summarisation, and text generation.
Alongside this, there have been many studies focused on online authorship
attribution (AA). That is, the use of models to identify the authors of online
texts. Given the power of natural language models in generating convincing
texts, this paper examines the degree to which these language models can
generate texts capable of deceiving online AA models. Experimenting with both
blog and Twitter data, we utilise GPT-2 language models to generate texts using
the existing posts of online users. We then examine whether these AI-based text
generators are capable of mimicking authorial style to such a degree that they
can deceive typical AA models. From this, we find that current AI-based text
generators are able to successfully mimic authorship, showing capabilities
towards this on both datasets. Our findings, in turn, highlight the current
capacity of powerful natural language models to generate original online posts
capable of mimicking authorial style sufficiently to deceive popular AA
methods; a key finding given the proposed role of AA in real world applications
such as spam-detection and forensic investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 4 tables, Accepted for publication in the
  proceedings of the sixteenth International AAAI Conference on Web and Social
  Media (ICWSM-22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Prototypical Verbalizer for <span class="highlight-title">Prompt</span>-based Few-shot Tuning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, <span class="highlight-author">Zhiyuan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRS: Combining Generation and Revision in Unsupervised Sentence
  Simplification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Dehghan, Dhruv Kumar, Lukasz Golab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose GRS: an unsupervised approach to sentence simplification that
combines text generation and text revision. We start with an iterative
framework in which an input sentence is revised using explicit edit operations,
and add paraphrasing as a new edit operation. This allows us to combine the
advantages of generative and revision-based approaches: paraphrasing captures
complex edit operations, and the use of explicit edit operations in an
iterative manner provides controllability and interpretability. We demonstrate
these advantages of GRS compared to existing methods on the Newsela and ASSET
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRBoost: <span class="highlight-title">Prompt</span>-Based Rule Discovery and Boosting for Interactive
  Weakly-Supervised Learning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised learning (WSL) has shown promising results in addressing
label scarcity on many NLP tasks, but manually designing a comprehensive,
high-quality labeling rule set is tedious and difficult. We study interactive
weakly-supervised learning -- the problem of iteratively and automatically
discovering novel labeling rules from data to improve the WSL model. Our
proposed model, named PRBoost, achieves this goal via iterative prompt-based
rule discovery and model boosting. It uses boosting to identify large-error
instances and then discovers candidate rules from them by prompting pre-trained
LMs with rule templates. The candidate rules are judged by human experts, and
the accepted rules are used to generate complementary weak labels and
strengthen the current model. Experiments on four tasks show PRBoost
outperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with
fully supervised models. Our Implementation is available at
\url{https://github.com/rz-zhang/PRBoost}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 (Main Conference). Code: https://github.com/rz-zhang/PRBoost</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEAM: <span class="highlight-title">Dialogue</span> Coherence Evaluation using AMR-based Semantic
  Manipulations <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarik Ghazarian, Nuan Wen, Aram Galstyan, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation metrics are essential for the rapid development of
open-domain dialogue systems as they facilitate hyper-parameter tuning and
comparison between models. Although recently proposed trainable
conversation-level metrics have shown encouraging results, the quality of the
metrics is strongly dependent on the quality of training data. Prior works
mainly resort to heuristic text-level manipulations (e.g. utterances shuffling)
to bootstrap incoherent conversations (negative examples) from coherent
dialogues (positive examples). Such approaches are insufficient to
appropriately reflect the incoherence that occurs in interactions between
advanced dialogue models and humans. To tackle this problem, we propose DEAM, a
Dialogue coherence Evaluation metric that relies on Abstract Meaning
Representation (AMR) to apply semantic-level Manipulations for incoherent
(negative) data generation. AMRs naturally facilitate the injection of various
types of incoherence sources, such as coreference inconsistency, irrelevancy,
contradictions, and decrease engagement, at the semantic level, thus resulting
in more natural incoherent samples. Our experiments show that DEAM achieves
higher correlations with human judgments compared to baseline methods on
several dialog datasets by significant margins. We also show that DEAM can
distinguish between coherent and incoherent dialogues generated by baseline
manipulations, whereas those baseline models cannot detect incoherent examples
generated by DEAM. Our results demonstrate the potential of AMR-based semantic
manipulations for natural negative example generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Association for Computational Linguistics (ACL 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve few-shot voice cloning using <span class="highlight-title">multi-modal</span> learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Zhang, Yue Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, few-shot voice cloning has achieved a significant improvement.
However, most models for few-shot voice cloning are single-modal, and
multi-modal few-shot voice cloning has been understudied. In this paper, we
propose to use multi-modal learning to improve the few-shot voice cloning
performance. Inspired by the recent works on unsupervised speech
representation, the proposed multi-modal system is built by extending Tacotron2
with an unsupervised speech representation module. We evaluate our proposed
system in two few-shot voice cloning scenarios, namely few-shot
text-to-speech(TTS) and voice conversion(VC). Experimental results demonstrate
that the proposed multi-modal learning can significantly improve the few-shot
voice cloning performance over their counterpart single-modal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 IEEE International Conference on Acoustics, Speech and Signal
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A$^3$T: Alignment-Aware Acoustic and Text <span class="highlight-title">Pretrain</span>ing for Speech
  Synthesis and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Bai, Renjie Zheng, Junkun Chen, Xintong Li, Mingbo Ma, Liang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, speech representation learning has improved many speech-related
tasks such as speech recognition, speech classification, and speech-to-text
translation. However, all the above tasks are in the direction of speech
understanding, but for the inverse direction, speech synthesis, the potential
of representation learning is yet to be realized, due to the challenging nature
of generating high-quality speech. To address this problem, we propose our
framework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which
reconstructs masked acoustic signals with text input and acoustic-text
alignment during training. In this way, the pretrained model can generate high
quality of reconstructed spectrogram, which can be applied to the speech
editing and unseen speaker TTS directly. Experiments show A$^3$T outperforms
SOTA models on speech editing, and improves multi-speaker speech synthesis
without the external speaker verification model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review, 12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Intensification for Sign Language Generation: A Computational
  Approach <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert İnan, Yang Zhong, Sabit Hassan, Lorna Quandt, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end sign language generation models do not accurately represent the
prosody in sign language. A lack of temporal and spatial variations leads to
poor-quality generated presentations that confuse human interpreters. In this
paper, we aim to improve the prosody in generated sign languages by modeling
intensification in a data-driven manner. We present different strategies
grounded in linguistics of sign language that inform how intensity modifiers
can be represented in gloss annotations. To employ our strategies, we first
annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,
with different levels of intensification. We then use a supervised intensity
tagger to extend the annotated dataset and obtain labels for the remaining
portion of it. This enhanced dataset is then used to train state-of-the-art
transformer models for sign language generation. We find that our efforts in
intensification modeling yield better results when evaluated with automatic
metrics. Human evaluation also indicates a higher preference of the videos
generated using our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, Findings of the Association for Computational Linguistics:
  ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hate speech, Censorship, and Freedom of Speech: The Changing Policies of
  Reddit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elissa Nakajima Wickham, Emily Öhman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the shift in focus on content policies and user attitudes
on the social media platform Reddit. We do this by focusing on comments from
general Reddit users from five posts made by admins (moderators) on updates to
Reddit Content Policy. All five concern the nature of what kind of content is
allowed to be posted on Reddit, and which measures will be taken against
content that violates these policies. We use topic modeling to probe how the
general discourse for Redditors has changed around limitations on content, and
later, limitations on hate speech, or speech that incites violence against a
particular group. We show that there is a clear shift in both the contents and
the user attitudes that can be linked to contemporary societal upheaval as well
as newly passed laws and regulations, and contribute to the wider discussion on
hate speech moderation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Journal of Data Mining and Digital Humanities</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correcting diacritics and typos with a ByT5 <span class="highlight-title">transformer</span> model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius, Jurgita Kapočiūtė-Dzikienė, Monika Briedienė, Tomas Krilavičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the fast pace of life and online communications and the prevalence of
English and the QWERTY keyboard, people tend to forgo using diacritics, make
typographical errors (typos) when typing in other languages. Restoring
diacritics and correcting spelling is important for proper language use and the
disambiguation of texts for both humans and downstream algorithms. However,
both of these problems are typically addressed separately: the state-of-the-art
diacritics restoration methods do not tolerate other typos, but classical
spellcheckers also cannot deal adequately with all the diacritics missing. In
this work, we tackle both problems at once by employing the newly-developed
universal ByT5 byte-level seq2seq transformer model that requires no
language-specific model structures. For a comparison, we perform diacritics
restoration on benchmark datasets of 12 languages, with the addition of
Lithuanian. The experimental investigation proves that our approach is able to
achieve results (> 98%) comparable to the previous state-of-the-art, despite
being trained less and on fewer data. Our approach is also able to restore
diacritics in words not seen during training with > 76% accuracy. Our
simultaneous diacritics restoration and typos correction approach reaches > 94%
alpha-word accuracy on the 13 languages. It has no direct competitors and
strongly outperforms classical spell-checking or dictionary-based approaches.
We also demonstrate all the accuracies to further improve with more training.
Taken together, this shows the great real-world application potential of our
suggested methods to more data, languages, and error classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Knowledgeable <span class="highlight-title">Prompt</span>-tuning: Incorporating Knowledge into <span class="highlight-title">Prompt</span>
  Verbalizer for Text Classification <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.02035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.02035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengding Hu, Ning Ding, Huadong Wang, <span class="highlight-author">Zhiyuan Liu</span>, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tuning pre-trained language models (PLMs) with task-specific prompts has been
a promising approach for text classification. Particularly, previous studies
suggest that prompt-tuning has remarkable superiority in the low-data scenario
over the generic fine-tuning methods with extra classifiers. The core idea of
prompt-tuning is to insert text pieces, i.e., template, to the input and
transform a classification problem into a masked language modeling problem,
where a crucial step is to construct a projection, i.e., verbalizer, between a
label space and a label word space. A verbalizer is usually handcrafted or
searched by gradient descent, which may lack coverage and bring considerable
bias and high variances to the results. In this work, we focus on incorporating
external knowledge into the verbalizer, forming a knowledgeable prompt-tuning
(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the
label word space of the verbalizer using external knowledge bases (KBs) and
refine the expanded label word space with the PLM itself before predicting with
the expanded label word space. Extensive experiments on zero and few-shot text
classification tasks demonstrate the effectiveness of knowledgeable
prompt-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shellcode_IA32: A <span class="highlight-title">Dataset</span> for Automatic Shellcode Generation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.13100v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.13100v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Liguori, Erfan Al-Hossami, Domenico Cotroneo, Roberto Natella, Bojan Cukic, Samira Shaikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We take the first step to address the task of automatically generating
shellcodes, i.e., small pieces of code used as a payload in the exploitation of
a software vulnerability, starting from natural language comments. We assemble
and release a novel dataset (Shellcode_IA32), consisting of challenging but
common assembly instructions with their natural language descriptions. We
experiment with standard methods in neural machine translation (NMT) to
establish baseline performance levels on this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted to NLP4Prog Workshop 2021 co-located with ACL-IJCNLP
  2021. Extended journal version of this work has been published in the
  Automated Software Engineering journal, Volume 29, Article no. 30, March
  2022, DOI: 10.1007/s10515-022-00331-3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiRdQA: A Bilingual <span class="highlight-title">Dataset</span> for Question Answering on Tricky Riddles <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.11087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.11087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Zhang, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A riddle is a question or statement with double or veiled meanings, followed
by an unexpected answer. Solving riddle is a challenging task for both machine
and human, testing the capability of understanding figurative, creative natural
language and reasoning with commonsense knowledge. We introduce BiRdQA, a
bilingual multiple-choice question answering dataset with 6614 English riddles
and 8751 Chinese riddles. For each riddle-answer pair, we provide four
distractors with additional information from Wikipedia. The distractors are
automatically generated at scale with minimal bias. Existing monolingual and
multilingual QA models fail to perform well on our dataset, indicating that
there is a long way to go before machine can beat human on solving tricky
riddles. The dataset has been released to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing <span class="highlight-title">BERT</span>'s priors with serial reproduction chains <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takateru Yamakoshi, Thomas L. Griffiths, Robert D. Hawkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling is a promising bottom-up method for exposing what generative models
have learned about language, but it remains unclear how to generate
representative samples from popular masked language models (MLMs) like BERT.
The MLM objective yields a dependency network with no guarantee of consistent
conditional distributions, posing a problem for naive approaches. Drawing from
theories of iterated learning in cognitive science, we explore the use of
serial reproduction chains to sample from BERT's priors. In particular, we
observe that a unique and consistent estimator of the ground-truth joint
distribution is given by a Generative Stochastic Network (GSN) sampler, which
randomly selects which token to mask and reconstruct on each step. We show that
the lexical and syntactic statistics of sentences from GSN chains closely match
the ground-truth corpus distribution and perform better than other methods in a
large corpus of naturalness judgments. Our findings establish a firmer
theoretical foundation for bottom-up probing and highlight richer deviations
from human priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting the Robustness of Neural NLP Models to Textual
  Perturbations <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Zhang, Liangming Pan, Samson Tan, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Natural Language Processing (NLP) models are known to be sensitive to
input perturbations and their performance can decrease when applied to
real-world, noisy data. However, it is still unclear why models are less robust
to some perturbations than others. In this work, we test the hypothesis that
the extent to which a model is affected by an unseen textual perturbation
(robustness) can be explained by the learnability of the perturbation (defined
as how well the model learns to identify the perturbation with a small amount
of evidence). We further give a causal justification for the learnability
metric. We conduct extensive experiments with four prominent NLP models --
TextRNN, BERT, RoBERTa and XLNet -- over eight types of textual perturbations
on three datasets. We show that a model which is better at identifying a
perturbation (higher learnability) becomes worse at ignoring such a
perturbation at test time (lower robustness), providing empirical support for
our hypothesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS
  With Accurate Phoneme Duration Control <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchao He, Jian Luan, Yujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence expansion between encoder and decoder is a critical challenge in
sequence-to-sequence TTS. Attention-based methods achieve great naturalness but
suffer from unstable issues like missing and repeating phonemes, not to mention
accurate duration control. Duration-informed methods, on the contrary, seem to
easily adjust phoneme duration but show obvious degradation in speech
naturalness. This paper proposes PAMA-TTS to address the problem. It takes the
advantage of both flexible attention and explicit duration models. Based on the
monotonic attention mechanism, PAMA-TTS also leverages token duration and
relative position of a frame, especially countdown information, i.e. in how
many future frames the present phoneme will end. They help the attention to
move forward along the token sequence in a soft but reliable control.
Experimental results prove that PAMA-TTS achieves the highest naturalness,
while has on-par or even better duration controllability than the
duration-informed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2022. 5 pages, 4 figures, 3 tables. Audio samples
  are available at: https://pama-tts.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced
  Training for Neural Machine Translation <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenze Shao, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MELM: Data Augmentation with Masked Entity Language Modeling for
  Low-Resource NER <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is an effective solution to data scarcity in low-resource
scenarios. However, when applied to token-level tasks such as NER, data
augmentation methods often suffer from token-label misalignment, which leads to
unsatsifactory performance. In this work, we propose Masked Entity Language
Modeling (MELM) as a novel data augmentation framework for low-resource NER. To
alleviate the token-label misalignment issue, we explicitly inject NER labels
into sentence context, and thus the fine-tuned MELM is able to predict masked
entity tokens by explicitly conditioning on their labels. Thereby, MELM
generates high-quality augmented data with novel entities, which provides rich
entity regularity knowledge and boosts NER performance. When training data from
multiple languages are available, we also integrate MELM with code-mixing for
further improvement. We demonstrate the effectiveness of MELM on monolingual,
cross-lingual and multilingual NER across various low-resource levels.
Experimental results show that our MELM presents substantial improvement over
the baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BEAT: A Large-Scale Semantic and Emotional <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Dataset</span> for
  <span class="highlight-title">Conversation</span>al Gestures Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai<span class="highlight-author">yang Liu</span>, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> An Empirical Study of Training End-to-End <span class="highlight-title">Vision-and-Language</span>
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Yi Dou, Yichong Xu, <span class="highlight-author">Zhe Gan</span>, Jianfeng Wang, Shuohang Wang, <span class="highlight-author">Lijuan Wang</span>, Chenguang Zhu, Pengchuan Zhang, <span class="highlight-author">Lu Yuan</span>, Nanyun Peng, Zicheng Liu, Michael Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks often
degrades significantly. In this paper, we present METER, a Multimodal
End-to-end TransformER framework, through which we investigate how to design
and pre-train a fully transformer-based VL model in an end-to-end manner.
Specifically, we dissect the model designs along multiple dimensions: vision
encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,
DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),
architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training
objectives (e.g., masked image modeling). We conduct comprehensive experiments
and provide insights on how to train a performant VL transformer. METER
achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images
for pre-training, surpassing the state-of-the-art region-feature-based model by
1.04%, and outperforming the previous best fully transformer-based model by
1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy
of 80.54%. Code and pre-trained models are released at
https://github.com/zdou0830/METER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Many Data Samples is an Additional Instruction Worth? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in the expanded
version of NATURAL INSTRUCTIONS with additional instructions and find that
these significantly improve model performance (up to 35%), especially in the
low-data regime. Our results indicate that an additional instruction can be
equivalent to ~200 data samples on average across tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Inversion for Nonlinear Imaging Models using Deep Generative
  Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pakshal Bohra, Thanh-an Pham, Jonathan Dong, Michael Unser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most modern imaging systems involve a computational reconstruction pipeline
to infer the image of interest from acquired measurements. The Bayesian
reconstruction framework relies on the characterization of the posterior
distribution, which depends on a model of the imaging system and prior
knowledge on the image, for solving such inverse problems. Here, the choice of
the prior distribution is critical for obtaining high-quality estimates. In
this work, we use deep generative models to represent the prior distribution.
We develop a posterior sampling scheme for the class of nonlinear inverse
problems where the forward model has a neural-network-like structure. This
class includes most existing imaging modalities. We introduce the notion of
augmented generative models in order to suitably handle quantitative image
recovery. We illustrate the advantages of our framework by applying it to two
nonlinear imaging modalities-phase retrieval and optical diffraction
tomography.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lunar Rover Localization Using Craters as Landmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Matthies, Shreyansh Daftry, Scott Tepsuporn, Yang Cheng, Deegan Atha, R. Michael Swan, Sanjna Ravichandar, Masahiro Ono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Onboard localization capabilities for planetary rovers to date have used
relative navigation, by integrating combinations of wheel odometry, visual
odometry, and inertial measurements during each drive to track position
relative to the start of each drive. At the end of each drive, a
ground-in-the-loop (GITL) interaction is used to get a position update from
human operators in a more global reference frame, by matching images or local
maps from onboard the rover to orbital reconnaissance images or maps of a large
region around the rover's current position. Autonomous rover drives are limited
in distance so that accumulated relative navigation error does not risk the
possibility of the rover driving into hazards known from orbital images.
However, several rover mission concepts have recently been studied that require
much longer drives between GITL cycles, particularly for the Moon. These
concepts require greater autonomy to minimize GITL cycles to enable such large
range; onboard global localization is a key element of such autonomy. Multiple
techniques have been studied in the past for onboard rover global localization,
but a satisfactory solution has not yet emerged. For the Moon, the ubiquitous
craters offer a new possibility, which involves mapping craters from orbit,
then recognizing crater landmarks with cameras and-or a lidar onboard the
rover. This approach is applicable everywhere on the Moon, does not require
high resolution stereo imaging from orbit as some other approaches do, and has
potential to enable position knowledge with order of 5 to 10 m accuracy at all
times. This paper describes our technical approach to crater-based lunar rover
localization and presents initial results on crater detection using 3D point
cloud data from onboard lidar or stereo cameras, as well as using shading cues
in monocular onboard imagery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Aerospace Conference, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imaging-based histological features are predictive of MET alterations in
  Non-Small Cell Lung Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan P. Joshi, Bo Osinski, Niha Beig, Lingdao Sha, Kshitij Ingale, Martin C. Stumpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MET is a proto-oncogene whose somatic activation in non-small cell lung
cancer leads to increased cell growth and tumor progression. The two major
classes of MET alterations are gene amplification and exon 14 deletion, both of
which are therapeutic targets and detectable using existing molecular assays.
However, existing tests are limited by their consumption of valuable tissue,
cost and complexity that prevent widespread use. MET alterations could have an
effect on cell morphology, and quantifying these associations could open new
avenues for research and development of morphology-based screening tools. Using
H&E-stained whole slide images (WSIs), we investigated the association of
distinct cell-morphological features with MET amplifications and MET exon 14
deletions. We found that cell shape, color, grayscale intensity and
texture-based features from both tumor infiltrating lymphocytes and tumor cells
distinguished MET wild-type from MET amplified or MET exon 14 deletion cases.
The association of individual cell features with MET alterations suggested a
predictive model could distinguish MET wild-type from MET amplification or MET
exon 14 deletion. We therefore developed an L1-penalized logistic regression
model, achieving a mean Area Under the Receiver Operating Characteristic Curve
(ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent
holdout test set. A sparse set of 43 features differentiated these classes,
which included features similar to what was found in the univariate analysis as
well as the percent of tumor cells in the tissue. Our study demonstrates that
MET alterations result in a detectable morphological signal in tumor cells and
lymphocytes. These results suggest that development of low-cost predictive
models based on H&E-stained WSIs may improve screening for MET altered tumors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-input segmentation of damaged brain in acute ischemic stroke
  patients using slow fusion with skip connection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Tomasetti, Mahdieh Khanmohammadi, Kjersti Engan, Liv Jorunn Høllesli, Kathinka Dæhli Kurz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time is a fundamental factor during stroke treatments. A fast, automatic
approach that segments the ischemic regions helps treatment decisions. In
clinical use today, a set of color-coded parametric maps generated from
computed tomography perfusion (CTP) images are investigated manually to decide
a treatment plan. We propose an automatic method based on a neural network
using a set of parametric maps to segment the two ischemic regions (core and
penumbra) in patients affected by acute ischemic stroke. Our model is based on
a convolution-deconvolution bottleneck structure with multi-input and slow
fusion. A loss function based on the focal Tversky index addresses the data
imbalance issue. The proposed architecture demonstrates effective performance
and results comparable to the ground truth annotated by neuroradiologists. A
Dice coefficient of 0.81 for penumbra and 0.52 for core over the large vessel
occlusion test set is achieved. The full implementation is available at:
https://git.io/JtFGb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHREC 2021: Classification in cryo-electron tomograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilja Gubins, Marten L. Chaillet, Gijs van der Schot, M. Cristina Trueba, Remco C. Veltkamp, Friedrich Förster, Xiao Wang, Daisuke Kihara, Emmanuel Moebel, Nguyen P. Nguyen, Tommi White, Filiz Bunyak, Giorgos Papoulias, Stavros Gerolymatos, Evangelia I. Zacharaki, Konstantinos Moustakas, Xiangrui Zeng, Sinuo Liu, Min Xu, Yaoyu Wang, Cheng Chen, Xuefeng Cui, Fa Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron tomography (cryo-ET) is an imaging technique that allows
three-dimensional visualization of macro-molecular assemblies under near-native
conditions. Cryo-ET comes with a number of challenges, mainly low
signal-to-noise and inability to obtain images from all angles. Computational
methods are key to analyze cryo-electron tomograms.
  To promote innovation in computational methods, we generate a novel simulated
dataset to benchmark different methods of localization and classification of
biological macromolecules in tomograms. Our publicly available dataset contains
ten tomographic reconstructions of simulated cell-like volumes. Each volume
contains twelve different types of complexes, varying in size, function and
structure.
  In this paper, we have evaluated seven different methods of finding and
classifying proteins. Seven research groups present results obtained with
learning-based methods and trained on the simulated dataset, as well as a
baseline template matching (TM), a traditional method widely used in cryo-ET
research. We show that learning-based approaches can achieve notably better
localization and classification performance than TM. We also experimentally
confirm that there is a negative relationship between particle size and
performance for all methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop version of the paper can be found here:
  https://diglib.eg.org/handle/10.2312/3dor20211307</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonnegative-Constrained Joint Collaborative Representation with Union
  Dictionary for Hyperspectral Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhen Chang, Pedram Ghamisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many collaborative representation-based (CR) algorithms have been
proposed for hyperspectral anomaly detection. CR-based detectors approximate
the image by a linear combination of background dictionaries and the
coefficient matrix, and derive the detection map by utilizing recovery
residuals. However, these CR-based detectors are often established on the
premise of precise background features and strong image representation, which
are very difficult to obtain. In addition, pursuing the coefficient matrix
reinforced by the general $l_2$-min is very time consuming. To address these
issues, a nonnegative-constrained joint collaborative representation model is
proposed in this paper for the hyperspectral anomaly detection task. To extract
reliable samples, a union dictionary consisting of background and anomaly
sub-dictionaries is designed, where the background sub-dictionary is obtained
at the superpixel level and the anomaly sub-dictionary is extracted by the
pre-detection process. And the coefficient matrix is jointly optimized by the
Frobenius norm regularization with a nonnegative constraint and a sum-to-one
constraint. After the optimization process, the abnormal information is finally
derived by calculating the residuals that exclude the assumed background
information. To conduct comparable experiments, the proposed
nonnegative-constrained joint collaborative representation (NJCR) model and its
kernel version (KNJCR) are tested in four HSI data sets and achieve superior
results compared with other state-of-the-art detectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Subclass Regularization for Semi-Supervised Semantic
  Segmentation <span class="chip">ICCV 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised semantic segmentation learns from small amounts of labelled
images and large amounts of unlabelled images, which has witnessed impressive
progress with the recent advance of deep neural networks. However, it often
suffers from severe class-bias problem while exploring the unlabelled images,
largely due to the clear pixel-wise class imbalance in the labelled images.
This paper presents an unbiased subclass regularization network (USRN) that
alleviates the class imbalance issue by learning class-unbiased segmentation
from balanced subclass distributions. We build the balanced subclass
distributions by clustering pixels of each original class into multiple
subclasses of similar sizes, which provide class-balanced pseudo supervision to
regularize the class-biased segmentation. In addition, we design an
entropy-based gate mechanism to coordinate learning between the original
classes and the clustered subclasses which facilitates subclass regularization
effectively by suppressing unconfident subclass predictions. Extensive
experiments over multiple public benchmarks show that USRN achieves superior
performance as compared with the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2021. Code is available at
  https://github.com/Dayan-Guan/USRN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESS: Learning Event-based Semantic Segmentation from Still Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoning Sun, Nico Messikommer, Daniel Gehrig, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving accurate semantic information in challenging high dynamic range
(HDR) and high-speed conditions remains an open challenge for image-based
algorithms due to severe image degradations. Event cameras promise to address
these challenges since they feature a much higher dynamic range and are
resilient to motion blur. Nonetheless, semantic segmentation with event cameras
is still in its infancy which is chiefly due to the novelty of the sensor, and
the lack of high-quality, labeled datasets. In this work, we introduce ESS,
which tackles this problem by directly transferring the semantic segmentation
task from existing labeled image datasets to unlabeled events via unsupervised
domain adaptation (UDA). Compared to existing UDA methods, our approach aligns
recurrent, motion-invariant event embeddings with image embeddings. For this
reason, our method neither requires video data nor per-pixel alignment between
images and events and, crucially, does not need to hallucinate motion from
still images. Additionally, to spur further research in event-based semantic
segmentation, we introduce DSEC-Semantic, the first large-scale event-based
dataset with fine-grained labels. We show that using image labels alone, ESS
outperforms existing UDA approaches, and when combined with event labels, it
even outperforms state-of-the-art supervised approaches on both DDD17 and
DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount
of existing labeled image datasets and paves the way for new and exciting
research directions in new fields previously inaccessible for event cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parametric Scaling of Preprocessing assisted U-net Architecture for
  Improvised Retinal Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kundan Kumar, Sumanshu Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting blood vessels from retinal fundus images plays a decisive role in
diagnosing the progression in pertinent diseases. In medical image analysis,
vessel extraction is a semantic binary segmentation problem, where blood
vasculature needs to be extracted from the background. Here, we present an
image enhancement technique based on the morphological preprocessing coupled
with a scaled U-net architecture. Despite a relatively less number of trainable
network parameters, the scaled version of U-net architecture provides better
performance compare to other methods in the domain. We validated the proposed
method on retinal fundus images from the DRIVE database. A significant
improvement as compared to the other algorithms in the domain, in terms of the
area under ROC curve (>0.9762) and classification accuracy (>95.47%) are
evident from the results. Furthermore, the proposed method is resistant to the
central vessel reflex while sensitive to detect blood vessels in the presence
of background items viz. exudates, optic disc, and fovea.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, ICAIHC-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing EEG Data with Machine and Deep Learning: A Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Avola, Marco Cascio, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti, Marco Raoul Marini, Daniele Pannone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, machine and deep learning techniques are widely used in different
areas, ranging from economics to biology. In general, these techniques can be
used in two ways: trying to adapt well-known models and architectures to the
available data, or designing custom architectures. In both cases, to speed up
the research process, it is useful to know which type of models work best for a
specific problem and/or data type. By focusing on EEG signal analysis, and for
the first time in literature, in this paper a benchmark of machine and deep
learning for EEG signal classification is proposed. For our experiments we used
the four most widespread models, i.e., multilayer perceptron, convolutional
neural network, long short-term memory, and gated recurrent unit, highlighting
which one can be a good starting point for developing EEG classification
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference, 11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra-low Latency Spiking Neural Networks with Spatio-Temporal
  Compression and Synaptic Convolutional Block 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changqing Xu, Yi Liu, Yintang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), as one of the brain-inspired models, has
spatio-temporal information processing capability, low power feature, and high
biological plausibility. The effective spatio-temporal feature makes it
suitable for event streams classification. However, neuromorphic datasets, such
as N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events
into frames with a new higher temporal resolution for event stream
classification, which causes high training and inference latency. In this work,
we proposed a spatio-temporal compression method to aggregate individual events
into a few time steps of synaptic current to reduce the training and inference
latency. To keep the accuracy of SNNs under high compression ratios, we also
proposed a synaptic convolutional block to balance the dramatic change between
adjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with
learnable membrane time constant is introduced to increase its information
processing capability. We evaluate the proposed method for event streams
classification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture
datasets. The experiment results show that our proposed method outperforms the
state-of-the-art accuracy on nearly all datasets, using fewer time steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of Top-hat Transformation for Enhanced Blood Vessel
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tithi Parna Das, Sheetal Praharaj, Sarita Swain, Sumanshu Agarwal, Kundan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the medical domain, different computer-aided diagnosis systems have been
proposed to extract blood vessels from retinal fundus images for the clinical
treatment of vascular diseases. Accurate extraction of blood vessels from the
fundus images using a computer-generated method can help the clinician to
produce timely and accurate reports for the patient suffering from these
diseases. In this article, we integrate top-hat based preprocessing approach
with fine-tuned B-COSFIRE filter to achieve more accurate segregation of blood
vessel pixels from the background. The use of top-hat transformation in the
preprocessing stage enhances the efficacy of the algorithm to extract blood
vessels in presence of structures like fovea, exudates, haemorrhages, etc.
Furthermore, to reduce the false positives, small clusters of blood vessel
pixels are removed in the postprocessing stage. Further, we find that the
proposed algorithm is more efficient as compared to various modern algorithms
reported in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, ICAIHC-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elastica Models for Color Image Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Xue-Cheng Tai, Ron Kimmel, Roland Glowinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One classical approach to regularize color is to tream them as two
dimensional surfaces embedded in a five dimensional spatial-chromatic space. In
this case, a natural regularization term arises as the image surface area.
Choosing the chromatic coordinates as dominating over the spatial ones, the
image spatial coordinates could be thought of as a paramterization of the image
surface manifold in a three dimensional color space. Minimizing the area of the
image manifold leads to the Beltrami flow or mean curvature flow of the image
surface in the 3D color space, while minimizing the elastica of the image
surface yields an additional interesting regularization. Recently, the authors
proposed a color elastica model, which minimizes both the surface area and
elastica of the image manifold. In this paper, we propose to modify the color
elastica and introduce two new models for color image regularization. The
revised measures are motivated by the relations between the color elastica
model, Euler's elastica model and the total variation model for gray level
images. Compared to our previous color elastica model, the new models are
direct extensions of Euler's elastica model to color images. The proposed
models are nonlinear and challenging to minimize. To overcome this difficulty,
two operator-splitting methods are suggested. Specifically, nonlinearities are
decoupled by introducing new vector- and matrix-valued variables. Then, the
minimization problems are converted to solving initial value problems which are
time-discretized by operator splitting. Each subproblem, after splitting
either, has a closed-form solution or can be solved efficiently. The
effectiveness and advantages of the proposed models are demonstrated by
comprehensive experiments. The benefits of incorporating the elastica of the
image surface as regularization terms compared to common alternatives are
empirically validated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion and Volume Maximization-Based Clustering of Highly Mixed
  Hyperspectral Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam L. Polk, Kangning Cui, Robert J. Plemmons, James M. Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images of a scene or object are a rich data source, often
encoding a hundred or more spectral bands of reflectance at each pixel. Despite
being very high-dimensional, these images typically encode latent
low-dimensional structure that can be exploited for material discrimination.
However, due to an inherent trade-off between spectral and spatial resolution,
many hyperspectral images are generated at a coarse spatial scale, and single
pixels may correspond to spatial regions containing multiple materials. This
article introduces the \emph{Diffusion and Volume maximization-based Image
Clustering} (\emph{D-VIC}) algorithm for unsupervised material discrimination.
D-VIC locates cluster modes -- high-density, high-purity pixels in the
hyperspectral image that are far in diffusion distance (a data-dependent
distance metric) from other high-density, high-purity pixels -- and assigns
these pixels unique labels, as these points are meant to exemplify underlying
material structure. Non-modal pixels are labeled according to their diffusion
distance nearest neighbor of higher density and purity that is already labeled.
By directly incorporating pixel purity into its modal and non-modal labeling,
D-VIC upweights pixels that correspond to a spatial region containing just a
single material, yielding more interpretable clusterings. D-VIC is shown to
outperform baseline and comparable state-of-the-art methods in extensive
numerical experiments on a range of hyperspectral images, implying that it is
well-equipped for material discrimination and clustering of these data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GiNGR: Generalized Iterative Non-Rigid Point Cloud and Surface
  Registration Using Gaussian Process Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Madsen, Jonathan Aellen, Andreas Morel-Forster, Thomas Vetter, Marcel Lüthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we unify popular non-rigid registration methods for point sets
and surfaces under our general framework, GiNGR. GiNGR builds upon Gaussian
Process Morphable Models (GPMM) and hence separates modeling the deformation
prior from model adaptation for registration. In addition, it provides
explainable hyperparameters, multi-resolution registration, trivial inclusion
of expert annotation, and the ability to use and combine analytical and
statistical deformation priors. But more importantly, the reformulation allows
for a direct comparison of registration methods. Instead of using a general
solver in the optimization step, we show how Gaussian process regression (GPR)
iteratively can warp a reference onto a target, leading to smooth deformations
following the prior for any dense, sparse, or partial estimated correspondences
in a principled way. We show how the popular CPD and ICP algorithms can be
directly explained with GiNGR. Furthermore, we show how existing algorithms in
the GiNGR framework can perform probabilistic registration to obtain a
distribution of different registrations instead of a single best registration.
This can be used to analyze the uncertainty e.g. when registering partial
observations. GiNGR is publicly available and fully modular to allow for
domain-specific prior construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthStrip: Skull-Stripping for Any Brain Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Hoopes, Jocelyn S. Mora, Adrian V. Dalca, Bruce Fischl, Malte Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The removal of non-brain signal from magnetic resonance imaging (MRI) data,
known as skull-stripping, is an integral component of many neuroimage analysis
streams. Despite their abundance, popular classical skull-stripping methods are
usually tailored to images with specific acquisition properties, namely
near-isotropic resolution and T1-weighted (T1w) MRI contrast, which are
prevalent in research settings. As a result, existing tools tend to adapt
poorly to other image types, such as stacks of thick slices acquired with fast
spin-echo (FSE) MRI that are common in the clinic. While learning-based
approaches for brain extraction have gained traction in recent years, these
methods face a similar burden, as they are only effective for image types seen
during the training procedure. To achieve robust skull-stripping across a
landscape of protocols, we introduce SynthStrip, a rapid, learning-based
brain-extraction tool. By leveraging anatomical segmentations to generate an
entirely synthetic training dataset with anatomies, intensity distributions,
and artifacts that far exceed the realistic range of medical images, SynthStrip
learns to successfully generalize to a variety of real acquired brain images,
removing the need for training data with target contrasts. We demonstrate the
efficacy of SynthStrip for a diverse set of image acquisitions and resolutions
across subject populations, ranging from newborn to adult. We show substantial
improvements in accuracy over popular skull-stripping baselines - all with a
single trained model. Our method and labeled evaluation data are available at
https://w3id.org/synthstrip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, 7 tables, skull stripping, brain extraction,
  image synthesis, MRI contrast agnosticism, deep learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancement of Novel View Synthesis Using Omnidirectional Image
  Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takayuki Hara, Tatsuya Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for synthesizing novel views from a single 360-degree
image based on the neural radiance field (NeRF) . Prior studies rely on the
neighborhood interpolation capability of multi-layer perceptrons to complete
missing regions caused by occlusion and zooming, and this leads to artifacts.
In the proposed method, the input image is reprojected to 360-degree images at
other camera positions, the missing regions of the reprojected images are
completed by a self-supervised trained generative model, and the completed
images are utilized to train the NeRF. Because multiple completed images
contain inconsistencies in 3D, we introduce a method to train NeRF while
dynamically selecting a sparse set of completed images, to reduce the
discrimination error of the synthesized views with real images. Experiments
indicate that the proposed method can synthesize plausible novel views while
preserving the features of the scene for both artificial and real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on
  Synthetic Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Guarnera, Oliver Giudice, Sebastiano Battiato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent style-transfer techniques based on generative architectures are
able to obtain synthetic multimedia contents, or commonly called deepfakes,
with almost no artifacts. Researchers already demonstrated that synthetic
images contain patterns that can determine not only if it is a deepfake but
also the generative architecture employed to create the image data itself.
These traces can be exploited to study problems that have never been addressed
in the context of deepfakes. To this aim, in this paper a first approach to
investigate the image ballistics on deepfake images subject to style-transfer
manipulations is proposed. Specifically, this paper describes a study on
detecting how many times a digital image has been processed by a generative
architecture for style transfer. Moreover, in order to address and study
accurately forensic ballistics on deepfake images, some mathematical properties
of style-transfer operations were investigated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Simultaneous Sparse Approximation with Applications to
  RGB-NIR Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farshad G. Veshki, Sergiy A. Vorobyov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous sparse approximation (SSA) seeks to represent a set of dependent
signals using sparse vectors with identical supports. The SSA model has been
used in various signal and image processing applications involving multiple
correlated input signals. In this paper, we propose algorithms for
convolutional SSA (CSSA) based on the alternating direction method of
multipliers. Specifically, we address the CSSA problem with different sparsity
structures and the convolutional feature learning problem in multimodal
data/signals based on the SSA model. We evaluate the proposed algorithms by
applying them to multimodal and multifocus image fusion problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier Document Restoration for Robust Document Dewarping and
  Recognition <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhui Xue, Zichen Tian, Fangneng Zhan, Shijian Lu, Song Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art document dewarping techniques learn to predict 3-dimensional
information of documents which are prone to errors while dealing with documents
with irregular distortions or large variations in depth. This paper presents
FDRNet, a Fourier Document Restoration Network that can restore documents with
different distortions and improve document recognition in a reliable and
simpler manner. FDRNet focuses on high-frequency components in the Fourier
space that capture most structural information but are largely free of
degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline
transformation which can handle various deformations effectively without
requiring deformation annotations in training. These features allow FDRNet to
learn from a small amount of simply labeled training images, and the learned
model can dewarp documents with complex geometric distortion and recognize the
restored texts accurately. To facilitate document restoration research, we
create a benchmark dataset consisting of over one thousand camera documents
with different types of geometric and photometric distortion. Extensive
experiments show that FDRNet outperforms the state-of-the-art by large margins
on both dewarping and text recognition tasks. In addition, FDRNet requires a
small amount of simply labeled training data and is easy to deploy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Affordance Grounding from Exocentric Images <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affordance grounding, a task to ground (i.e., localize) action possibility
region in objects, which faces the challenge of establishing an explicit link
with object parts due to the diversity of interactive affordance. Human has the
ability that transform the various exocentric interactions to invariant
egocentric affordance so as to counter the impact of interactive diversity. To
empower an agent with such ability, this paper proposes a task of affordance
grounding from exocentric view, i.e., given exocentric human-object interaction
and egocentric object images, learning the affordance knowledge of the object
and transferring it to the egocentric image using only the affordance label as
supervision. To this end, we devise a cross-view knowledge transfer framework
that extracts affordance-specific features from exocentric interactions and
enhances the perception of affordance regions by preserving affordance
correlation. Specifically, an Affordance Invariance Mining module is devised to
extract specific clues by minimizing the intra-class differences originated
from interaction habits in exocentric images. Besides, an Affordance
Co-relation Preserving strategy is presented to perceive and localize
affordance by aligning the co-relation matrix of predicted results between the
two views. Particularly, an affordance grounding dataset named AGD20K is
constructed by collecting and labeling over 20K images from 36 affordance
categories. Experimental results demonstrate that our method outperforms the
representative models in terms of objective metrics and visual quality. Code:
github.com/lhc1224/Cross-View-AG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodedVTR: Codebook-based Sparse Voxel <span class="highlight-title">Transformer</span> with Geometric
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchen Zhao, Niansong Zhang, Xuefei Ning, He Wang, Li Yi, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have gained much attention by outperforming convolutional neural
networks in many 2D vision tasks. However, they are known to have
generalization problems and rely on massive-scale pre-training and
sophisticated training techniques. When applying to 3D tasks, the irregular
data structure and limited data scale add to the difficulty of transformer's
application. We propose CodedVTR (Codebook-based Voxel TRansformer), which
improves data efficiency and generalization ability for 3D sparse voxel
transformers. On the one hand, we propose the codebook-based attention that
projects an attention space into its subspace represented by the combination of
"prototypes" in a learnable codebook. It regularizes attention learning and
improves generalization. On the other hand, we propose geometry-aware
self-attention that utilizes geometric information (geometric pattern, density)
to guide attention learning. CodedVTR could be embedded into existing sparse
convolution-based methods, and bring consistent performance improvements for
indoor and outdoor 3D semantic segmentation tasks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyang Luo, Dunyuan Xu, Hao Chen, Tien-Tsin Wong, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models were frequently reported to learn from shortcuts like
dataset biases. As deep learning is playing an increasingly important role in
the modern healthcare system, it is of great need to combat shortcut learning
in medical data as well as develop unbiased and trustworthy models. In this
paper, we study the problem of developing debiased chest X-ray diagnosis models
from the biased training data without knowing exactly the bias labels. We start
with the observations that the imbalance of bias distribution is one of the key
reasons causing shortcut learning, and the dataset biases are preferred by the
model if they were easier to be learned than the intended features. Based on
these observations, we propose a novel algorithm, pseudo bias-balanced
learning, which first captures and predicts per-sample bias labels via
generalized cross entropy loss and then trains a debiased model using pseudo
bias labels and bias-balanced softmax function. To our best knowledge, we are
pioneered in tackling dataset biases in medical images without explicit
labeling on the bias attributes. We constructed several chest X-ray datasets
with various dataset bias situations and demonstrated with extensive
experiments that our proposed method achieved consistent improvements over
other state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Debias; Shortcut Learning; Chest X-ray</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-Modal</span> Masked <span class="highlight-title">Pre-Train</span>ing for Monocular Panoramic Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Yan, Xiang Li, Kun Wang, Zhenyu Zhang, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we formulate a potentially valuable panoramic depth completion
(PDC) task as panoramic 3D cameras often produce 360{\deg} depth with missing
data in complex scenes. Its goal is to recover dense panoramic depths from raw
sparse ones and panoramic RGB images. To deal with the PDC task, we train a
deep network that takes both depth and image as inputs for the dense panoramic
depth recovery. However, it needs to face a challenging optimization problem of
the network parameters due to its non-convex objective function. To address
this problem, we propose a simple yet effective approach termed M{^3}PT:
multi-modal masked pre-training. Specifically, during pre-training, we
simultaneously cover up patches of the panoramic RGB image and sparse depth by
shared random mask, then reconstruct the sparse depth in the masked regions. To
our best knowledge, it is the first time that we show the effectiveness of
masked pre-training in a multi-modal vision task, instead of the single-modal
task resolved by masked autoencoders (MAE). Different from MAE where
fine-tuning completely discards the decoder part of pre-training, there is no
architectural difference between the pre-training and fine-tuning stages in our
M$^{3}$PT as they only differ in the prediction density, which potentially
makes the transfer learning more convenient and effective. Extensive
experiments verify the effectiveness of M{^3}PT on three panoramic datasets.
Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,
51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.
Codes and pre-trained models are available at
https://github.com/anonymoustbd/MMMPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Location-Free Camouflage Generation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Li, Wei Zhai, Yang Cao, Zheng-jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflage is a common visual phenomenon, which refers to hiding the
foreground objects into the background images, making them briefly invisible to
the human eye. Previous work has typically been implemented by an iterative
optimization process. However, these methods struggle in 1) efficiently
generating camouflage images using foreground and background with arbitrary
structure; 2) camouflaging foreground objects to regions with multiple
appearances (e.g. the junction of the vegetation and the mountains), which
limit their practical application. To address these problems, this paper
proposes a novel Location-free Camouflage Generation Network (LCG-Net) that
fuse high-level features of foreground and background image, and generate
result by one inference. Specifically, a Position-aligned Structure Fusion
(PSF) module is devised to guide structure feature fusion based on the
point-to-point structure similarity of foreground and background, and introduce
local appearance features point-by-point. To retain the necessary identifiable
features, a new immerse loss is adopted under our pipeline, while a background
patch appearance loss is utilized to ensure that the hidden objects look
continuous and natural at regions with multiple appearances. Experiments show
that our method has results as satisfactory as state-of-the-art in the
single-appearance regions and are less likely to be completely invisible, but
far exceed the quality of the state-of-the-art in the multi-appearance regions.
Moreover, our method is hundreds of times faster than previous methods.
Benefitting from the unique advantages of our method, we provide some
downstream applications for camouflage generation, which show its potential.
The related code and dataset will be released at
https://github.com/Tale17/LCG-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinlin Hu, Pascal Fua, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent 6D object pose estimation methods, including unsupervised ones,
require many real training images. Unfortunately, for some applications, such
as those in space or deep under water, acquiring real images, even unannotated,
is virtually impossible. In this paper, we propose a method that can be trained
solely on synthetic images, or optionally using a few additional real ones.
Given a rough pose estimate obtained from a first network, it uses a second
network to predict a dense 2D correspondence field between the image rendered
using the rough pose and the real image and infers the required pose
correction. This approach is much less sensitive to the domain shift between
synthetic and real images than state-of-the-art methods. It performs on par
with methods that require annotated real images for training when not using
any, and outperforms them considerably when using as few as twenty real images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTA: Physical Camouflage Attacks using Differentiable Transformation
  Network <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, Howon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform adversarial attacks in the physical world, many studies have
proposed adversarial camouflage, a method to hide a target object by applying
camouflage patterns on 3D object surfaces. For obtaining optimal physical
adversarial camouflage, previous studies have utilized the so-called neural
renderer, as it supports differentiability. However, existing neural renderers
cannot fully represent various real-world transformations due to a lack of
control of scene parameters compared to the legacy photo-realistic renderers.
In this paper, we propose the Differentiable Transformation Attack (DTA), a
framework for generating a robust physical adversarial pattern on a target
object to camouflage it against object detection models with a wide range of
transformations. It utilizes our novel Differentiable Transformation Network
(DTN), which learns the expected transformation of a rendered object when the
texture is changed while preserving the original properties of the target
object. Using our attack framework, an adversary can gain both the advantages
of the legacy photo-realistic renderers including various physical-world
transformations and the benefit of white-box access by offering
differentiability. Our experiments show that our camouflaged 3D vehicles can
successfully evade state-of-the-art object detection models in the
photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our
demonstration on a scaled Tesla Model 3 proves the applicability and
transferability of our method to the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laneformer: Object-aware Row-Column <span class="highlight-title">Transformer</span>s for Lane Detection <span class="chip">AAAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Han, Xiajun Deng, Xinyue Cai, Zhen Yang, Hang Xu, Chunjing Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Laneformer, a conceptually simple yet powerful transformer-based
architecture tailored for lane detection that is a long-standing research topic
for visual perception in autonomous driving. The dominant paradigms rely on
purely CNN-based architectures which often fail in incorporating relations of
long-range lane points and global contexts induced by surrounding objects
(e.g., pedestrians, vehicles). Inspired by recent advances of the transformer
encoder-decoder architecture in various vision tasks, we move forwards to
design a new end-to-end Laneformer architecture that revolutionizes the
conventional transformers into better capturing the shape and semantic
characteristics of lanes, with minimal overhead in latency. First, coupling
with deformable pixel-wise self-attention in the encoder, Laneformer presents
two new row and column self-attention operations to efficiently mine point
context along with the lane shapes. Second, motivated by the appearing objects
would affect the decision of predicting lane segments, Laneformer further
includes the detected object instances as extra inputs of multi-head attention
blocks in the encoder and decoder to facilitate the lane point detection by
sensing semantic contexts. Specifically, the bounding box locations of objects
are added into Key module to provide interaction with each pixel and query
while the ROI-aligned features are inserted into Value module. Extensive
experiments demonstrate our Laneformer achieves state-of-the-art performances
on CULane benchmark, in terms of 77.1% F1 score. We hope our simple and
effective Laneformer will serve as a strong baseline for future research in
self-attention models for lane detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Cross-Modal</span> Perceptionist: Can Face Geometry be Gleaned from Voices? <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cho-Ying Wu, Chin-Cheng Hsu, Ulrich Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work digs into a root question in human perception: can face geometry be
gleaned from one's voices? Previous works that study this question only adopt
developments in image synthesis and convert voices into face images to show
correlations, but working on the image domain unavoidably involves predicting
attributes that voices cannot hint, including facial textures, hairstyles, and
backgrounds. We instead investigate the ability to reconstruct 3D faces to
concentrate on only geometry, which is much more physiologically grounded. We
propose our analysis framework, Cross-Modal Perceptionist, under both
supervised and unsupervised learning. First, we construct a dataset,
Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,
making supervised learning possible. Second, we use a knowledge distillation
mechanism to study whether face geometry can still be gleaned from voices
without paired voices and 3D face data under limited availability of 3D face
scans. We break down the core question into four parts and perform visual and
numerical analyses as responses to the core question. Our findings echo those
in physiology and neuroscience about the correlation between voices and facial
structures. The work provides future human-centric cross-modal learning with
explainable foundations. See our project page:
https://choyingw.github.io/works/Voice2Mesh/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022. Project page:
  https://choyingw.github.io/works/Voice2Mesh/index.html. This version
  supersedes arXiv:2104.10299</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared
  Control on the Hannes Prosthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Vasile, Elisa Maiettini, Giulia Pasquale, Astrid Florio, Nicolò Boccardo, Lorenzo Natale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of object grasping with a prosthetic hand capable of
multiple grasp types. In this setting, communicating the intended grasp type
often requires a high user cognitive load which can be reduced adopting shared
autonomy frameworks. Among these, so-called eye-in-hand systems automatically
control the hand aperture and pre-shaping before the grasp, based on visual
input coming from a camera on the wrist. In this work, we present an
eye-in-hand learning-based approach for hand pre-shape classification from RGB
sequences. In order to reduce the need for tedious data collection sessions for
training the system, we devise a pipeline for rendering synthetic visual
sequences of hand trajectories for the purpose. We tackle the peculiarity of
the eye-in-hand setting by means of a model for the human arm trajectories,
with domain randomization over relevant visual elements. We develop a
sensorized setup to acquire real human grasping sequences for benchmarking and
show that, compared on practical use cases, models trained with our synthetic
dataset achieve better generalization performance than models trained on real
data. We finally integrate our model on the Hannes prosthetic hand and show its
practical effectiveness. Our code, real and synthetic datasets will be released
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased
  Scene Graph Generation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingning Dong, Tian Gan, Xuemeng Song, Jianlong Wu, Yuan Cheng, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Graph Generation, which generally follows a regular encoder-decoder
pipeline, aims to first encode the visual contents within the given image and
then parse them into a compact summary graph. Existing SGG approaches generally
not only neglect the insufficient modality fusion between vision and language,
but also fail to provide informative predicates due to the biased relationship
predictions, leading SGG far from practical. Towards this end, in this paper,
we first present a novel Stacked Hybrid-Attention network, which facilitates
the intra-modal refinement as well as the inter-modal interaction, to serve as
the encoder. We then devise an innovative Group Collaborative Learning strategy
to optimize the decoder. Particularly, based upon the observation that the
recognition capability of one classifier is limited towards an extremely
unbalanced dataset, we first deploy a group of classifiers that are expert in
distinguishing different subsets of classes, and then cooperatively optimize
them from two aspects to promote the unbiased SGG. Experiments conducted on VG
and GQA datasets demonstrate that, we not only establish a new state-of-the-art
in the unbiased metric, but also nearly double the performance compared with
two baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022, the code is available at
  https://github.com/dongxingning/SHA-GCL-for-SGG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Consistency from High-quality Pseudo-labels for Weakly
  Supervised Object Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangbo Sun, Jie Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo-supervised learning methods have been shown to be effective for weakly
supervised object localization tasks. However, the effectiveness depends on the
powerful regularization ability of deep neural networks. Based on the
assumption that the localization network should have similar location
predictions on different versions of the same image, we propose a two-stage
approach to learn more consistent localization. In the first stage, we propose
a mask-based pseudo label generator algorithm, and use the pseudo-supervised
learning method to initialize an object localization network. In the second
stage, we propose a simple and effective method for evaluating the confidence
of pseudo-labels based on classification discrimination, and by learning
consistency from high-quality pseudo-labels, we further refine the localization
network to get better localization performance. Experimental results show that
our proposed approach achieves excellent performance in three benchmark
datasets including CUB-200-2011, ImageNet-1k and Tiny-ImageNet, which
demonstrates its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three things everyone should know about Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, Hervé Jégou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After their initial success in natural language processing, transformer
architectures have rapidly gained traction in computer vision, providing
state-of-the-art results for tasks such as image classification, detection,
segmentation, and video analysis. We offer three insights based on simple and
easy to implement variants of vision transformers. (1) The residual layers of
vision transformers, which are usually processed sequentially, can to some
extent be processed efficiently in parallel without noticeably affecting the
accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to
adapt vision transformers to a higher resolution and to other classification
tasks. This saves compute, reduces the peak memory consumption at fine-tuning
time, and allows sharing the majority of weights across tasks. (3) Adding
MLP-based patch pre-processing layers improves Bert-like self-supervised
training based on patch masking. We evaluate the impact of these design choices
using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test
set. Transfer performance is measured across six smaller datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust 2D Convolution for Reliable Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lida Li, Shuai Li, Kun Wang, Xiangchu Feng, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D convolution (Conv2d), which is responsible for extracting features from
the input image, is one of the key modules of a convolutional neural network
(CNN). However, Conv2d is vulnerable to image corruptions and adversarial
samples. It is an important yet rarely investigated problem that whether we can
design a more robust alternative of Conv2d for more reliable feature
extraction. In this paper, inspired by the recently developed learnable sparse
transform that learns to convert the CNN features into a compact and sparse
latent space, we design a novel building block, denoted by RConv-MK, to
strengthen the robustness of extracted convolutional features. Our method
leverages a set of learnable kernels of different sizes to extract features at
different frequencies and employs a normalized soft thresholding operator to
adaptively remove noises and trivial features at different corruption levels.
Extensive experiments on clean images, corrupted images as well as adversarial
samples validate the effectiveness of the proposed robust module for reliable
visual recognition. The source codes are enclosed in the submission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Sparse Fuse Dense: Towards High Quality 3D Detection with Depth
  Completion <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng, Haifeng Liu, <span class="highlight-author">Deng Cai</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current LiDAR-only 3D detection methods inevitably suffer from the sparsity
of point clouds. Many multi-modal methods are proposed to alleviate this issue,
while different representations of images and point clouds make it difficult to
fuse them, resulting in suboptimal performance. In this paper, we present a
novel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo
point clouds generated from depth completion to tackle the issues mentioned
above. Different from prior works, we propose a new RoI fusion strategy 3D-GAF
(3D Grid-wise Attentive Fusion) to make fuller use of information from
different types of point clouds. Specifically, 3D-GAF fuses 3D RoI features
from the couple of point clouds in a grid-wise attentive way, which is more
fine-grained and more precise. In addition, we propose a SynAugment
(Synchronized Augmentation) to enable our multi-modal framework to utilize all
data augmentation approaches tailored to LiDAR-only methods. Lastly, we
customize an effective and efficient feature extractor CPConv (Color Point
Convolution) for pseudo point clouds. It can explore 2D image features and 3D
geometric features of pseudo point clouds simultaneously. Our method holds the
highest entry on the KITTI car 3D object detection leaderboard, demonstrating
the effectiveness of our SFD. Code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Class-Modelling for Decentralized Source Attribution of
  GAN-Generated Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon B. G. Khoo, Chern Hong Lim, Raphael C. -W. Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GAN-generated deepfakes as a genre of digital images are gaining ground as
both catalysts of artistic expression and malicious forms of deception,
therefore demanding systems to enforce and accredit their ethical use. Existing
techniques for the source attribution of synthetic images identify subtle
intrinsic fingerprints using multiclass classification neural nets limited in
functionality and scalability. Hence, we redefine the deepfake detection and
source attribution problems as a series of related binary classification tasks.
We leverage transfer learning to rapidly adapt forgery detection networks for
multiple independent attribution problems, by proposing a semi-decentralized
modular design to solve them simultaneously and efficiently. Class activation
mapping is also demonstrated as an effective means of feature localization for
model interpretation. Our models are determined via experimentation to be
competitive with current benchmarks, and capable of decent performance on human
portraits in ideal conditions. Decentralized fingerprint-based attribution is
found to retain validity in the presence of novel sources, but is more
susceptible to type II errors that intensify with image perturbations and
attributive uncertainty. We describe both our conceptual framework and model
prototypes for further enhancement when investigating the technical limits of
reactive deepfake attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures. Code:
  https://github.com/quarxilon/Generator_Attribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContrastMask: <span class="highlight-title">Contrastive Learning</span> to Segment Every Thing <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehui Wang, Kai Zhao, Ruixin Zhang, Shouhong Ding, Yan Wang, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially-supervised instance segmentation is a task which requests
segmenting objects from novel unseen categories via learning on limited seen
categories with annotated masks thus eliminating demands of heavy annotation
burden. The key to addressing this task is to build an effective class-agnostic
mask segmentation model. Unlike previous methods that learn such models only on
seen categories, in this paper, we propose a new method, named ContrastMask,
which learns a mask segmentation model on both seen and unseen categories under
a unified pixel-level contrastive learning framework. In this framework,
annotated masks of seen categories and pseudo masks of unseen categories serve
as a prior for contrastive learning, where features from the mask regions
(foreground) are pulled together, and are contrasted against those from the
background, and vice versa. Through this framework, feature discrimination
between foreground and background is largely improved, facilitating learning of
the class-agnostic mask segmentation model. Exhaustive experiments on the COCO
dataset demonstrate the superiority of our method, which outperforms previous
state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local-Global Context Aware <span class="highlight-title">Transformer</span> for Language-Guided Video
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Liang, Wenguan Wang, Tianfei Zhou, Jiaxu Miao, Yawei Luo, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the task of language-guided video segmentation (LVS). Previous
algorithms mostly adopt 3D CNNs to learn video representation, struggling to
capture long-term context and easily suffering from visual-linguistic
misalignment. In light of this, we present Locater (local-global context aware
Transformer), which augments the Transformer architecture with a finite memory
so as to query the entire video with the language expression in an efficient
manner. The memory is designed to involve two components -- one for
persistently preserving global video content, and one for dynamically gathering
local temporal context and segmentation history. Based on the memorized
local-global context and the particular content of each frame, Locater
holistically and flexibly comprehends the expression as an adaptive query
vector for each frame. The vector is used to query the corresponding frame for
mask generation. The memory also allows Locater to process videos with linear
time complexity and constant size memory, while Transformer-style
self-attention computation scales quadratically with sequence length. To
thoroughly examine the visual grounding capability of LVS models, we contribute
a new LVS dataset, A2D-S+, which is built upon A2D-S dataset but poses
increased challenges in disambiguating among similar objects. Experiments on
three LVS datasets and our A2D-S+ show that Locater outperforms previous
state-of-the-arts. Further, our Locater based solution achieved the 1st place
in the Referring Video Object Segmentation Track of the 3rd Large-scale Video
Object Segmentation Challenge. Our code and dataset are available at:
https://github.com/leonnnop/Locater
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data: https://github.com/leonnnop/Locater</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Completing Partial Point Clouds with Outliers by Collaborative
  Completion and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changfeng Ma, Yang Yang, Jie Guo, Chongjun Wang, Yanwen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing point cloud completion methods are only applicable to partial
point clouds without any noises and outliers, which does not always hold in
practice. We propose in this paper an end-to-end network, named CS-Net, to
complete the point clouds contaminated by noises or containing outliers. In our
CS-Net, the completion and segmentation modules work collaboratively to promote
each other, benefited from our specifically designed cascaded structure. With
the help of segmentation, more clean point cloud is fed into the completion
module. We design a novel completion decoder which harnesses the labels
obtained by segmentation together with FPS to purify the point cloud and
leverages KNN-grouping for better generation. The completion and segmentation
modules work alternately share the useful information from each other to
gradually improve the quality of prediction. To train our network, we build a
dataset to simulate the real case where incomplete point clouds contain
outliers. Our comprehensive experiments and comparisons against
state-of-the-art completion methods demonstrate our superiority. We also
compare with the scheme of segmentation followed by completion and their
end-to-end fusion, which also proves our efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach
  to Continuous Image Transition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang, Peiran Ren, Xuansong Xie, Xiansheng Hua, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video frame interpolation (VFI) aims to improve the temporal resolution of a
video sequence. Most of the existing deep learning based VFI methods adopt
off-the-shelf optical flow algorithms to estimate the bidirectional flows and
interpolate the missing frames accordingly. Though having achieved a great
success, these methods require much human experience to tune the bidirectional
flows and often generate unpleasant results when the estimated flows are not
accurate. In this work, we rethink the VFI problem and formulate it as a
continuous image transition (CIT) task, whose key issue is to transition an
image from one space to another space continuously. More specifically, we learn
to implicitly decouple the images into a translatable flow space and a
non-translatable feature space. The former depicts the translatable states
between the given images, while the later aims to reconstruct the intermediate
features that cannot be directly translated. In this way, we can easily perform
image interpolation in the flow space and intermediate image synthesis in the
feature space, obtaining a CIT model. The proposed space decoupled learning
(SDL) approach is simple to implement, while it provides an effective framework
to a variety of CIT problems beyond VFI, such as style transfer and image
morphing. Our extensive experiments on a variety of CIT tasks demonstrate the
superiority of SDL to existing methods. The source code and models can be found
at \url{https://github.com/yangxy/SDL}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqiao Li, Xiaotao Liu, Jian Zhao, Furao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have been proven to be vulnerable to adversarial
examples. A special branch of adversarial examples, namely sparse adversarial
examples, can fool the target DNNs by perturbing only a few pixels. However,
many existing sparse adversarial attacks use heuristic methods to select the
pixels to be perturbed, and regard the pixel selection and the adversarial
attack as two separate steps. From the perspective of neural network pruning,
we propose a novel end-to-end sparse adversarial attack method, namely
AutoAdversary, which can find the most important pixels automatically by
integrating the pixel selection into the adversarial attack. Specifically, our
method utilizes a trainable neural network to generate a binary mask for the
pixel selection. After jointly optimizing the adversarial perturbation and the
neural network, only the pixels corresponding to the value 1 in the mask are
perturbed. Experiments demonstrate the superiority of our proposed method over
several state-of-the-art methods. Furthermore, since AutoAdversary does not
require a heuristic pixel selection process, it does not slow down excessively
as other methods when the image size increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot peels banana with goal-conditioned dual-action deep imitation
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A long-horizon dexterous robot manipulation task of deformable objects, such
as banana peeling, is problematic because of difficulties in object modeling
and a lack of knowledge about stable and dexterous manipulation skills. This
paper presents a goal-conditioned dual-action deep imitation learning (DIL)
which can learn dexterous manipulation skills using human demonstration data.
Previous DIL methods map the current sensory input and reactive action, which
easily fails because of compounding errors in imitation learning caused by
recurrent computation of actions. The proposed method predicts reactive action
when the precise manipulation of the target object is required (local action)
and generates the entire trajectory when the precise manipulation is not
required. This dual-action formulation effectively prevents compounding error
with the trajectory-based global action while respond to unexpected changes in
the target object with the reactive local action. Furthermore, in this
formulation, both global/local actions are conditioned by a goal state which is
defined as the last step of each subtask, for robust policy prediction. The
proposed method was tested in the real dual-arm robot and successfully
accomplished the banana peeling task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic
  Segmentation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihuang Li, Shuai Li, Chenhang He, Yabin Zhang, Xu Jia, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation aims to learn a model with the
supervision of source domain data, and produce satisfactory dense predictions
on unlabeled target domain. One popular solution to this challenging task is
self-training, which selects high-scoring predictions on target samples as
pseudo labels for training. However, the produced pseudo labels often contain
much noise because the model is biased to source domain as well as majority
categories. To address the above issues, we propose to directly explore the
intrinsic pixel distributions of target domain data, instead of heavily relying
on the source domain. Specifically, we simultaneously cluster pixels and
rectify pseudo labels with the obtained cluster assignments. This process is
done in an online fashion so that pseudo labels could co-evolve with the
segmentation model without extra training rounds. To overcome the class
imbalance problem on long-tailed categories, we employ a distribution alignment
technique to enforce the marginal class distribution of cluster assignments to
be close to that of pseudo labels. The proposed method, namely Class-balanced
Pixel-level Self-Labeling (CPSL), improves the segmentation performance on
target domain over state-of-the-arts by a large margin, especially on
long-tailed categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Deep Networks Transfer Invariances Across Classes? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To generalize well, classifiers must learn to be invariant to nuisance
transformations that do not alter an input's class. Many problems have
"class-agnostic" nuisance transformations that apply similarly to all classes,
such as lighting and background changes for image classification. Neural
networks can learn these invariances given sufficient data, but many real-world
datasets are heavily class imbalanced and contain only a few examples for most
of the classes. We therefore pose the question: how well do neural networks
transfer class-agnostic invariances learned from the large classes to the small
ones? Through careful experimentation, we observe that invariance to
class-agnostic transformations is still heavily dependent on class size, with
the networks being much less invariant on smaller classes. This result holds
even when using data balancing techniques, and suggests poor invariance
transfer across classes. Our results provide one explanation for why
classifiers generalize poorly on unbalanced and long-tailed distributions.
Based on this analysis, we show how a generative approach for learning the
nuisance transformations can help transfer invariances across classes and
improve performance on a set of imbalanced image classification benchmarks.
Source code for our experiments is available at
https://github.com/AllanYangZhou/generative-invariance-transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning with Mutual <span class="highlight-title">Distillation</span> for Monocular Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongbeom Baek, Gyeongnyeon Kim, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a semi-supervised learning framework for monocular depth
estimation. Compared to existing semi-supervised learning methods, which
inherit limitations of both sparse supervised and unsupervised loss functions,
we achieve the complementary advantages of both loss functions, by building two
separate network branches for each loss and distilling each other through the
mutual distillation loss function. We also present to apply different data
augmentation to each branch, which improves the robustness. We conduct
experiments to demonstrate the effectiveness of our framework over the latest
methods and provide extensive ablation studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Series Photo Selection via Multi-view Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Huang, Lu Zhang, Yongshun Gong, Jian Zhang, Xiushan Nie, Yilong Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Series photo selection (SPS) is an important branch of the image aesthetics
quality assessment, which focuses on finding the best one from a series of
nearly identical photos. While a great progress has been observed, most of the
existing SPS approaches concentrate solely on extracting features from the
original image, neglecting that multiple views, e.g, saturation level, color
histogram and depth of field of the image, will be of benefit to successfully
reflecting the subtle aesthetic changes. Taken multi-view into consideration,
we leverage a graph neural network to construct the relationships between
multi-view features. Besides, multiple views are aggregated with an
adaptive-weight self-attention module to verify the significance of each view.
Finally, a siamese network is proposed to select the best one from a series of
nearly identical photos. Experimental results demonstrate that our model
accomplish the highest success rates compared with competitive methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distortion-Tolerant Monocular Depth Estimation On Omnidirectional Images
  Using Dual-cubemap <span class="chip">ICME2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Shen, Chunyu Lin, Lang Nie, Kang Liao, Yao zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the depth of omnidirectional images is more challenging than that
of normal field-of-view (NFoV) images because the varying distortion can
significantly twist an object's shape. The existing methods suffer from
troublesome distortion while estimating the depth of omnidirectional images,
leading to inferior performance. To reduce the negative impact of the
distortion influence, we propose a distortion-tolerant omnidirectional depth
estimation algorithm using a dual-cubemap. It comprises two modules:
Dual-Cubemap Depth Estimation (DCDE) module and Boundary Revision (BR) module.
In DCDE module, we present a rotation-based dual-cubemap model to estimate the
accurate NFoV depth, reducing the distortion at the cost of boundary
discontinuity on omnidirectional depths. Then a boundary revision module is
designed to smooth the discontinuous boundaries, which contributes to the
precise and visually continuous omnidirectional depths. Extensive experiments
demonstrate the superiority of our method over other state-of-the-art
solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME2021, poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual Weighting Label Assignment Scheme for Object Detection <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Li, Chenhang He, Ruihuang Li, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label assignment (LA), which aims to assign each training sample a positive
(pos) and a negative (neg) loss weight, plays an important role in object
detection. Existing LA methods mostly focus on the design of pos weighting
function, while the neg weight is directly derived from the pos weight. Such a
mechanism limits the learning capacity of detectors. In this paper, we explore
a new weighting paradigm, termed dual weighting (DW), to specify pos and neg
weights separately. We first identify the key influential factors of pos/neg
weights by analyzing the evaluation metrics in object detection, and then
design the pos and neg weighting functions based on them. Specifically, the pos
weight of a sample is determined by the consistency degree between its
classification and localization scores, while the neg weight is decomposed into
two terms: the probability that it is a neg sample and its importance
conditioned on being a neg sample. Such a weighting strategy offers greater
flexibility to distinguish between important and less important samples,
resulting in a more effective object detector. Equipped with the proposed DW
method, a single FCOS-ResNet-50 detector can reach 41.5% mAP on COCO under 1x
schedule, outperforming other existing LA methods. It consistently improves the
baselines on COCO by a large margin under various backbones without bells and
whistles. Code is available at https://github.com/strongwolf/DW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REALY: Rethinking the Evaluation of 3D Face Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenghao Chai, Haoxian Zhang, Jing Ren, Di Kang, Zhengzhuo Xu, Xuefei Zhe, Chun Yuan, Linchao Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of 3D face reconstruction results typically relies on a rigid
shape alignment between the estimated 3D model and the ground-truth scan. We
observe that aligning two shapes with different reference points can largely
affect the evaluation results. This poses difficulties for precisely diagnosing
and improving a 3D face reconstruction method. In this paper, we propose a
novel evaluation approach with a new benchmark REALY, consists of 100 globally
aligned face scans with accurate facial keypoints, high-quality region masks,
and topology-consistent meshes. Our approach performs region-wise shape
alignment and leads to more accurate, bidirectional correspondences during
computing the shape errors. The fine-grained, region-wise evaluation results
provide us detailed understandings about the performance of state-of-the-art 3D
face reconstruction methods. For example, our experiments on single-image based
reconstruction methods reveal that DECA performs the best on nose regions,
while GANFit performs better on cheek regions. Besides, a new and high-quality
3DMM basis, HIFI3D++, is further derived using the same procedure as we
construct REALY to align and retopologize several 3D face datasets. We will
release REALY, HIFI3D++, and our new evaluation pipeline at
https://realy3dface.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://realy3dface.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the optimization process for <span class="highlight-title">self-supervised</span> model-driven MRI
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Huang, Cheng Li, Wenxin Fan, Yongjin Zhou, Qiegen Liu, Hairong Zheng, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality images from undersampled measurements is critical for
accelerated MRI reconstruction. Recently, various supervised deep
learning-based MRI reconstruction methods have been developed. Despite the
achieved promising performances, these methods require fully sampled reference
data, the acquisition of which is resource-intensive and time-consuming.
Self-supervised learning has emerged as a promising solution to alleviate the
reliance on fully sampled datasets. However, existing self-supervised methods
suffer from reconstruction errors due to the insufficient constraint enforced
on the non-sampled data points and the error accumulation happened alongside
the iterative image reconstruction process for model-driven deep learning
reconstrutions. To address these challenges, we propose K2Calibrate, a K-space
adaptation strategy for self-supervised model-driven MR reconstruction
optimization. By iteratively calibrating the learned measurements, K2Calibrate
can reduce the network's reconstruction deterioration caused by statistically
dependent noise. Extensive experiments have been conducted on the open-source
dataset FastMRI, and K2Calibrate achieves better results than five
state-of-the-art methods. The proposed K2Calibrate is plug-and-play and can be
easily integrated with different model-driven deep learning reconstruction
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deterministic Bridge Regression for Compressive Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kar-Ann Toh, Giuseppe Molteni, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pattern classification with compact representation is an important component
in machine intelligence. In this work, an analytic bridge solution is proposed
for compressive classification. The proposal has been based upon solving a
penalized error formulation utilizing an approximated $\ell_p$-norm. The
solution comes in a primal form for over-determined systems and in a dual form
for under-determined systems. While the primal form is suitable for problems of
low dimension with large data samples, the dual form is suitable for problems
of high dimension but with a small number of data samples. The solution has
also been extended for problems with multiple classification outputs. Numerical
studies based on simulated and real-world data validated the effectiveness of
the proposed solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial
  Attention <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengheng Deng, Zhihao Liang, Lin Sun, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting objects from LiDAR point clouds is of tremendous significance in
autonomous driving. In spite of good progress, accurate and reliable 3D
detection is yet to be achieved due to the sparsity and irregularity of LiDAR
point clouds. Among existing strategies, multi-view methods have shown great
promise by leveraging the more comprehensive information from both bird's eye
view (BEV) and range view (RV). These multi-view methods either refine the
proposals predicted from single view via fused features, or fuse the features
without considering the global spatial context; their performance is limited
consequently. In this paper, we propose to adaptively fuse multi-view features
in a global spatial context via Dual Cross-VIew SpaTial Attention (VISTA). The
proposed VISTA is a novel plug-and-play fusion module, wherein the multi-layer
perceptron widely adopted in standard attention modules is replaced with a
convolutional one. Thanks to the learned attention mechanism, VISTA can produce
fused features of high quality for prediction of proposals. We decouple the
classification and regression tasks in VISTA, and an additional constraint of
attention variance is applied that enables the attention module to focus on
specific targets instead of generic points. We conduct thorough experiments on
the benchmarks of nuScenes and Waymo; results confirm the efficacy of our
designs. At the time of submission, our method achieves 63.0% in overall mAP
and 69.8% in NDS on the nuScenes benchmark, outperforming all published methods
by up to 24% in safety-crucial categories such as cyclist. The source code in
PyTorch is available at https://github.com/Gorilla-Lab-SCUT/VISTA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Contextualization for Video Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbin Hao, Hao Zhang, Chong-Wah Ngo, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning discriminative representation from the complex spatio-temporal
dynamic space is essential for video recognition. On top of those stylized
spatio-temporal computational units, further refining the learnt feature with
axial contexts is demonstrated to be promising in achieving this goal. However,
previous works generally focus on utilizing a single kind of contexts to
calibrate entire feature channels and could hardly apply to deal with diverse
video activities. The problem can be tackled by using pair-wise spatio-temporal
attentions to recompute feature response with cross-axis contexts at the
expense of heavy computations. In this paper, we propose an efficient feature
refinement method that decomposes the feature channels into several groups and
separately refines them with different axial contexts in parallel. We refer
this lightweight feature calibration as group contextualization (GC).
Specifically, we design a family of efficient element-wise calibrators, i.e.,
ECal-G/S/T/L, where their axial contexts are information dynamics aggregated
from other axes either globally or locally, to contextualize feature channel
groups. The GC module can be densely plugged into each residual layer of the
off-the-shelf video networks. With little computational overhead, consistent
improvement is observed when plugging in GC on different networks. By utilizing
calibrators to embed feature with four different kinds of contexts in parallel,
the learnt representation is expected to be more resilient to diverse types of
activities. On videos with rich temporal variations, empirically GC can boost
the performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the
state-of-the-art video networks. Code is available at
https://github.com/haoyanbin918/Group-Contextualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Geometric Detail Recovery via Implicit Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Ren, Alexandros Lattas, Baris Gecer, Jiankang Deng, Chao Ma, Xiaokang Yang, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a dense 3D model with fine-scale details from a single facial image
is highly challenging and ill-posed. To address this problem, many approaches
fit smooth geometries through facial prior while learning details as additional
displacement maps or personalized basis. However, these techniques typically
require vast datasets of paired multi-view data or 3D scans, whereas such
datasets are scarce and expensive. To alleviate heavy data dependency, we
present a robust texture-guided geometric detail recovery approach using only a
single in-the-wild facial image. More specifically, our method combines
high-quality texture completion with the powerful expressiveness of implicit
surfaces. Initially, we inpaint occluded facial parts, generate complete
textures, and build an accurate multi-view dataset of the same subject. In
order to estimate the detailed geometry, we define an implicit signed distance
function and employ a physically-based implicit renderer to reconstruct fine
geometric details from the generated multi-view images. Our method not only
recovers accurate facial details but also decomposes normals, albedos, and
shading parts in a self-supervised way. Finally, we register the implicit shape
details to a 3D Morphable Model template, which can be used in traditional
modeling and rendering pipelines. Extensive experiments demonstrate that the
proposed approach can reconstruct impressive facial details from a single
image, especially when compared with state-of-the-art methods trained on large
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Intensification for Sign Language Generation: A Computational
  Approach <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert İnan, Yang Zhong, Sabit Hassan, Lorna Quandt, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end sign language generation models do not accurately represent the
prosody in sign language. A lack of temporal and spatial variations leads to
poor-quality generated presentations that confuse human interpreters. In this
paper, we aim to improve the prosody in generated sign languages by modeling
intensification in a data-driven manner. We present different strategies
grounded in linguistics of sign language that inform how intensity modifiers
can be represented in gloss annotations. To employ our strategies, we first
annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,
with different levels of intensification. We then use a supervised intensity
tagger to extend the annotated dataset and obtain labels for the remaining
portion of it. This enhanced dataset is then used to train state-of-the-art
transformer models for sign language generation. We find that our efforts in
intensification modeling yield better results when evaluated with automatic
metrics. Human evaluation also indicates a higher preference of the videos
generated using our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, Findings of the Association for Computational Linguistics:
  ACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A workflow for segmenting soil and plant X-ray CT images with deep
  learning in Googles Colaboratory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devin A. Rippner, Pranav Raja, J. Mason Earles, Alexander Buchko, Mina Momayyezi, Fiona Duong, Dilworth Parkinson, Elizabeth Forrestel, Ken Shackel, Andrew J. McElrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray micro-computed tomography (X-ray microCT) has enabled the
characterization of the properties and processes that take place in plants and
soils at the micron scale. Despite the widespread use of this advanced
technique, major limitations in both hardware and software limit the speed and
accuracy of image processing and data analysis. Recent advances in machine
learning, specifically the application of convolutional neural networks to
image analysis, have enabled rapid and accurate segmentation of image data.
Yet, challenges remain in applying convolutional neural networks to the
analysis of environmentally and agriculturally relevant images. Specifically,
there is a disconnect between the computer scientists and engineers, who build
these AI/ML tools, and the potential end users in agricultural research, who
may be unsure of how to apply these tools in their work. Additionally, the
computing resources required for training and applying deep learning models are
unique, more common to computer gaming systems or graphics design work, than to
traditional computational systems. To navigate these challenges, we developed a
modular workflow for applying convolutional neural networks to X-ray microCT
images, using low-cost resources in Googles Colaboratory web application. Here
we present the results of the workflow, illustrating how parameters can be
optimized to achieve best results using example scans from walnut leaves,
almond flower buds, and a soil aggregate. We expect that this framework will
accelerate the adoption and use of emerging deep learning techniques within the
plant and soil sciences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All You Need is RAW: Defending Against Adversarial Attacks with Camera
  Image Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Bo Dong, Felix Heide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing neural networks for computer vision tasks are vulnerable to
adversarial attacks: adding imperceptible perturbations to the input images can
fool these methods to make a false prediction on an image that was correctly
predicted without the perturbation. Various defense methods have proposed
image-to-image mapping methods, either including these perturbations in the
training process or removing them in a preprocessing denoising step. In doing
so, existing methods often ignore that the natural RGB images in today's
datasets are not captured but, in fact, recovered from RAW color filter array
captures that are subject to various degradations in the capture. In this work,
we exploit this RAW data distribution as an empirical prior for adversarial
defense. Specifically, we proposed a model-agnostic adversarial defensive
method, which maps the input RGB images to Bayer RAW space and back to output
RGB using a learned camera image signal processing (ISP) pipeline to eliminate
potential adversarial patterns. The proposed method acts as an off-the-shelf
preprocessing module and, unlike model-specific adversarial training methods,
does not require adversarial images to train. As a result, the method
generalizes to unseen tasks without additional retraining. Experiments on
large-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g.,
classification, semantic segmentation, object detection) validate that the
method significantly outperforms existing methods across task domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multiscale spatiotemporal approach for smallholder irrigation
  detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terence Conlon, Christopher Small, Vijay Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In presenting an irrigation detection methodology that leverages multiscale
satellite imagery of vegetation abundance, this paper introduces a process to
supplement limited ground-collected labels and ensure classifier applicability
in an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced
Vegetation Index (EVI) timeseries characterizes native vegetation phenologies
at regional scale to provide the basis for a continuous phenology map that
guides supplementary label collection over irrigated and non-irrigated
agriculture. Subsequently, validated dry season greening and senescence cycles
observed in 10m Sentinel-2 imagery are used to train a suite of classifiers for
automated detection of potential smallholder irrigation. Strategies to improve
model robustness are demonstrated, including a method of data augmentation that
randomly shifts training samples; and an assessment of classifier types that
produce the best performance in withheld target regions. The methodology is
applied to detect smallholder irrigation in two states in the Ethiopian
highlands, Tigray and Amhara. Results show that a transformer-based neural
network architecture allows for the most robust prediction performance in
withheld regions, followed closely by a CatBoost random forest model. Over
withheld ground-collection survey labels, the transformer-based model achieves
96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated
samples. Over a larger set of samples independently collected via the
introduced method of label supplementation, non-irrigated and irrigated labels
are predicted with 98.3% and 95.5% accuracy, respectively. The detection model
is then deployed over Tigray and Amhara, revealing crop rotation patterns and
year-over-year irrigated area change. Predictions suggest that irrigated area
in these two states has decreased by approximately 40% from 2020 to 2021.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMCNet for Spatial-configuration-based Classification: A Summary of
  Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Farhadloo, Carl Molnar, Gaoxiang Luo, Yan Li, Shashi Shekhar, Rachel L. Maus, Svetomir N. Markovic, Raymond Moore, Alexey Leontovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of spatial-configuration-based classification is to build a
classifier to distinguish two classes (e.g., responder, non-responder) based on
the spatial arrangements (e.g., spatial interactions between different point
categories) given multi-category point data from two classes. This problem is
important for generating hypotheses in medical pathology towards discovering
new immunotherapies for cancer treatment as well as for other applications in
biomedical research and microbial ecology. This problem is challenging due to
an exponential number of category subsets which may vary in the strength of
spatial interactions. Most prior efforts on using human selected spatial
association measures may not be sufficient for capturing the relevant (e.g.,
surrounded by) spatial interactions which may be of biological significance. In
addition, the related deep neural networks are limited to category pairs and do
not explore larger subsets of point categories. To overcome these limitations,
we propose a Spatial-interaction Aware Multi-Category deep neural Network
(SAMCNet) architecture and contribute novel local reference frame
characterization and point pair prioritization layers for
spatial-configuration-based classification. Extensive experimental results on
multiple cancer datasets show that the proposed architecture provides higher
prediction accuracy over baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05340v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05340v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Wang, Zezheng Wang, Zitong Yu, Weihong Deng, Jiahong Li, Tingting Gao, Zhongyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With diverse presentation attacks emerging continually, generalizable face
anti-spoofing (FAS) has drawn growing attention. Most existing methods
implement domain generalization (DG) on the complete representations. However,
different image statistics may have unique properties for the FAS tasks. In
this work, we separate the complete representation into content and style ones.
A novel Shuffled Style Assembly Network (SSAN) is proposed to extract and
reassemble different content and style features for a stylized feature space.
Then, to obtain a generalized representation, a contrastive learning strategy
is developed to emphasize liveness-related style information while suppress the
domain-specific one. Finally, the representations of the correct assemblies are
used to distinguish between living and spoofing during the inferring. On the
other hand, despite the decent performance, there still exists a gap between
academia and industry, due to the difference in data quantity and distribution.
Thus, a new large-scale benchmark for FAS is built up to further evaluate the
performance of algorithms in reality. Both qualitative and quantitative results
on existing and proposed benchmarks demonstrate the effectiveness of our
methods. The codes will be available at https://github.com/wangzhuo2019/SSAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptive Hand Keypoint and Pixel Localization in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takehiko Ohkawa, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Trans2Cap: <span class="highlight-title">Cross-Modal</span> Knowledge Transfer using <span class="highlight-title">Transformer</span> for 3D
  Dense Captioning <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Zhen Li, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D dense captioning aims to describe individual objects by natural language
in 3D scenes, where 3D scenes are usually represented as RGB-D scans or point
clouds. However, only exploiting single modal information, e.g., point cloud,
previous approaches fail to produce faithful descriptions. Though aggregating
2D features into point clouds may be beneficial, it introduces an extra
computational burden, especially in inference phases. In this study, we
investigate a cross-modal knowledge transfer using Transformer for 3D dense
captioning, X-Trans2Cap, to effectively boost the performance of single-modal
3D caption through knowledge distillation using a teacher-student framework. In
practice, during the training phase, the teacher network exploits auxiliary 2D
modality and guides the student network that only takes point clouds as input
through the feature consistency constraints. Owing to the well-designed
cross-modal feature fusion module and the feature alignment in the training
phase, X-Trans2Cap acquires rich appearance information embedded in 2D images
with ease. Thus, a more faithful caption can be generated only using point
clouds during the inference. Qualitative and quantitative results confirm that
X-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,
about +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Aware Single-Stage Models for Multi-Person 3D Pose
  Estimation <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07697v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07697v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Wang, Xuecheng Nie, Xiaochao Qu, Yunpeng Chen, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel Distribution-Aware Single-stage (DAS) model
for tackling the challenging multi-person 3D pose estimation problem. Different
from existing top-down and bottom-up methods, the proposed DAS model
simultaneously localizes person positions and their corresponding body joints
in the 3D camera space in a one-pass manner. This leads to a simplified
pipeline with enhanced efficiency. In addition, DAS learns the true
distribution of body joints for the regression of their positions, rather than
making a simple Laplacian or Gaussian assumption as previous works. This
provides valuable priors for model prediction and thus boosts the
regression-based scheme to achieve competitive performance with volumetric-base
ones. Moreover, DAS exploits a recursive update strategy for progressively
approaching to regression target, alleviating the optimization difficulty and
further lifting the regression performance. DAS is implemented with a fully
Convolutional Neural Network and end-to-end learnable. Comprehensive
experiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior
efficiency of the proposed DAS model, specifically 1.5x speedup over previous
best model, and its stat-of-the-art accuracy for multi-person 3D pose
estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2022. Code will be released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Dirichlet uncertainty for unsupervised out-of-distribution
  detection of eye fundus photographs in glaucoma screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Araújo, Guilherme Aresta, Hrvoje Bogunovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of automatic tools for early glaucoma diagnosis with color
fundus photographs can significantly reduce the impact of this disease.
However, current state-of-the-art solutions are not robust to real-world
scenarios, providing over-confident predictions for out-of-distribution cases.
With this in mind, we propose a model based on the Dirichlet distribution that
allows to obtain class-wise probabilities together with an uncertainty
estimation without exposure to out-of-distribution cases. We demonstrate our
approach on the AIROGS challenge. At the start of the final test phase (8 Feb.
2022), our method had the highest average score among all submissions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ISBI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift
  Estimation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joachim Nyborg, Charlotte Pelletier, Sébastien Lefèvre, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent developments of deep learning models that capture the complex
temporal patterns of crop phenology have greatly advanced crop classification
of Satellite Image Time Series (SITS). However, when applied to target regions
spatially different from the training region, these models perform poorly
without any target labels due to the temporal shift of crop phenology between
regions. To address this unsupervised cross-region adaptation setting, existing
methods learn domain-invariant features without any target supervision, but not
the temporal shift itself. As a consequence, these techniques provide only
limited benefits for SITS. In this paper, we propose TimeMatch, a new
unsupervised domain adaptation method for SITS that directly accounts for the
temporal shift. TimeMatch consists of two components: 1) temporal shift
estimation, which estimates the temporal shift of the unlabeled target region
with a source-trained model, and 2) TimeMatch learning, which combines temporal
shift estimation with semi-supervised learning to adapt a classifier to an
unlabeled target region. We also introduce an open-access dataset for
cross-region adaptation with SITS from four different regions in Europe. On
this dataset, we demonstrate that TimeMatch outperforms all competing methods
by 11% in F1-score across five different adaptation scenarios, setting a new
state-of-the-art for cross-region adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to the ISPRS Journal of Photogrammetry and Remote
  Sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D
  MRI Scans with Geometric Deep Neural Networks <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Bongratz, Anne-Marie Rickmann, Sebastian Pölsterl, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reconstruction of cortical surfaces from brain magnetic resonance imaging
(MRI) scans is essential for quantitative analyses of cortical thickness and
sulcal morphology. Although traditional and deep learning-based algorithmic
pipelines exist for this purpose, they have two major drawbacks: lengthy
runtimes of multiple hours (traditional) or intricate post-processing, such as
mesh extraction and topology correction (deep learning-based). In this work, we
address both of these issues and propose Vox2Cortex, a deep learning-based
algorithm that directly yields topologically correct, three-dimensional meshes
of the boundaries of the cortex. Vox2Cortex leverages convolutional and graph
convolutional neural networks to deform an initial template to the densely
folded geometry of the cortex represented by an input MRI scan. We show in
extensive experiments on three brain MRI datasets that our meshes are as
accurate as the ones reconstructed by state-of-the-art methods in the field,
without the need for time- and resource-intensive post-processing. To
accurately reconstruct the tightly folded cortex, we work with meshes
containing about 168,000 vertices at test time, scaling deep explicit
reconstruction methods to a new level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transframer: Arbitrary Frame Prediction with Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, Peter Battaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a general-purpose framework for image modelling and vision tasks
based on probabilistic frame prediction. Our approach unifies a broad range of
tasks, from image segmentation, to novel view synthesis and video
interpolation. We pair this framework with an architecture we term Transframer,
which uses U-Net and Transformer components to condition on annotated context
frames, and outputs sequences of sparse, compressed image features. Transframer
is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can
generate coherent 30 second videos from a single image without any explicit
geometric information. A single generalist Transframer simultaneously produces
promising results on 8 tasks, including semantic segmentation, image
classification and optical flow prediction with no task-specific architectural
components, demonstrating that multi-task computer vision can be tackled using
probabilistic image models. Our approach can in principle be applied to a wide
range of applications that require learning the conditional structure of
annotated image-formatted data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Learning to Affiliate: Mutual Centralized Learning for Few-shot
  Classification <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.05517v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.05517v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Yang Liu</span>, Weifeng Zhang, Chao Xiang, Tu Zheng, <span class="highlight-author">Deng Cai</span>, Xiaofei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning (FSL) aims to learn a classifier that can be easily adapted
to accommodate new tasks not seen during training, given only a few examples.
To handle the limited-data problem in few-shot regimes, recent methods tend to
collectively use a set of local features to densely represent an image instead
of using a mixed global feature. They generally explore a unidirectional
query-to-support paradigm in FSL, e.g., find the nearest/optimal support
feature for each query feature and aggregate these local matches for a joint
classification. In this paper, we propose a new method Mutual Centralized
Learning (MCL) to fully affiliate the two disjoint sets of dense features in a
bidirectional paradigm. We associate each local feature with a particle that
can bidirectionally random walk in a discrete feature space by the
affiliations. To estimate the class probability, we propose the features'
accessibility that measures the expected number of visits to the support
features of that class in a Markov process. We relate our method to learning a
centrality on an affiliation network and demonstrate its capability to be
plugged in existing methods by highlighting centralized local features.
Experiments show that our method achieves the state-of-the-art on both
miniImageNet and tieredImageNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CycleMLP: A MLP-like Architecture for Dense Prediction <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.10224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.10224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoufa Chen, Enze Xie, Chongjian Ge, Runjian Chen, Ding Liang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a simple MLP-like architecture, CycleMLP, which is a
versatile backbone for visual recognition and dense predictions. As compared to
modern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose
architectures are correlated to image size and thus are infeasible in object
detection and segmentation, CycleMLP has two advantages compared to modern
approaches. (1) It can cope with various image sizes. (2) It achieves linear
computational complexity to image size by using local windows. In contrast,
previous MLPs have $O(N^2)$ computations due to fully spatial connections. We
build a family of models which surpass existing MLPs and even state-of-the-art
Transformer-based models, e.g., Swin Transformer, while using fewer parameters
and FLOPs. We expand the MLP-like models' applicability, making them a
versatile backbone for dense prediction tasks. CycleMLP achieves competitive
results on object detection, instance segmentation, and semantic segmentation.
In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K
dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot
robustness on ImageNet-C dataset. Code is available at
https://github.com/ShoufaChen/CycleMLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 (Oral). Camera-ready Code:
  https://github.com/ShoufaChen/CycleMLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Hyperbolic Embeddings in 2D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Lang, Alexander Braun, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, for the most part, has been formulated in the euclidean
space, where euclidean or spherical geodesic distances measure the similarity
of an image region to an object class prototype. In this work, we study whether
a hyperbolic geometry better matches the underlying structure of the object
classification space. We incorporate a hyperbolic classifier in two-stage,
keypoint-based, and transformer-based object detection architectures and
evaluate them on large-scale, long-tailed, and zero-shot object detection
benchmarks. In our extensive experimental evaluations, we observe categorical
class hierarchies emerging in the structure of the classification space,
resulting in lower classification errors and boosting the overall object
detection performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HDMapNet: An Online HD Map Construction and Evaluation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.06307v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.06307v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Li, Yue Wang, Yilun Wang, <span class="highlight-author">Hang Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing HD semantic maps is a central component of autonomous driving.
However, traditional pipelines require a vast amount of human efforts and
resources in annotating and maintaining the semantics in the map, which limits
its scalability. In this paper, we introduce the problem of HD semantic map
learning, which dynamically constructs the local semantics based on onboard
sensor observations. Meanwhile, we introduce a semantic map learning method,
dubbed HDMapNet. HDMapNet encodes image features from surrounding cameras
and/or point clouds from LiDAR, and predicts vectorized map elements in the
bird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all
settings, it performs better than baseline methods. Of note, our camera-LiDAR
fusion-based HDMapNet outperforms existing methods by more than 50% in all
metrics. In addition, we develop semantic-level and instance-level metrics to
evaluate the map learning performance. Finally, we showcase our method is
capable of predicting a locally consistent map. By introducing the method and
metrics, we invite the community to study this novel map learning problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClawCraneNet: Leveraging Object-level Relation for Text-based Video
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.10702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.10702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Liang, Yu Wu, Yawei Luo, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based video segmentation is a challenging task that segments out the
natural language referred objects in videos. It essentially requires semantic
comprehension and fine-grained video understanding. Existing methods introduce
language representation into segmentation models in a bottom-up manner, which
merely conducts vision-language interaction within local receptive fields of
ConvNets. We argue that such interaction is not fulfilled since the model can
barely construct region-level relationships given partial observations, which
is contrary to the description logic of natural language/referring expressions.
In fact, people usually describe a target object using relations with other
objects, which may not be easily understood without seeing the whole video. To
address the issue, we introduce a novel top-down approach by imitating how we
human segment an object with the language guidance. We first figure out all
candidate objects in videos and then choose the refereed one by parsing
relations among those high-level objects. Three kinds of object-level relations
are investigated for precise relationship understanding, i.e., positional
relation, text-guided semantic relation, and temporal relation. Extensive
experiments on A2D Sentences and J-HMDB Sentences show our method outperforms
state-of-the-art methods by a large margin. Qualitative results also show our
results are more explainable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Coreset Selection for Rehearsal-based Continual Learning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.01085v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.01085v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a
current dataset while selecting high-affinity samples to past tasks, which
directly inhibits catastrophic forgetting. We validate the effectiveness of our
coreset selection mechanism over various standard, imbalanced, and noisy
datasets against strong continual learning baselines, demonstrating that it
improves task adaptation and prevents catastrophic forgetting in a
sample-efficient manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M5Product: Self-harmonized <span class="highlight-title">Contrastive Learning</span> for E-commercial
  <span class="highlight-title">Multi-modal</span> <span class="highlight-title">Pretrain</span>ing <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04275v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04275v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Michael C. Kampffmeyer, Xiaoyong Wei, Minlong Lu, Yaowei Wang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the potential of multi-modal pre-training to learn highly
discriminative feature representations from complementary data modalities,
current progress is being slowed by the lack of large-scale modality-diverse
datasets. By leveraging the natural suitability of E-commerce, where different
modalities capture complementary semantic information, we contribute a
large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5
modalities (image, text, table, video, and audio), covers over 6,000 categories
and 5,000 attributes, and is 500 larger than the largest publicly available
dataset with a similar number of modalities. Furthermore, M5Product contains
incomplete modality pairs and noise while also having a long-tailed
distribution, resembling most real-world problems. We further propose
Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework
that integrates the different modalities into a unified model through an
adaptive feature fusion mechanism, where the importance of each modality is
learned directly from the modality embeddings and impacts the inter-modality
contrastive learning and masked tasks within a multi-modal transformer model.
We evaluate the current multi-modal pre-training state-of-the-art approaches
and benchmark their ability to learn from unlabeled data when faced with the
large number of modalities in the M5Product dataset. We conduct extensive
experiments on four downstream tasks and demonstrate the superiority of our
SCALE model, providing insights into the importance of dataset scale and
diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interacting Attention Graph for Single Image Two-Hand Reconstruction <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu, Feng Chen, Tao Yu, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph convolutional network (GCN) has achieved great success in single hand
reconstruction task, while interacting two-hand reconstruction by GCN remains
unexplored. In this paper, we present Interacting Attention Graph Hand
(IntagHand), the first graph convolution based network that reconstructs two
interacting hands from a single RGB image. To solve occlusion and interaction
challenges of two-hand reconstruction, we introduce two novel attention based
modules in each upsampling step of the original GCN. The first module is the
pyramid image feature attention (PIFA) module, which utilizes multiresolution
features to implicitly obtain vertex-to-image alignment. The second module is
the cross hand attention (CHA) module that encodes the coherence of interacting
hands by building dense cross-attention between two hand vertices. As a result,
our model outperforms all existing two-hand reconstruction methods by a large
margin on InterHand2.6M benchmark. Moreover, ablation studies verify the
effectiveness of both PIFA and CHA modules for improving the reconstruction
accuracy. Results on in-the-wild images and live video streams further
demonstrate the generalization ability of our network. Our code is available at
https://github.com/Dw1010/IntagHand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR 2022. Project page:
  http://www.liuyebin.com/IntagHand/Intaghand.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple Mutual Information based Registration Method for
  Thermal-Optical Image Pairs applied on a Novel <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.06910v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.06910v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suranjan Goswami, Satish Kumar Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While thermal optical registered datasets are becoming widely available, most
of these works are based on image pairs which are pre-registered. However,
thermal imagers where these images are registered by default are quite
expensive. We present in this work, a thermal image registration technique
which is computationally lightweight, and can be employed regardless of the
resolution of the images captured. We use 2 different thermal imagers to create
a completely new database and introduce it as a part of this work as well. The
images captured are based on 5 different classes and encompass subjects like
the Prayagraj Kumbh Mela, one of the largest public fairs in the world,
captured over a period of 2 years.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Texture for Fooling Person Detectors in the Physical World <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03373v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03373v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Xiaolin Hu, Fuchun Sun, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, cameras equipped with AI systems can capture and analyze images to
detect people automatically. However, the AI system can make mistakes when
receiving deliberately designed patterns in the real world, i.e., physical
adversarial examples. Prior works have shown that it is possible to print
adversarial patches on clothes to evade DNN-based person detectors. However,
these adversarial examples could have catastrophic drops in the attack success
rate when the viewing angle (i.e., the camera's angle towards the object)
changes. To perform a multi-angle attack, we propose Adversarial Texture
(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people
wearing such clothes can hide from person detectors from different viewing
angles. We propose a generative method, named Toroidal-Cropping-based
Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive
structures. We printed several pieces of cloth with AdvTexure and then made
T-shirts, skirts, and dresses in the physical world. Experiments showed that
these clothes could fool person detectors in the physical world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adversarial Neural Networks for Domain Generalization: When It
  Works and How to Improve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.03924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.03924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Sicilia, Xingchen Zhao, Seong Jae Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theoretically, domain adaptation is a well-researched problem. Further, this
theory has been well-used in practice. In particular, we note the bound on
target error given by Ben-David et al. (2010) and the well-known
domain-aligning algorithm based on this work using Domain Adversarial Neural
Networks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple
variants of DANN have been proposed for the related problem of domain
generalization, but without much discussion of the original motivating bound.
In this paper, we investigate the validity of DANN in domain generalization
from this perspective. We investigate conditions under which application of
DANN makes sense and further consider DANN as a dynamic process during
training. Our investigation suggests that the application of DANN to domain
generalization may not be as straightforward as it seems. To address this, we
design an algorithmic extension to DANN in the domain generalization case. Our
experimentation validates both theory and algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Universal Backward-Compatible Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binjie Zhang, Yixiao Ge, Yantao Shen, Shupeng Su, Fanzi Wu, Chun Yuan, Xuyuan Xu, Yexin Wang, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional model upgrades for visual search systems require offline refresh
of gallery features by feeding gallery images into new models (dubbed as
"backfill"), which is time-consuming and expensive, especially in large-scale
applications. The task of backward-compatible representation learning is
therefore introduced to support backfill-free model upgrades, where the new
query features are interoperable with the old gallery features. Despite the
success, previous works only investigated a close-set training scenario (i.e.,
the new training set shares the same classes as the old one), and are limited
by more realistic and challenging open-set scenarios. To this end, we first
introduce a new problem of universal backward-compatible representation
learning, covering all possible data split in model upgrades. We further
propose a simple yet effective method, dubbed as Universal Backward-Compatible
Training (UniBCT) with a novel structural prototype refinement algorithm, to
learn compatible representations in all kinds of model upgrading benchmarks in
a unified manner. Comprehensive experiments on the large-scale face recognition
datasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03814v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03814v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo, Tong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoptic segmentation involves a combination of joint semantic segmentation
and instance segmentation, where image contents are divided into two types:
things and stuff. We present Panoptic SegFormer, a general framework for
panoptic segmentation with transformers. It contains three innovative
components: an efficient deeply-supervised mask decoder, a query decoupling
strategy, and an improved post-processing method. We also use Deformable DETR
to efficiently process multi-scale features, which is a fast and efficient
version of DETR. Specifically, we supervise the attention modules in the mask
decoder in a layer-wise manner. This deep supervision strategy lets the
attention modules quickly focus on meaningful semantic regions. It improves
performance and reduces the number of required training epochs by half compared
to Deformable DETR. Our query decoupling strategy decouples the
responsibilities of the query set and avoids mutual interference between things
and stuff. In addition, our post-processing strategy improves performance
without additional costs by jointly considering classification and segmentation
qualities to resolve conflicting mask overlaps. Our approach increases the
accuracy 6.2\% PQ over the baseline DETR model. Panoptic SegFormer achieves
state-of-the-art results on COCO test-dev with 56.2\% PQ. It also shows
stronger zero-shot robustness over existing methods. The code is released at
\url{https://github.com/zhiqi-li/Panoptic-SegFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Compression-Based Feature Learning for Video Restoration <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Huang, Jiahao Li, Bin Li, Dong Liu, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to efficiently utilize the temporal features is crucial, yet challenging,
for video restoration. The temporal features usually contain various noisy and
uncorrelated information, and they may interfere with the restoration of the
current frame. This paper proposes learning noise-robust feature
representations to help video restoration. We are inspired by that the neural
codec is a natural denoiser. In neural codec, the noisy and uncorrelated
contents which are hard to predict but cost lots of bits are more inclined to
be discarded for bitrate saving. Therefore, we design a neural compression
module to filter the noise and keep the most useful information in features for
video restoration. To achieve robustness to noise, our compression module
adopts a spatial channel-wise quantization mechanism to adaptively determine
the quantization step size for each position in the latent. Experiments show
that our method can significantly boost the performance on video denoising,
where we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs.
Meanwhile, our method also obtains SOTA results on video deraining and
dehazing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Disturbance-Free Visual Mobile Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Ni, Kiana Ehsani, Luca Weihs, Jordi Salvador
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has shown promising results on an abundance of
robotic tasks in simulation, including visual navigation and manipulation.
Prior work generally aims to build embodied agents that solve their assigned
tasks as quickly as possible, while largely ignoring the problems caused by
collision with objects during interaction. This lack of prioritization is
understandable: there is no inherent cost in breaking virtual objects. As a
result, "well-trained" agents frequently collide with objects before achieving
their primary goals, a behavior that would be catastrophic in the real world.
In this paper, we study the problem of training agents to complete the task of
visual mobile manipulation in the ManipulaTHOR environment while avoiding
unnecessary collision (disturbance) with objects. We formulate disturbance
avoidance as a penalty term in the reward function, but find that directly
training with such penalized rewards often results in agents being unable to
escape poor local optima. Instead, we propose a two-stage training curriculum
where an agent is first allowed to freely explore and build basic competencies
without penalization, after which a disturbance penalty is introduced to refine
the agent's behavior. Results on testing scenes show that our curriculum not
only avoids these poor local optima, but also leads to 10% absolute gains in
success rate without disturbance, compared to our state-of-the-art baselines.
Moreover, we propose a novel disturbance-prediction auxiliary task accelerating
learning and further improving success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project site: https://sites.google.com/view/disturb-free</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BEAT: A Large-Scale Semantic and Emotional <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Dataset</span> for
  <span class="highlight-title">Conversation</span>al Gestures Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai<span class="highlight-author">yang Liu</span>, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MODNet: Real-Time Trimap-Free Portrait Matting via Objective
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.11961v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.11961v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing portrait matting methods either require auxiliary inputs that are
costly to obtain or involve multiple stages that are computationally expensive,
making them less suitable for real-time applications. In this work, we present
a light-weight matting objective decomposition network (MODNet) for portrait
matting in real-time with a single input image. The key idea behind our
efficient design is by optimizing a series of sub-objectives simultaneously via
explicit constraints. In addition, MODNet includes two novel techniques for
improving model efficiency and robustness. First, an Efficient Atrous Spatial
Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for
semantic estimation. Second, a self-supervised sub-objectives consistency (SOC)
strategy is proposed to adapt MODNet to real-world data to address the domain
shift problem common to trimap-free methods. MODNet is easy to be trained in an
end-to-end manner. It is much faster than contemporaneous methods and runs at
67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms
prior trimap-free methods by a large margin on both Adobe Matting Dataset and a
carefully designed photographic portrait matting (PPM-100) benchmark proposed
by us. Further, MODNet achieves remarkable results on daily photos and videos.
Our code and models are available at https://github.com/ZHKKKe/MODNet, and the
PPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeILF: Neural Incident Light Field for Physically-based Material
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a differentiable rendering framework for material and lighting
estimation from multi-view images and a reconstructed geometry. In the
framework, we represent scene lightings as the Neural Incident Light Field
(NeILF) and material properties as the surface BRDF modelled by multi-layer
perceptrons. Compared with recent approaches that approximate scene lightings
as the 2D environment map, NeILF is a fully 5D light field that is capable of
modelling illuminations of any static scenes. In addition, occlusions and
indirect lights can be handled naturally by the NeILF representation without
requiring multiple bounces of ray tracing, making it possible to estimate
material properties even for scenes with complex lightings and geometries. We
also propose a smoothness regularization and a Lambertian assumption to reduce
the material-lighting ambiguity during the optimization. Our method strictly
follows the physically-based rendering equation, and jointly optimizes material
and lighting through the differentiable rendering process. We have intensively
evaluated the proposed method on our in-house synthetic dataset, the DTU MVS
dataset, and real-world BlendedMVS scenes. Our method is able to outperform
previous methods by a significant margin in terms of novel view rendering
quality, setting a new state-of-the-art for image-based material and lighting
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ METEOR:A Dense, Heterogeneous, and Unstructured Traffic <span class="highlight-title">Dataset</span> With
  Rare Behaviors <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.07648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.07648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Chandra, Xijun Wang, Mridul Mahajan, Rahul Kala, Rishitha Palugulla, Chandrababu Naidu, Alok Jain, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new traffic dataset, METEOR, which captures traffic patterns and
multi-agent driving behaviors in unstructured scenarios. METEOR consists of
more than 1000 one-minute videos, over 2 million annotated frames with bounding
boxes and GPS trajectories for 16 unique agent categories, and more than 13
million bounding boxes for traffic agents. METEOR is a dataset for rare and
interesting, multi-agent driving behaviors that are grouped into traffic
violations, atypical interactions, and diverse scenarios. Every video in METEOR
is tagged using a diverse range of factors corresponding to weather, time of
the day, road conditions, and traffic density. We use METEOR to benchmark
perception methods for object detection and multi-agent behavior prediction.
Our key finding is that state-of-the-art models for object detection and
behavior prediction, which otherwise succeed on existing datasets such as
Waymo, fail on the METEOR dataset. METEOR marks the first step towards the
development of more sophisticated perception models for dense, heterogeneous,
and unstructured scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IROS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud
  Completion <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03530v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03530v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud is an important 3D representation for capturing real world 3D
objects. However, real-scanned 3D point clouds are often incomplete, and it is
important to recover complete point clouds for downstream applications. Most
existing point cloud completion methods use Chamfer Distance (CD) loss for
training. The CD loss estimates correspondences between two point clouds by
searching nearest neighbors, which does not capture the overall point density
distribution on the generated shape, and therefore likely leads to non-uniform
point cloud generation. To tackle this problem, we propose a novel Point
Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of
a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The
CGNet uses a conditional generative model called the denoising diffusion
probabilistic model (DDPM) to generate a coarse completion conditioned on the
partial observation. DDPM establishes a one-to-one pointwise mapping between
the generated point cloud and the uniform ground truth, and then optimizes the
mean squared error loss to realize uniform generation. The RFNet refines the
coarse output of the CGNet and further improves quality of the completed point
cloud. Furthermore, we develop a novel dual-path architecture for both
networks. The architecture can (1) effectively and efficiently extract
multi-level features from partially observed point clouds to guide completion,
and (2) accurately manipulate spatial locations of 3D points to obtain smooth
surfaces and sharp details. Extensive experimental results on various benchmark
datasets show that our PDR paradigm outperforms previous state-of-the-art
methods for point cloud completion. Remarkably, with the help of the RFNet, we
can accelerate the iterative generation process of the DDPM by up to 50 times
without much performance drop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022. Code is released at
  https://github.com/ZhaoyangLyu/Point_Diffusion_Refinement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iSegFormer: Interactive Segmentation via <span class="highlight-title">Transformer</span>s with Application
  to 3D Knee MR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11325v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11325v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Zhenlin Xu, Yining Jiao, Marc Niethammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose iSegFormer, a memory-efficient transformer that combines a Swin
transformer with a lightweight multilayer perceptron (MLP) decoder. With the
efficient Swin transformer blocks for hierarchical self-attention and the
simple MLP decoder for aggregating both local and global attention, iSegFormer
learns powerful representations while achieving high computational
efficiencies. Specifically, we apply iSegFormer to interactive 3D medical image
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> An Empirical Study of Training End-to-End <span class="highlight-title">Vision-and-Language</span>
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Yi Dou, Yichong Xu, <span class="highlight-author">Zhe Gan</span>, Jianfeng Wang, Shuohang Wang, <span class="highlight-author">Lijuan Wang</span>, Chenguang Zhu, Pengchuan Zhang, <span class="highlight-author">Lu Yuan</span>, Nanyun Peng, Zicheng Liu, Michael Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks often
degrades significantly. In this paper, we present METER, a Multimodal
End-to-end TransformER framework, through which we investigate how to design
and pre-train a fully transformer-based VL model in an end-to-end manner.
Specifically, we dissect the model designs along multiple dimensions: vision
encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,
DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),
architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training
objectives (e.g., masked image modeling). We conduct comprehensive experiments
and provide insights on how to train a performant VL transformer. METER
achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images
for pre-training, surpassing the state-of-the-art region-feature-based model by
1.04%, and outperforming the previous best fully transformer-based model by
1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy
of 80.54%. Code and pre-trained models are released at
https://github.com/zdou0830/METER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Text Attention Network for Spatial Deformation Robust Scene Text Image
  Super-resolution <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqi Ma, Zhetong Liang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene text image super-resolution aims to increase the resolution and
readability of the text in low-resolution images. Though significant
improvement has been achieved by deep convolutional neural networks (CNNs), it
remains difficult to reconstruct high-resolution images for spatially deformed
texts, especially rotated and curve-shaped ones. This is because the current
CNN-based methods adopt locality-based operations, which are not effective to
deal with the variation caused by deformations. In this paper, we propose a CNN
based Text ATTention network (TATT) to address this problem. The semantics of
the text are firstly extracted by a text recognition module as text prior
information. Then we design a novel transformer-based module, which leverages
global attention mechanism, to exert the semantic guidance of text prior to the
text reconstruction process. In addition, we propose a text structure
consistency loss to refine the visual appearance by imposing structural
consistency on the reconstructions of regular and deformed texts. Experiments
on the benchmark TextZoom dataset show that the proposed TATT not only achieves
state-of-the-art performance in terms of PSNR/SSIM metrics, but also
significantly improves the recognition accuracy in the downstream text
recognition task, particularly for text instances with multi-orientation and
curved shapes. Code is available at https://github.com/mjq11302010044/TATT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Many Data Samples is an Additional Instruction Worth? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in the expanded
version of NATURAL INSTRUCTIONS with additional instructions and find that
these significantly improve model performance (up to 35%), especially in the
low-data regime. Our results indicate that an additional instruction can be
equivalent to ~200 data samples on average across tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Region-Aware Face Swapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Xu, Jiangning Zhang, Miao Hua, Qian He, Zili Yi, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cervical Optical Coherence Tomography Image Classification Based on
  <span class="highlight-title">Contrastive</span> <span class="highlight-title">Self-Supervised</span> Texture Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.05081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.05081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyi Chen, Qingbin Wang, Yutao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Cervical cancer seriously affects the health of the female
reproductive system. Optical coherence tomography (OCT) emerged as a
non-invasive, high-resolution imaging technology for cervical disease
detection. However, OCT image annotation is knowledge-intensive and
time-consuming, which impedes the training process of deep-learning-based
classification models. Purpose: This study aims to develop a computer-aided
diagnosis (CADx) approach to classifying in-vivo cervical OCT images based on
self-supervised learning. Methods: In addition to high-level semantic features
extracted by a convolutional neural network (CNN), the proposed CADx approach
leverages unlabeled cervical OCT images' texture features learned by
contrastive texture learning. We conducted ten-fold cross-validation on the OCT
image dataset from a multi-center clinical study on 733 patients from China.
Results: In a binary classification task for detecting high-risk diseases,
including high-grade squamous intraepithelial lesion and cervical cancer, our
method achieved an area-under-the-curve value of 0.9798 plus or minus 0.0157
with a sensitivity of 91.17 plus or minus 4.99% and a specificity of 93.96 plus
or minus 4.72% for OCT image patches; also, it outperformed two out of four
medical experts on the test set. Furthermore, our method achieved a 91.53%
sensitivity and 97.37% specificity on an external validation dataset containing
287 3D OCT volumes from 118 Chinese patients in a new hospital using a
cross-shaped threshold voting strategy. Conclusions: The proposed
contrastive-learning-based CADx method outperformed the end-to-end CNN models
and provided better interpretability based on texture features, which holds
great potential to be used in the clinical protocol of "see-and-treat."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures, and 7 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offensive Language Detection in Under-resourced Algerian Dialectal
  Arabic Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oussama Boucherit, Kheireddine Abainia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of detecting the offensive and abusive
content in Facebook comments, where we focus on the Algerian dialectal Arabic
which is one of under-resourced languages. The latter has a variety of dialects
mixed with different languages (i.e. Berber, French and English). In addition,
we deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due
to the scarcity of works on the same language, we have built a new corpus
regrouping more than 8.7k texts manually annotated as normal, abusive and
offensive. We have conducted a series of experiments using the state-of-the-art
classifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.
The results showed acceptable performances, but the problem requires further
investigation on linguistic features to increase the identification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BigDML 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FORCE: A Framework of Rule-Based <span class="highlight-title">Conversation</span>al Recommender System <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Quan, Ze Wei, Qiang Gan, Jingqi Yao, Jingyi Lu, Yuchen Dong, Yiming Liu, Yi Zeng, Chao Zhang, Yongzhi Li, Huang Hu, Yingying He, Yang Yang, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 (Demonstration Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Lithuanian grammatical error correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyone wants to write beautiful and correct text, yet the lack of language
skills, experience, or hasty typing can result in errors. By employing the
recent advances in transformer architectures, we construct a grammatical error
correction model for Lithuanian, the language rich in archaic features. We
compare subword and byte-level approaches and share our best trained model,
achieving F$_{0.5}$=0.92, and accompanying code, in an online open-source
repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Secondary complementary balancing compressive imaging with a free-space
  balanced amplified photodetector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-<span class="highlight-author">Kai Yu</span>, Ying Yang, Jin-Rui Liu, Ning Wei, Shuo-Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-pixel imaging (SPI) has attracted widespread attention because it
generally uses a non-pixelated photodetector and a digital micromirror device
(DMD) to acquire the object image. Since the modulated patterns seen from two
reflection directions of the DMD are naturally complementary, one can apply
complementary balanced measurements to greatly improve the measurement
signal-to-noise ratio and reconstruction quality. However, the balance between
two reflection arms significantly determines the quality of differential
measurements. In this work, we propose and demonstrate a simple secondary
complementary balancing mechanism to minimize the impact of the imbalance on
the imaging system. In our SPI setup, we used a silicon free-space balanced
amplified photodetector with 5 mm active diameter which could directly output
the difference between two optical input signals in two reflection arms. Both
simulation and experimental results have demonstrated that the use of secondary
complementary balancing can result in a better cancellation of direct current
components of measurements and a better image restoration quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Personalized Item-to-Item Recommendation Metric via Implicit
  Feedback <span class="chip">AISTATS-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong Nghia Hoang, Anoop Deoras, Tong Zhao, Jin Li, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the item-to-item recommendation problem in recommender
systems from a new perspective of metric learning via implicit feedback. We
develop and investigate a personalizable deep metric model that captures both
the internal contents of items and how they were interacted with by users.
There are two key challenges in learning such model. First, there is no
explicit similarity annotation, which deviates from the assumption of most
metric learning methods. Second, these approaches ignore the fact that items
are often represented by multiple sources of meta data and different users use
different combinations of these sources to form their own notion of similarity.
  To address these challenges, we develop a new metric representation embedded
as kernel parameters of a probabilistic model. This helps express the
correlation between items that a user has interacted with, which can be used to
predict user interaction with new items. Our approach hinges on the intuition
that similar items induce similar interactions from the same user, thus fitting
a metric-parameterized model to predict an implicit feedback signal could
indirectly guide it towards finding the most suitable metric for each user. To
this end, we also analyze how and when the proposed method is effective from a
theoretical lens. Its empirical effectiveness is also demonstrated on several
real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS-22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correcting diacritics and typos with a ByT5 <span class="highlight-title">transformer</span> model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius, Jurgita Kapočiūtė-Dzikienė, Monika Briedienė, Tomas Krilavičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the fast pace of life and online communications and the prevalence of
English and the QWERTY keyboard, people tend to forgo using diacritics, make
typographical errors (typos) when typing in other languages. Restoring
diacritics and correcting spelling is important for proper language use and the
disambiguation of texts for both humans and downstream algorithms. However,
both of these problems are typically addressed separately: the state-of-the-art
diacritics restoration methods do not tolerate other typos, but classical
spellcheckers also cannot deal adequately with all the diacritics missing. In
this work, we tackle both problems at once by employing the newly-developed
universal ByT5 byte-level seq2seq transformer model that requires no
language-specific model structures. For a comparison, we perform diacritics
restoration on benchmark datasets of 12 languages, with the addition of
Lithuanian. The experimental investigation proves that our approach is able to
achieve results (> 98%) comparable to the previous state-of-the-art, despite
being trained less and on fewer data. Our approach is also able to restore
diacritics in words not seen during training with > 76% accuracy. Our
simultaneous diacritics restoration and typos correction approach reaches > 94%
alpha-word accuracy on the 13 languages. It has no direct competitors and
strongly outperforms classical spell-checking or dictionary-based approaches.
We also demonstrate all the accuracies to further improve with more training.
Taken together, this shows the great real-world application potential of our
suggested methods to more data, languages, and error classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning Approach for Repairing Missing Activity Labels in Event
  Logs for Process Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Lu, Qifan Chen, Simon K. Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process mining is a relatively new subject that builds a bridge between
traditional process modeling and data mining. Process discovery is one of the
most critical parts of process mining, which aims at discovering process models
automatically from event logs. The performance of existing process discovery
algorithms can be affected when there are missing activity labels in event
logs. Several methods have been proposed to repair missing activity labels, but
their accuracy can drop when a large number of activity labels are missing. In
this paper, we propose an LSTM-based prediction model to predict the missing
activity labels in event logs. The proposed model takes both the prefix and
suffix sequences of the events with missing activity labels as input.
Additional attributes of event logs are also utilized to improve the
performance. Our evaluation of several publicly available datasets shows that
the proposed method performed consistently better than existing methods in
terms of repairing missing activity labels in event logs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearest Neighbor Classifier with Margin Penalty for Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Cao, Zhiqiao Gao, Jie Hu, Mingchuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning becomes the mainstream in the field of natural language
processing, the need for suitable active learning method are becoming
unprecedented urgent. Active Learning (AL) methods based on nearest neighbor
classifier are proposed and demonstrated superior results. However, existing
nearest neighbor classifier are not suitable for classifying mutual exclusive
classes because inter-class discrepancy cannot be assured by nearest neighbor
classifiers. As a result, informative samples in the margin area can not be
discovered and AL performance are damaged. To this end, we propose a novel
Nearest neighbor Classifier with Margin penalty for Active Learning(NCMAL).
Firstly, mandatory margin penalty are added between classes, therefore both
inter-class discrepancy and intra-class compactness are both assured. Secondly,
a novel sample selection strategy are proposed to discover informative samples
within the margin area. To demonstrate the effectiveness of the methods, we
conduct extensive experiments on for datasets with other state-of-the-art
methods. The experimental results demonstrate that our method achieves better
results with fewer annotated samples than all baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Domain Multi-Task Learning for Sequential Sentence Classification
  in Research Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.06008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.06008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Brack, Anett Hoppe, Pascal Buschermöhle, Ralph Ewerth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential sentence classification deals with the categorisation of sentences
based on their content and context. Applied to scientific texts, it enables the
automatic structuring of research papers and the improvement of academic search
engines. However, previous work has not investigated the potential of transfer
learning for sentence classification across different scientific domains and
the issue of different text structure of full papers and abstracts. In this
paper, we derive seven related research questions and present several
contributions to address them: First, we suggest a novel uniform deep learning
architecture and multi-task learning for cross-domain sequential sentence
classification in scientific texts. Second, we tailor two common transfer
learning methods, sequential transfer learning and multi-task learning, to deal
with the challenges of the given task. Semantic relatedness of tasks is a
prerequisite for successful transfer learning of neural models. Consequently,
our third contribution is an approach to semi-automatically identify
semantically related classes from different annotation schemes and we present
an analysis of four annotation schemes. Comprehensive experimental results
indicate that models, which are trained on datasets from different scientific
domains, benefit from one another when using the proposed multi-task learning
architecture. We also report comparisons with several state-of-the-art
approaches. Our approach outperforms the state of the art on full paper
datasets significantly while being on par for datasets consisting of abstracts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in ACM/IEEE Joint Conference on Digital
  Libraries (JCDL), 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ But that's not why: Inference adjustment by interactive prototype
  deselection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Gerstenberger, Sebastian Lapuschkin, Peter Eisert, Sebastian Bosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advances in machine learning, decision-making of
artificial agents is still not perfect and often requires post-hoc human
interventions. If the prediction of a model relies on unreasonable factors it
is desirable to remove their effect. Deep interactive prototype adjustment
enables the user to give hints and correct the model's reasoning. In this
paper, we demonstrate that prototypical-part models are well suited for this
task as their prediction is based on prototypical image patches that can be
interpreted semantically by the user. It shows that even correct
classifications can rely on unreasonable prototypes that result from
confounding variables in a dataset. Hence, we propose simple yet effective
interaction schemes for inference adjustment: The user is consulted
interactively to identify faulty prototypes. Non-object prototypes can be
removed by prototype masking or a custom mode of deselection training.
Interactive prototype rejection allows machine learning na\"{i}ve users to
adjust the logic of reasoning without compromising the accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Know Therefore I Score: Label-Free Crafting of Scoring Functions using
  Constraints Based on Domain Expertise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ragja Palakkadavath, Sarath Sivaprasad, Shirish Karande, Niranjan Pedanekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several real-life applications require crafting concise, quantitative scoring
functions (also called rating systems) from measured observations. For example,
an effectiveness score needs to be created for advertising campaigns using a
number of engagement metrics. Experts often need to create such scoring
functions in the absence of labelled data, where the scores need to reflect
business insights and rules as understood by the domain experts. Without a way
to capture these inputs systematically, this becomes a time-consuming process
involving trial and error. In this paper, we introduce a label-free practical
approach to learn a scoring function from multi-dimensional numerical data. The
approach incorporates insights and business rules from domain experts in the
form of easily observable and specifiable constraints, which are used as weak
supervision by a machine learning model. We convert such constraints into loss
functions that are optimized simultaneously while learning the scoring
function. We examine the efficacy of the approach using a synthetic dataset as
well as four real-life datasets, and also compare how it performs vis-a-vis
supervised learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lunar Rover Localization Using Craters as Landmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Matthies, Shreyansh Daftry, Scott Tepsuporn, Yang Cheng, Deegan Atha, R. Michael Swan, Sanjna Ravichandar, Masahiro Ono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Onboard localization capabilities for planetary rovers to date have used
relative navigation, by integrating combinations of wheel odometry, visual
odometry, and inertial measurements during each drive to track position
relative to the start of each drive. At the end of each drive, a
ground-in-the-loop (GITL) interaction is used to get a position update from
human operators in a more global reference frame, by matching images or local
maps from onboard the rover to orbital reconnaissance images or maps of a large
region around the rover's current position. Autonomous rover drives are limited
in distance so that accumulated relative navigation error does not risk the
possibility of the rover driving into hazards known from orbital images.
However, several rover mission concepts have recently been studied that require
much longer drives between GITL cycles, particularly for the Moon. These
concepts require greater autonomy to minimize GITL cycles to enable such large
range; onboard global localization is a key element of such autonomy. Multiple
techniques have been studied in the past for onboard rover global localization,
but a satisfactory solution has not yet emerged. For the Moon, the ubiquitous
craters offer a new possibility, which involves mapping craters from orbit,
then recognizing crater landmarks with cameras and-or a lidar onboard the
rover. This approach is applicable everywhere on the Moon, does not require
high resolution stereo imaging from orbit as some other approaches do, and has
potential to enable position knowledge with order of 5 to 10 m accuracy at all
times. This paper describes our technical approach to crater-based lunar rover
localization and presents initial results on crater detection using 3D point
cloud data from onboard lidar or stereo cameras, as well as using shading cues
in monocular onboard imagery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Aerospace Conference, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SURF: Semi-supervised Reward Learning with Data Augmentation for
  Feedback-efficient Preference-based Reinforcement Learning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongjin Park, Younggyo Seo, Jinwoo Shin, Honglak Lee, Pieter Abbeel, Kimin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference-based reinforcement learning (RL) has shown potential for teaching
agents to perform the target tasks without a costly, pre-defined reward
function by learning the reward with a supervisor's preference between the two
agent behaviors. However, preference-based learning often requires a large
amount of human feedback, making it difficult to apply this approach to various
applications. This data-efficiency problem, on the other hand, has been
typically addressed by using unlabeled samples or data augmentation techniques
in the context of supervised learning. Motivated by the recent success of these
approaches, we present SURF, a semi-supervised reward learning framework that
utilizes a large amount of unlabeled samples with data augmentation. In order
to leverage unlabeled samples for reward learning, we infer pseudo-labels of
the unlabeled samples based on the confidence of the preference predictor. To
further improve the label-efficiency of reward learning, we introduce a new
data augmentation that temporally crops consecutive subsequences from the
original behaviors. Our experiments demonstrate that our approach significantly
improves the feedback-efficiency of the state-of-the-art preference-based
method on a variety of locomotion and robotic manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-Sensitive Bayesian Games for Multi-Agent Reinforcement Learning
  under Policy Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Eriksson, Debabrota Basu, Mina Alibeigi, Christos Dimitrakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In stochastic games with incomplete information, the uncertainty is evoked by
the lack of knowledge about a player's own and the other players' types, i.e.
the utility function and the policy space, and also the inherent stochasticity
of different players' interactions. In existing literature, the risk in
stochastic games has been studied in terms of the inherent uncertainty evoked
by the variability of transitions and actions. In this work, we instead focus
on the risk associated with the \textit{uncertainty over types}. We contrast
this with the multi-agent reinforcement learning framework where the other
agents have fixed stationary policies and investigate risk-sensitiveness due to
the uncertainty about the other agents' adaptive policies. We propose
risk-sensitive versions of existing algorithms proposed for risk-neutral
stochastic games, such as Iterated Best Response (IBR), Fictitious Play (FP)
and a general multi-objective gradient approach using dual ascent (DAPG). Our
experimental analysis shows that risk-sensitive DAPG performs better than
competing algorithms for both social welfare and general-sum stochastic games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Low-rank Matrix Completion with Dual-graph Embedding: Prior
  Analysis and Tuning-free Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangge Chen, Lei Cheng, Yik-Chung Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there is a revival of interest in low-rank matrix completion-based
unsupervised learning through the lens of dual-graph regularization, which has
significantly improved the performance of multidisciplinary machine learning
tasks such as recommendation systems, genotype imputation and image inpainting.
While the dual-graph regularization contributes a major part of the success,
computational costly hyper-parameter tunning is usually involved. To circumvent
such a drawback and improve the completion performance, we propose a novel
Bayesian learning algorithm that automatically learns the hyper-parameters
associated with dual-graph regularization, and at the same time, guarantees the
low-rankness of matrix completion. Notably, a novel prior is devised to promote
the low-rankness of the matrix and encode the dual-graph information
simultaneously, which is more challenging than the single-graph counterpart. A
nontrivial conditional conjugacy between the proposed priors and likelihood
function is then explored such that an efficient algorithm is derived under
variational inference framework. Extensive experiments using synthetic and
real-world datasets demonstrate the state-of-the-art performance of the
proposed learning algorithm for various data analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Generalization Mystery in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satrajit Chatterjee, Piotr Zielinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization mystery in deep learning is the following: Why do
over-parameterized neural networks trained with gradient descent (GD)
generalize well on real datasets even though they are capable of fitting random
datasets of comparable size? Furthermore, from among all solutions that fit the
training data, how does GD find one that generalizes well (when such a
well-generalizing solution exists)?
  We argue that the answer to both questions lies in the interaction of the
gradients of different examples during training. Intuitively, if the
per-example gradients are well-aligned, that is, if they are coherent, then one
may expect GD to be (algorithmically) stable, and hence generalize well. We
formalize this argument with an easy to compute and interpretable metric for
coherence, and show that the metric takes on very different values on real and
random datasets for several common vision networks. The theory also explains a
number of other phenomena in deep learning, such as why some examples are
reliably learned earlier than others, why early stopping works, and why it is
possible to learn from noisy labels. Moreover, since the theory provides a
causal explanation of how GD finds a well-generalizing solution when one
exists, it motivates a class of simple modifications to GD that attenuate
memorization and improve generalization.
  Generalization in deep learning is an extremely broad phenomenon, and
therefore, it requires an equally general explanation. We conclude with a
survey of alternative lines of attack on this problem, and argue that the
proposed approach is the most viable one on this basis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skill-based Multi-objective Reinforcement Learning of Industrial Robot
  Tasks with Planning and Knowledge Integration <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Mayr, Faseeh Ahmad, Konstantinos Chatzilygeroudis, Luigi Nardi, Volker Krueger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern industrial settings with small batch sizes it should be easy to set
up a robot system for a new task. Strategies exist, e.g. the use of skills, but
when it comes to handling forces and torques, these systems often fall short.
We introduce an approach that provides a combination of task-level planning
with targeted learning of scenario-specific parameters for skill-based systems.
We propose the following pipeline: (1) the user provides a task goal in the
planning language PDDL, (2) a plan (i.e., a sequence of skills) is generated
and the learnable parameters of the skills are automatically identified. An
operator then chooses (3) reward functions and hyperparameters for the learning
process. Two aspects of our methodology are critical: (a) learning is tightly
integrated with a knowledge framework to support symbolic planning and to
provide priors for learning, (b) using multi-objective optimization. This can
help to balance key performance indicators (KPIs) such as safety and task
performance since they can often affect each other. We adopt a multi-objective
Bayesian optimization approach and learn entirely in simulation. We demonstrate
the efficacy and versatility of our approach by learning skill parameters for
two different contact-rich tasks. We show their successful execution on a real
7-DOF KUKA-iiwa manipulator and outperform the manual parameterization by human
robot operators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parametric Scaling of Preprocessing assisted U-net Architecture for
  Improvised Retinal Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kundan Kumar, Sumanshu Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting blood vessels from retinal fundus images plays a decisive role in
diagnosing the progression in pertinent diseases. In medical image analysis,
vessel extraction is a semantic binary segmentation problem, where blood
vasculature needs to be extracted from the background. Here, we present an
image enhancement technique based on the morphological preprocessing coupled
with a scaled U-net architecture. Despite a relatively less number of trainable
network parameters, the scaled version of U-net architecture provides better
performance compare to other methods in the domain. We validated the proposed
method on retinal fundus images from the DRIVE database. A significant
improvement as compared to the other algorithms in the domain, in terms of the
area under ROC curve (>0.9762) and classification accuracy (>95.47%) are
evident from the results. Furthermore, the proposed method is resistant to the
central vessel reflex while sensitive to detect blood vessels in the presence
of background items viz. exudates, optic disc, and fovea.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, ICAIHC-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Report from the NSF Future Directions Workshop on Automatic Evaluation
  of <span class="highlight-title">Dialog</span>: Research Directions and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikib Mehri, Jinho Choi, Luis Fernando D'Haro, Jan Deriu, Maxine Eskenazi, Milica Gasic, Kallirroi Georgila, Dilek Hakkani-Tur, Zekang Li, Verena Rieser, Samira Shaikh, David Traum, Yi-Ting Yeh, <span class="highlight-author">Zhou Yu</span>, Yizhe Zhang, Chen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Report from the NSF AED Workshop (http://dialrc.org/AED/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing EEG Data with Machine and Deep Learning: A Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Avola, Marco Cascio, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti, Marco Raoul Marini, Daniele Pannone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, machine and deep learning techniques are widely used in different
areas, ranging from economics to biology. In general, these techniques can be
used in two ways: trying to adapt well-known models and architectures to the
available data, or designing custom architectures. In both cases, to speed up
the research process, it is useful to know which type of models work best for a
specific problem and/or data type. By focusing on EEG signal analysis, and for
the first time in literature, in this paper a benchmark of machine and deep
learning for EEG signal classification is proposed. For our experiments we used
the four most widespread models, i.e., multilayer perceptron, convolutional
neural network, long short-term memory, and gated recurrent unit, highlighting
which one can be a good starting point for developing EEG classification
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference, 11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra-low Latency Spiking Neural Networks with Spatio-Temporal
  Compression and Synaptic Convolutional Block 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changqing Xu, Yi Liu, Yintang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), as one of the brain-inspired models, has
spatio-temporal information processing capability, low power feature, and high
biological plausibility. The effective spatio-temporal feature makes it
suitable for event streams classification. However, neuromorphic datasets, such
as N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events
into frames with a new higher temporal resolution for event stream
classification, which causes high training and inference latency. In this work,
we proposed a spatio-temporal compression method to aggregate individual events
into a few time steps of synaptic current to reduce the training and inference
latency. To keep the accuracy of SNNs under high compression ratios, we also
proposed a synaptic convolutional block to balance the dramatic change between
adjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with
learnable membrane time constant is introduced to increase its information
processing capability. We evaluate the proposed method for event streams
classification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture
datasets. The experiment results show that our proposed method outperforms the
state-of-the-art accuracy on nearly all datasets, using fewer time steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of Top-hat Transformation for Enhanced Blood Vessel
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tithi Parna Das, Sheetal Praharaj, Sarita Swain, Sumanshu Agarwal, Kundan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the medical domain, different computer-aided diagnosis systems have been
proposed to extract blood vessels from retinal fundus images for the clinical
treatment of vascular diseases. Accurate extraction of blood vessels from the
fundus images using a computer-generated method can help the clinician to
produce timely and accurate reports for the patient suffering from these
diseases. In this article, we integrate top-hat based preprocessing approach
with fine-tuned B-COSFIRE filter to achieve more accurate segregation of blood
vessel pixels from the background. The use of top-hat transformation in the
preprocessing stage enhances the efficacy of the algorithm to extract blood
vessels in presence of structures like fovea, exudates, haemorrhages, etc.
Furthermore, to reduce the false positives, small clusters of blood vessel
pixels are removed in the postprocessing stage. Further, we find that the
proposed algorithm is more efficient as compared to various modern algorithms
reported in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, ICAIHC-2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FORCE: A Framework of Rule-Based <span class="highlight-title">Conversation</span>al Recommender System <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Quan, Ze Wei, Qiang Gan, Jingqi Yao, Jingyi Lu, Yuchen Dong, Yiming Liu, Yi Zeng, Chao Zhang, Yongzhi Li, Huang Hu, Yingying He, Yang Yang, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2022 (Demonstration Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Text <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Pre-train</span>ing for Medical Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjin Park, Seongsu Bae, Jiho Kim, Tackeun Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the volume of Electronic Health Records (EHR) sharply grows, there has
been emerging interest in learning the representation of EHR for healthcare
applications. Representation learning of EHR requires appropriate modeling of
the two dominant modalities in EHR: structured data and unstructured text. In
this paper, we present MedGTX, a pre-trained model for multi-modal
representation learning of the structured and textual EHR data. MedGTX uses a
novel graph encoder to exploit the graphical nature of structured EHR data, and
a text encoder to handle unstructured text, and a cross-modal encoder to learn
a joint representation space. We pre-train our model through four proxy tasks
on MIMIC-III, an open-source EHR data, and evaluate our model on two clinical
benchmarks and three novel downstream tasks which tackle real-world problems in
EHR data. The results consistently show the effectiveness of pre-training the
model for joint representation of both structured and unstructured information
from EHR. Given the promising performance of MedGTX, we believe this work opens
a new door to jointly understanding the two fundamental modalities of EHR data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of the Conference on Health, Inference, and
  Learning (CHIL 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion and Volume Maximization-Based Clustering of Highly Mixed
  Hyperspectral Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam L. Polk, Kangning Cui, Robert J. Plemmons, James M. Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images of a scene or object are a rich data source, often
encoding a hundred or more spectral bands of reflectance at each pixel. Despite
being very high-dimensional, these images typically encode latent
low-dimensional structure that can be exploited for material discrimination.
However, due to an inherent trade-off between spectral and spatial resolution,
many hyperspectral images are generated at a coarse spatial scale, and single
pixels may correspond to spatial regions containing multiple materials. This
article introduces the \emph{Diffusion and Volume maximization-based Image
Clustering} (\emph{D-VIC}) algorithm for unsupervised material discrimination.
D-VIC locates cluster modes -- high-density, high-purity pixels in the
hyperspectral image that are far in diffusion distance (a data-dependent
distance metric) from other high-density, high-purity pixels -- and assigns
these pixels unique labels, as these points are meant to exemplify underlying
material structure. Non-modal pixels are labeled according to their diffusion
distance nearest neighbor of higher density and purity that is already labeled.
By directly incorporating pixel purity into its modal and non-modal labeling,
D-VIC upweights pixels that correspond to a spatial region containing just a
single material, yielding more interpretable clusterings. D-VIC is shown to
outperform baseline and comparable state-of-the-art methods in extensive
numerical experiments on a range of hyperspectral images, implying that it is
well-equipped for material discrimination and clustering of these data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Storage on Synthetic DNA Using Autoencoders <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Pic, Marc Antonini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past years, the ever-growing trend on data storage demand, more
specifically for "cold" data (rarely accessed data), has motivated research for
alternative systems of data storage. Because of its biochemical
characteristics, synthetic DNA molecules are now considered as serious
candidates for this new kind of storage. This paper presents some results on
lossy image compression methods based on convolutional autoencoders adapted to
DNA data storage.
  The model architectures presented here have been designed to efficiently
compress images, encode them into a quaternary code, and finally store them
into synthetic DNA molecules. This work also aims at making the compression
models better fit the problematics that we encounter when storing data into
DNA, namely the fact that the DNA writing, storing and reading methods are
error prone processes. The main take away of this kind of compressive
autoencoder is our quantization and the robustness to substitution errors
thanks to the noise model that we use during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICIP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad-Javad Darvishi-Bayazi, Guillaume Dumas, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models often fail to generalize well under distributional
shifts. Understanding and overcoming these failures have led to a research
field of Out-of-Distribution (OOD) generalization. Despite being extensively
studied for static computer vision tasks, OOD generalization has been
underexplored for time series tasks. To shine light on this gap, we present
WOODS: eight challenging open-source time series benchmarks covering a diverse
range of data modalities, such as videos, brain recordings, and sensor signals.
We revise the existing OOD generalization algorithms for time series tasks and
evaluate them using our systematic framework. Our experiments show a large room
for improvement for empirical risk minimization and OOD generalization
algorithms on our datasets, thus underscoring the new challenges posed by time
series tasks. Code and documentation are available at
https://woods-benchmarks.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIOS: An Algorithmically Generated Biomedical Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Yu, Zheng Yuan, Jun Xia, Shengxuan Luo, Huaiyuan Ying, Sihang Zeng, Jingyi Ren, Hongyi Yuan, Zhengyun Zhao, Yucong Lin, Keming Lu, Jing Wang, Yutao Xie, Heung-Yeung Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for
biomedical and healthcare big data and artificial intelligence (AI),
facilitating natural language processing, model development, and data exchange.
For many decades, these knowledge graphs have been built via expert curation,
which can no longer catch up with the speed of today's AI development, and a
transition to algorithmically generated BioMedKGs is necessary. In this work,
we introduce the Biomedical Informatics Ontology System (BIOS), the first large
scale publicly available BioMedKG that is fully generated by machine learning
algorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in
two languages, and 7.3 million relation triplets. We introduce the methodology
for developing BIOS, which covers curation of raw biomedical terms,
computationally identifying synonymous terms and aggregating them to create
concept nodes, semantic type classification of the concepts, relation
identification, and biomedical machine translation. We provide statistics about
the current content of BIOS and perform preliminary assessment for term
quality, synonym grouping, and relation extraction. Results suggest that
machine learning-based BioMedKG development is a totally viable solution for
replacing traditional expert curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Lithuanian grammatical error correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyone wants to write beautiful and correct text, yet the lack of language
skills, experience, or hasty typing can result in errors. By employing the
recent advances in transformer architectures, we construct a grammatical error
correction model for Lithuanian, the language rich in archaic features. We
compare subword and byte-level approaches and share our best trained model,
achieving F$_{0.5}$=0.92, and accompanying code, in an online open-source
repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SS-SAM : Stochastic Scheduled Sharpness-Aware Minimization for
  Efficiently Training Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Hao Zhang, Xiuyuan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By driving optimizers to converge to flat minima, sharpness-aware
minimization (SAM) has shown the power to improve the model generalization.
However, SAM requires to perform two forward-backward propagations for one
parameter update, which largely burdens the practical computation. In this
paper, we propose a novel and efficient training scheme, called Stochastic
Scheduled SAM (SS-SAM). Specifically, in SS-SAM, the optimizer is arranged by a
predefined scheduling function to perform a random trial at each update step,
which would randomly select to perform the SGD optimization or the SAM
optimization. In this way, the overall count of propagation pair could be
largely reduced. Then, we empirically investigate four typical types of
scheduling functions, and demonstrates the computational efficiency and their
impact on model performance respectively. We show that with proper scheduling
functions, models could be trained to achieve comparable or even better
performance with much lower computation cost compared to models trained with
only SAM training scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defending Variational Autoencoders from Adversarial Attacks with MCMC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kuzina, Max Welling, Jakub M. Tomczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational autoencoders (VAEs) are deep generative models used in various
domains. VAEs can generate complex objects and provide meaningful latent
representations, which can be further used in downstream tasks such as
classification. As previous work has shown, one can easily fool VAEs to produce
unexpected latent representations and reconstructions for a visually slightly
modified input. Here, we examine several objective functions for adversarial
attacks construction, suggest metrics assess the model robustness, and propose
a solution to alleviate the effect of an attack. Our method utilizes the Markov
Chain Monte Carlo (MCMC) technique in the inference step and is motivated by
our theoretical analysis. Thus, we do not incorporate any additional costs
during training or we do not decrease the performance on non-attacked inputs.
We validate our approach on a variety of datasets (MNIST, Fashion MNIST, Color
MNIST, CelebA) and VAE configurations ($\beta$-VAE, NVAE, TC-VAE) and show that
it consistently improves the model robustness to adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing SONN Types for Efficient Robot Motion Planning in the
  Configuration Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Steffen, Tobias Weyer, Katharina Glueck, Stefan Ulbrich, Arne Roennau, Rüdiger Dillmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning in the configuration space (C-space) induces benefits, such
as smooth trajectories. It becomes more complex as the degrees of freedom (DOF)
increase. This is due to the direct relation between the dimensionality of the
search space and the DOF. Self-organizing neural networks (SONN) and their
famous candidate, the Self-Organizing Map, have been proven to be useful tools
for C-space reduction while preserving its underlying topology, as presented in
[29]. In this work, we extend our previous study with additional models and
adapt the approach from human motion data towards robots' kinematics. The
evaluation includes the best performant models from [29] and three additional
SONN architectures, representing the consequent continuation of this previous
work. Generated Trajectories, planned with the different SONN models, were
successfully tested in a robot simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Simultaneous Sparse Approximation with Applications to
  RGB-NIR Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farshad G. Veshki, Sergiy A. Vorobyov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous sparse approximation (SSA) seeks to represent a set of dependent
signals using sparse vectors with identical supports. The SSA model has been
used in various signal and image processing applications involving multiple
correlated input signals. In this paper, we propose algorithms for
convolutional SSA (CSSA) based on the alternating direction method of
multipliers. Specifically, we address the CSSA problem with different sparsity
structures and the convolutional feature learning problem in multimodal
data/signals based on the SSA model. We evaluate the proposed algorithms by
applying them to multimodal and multifocus image fusion problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why we need biased AI -- How including cognitive and ethical machine
  biases can enhance AI systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Fabi, Thilo Hagendorff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper stresses the importance of biases in the field of artificial
intelligence (AI) in two regards. First, in order to foster efficient
algorithmic decision-making in complex, unstable, and uncertain real-world
environments, we argue for the structurewise implementation of human cognitive
biases in learning algorithms. Secondly, we argue that in order to achieve
ethical machine behavior, filter mechanisms have to be applied for selecting
biased training stimuli that represent social or behavioral traits that are
ethically desirable. We use insights from cognitive science as well as ethics
and apply them to the AI field, combining theoretical considerations with seven
case studies depicting tangible bias implementation scenarios. Ultimately, this
paper is the first tentative step to explicitly pursue the idea of a
re-evaluation of the ethical significance of machine biases, as well as putting
the idea forth to implement cognitive biases into machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight Instrument-Agnostic Model for Polyphonic Note
  Transcription and Multipitch Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel M. Bittner, Juan José Bosch, David Rubinstein, Gabriel Meseguer-Brocal, Sebastian Ewert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Music Transcription (AMT) has been recognized as a key enabling
technology with a wide range of applications. Given the task's complexity, best
results have typically been reported for systems focusing on specific settings,
e.g. instrument-specific systems tend to yield improved results over
instrument-agnostic methods. Similarly, higher accuracy can be obtained when
only estimating frame-wise $f_0$ values and neglecting the harder note event
detection. Despite their high accuracy, such specialized systems often cannot
be deployed in the real-world. Storage and network constraints prohibit the use
of multiple specialized models, while memory and run-time constraints limit
their complexity. In this paper, we propose a lightweight neural network for
musical instrument transcription, which supports polyphonic outputs and
generalizes to a wide variety of instruments (including vocals). Our model is
trained to jointly predict frame-wise onsets, multipitch and note activations,
and we experimentally show that this multi-output structure improves the
resulting frame-level note accuracy. Despite its simplicity, benchmark results
show our system's note estimation to be substantially better than a comparable
baseline, and its frame-level accuracy to be only marginally below those of
specialized state-of-the-art AMT systems. With this work we hope to encourage
the community to further investigate low-resource, instrument-agnostic AMT
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypergraph Modeling via Spectral Embedding Connection: Hypergraph Cut,
  Weighted Kernel $k$-means, and Heat Kernel <span class="chip">AAAI-22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shota Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a theoretical framework of multi-way similarity to model
real-valued data into hypergraphs for clustering via spectral embedding. For
graph cut based spectral clustering, it is common to model real-valued data
into graph by modeling pairwise similarities using kernel function. This is
because the kernel function has a theoretical connection to the graph cut. For
problems where using multi-way similarities are more suitable than pairwise
ones, it is natural to model as a hypergraph, which is generalization of a
graph. However, although the hypergraph cut is well-studied, there is not yet
established a hypergraph cut based framework to model multi-way similarity. In
this paper, we formulate multi-way similarities by exploiting the theoretical
foundation of kernel function. We show a theoretical connection between our
formulation and hypergraph cut in two ways, generalizing both weighted kernel
$k$-means and the heat kernel, by which we justify our formulation. We also
provide a fast algorithm for spectral clustering. Our algorithm empirically
shows better performance than existing graph and other heuristic modeling
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of our AAAI-22 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification of Hypokinetic Dysarthria Using Acoustic Analysis of Poem
  Recitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Mucha, Zoltan Galaz, Jiri Mekyska, Tomas Kiska, Vojtech Zvoncak, Zdenek Smekal, Ilona Eliasova, Martina Mrackova, Milena Kostalova, Irena Rektorova, Marcos Faundez-Zanuy, Jesus B. Alonso-Hernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Up to 90 % of patients with Parkinson's disease (PD) suffer from hypokinetic
dysarthria (HD). In this work, we analysed the power of conventional speech
features quantifying imprecise articulation, dysprosody, speech dysfluency and
speech quality deterioration extracted from a specialized poem recitation task
to discriminate dysarthric and healthy speech. For this purpose, 152 speakers
(53 healthy speakers, 99 PD patients) were examined. Only mildly strong
correlation between speech features and clinical status of the speakers was
observed. In the case of univariate classification analysis, sensitivity of
62.63% (imprecise articulation), 61.62% (dysprosody), 71.72% (speech
dysfluency) and 59.60% (speech quality deterioration) was achieved.
Multivariate classification analysis improved the classification performance.
Sensitivity of 83.42% using only two features describing imprecise articulation
and speech quality deterioration in HD was achieved. We showed the promising
potential of the selected speech features and especially the use of poem
recitation task to quantify and identify HD in PD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-wise Classifier Design Capable of Continual Learning using
  Adaptive Resonance Theory-based Topological Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Masuyama, Itsuki Tsubota, Yusuke Nojima, Hisao Ishibuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a supervised classification algorithm capable of
continual learning by utilizing an Adaptive Resonance Theory (ART)-based
growing self-organizing clustering algorithm. The ART-based clustering
algorithm is theoretically capable of continual learning, and the proposed
algorithm independently applies it to each class of training data for
generating classifiers. Whenever an additional training data set from a new
class is given, a new ART-based clustering will be defined in a different
learning space. Thanks to the above-mentioned features, the proposed algorithm
realizes continual learning capability. Simulation experiments showed that the
proposed algorithm has superior classification performance compared with
state-of-the-art clustering-based classification algorithms capable of
continual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review. arXiv admin note: substantial
  text overlap with arXiv:2201.10713</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite-sample analysis of identification of switched linear systems with
  arbitrary or restricted switching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengling Shi, Othmane Mazhar, Bart De Schutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims to derive a data-independent finite-sample error bound for the
least-squares (LS) estimation error of switched linear systems when the state
and the switching signal are measured. While the existing finite-sample bounds
for linear system identification extend to the problem under consideration, the
Gramian of the switched system, an essential term in the error bound, depends
on the measured switching signal. Therefore, data-independent bounds on the
spectrum of the Gramian are developed for globally asymptotically and
marginally stable switched systems when the switching is arbitrary or subject
to an average dwell time constraint. Combining the bounds on the spectrum of
the Gramian and the preliminary error bound extended from linear system
identification leads to the error bound for the LS estimate of the switched
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision-Making under Miscalibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy N. Rothblum, Gal Yona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ML-based predictions are used to inform consequential decisions about
individuals. How should we use predictions (e.g., risk of heart attack) to
inform downstream binary classification decisions (e.g., undergoing a medical
procedure)? When the risk estimates are perfectly calibrated, the answer is
well understood: a classification problem's cost structure induces an optimal
treatment threshold $j^{\star}$. In practice, however, some amount of
miscalibration is unavoidable, raising a fundamental question: how should one
use potentially miscalibrated predictions to inform binary decisions? We
formalize a natural (distribution-free) solution concept: given anticipated
miscalibration of $\alpha$, we propose using the threshold $j$ that minimizes
the worst-case regret over all $\alpha$-miscalibrated predictors, where the
regret is the difference in clinical utility between using the threshold in
question and using the optimal threshold in hindsight. We provide closed form
expressions for $j$ when miscalibration is measured using both expected and
maximum calibration error, which reveal that it indeed differs from $j^{\star}$
(the optimal threshold under perfect calibration). We validate our theoretical
findings on real data, demonstrating that there are natural cases in which
making decisions using $j$ improves the clinical utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Predictor for Black-Box Adversarial Attacks on Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marie Biolková, Bac Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have revealed the vulnerability of automatic speech recognition
(ASR) models to adversarial examples (AEs), i.e., small perturbations that
cause an error in the transcription of the audio signal. Studying audio
adversarial attacks is therefore the first step towards robust ASR. Despite the
significant progress made in attacking audio examples, the black-box attack
remains challenging because only the hard-label information of transcriptions
is provided. Due to this limited information, existing black-box methods often
require an excessive number of queries to attack a single audio example. In
this paper, we introduce NP-Attack, a neural predictor-based method, which
progressively evolves the search towards a small adversarial perturbation.
Given a perturbation direction, our neural predictor directly estimates the
smallest perturbation that causes a mistranscription. In particular, it enables
NP-Attack to accurately learn promising perturbation directions via
gradient-based optimization. Experimental results show that NP-Attack achieves
competitive results with other state-of-the-art black-box adversarial attacks
while requiring a significantly smaller number of queries. The code of
NP-Attack is available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gender classification by means of online uppercase handwriting: A
  text-dependent allographic approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enric Sesa-Nogueras, Marcos Faundez-Zanuy, Josep Roure-Alcobé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a gender classification schema based on online
handwriting. Using samples acquired with a digital tablet that captures the
dynamics of the writing, it classifies the writer as a male or a female. The
method proposed is allographic, regarding strokes as the structural units of
handwriting. Strokes performed while the writing device is not exerting any
pressure on the writing surface, pen-up (in-air) strokes, are also taken into
account. The method is also text-dependent meaning that training and testing is
done with exactly the same text. Text-dependency allows classification be
performed with very small amounts of text. Experimentation, performed with
samples from the BiosecurID database, yields results that fall in the range of
the classification averages expected from human judges. With only four
repetitions of a single uppercase word, the average rate of well classified
writers is 68%; with sixteen words, the rate rises to an average 72.6%.
Statistical analysis reveals that the aforementioned rates are highly
significant. In order to explore the classification potential of the pen-up
strokes, these are also considered. Although in this case results are not
conclusive, an outstanding average of 74% of well classified writers is
obtained when information from pen-up strokes is combined with information from
pen-down ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, published in Cogn Comput 8, pages 15 to 29, year 2016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Representative Subset Selection for <span class="highlight-title">Self-Supervised</span> Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech recognition models require considerable labeled
training data for learning high-fidelity representations for Automatic Speech
Recognition (ASR), which hinders their application to low-resource languages.
We consider the task of identifying an optimal subset of training data to
fine-tune self-supervised speech models for ASR. We make a surprising
observation that active learning strategies for sampling harder-to-learn
examples do not perform better than random subset selection for fine-tuning
self-supervised ASR. We then present the COWERAGE algorithm for better subset
selection in self-supervised ASR which is based on our finding that ensuring
the coverage of examples based on training WER in the early training epochs
leads to better generalization performance. Extensive experiments on the
wav2vec 2.0 model and TIMIT dataset show the effectiveness of COWERAGE, with up
to 27% absolute WER improvement over active learning methods. We also report
the connection between training WER and the phonemic cover and demonstrate that
our algorithm ensures inclusion of phonemically diverse examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Cross-Modal</span> Perceptionist: Can Face Geometry be Gleaned from Voices? <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cho-Ying Wu, Chin-Cheng Hsu, Ulrich Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work digs into a root question in human perception: can face geometry be
gleaned from one's voices? Previous works that study this question only adopt
developments in image synthesis and convert voices into face images to show
correlations, but working on the image domain unavoidably involves predicting
attributes that voices cannot hint, including facial textures, hairstyles, and
backgrounds. We instead investigate the ability to reconstruct 3D faces to
concentrate on only geometry, which is much more physiologically grounded. We
propose our analysis framework, Cross-Modal Perceptionist, under both
supervised and unsupervised learning. First, we construct a dataset,
Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,
making supervised learning possible. Second, we use a knowledge distillation
mechanism to study whether face geometry can still be gleaned from voices
without paired voices and 3D face data under limited availability of 3D face
scans. We break down the core question into four parts and perform visual and
numerical analyses as responses to the core question. Our findings echo those
in physiology and neuroscience about the correlation between voices and facial
structures. The work provides future human-centric cross-modal learning with
explainable foundations. See our project page:
https://choyingw.github.io/works/Voice2Mesh/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2022. Project page:
  https://choyingw.github.io/works/Voice2Mesh/index.html. This version
  supersedes arXiv:2104.10299</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are You Ro<span class="highlight-title">bert</span> or Ro<span class="highlight-title">BERT</span>a? Deceiving Online Authorship Attribution
  Models Using Neural Text Generators <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keenan Jones, Jason R. C. Nurse, Shujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a rise in the development of powerful pre-trained
natural language models, including GPT-2, Grover, and XLM. These models have
shown state-of-the-art capabilities towards a variety of different NLP tasks,
including question answering, content summarisation, and text generation.
Alongside this, there have been many studies focused on online authorship
attribution (AA). That is, the use of models to identify the authors of online
texts. Given the power of natural language models in generating convincing
texts, this paper examines the degree to which these language models can
generate texts capable of deceiving online AA models. Experimenting with both
blog and Twitter data, we utilise GPT-2 language models to generate texts using
the existing posts of online users. We then examine whether these AI-based text
generators are capable of mimicking authorial style to such a degree that they
can deceive typical AA models. From this, we find that current AI-based text
generators are able to successfully mimic authorship, showing capabilities
towards this on both datasets. Our findings, in turn, highlight the current
capacity of powerful natural language models to generate original online posts
capable of mimicking authorial style sufficiently to deceive popular AA
methods; a key finding given the proposed role of AA in real world applications
such as spam-detection and forensic investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 4 tables, Accepted for publication in the
  proceedings of the sixteenth International AAAI Conference on Web and Social
  Media (ICWSM-22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dencentralized learning in the presence of low-rank noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roula Nassif, Virginia Bordignon, Stefan Vlaski, Ali H. Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Observations collected by agents in a network may be unreliable due to
observation noise or interference. This paper proposes a distributed algorithm
that allows each node to improve the reliability of its own observation by
relying solely on local computations and interactions with immediate neighbors,
assuming that the field (graph signal) monitored by the network lies in a
low-dimensional subspace and that a low-rank noise is present in addition to
the usual full-rank noise. While oblique projections can be used to project
measurements onto a low-rank subspace along a direction that is oblique to the
subspace, the resulting solution is not distributed. Starting from the
centralized solution, we propose an algorithm that performs the oblique
projection of the overall set of observations onto the signal subspace in an
iterative and distributed manner. We then show how the oblique projection
framework can be extended to handle distributed learning and adaptation
problems over networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximal Policy Optimization with Adaptive Threshold for Symmetric
  Relative Density Ratio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taisuke Kobayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) is one of the promising approaches for
introducing robots into complicated environments. The recent remarkable
progress of DRL stands on regularization of policy, which allows the policy to
improve stably and efficiently. A popular method, so-called proximal policy
optimization (PPO), and its variants constrain density ratio of the latest and
baseline policies when the density ratio exceeds a given threshold. This
threshold can be designed relatively intuitively, and in fact its recommended
value range has been suggested. However, the density ratio is asymmetric for
its center, and the possible error scale from its center, which should be close
to the threshold, would depend on how the baseline policy is given. In order to
maximize the values of regularization of policy, this paper proposes a new PPO
derived using relative Pearson (RPE) divergence, therefore so-called PPO-RPE,
to design the threshold adaptively. In PPO-RPE, the relative density ratio,
which can be formed with symmetry, replaces the raw density ratio. Thanks to
this symmetry, its error scale from center can easily be estimated, hence, the
threshold can be adapted for the estimated error scale. From three simple
benchmark simulations, the importance of algorithm-dependent threshold design
is revealed. By simulating additional four locomotion tasks, it is verified
that the proposed method statistically contributes to task accomplishment by
appropriately restricting the policy updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdIoTack: Quantifying and Refining Resilience of Decision Tree Ensemble
  Inference Models against Adversarial Volumetric Attacks on IoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Pashamokhtari, Gustavo Batista, Hassan Habibi Gharakheili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning-based techniques have shown success in cyber intelligence.
However, they are increasingly becoming targets of sophisticated data-driven
adversarial attacks resulting in misprediction, eroding their ability to detect
threats on network devices. In this paper, we present AdIoTack, a system that
highlights vulnerabilities of decision trees against adversarial attacks,
helping cybersecurity teams quantify and refine the resilience of their trained
models for monitoring IoT networks. To assess the model for the worst-case
scenario, AdIoTack performs white-box adversarial learning to launch successful
volumetric attacks that decision tree ensemble models cannot flag. Our first
contribution is to develop a white-box algorithm that takes a trained decision
tree ensemble model and the profile of an intended network-based attack on a
victim class as inputs. It then automatically generates recipes that specify
certain packets on top of the indented attack packets (less than 15% overhead)
that together can bypass the inference model unnoticed. We ensure that the
generated attack instances are feasible for launching on IP networks and
effective in their volumetric impact. Our second contribution develops a method
to monitor the network behavior of connected devices actively, inject
adversarial traffic (when feasible) on behalf of a victim IoT device, and
successfully launch the intended attack. Our third contribution prototypes
AdIoTack and validates its efficacy on a testbed consisting of a handful of
real IoT devices monitored by a trained inference model. We demonstrate how the
model detects all non-adversarial volumetric attacks on IoT devices while
missing many adversarial ones. The fourth contribution develops systematic
methods for applying patches to trained decision tree ensemble models,
improving their resilience against adversarial volumetric attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 16 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constitutive model characterization and discovery using physics-informed
  deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Haghighat, Sahar Abouali, Reza Vaziri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classically, the mechanical response of materials is described through
constitutive models, often in the form of constrained ordinary differential
equations. These models have a very limited number of parameters, yet, they are
extremely efficient in reproducing complex responses observed in experiments.
Additionally, in their discretized form, they are computationally very
efficient, often resulting in a simple algebraic relation, and therefore they
have been extensively used within large-scale explicit and implicit finite
element models. However, it is very challenging to formulate new constitutive
models, particularly for materials with complex microstructures such as
composites. A recent trend in constitutive modeling leverages complex neural
network architectures to construct complex material responses where a
constitutive model does not yet exist. Whilst very accurate, they suffer from
two deficiencies. First, they are interpolation models and often do poorly in
extrapolation. Second, due to their complex architecture and numerous
parameters, they are inefficient to be used as a constitutive model within a
large-scale finite element model. In this study, we propose a novel approach
based on the physics-informed learning machines for the characterization and
discovery of constitutive models. Unlike data-driven constitutive models, we
leverage foundations of elastoplasticity theory as regularization terms in the
total loss function to find parametric constitutive models that are also
theoretically sound. We demonstrate that our proposed framework can efficiently
identify the underlying constitutive model describing different datasets from
the von Mises family.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISDE : Independence Structure Density Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Pujol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Density estimation appears as a subroutine in many learning procedures, so it
is of interest to have efficient methods for it to perform in practical
situations. Multidimensional density estimation suffers from the curse of
dimensionality. A solution to this problem is to add a structural hypothesis
through an undirected graphical model on the underlying distribution. We
propose ISDE (Independence Structure Density Estimation), an algorithm designed
to estimate a density and an undirected graphical model from a particular
family of graphs corresponding to Independence Structure (IS), a situation
where we can separate features into independent groups. ISDE works for
moderately high-dimensional data (up to a few dozen features), and it is
useable in parametric and nonparametric situations. Existing methods on
nonparametric graphical model estimation focus on multidimensional dependencies
only through pairwise ones: ISDE does not suffer from this restriction and can
address structures not yet covered by available algorithms. In this paper, we
present the existing theory about IS, explain the construction of our algorithm
and prove its effectiveness. This is done on synthetic data both
quantitatively, through measures of density estimation performance under
Kullback-Leibler loss, and qualitatively, in terms of capability to recover IS.
By applying ISDE on mass cytometry datasets, we also show how it performs both
quantitatively and qualitatively on real-world datasets. Then we provide
information about running time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Class-Modelling for Decentralized Source Attribution of
  GAN-Generated Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon B. G. Khoo, Chern Hong Lim, Raphael C. -W. Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GAN-generated deepfakes as a genre of digital images are gaining ground as
both catalysts of artistic expression and malicious forms of deception,
therefore demanding systems to enforce and accredit their ethical use. Existing
techniques for the source attribution of synthetic images identify subtle
intrinsic fingerprints using multiclass classification neural nets limited in
functionality and scalability. Hence, we redefine the deepfake detection and
source attribution problems as a series of related binary classification tasks.
We leverage transfer learning to rapidly adapt forgery detection networks for
multiple independent attribution problems, by proposing a semi-decentralized
modular design to solve them simultaneously and efficiently. Class activation
mapping is also demonstrated as an effective means of feature localization for
model interpretation. Our models are determined via experimentation to be
competitive with current benchmarks, and capable of decent performance on human
portraits in ideal conditions. Decentralized fingerprint-based attribution is
found to retain validity in the presence of novel sources, but is more
susceptible to type II errors that intensify with image perturbations and
attributive uncertainty. We describe both our conceptual framework and model
prototypes for further enhancement when investigating the technical limits of
reactive deepfake attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures. Code:
  https://github.com/quarxilon/Generator_Attribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Prototypical Verbalizer for <span class="highlight-title">Prompt</span>-based Few-shot Tuning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, <span class="highlight-author">Zhiyuan Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. ACL 2022 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker Embedding-aware Neural Diarization: a Novel Framework for
  Overlapped Speech Diarization in the Meeting Scenario <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Du, Shiliang Zhang, Siqi Zheng, Zhijie Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we reformulate overlapped speech diarization as a single-label
prediction problem, which is always treated as a multi-label classification
task in previous studies. Specifically, the multiple labels of each frame are
encoded into a single label with the power set, which represents the possible
combinations of different speakers. Through this formulation, we propose the
speaker embedding-aware neural diarization (SEND) system. In SEND, the speech
encoder, speaker encoder, similarity scores, and post-processing network are
optimized to predict the power set encoded labels according to the similarities
between speech features and speaker embeddings. Experimental results show that
our method significantly outperforms the variational Bayesian hidden Markov
model-based clustering algorithm (VBx). Besides, the proposed method has two
benefits compared with the target-speaker voice activity detection (TSVAD).
First, SEND can achieve lower diarization error rates in the real meeting
scenario. Second, when the training data has a high overlap ratio, the learning
process of SEND is more stable than TSVAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022, 5 parges, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqiao Li, Xiaotao Liu, Jian Zhao, Furao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have been proven to be vulnerable to adversarial
examples. A special branch of adversarial examples, namely sparse adversarial
examples, can fool the target DNNs by perturbing only a few pixels. However,
many existing sparse adversarial attacks use heuristic methods to select the
pixels to be perturbed, and regard the pixel selection and the adversarial
attack as two separate steps. From the perspective of neural network pruning,
we propose a novel end-to-end sparse adversarial attack method, namely
AutoAdversary, which can find the most important pixels automatically by
integrating the pixel selection into the adversarial attack. Specifically, our
method utilizes a trainable neural network to generate a binary mask for the
pixel selection. After jointly optimizing the adversarial perturbation and the
neural network, only the pixels corresponding to the value 1 in the mask are
perturbed. Experiments demonstrate the superiority of our proposed method over
several state-of-the-art methods. Furthermore, since AutoAdversary does not
require a heuristic pixel selection process, it does not slow down excessively
as other methods when the image size increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Sketching for Randomized Optimization: Exact
  Characterization, Concentration and Lower Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Bartan, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider distributed optimization methods for problems where forming the
Hessian is computationally challenging and communication is a significant
bottleneck. We leverage randomized sketches for reducing the problem dimensions
as well as preserving privacy and improving straggler resilience in
asynchronous distributed systems. We derive novel approximation guarantees for
classical sketching methods and establish tight concentration results that
serve as both upper and lower bounds on the error. We then extend our analysis
to the accuracy of parameter averaging for distributed sketches. Furthermore,
we develop unbiased parameter averaging methods for randomized second order
optimization for regularized problems that employ sketching of the Hessian.
Existing works do not take the bias of the estimators into consideration, which
limits their application to massively parallel computation. We provide
closed-form formulas for regularization parameters and step sizes that provably
minimize the bias for sketched Newton directions. Additionally, we demonstrate
the implications of our theoretical findings via large scale experiments on a
serverless cloud computing platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2002.06540</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look-Ahead Acquisition Functions for Bernoulli Level Set Estimation <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Letham, Phillip Guan, Chase Tymms, Eytan Bakshy, Michael Shvartsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Level set estimation (LSE) is the problem of identifying regions where an
unknown function takes values above or below a specified threshold. Active
sampling strategies for efficient LSE have primarily been studied in
continuous-valued functions. Motivated by applications in human psychophysics
where common experimental designs produce binary responses, we study LSE active
sampling with Bernoulli outcomes. With Gaussian process classification
surrogate models, the look-ahead model posteriors used by state-of-the-art
continuous-output methods are intractable. However, we derive analytic
expressions for look-ahead posteriors of sublevel set membership, and show how
these lead to analytic expressions for a class of look-ahead LSE acquisition
functions, including information-based methods. Benchmark experiments show the
importance of considering the global look-ahead impact on the entire posterior.
We demonstrate a clear benefit to using this new class of acquisition functions
on benchmark problems, and on a challenging real-world task of estimating a
high-dimensional contrast sensitivity function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In: Proceedings of the 25th International Conference on Artificial
  Intelligence and Statistics, AISTATS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Split-Mix Federated Learning for On-Demand and In-Situ
  Customization <span class="chip">ICLR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyuan Hong, Haotao Wang, Zhangyang Wang, Jiayu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) provides a distributed learning framework for
multiple participants to collaborate learning without sharing raw data. In many
practical FL scenarios, participants have heterogeneous resources due to
disparities in hardware and inference dynamics that require quickly loading
models of different sizes and levels of robustness. The heterogeneity and
dynamics together impose significant challenges to existing FL approaches and
thus greatly limit FL's applicability. In this paper, we propose a novel
Split-Mix FL strategy for heterogeneous participants that, once training is
done, provides in-situ customization of model sizes and robustness.
Specifically, we achieve customization by learning a set of base sub-networks
of different sizes and robustness levels, which are later aggregated on-demand
according to inference requirements. This split-mix strategy achieves
customization with high efficiency in communication, storage, and inference.
Extensive experiments demonstrate that our method provides better in-situ
customization than the existing heterogeneous-architecture FL methods. Codes
and pre-trained models are available: https://github.com/illidanlab/SplitMix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ICLR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Smoothness for Audio Inpainting Using a Latent Matrix Model in
  Delay-embedded Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuya Yokota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Here, we propose a new reconstruction method of smooth time-series signals. A
key concept of this study is not considering the model in signal space, but in
delay-embedded space. In other words, we indirectly represent a time-series
signal as an output of inverse delay-embedding of a matrix, and the matrix is
constrained. Based on the model under inverse delay-embedding, we propose to
constrain the matrix to be rank-1 with smooth factor vectors. The proposed
model is closely related to the convolutional model, and quadratic variation
(QV) regularization. Especially, the proposed method can be characterized as a
generalization of QV regularization. In addition, we show that the proposed
method provides the softer smoothness than QV regularization. Experiments of
audio inpainting and declipping are conducted to show its advantages in
comparison with several existing interpolation methods and sparse modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Deep Networks Transfer Invariances Across Classes? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To generalize well, classifiers must learn to be invariant to nuisance
transformations that do not alter an input's class. Many problems have
"class-agnostic" nuisance transformations that apply similarly to all classes,
such as lighting and background changes for image classification. Neural
networks can learn these invariances given sufficient data, but many real-world
datasets are heavily class imbalanced and contain only a few examples for most
of the classes. We therefore pose the question: how well do neural networks
transfer class-agnostic invariances learned from the large classes to the small
ones? Through careful experimentation, we observe that invariance to
class-agnostic transformations is still heavily dependent on class size, with
the networks being much less invariant on smaller classes. This result holds
even when using data balancing techniques, and suggests poor invariance
transfer across classes. Our results provide one explanation for why
classifiers generalize poorly on unbalanced and long-tailed distributions.
Based on this analysis, we show how a generative approach for learning the
nuisance transformations can help transfer invariances across classes and
improve performance on a set of imbalanced image classification benchmarks.
Source code for our experiments is available at
https://github.com/AllanYangZhou/generative-invariance-transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRBoost: <span class="highlight-title">Prompt</span>-Based Rule Discovery and Boosting for Interactive
  Weakly-Supervised Learning <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised learning (WSL) has shown promising results in addressing
label scarcity on many NLP tasks, but manually designing a comprehensive,
high-quality labeling rule set is tedious and difficult. We study interactive
weakly-supervised learning -- the problem of iteratively and automatically
discovering novel labeling rules from data to improve the WSL model. Our
proposed model, named PRBoost, achieves this goal via iterative prompt-based
rule discovery and model boosting. It uses boosting to identify large-error
instances and then discovers candidate rules from them by prompting pre-trained
LMs with rule templates. The candidate rules are judged by human experts, and
the accepted rules are used to generate complementary weak labels and
strengthen the current model. Experiments on four tasks show PRBoost
outperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with
fully supervised models. Our Implementation is available at
\url{https://github.com/rz-zhang/PRBoost}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 (Main Conference). Code: https://github.com/rz-zhang/PRBoost</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEFORM: A Practical, Universal Deep Beamforming System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai N. Nguyen, Guevara Noubir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce, design, and evaluate a set of universal receiver beamforming
techniques. Our approach and system DEFORM, a Deep Learning (DL) based RX
beamforming achieves significant gain for multi antenna RF receivers while
being agnostic to the transmitted signal features (e.g., modulation or
bandwidth). It is well known that combining coherent RF signals from multiple
antennas results in a beamforming gain proportional to the number of receiving
elements. However in practice, this approach heavily relies on explicit channel
estimation techniques, which are link specific and require significant
communication overhead to be transmitted to the receiver. DEFORM addresses this
challenge by leveraging Convolutional Neural Network to estimate the channel
characteristics in particular the relative phase to antenna elements. It is
specifically designed to address the unique features of wireless signals
complex samples, such as the ambiguous $2\pi$ phase discontinuity and the high
sensitivity of the link Bit Error Rate. The channel prediction is subsequently
used in the Maximum Ratio Combining algorithm to achieve an optimal combination
of the received signals. While being trained on a fixed, basic RF settings, we
show that DEFORM DL model is universal, achieving up to 3 dB of SNR gain for a
two antenna receiver in extensive experiments demonstrating various settings of
modulations, bandwidths, and channels. The universality of DEFORM is
demonstrated through joint beamforming relaying of LoRa (Chirp Spread Spectrum
modulation) and ZigBee signals, achieving significant improvements to Packet
Loss/Delivery Rates relatively to conventional Amplify and Forward (LoRa PLR
reduced by 23 times and ZigBee PDR increased by 8 times).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deterministic Bridge Regression for Compressive Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kar-Ann Toh, Giuseppe Molteni, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pattern classification with compact representation is an important component
in machine intelligence. In this work, an analytic bridge solution is proposed
for compressive classification. The proposal has been based upon solving a
penalized error formulation utilizing an approximated $\ell_p$-norm. The
solution comes in a primal form for over-determined systems and in a dual form
for under-determined systems. While the primal form is suitable for problems of
low dimension with large data samples, the dual form is suitable for problems
of high dimension but with a small number of data samples. The solution has
also been extended for problems with multiple classification outputs. Numerical
studies based on simulated and real-world data validated the effectiveness of
the proposed solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an AI-Driven Universal Anti-Jamming Solution with Convolutional
  Interference Cancellation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai N. Nguyen, Guevara Noubir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless links are increasingly used to deliver critical services, while
intentional interference (jamming) remains a very serious threat to such
services. In this paper, we are concerned with the design and evaluation of a
universal anti-jamming building block, that is agnostic to the specifics of the
communication link and can therefore be combined with existing technologies. We
believe that such a block should not require explicit probes, sounding,
training sequences, channel estimation, or even the cooperation of the
transmitter. To meet these requirements, we propose an approach that relies on
advances in Machine Learning, and the promises of neural accelerators and
software defined radios. We identify and address multiple challenges, resulting
in a convolutional neural network architecture and models for a multi-antenna
system to infer the existence of interference, the number of interfering
emissions and their respective phases. This information is continuously fed
into an algorithm that cancels the interfering signal. We develop a two-antenna
prototype system and evaluate our jamming cancellation approach in various
environment settings and modulation schemes using Software Defined Radio
platforms. We demonstrate that the receiving node equipped with our approach
can detect a jammer with over 99% of accuracy and achieve a Bit Error Rate
(BER) as low as $10^{-6}$ even when the jammer power is nearly two orders of
magnitude (18 dB) higher than the legitimate signal, and without requiring
modifications to the link modulation. In non-adversarial settings, our approach
can have other advantages such as detecting and mitigating collisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Stabilizable Deep Dynamics Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenji Kashima, Ryota Yoshiuchi, Yu Kawano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When neural networks are used to model dynamics, properties such as stability
of the dynamics are generally not guaranteed. In contrast, there is a recent
method for learning the dynamics of autonomous systems that guarantees global
exponential stability using neural networks. In this paper, we propose a new
method for learning the dynamics of input-affine control systems. An important
feature is that a stabilizing controller and control Lyapunov function of the
learned model are obtained as well. Moreover, the proposed method can also be
applied to solving Hamilton-Jacobi inequalities. The usefulness of the proposed
method is examined through numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for Privacy Preservation in Smart Healthcare Systems:
  A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansoor Ali, Faisal Naeem, Muhammad Tariq, Geroges Kaddoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in electronic devices and communication infrastructure have
revolutionized the traditional healthcare system into a smart healthcare system
by using IoMT devices. However, due to the centralized training approach of
artificial intelligence (AI), the use of mobile and wearable IoMT devices
raises privacy concerns with respect to the information that has been
communicated between hospitals and end users. The information conveyed by the
IoMT devices is highly confidential and can be exposed to adversaries. In this
regard, federated learning (FL), a distributive AI paradigm has opened up new
opportunities for privacy-preservation in IoMT without accessing the
confidential data of the participants. Further, FL provides privacy to end
users as only gradients are shared during training. For these specific
properties of FL, in this paper we present privacy related issues in IoMT.
Afterwards, we present the role of FL in IoMT networks for privacy preservation
and introduce some advanced FL architectures incorporating deep reinforcement
learning (DRL), digital twin, and generative adversarial networks (GANs) for
detecting privacy threats. Subsequently, we present some practical
opportunities of FL in smart healthcare systems. At the end, we conclude this
survey by providing open research challenges for FL that can be used in future
smart healthcare systems
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ tinyMAN: Lightweight Energy Manager using Reinforcement Learning for
  Energy Harvesting Wearable IoT Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toygun Basaklar, Yigit Tuncel, Umit Y. Ogras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in low-power electronics and machine learning techniques lead to
many novel wearable IoT devices. These devices have limited battery capacity
and computational power. Thus, energy harvesting from ambient sources is a
promising solution to power these low-energy wearable devices. They need to
manage the harvested energy optimally to achieve energy-neutral operation,
which eliminates recharging requirements. Optimal energy management is a
challenging task due to the dynamic nature of the harvested energy and the
battery energy constraints of the target device. To address this challenge, we
present a reinforcement learning-based energy management framework, tinyMAN,
for resource-constrained wearable IoT devices. The framework maximizes the
utilization of the target device under dynamic energy harvesting patterns and
battery constraints. Moreover, tinyMAN does not rely on forecasts of the
harvested energy which makes it a prediction-free approach. We deployed tinyMAN
on a wearable device prototype using TensorFlow Lite for Micro thanks to its
small memory footprint of less than 100 KB. Our evaluations show that tinyMAN
achieves less than 2.36 ms and 27.75 $\mu$J while maintaining up to 45% higher
utility compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, accepted as "Full Paper" for the 2022 tinyML
  Research Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Message-Passing Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.03717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.03717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Zhong, Cheng-Te Li, Jun Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become a prominent approach to machine
learning with graphs and have been increasingly applied in a multitude of
domains. Nevertheless, since most existing GNN models are based on flat
message-passing mechanisms, two limitations need to be tackled: (i) they are
costly in encoding long-range information spanning the graph structure; (ii)
they are failing to encode features in the high-order neighbourhood in the
graphs as they only perform information aggregation across the observed edges
in the original graph. To deal with these two issues, we propose a novel
Hierarchical Message-passing Graph Neural Networks framework. The key idea is
generating a hierarchical structure that re-organises all nodes in a flat graph
into multi-level super graphs, along with innovative intra- and inter-level
propagation manners. The derived hierarchy creates shortcuts connecting
far-away nodes so that informative long-range interactions can be efficiently
accessed via message passing and incorporates meso- and macro-level semantics
into the learned node representations. We present the first model to implement
this framework, termed Hierarchical Community-aware Graph Neural Network
(HC-GNN), with the assistance of a hierarchical community detection algorithm.
The theoretical analysis illustrates HC-GNN's remarkable capacity in capturing
long-range information without introducing heavy additional computation
complexity. Empirical experiments conducted on 9 datasets under transductive,
inductive, and few-shot settings exhibit that HC-GNN can outperform
state-of-the-art GNN models in network analysis tasks, including node
classification, link prediction, and community detection. Moreover, the model
analysis further demonstrates HC-GNN's robustness facing graph sparsity and the
flexibility in incorporating different GNN encoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multiscale spatiotemporal approach for smallholder irrigation
  detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terence Conlon, Christopher Small, Vijay Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In presenting an irrigation detection methodology that leverages multiscale
satellite imagery of vegetation abundance, this paper introduces a process to
supplement limited ground-collected labels and ensure classifier applicability
in an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced
Vegetation Index (EVI) timeseries characterizes native vegetation phenologies
at regional scale to provide the basis for a continuous phenology map that
guides supplementary label collection over irrigated and non-irrigated
agriculture. Subsequently, validated dry season greening and senescence cycles
observed in 10m Sentinel-2 imagery are used to train a suite of classifiers for
automated detection of potential smallholder irrigation. Strategies to improve
model robustness are demonstrated, including a method of data augmentation that
randomly shifts training samples; and an assessment of classifier types that
produce the best performance in withheld target regions. The methodology is
applied to detect smallholder irrigation in two states in the Ethiopian
highlands, Tigray and Amhara. Results show that a transformer-based neural
network architecture allows for the most robust prediction performance in
withheld regions, followed closely by a CatBoost random forest model. Over
withheld ground-collection survey labels, the transformer-based model achieves
96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated
samples. Over a larger set of samples independently collected via the
introduced method of label supplementation, non-irrigated and irrigated labels
are predicted with 98.3% and 95.5% accuracy, respectively. The detection model
is then deployed over Tigray and Amhara, revealing crop rotation patterns and
year-over-year irrigated area change. Predictions suggest that irrigated area
in these two states has decreased by approximately 40% from 2020 to 2021.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Priori Denoising Strategies for Sparse Identification of Nonlinear
  Dynamical Systems: A Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Cortiella, Kwang-Chun Park, Alireza Doostan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, identification of nonlinear dynamical systems from data has
become increasingly popular. Sparse regression approaches, such as Sparse
Identification of Nonlinear Dynamics (SINDy), fostered the development of novel
governing equation identification algorithms assuming the state variables are
known a priori and the governing equations lend themselves to sparse, linear
expansions in a (nonlinear) basis of the state variables. In the context of the
identification of governing equations of nonlinear dynamical systems, one faces
the problem of identifiability of model parameters when state measurements are
corrupted by noise. Measurement noise affects the stability of the recovery
process yielding incorrect sparsity patterns and inaccurate estimation of
coefficients of the governing equations. In this work, we investigate and
compare the performance of several local and global smoothing techniques to a
priori denoise the state measurements and numerically estimate the state
time-derivatives to improve the accuracy and robustness of two sparse
regression methods to recover governing equations: Sequentially Thresholded
Least Squares (STLS) and Weighted Basis Pursuit Denoising (WBPDN) algorithms.
We empirically show that, in general, global methods, which use the entire
measurement data set, outperform local methods, which employ a neighboring data
subset around a local point. We additionally compare Generalized Cross
Validation (GCV) and Pareto curve criteria as model selection techniques to
automatically estimate near optimal tuning parameters, and conclude that Pareto
curves yield better results. The performance of the denoising strategies and
sparse regression methods is empirically evaluated through well-known benchmark
problems of nonlinear dynamical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 24 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correcting diacritics and typos with a ByT5 <span class="highlight-title">transformer</span> model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Stankevičius, Mantas Lukoševičius, Jurgita Kapočiūtė-Dzikienė, Monika Briedienė, Tomas Krilavičius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the fast pace of life and online communications and the prevalence of
English and the QWERTY keyboard, people tend to forgo using diacritics, make
typographical errors (typos) when typing in other languages. Restoring
diacritics and correcting spelling is important for proper language use and the
disambiguation of texts for both humans and downstream algorithms. However,
both of these problems are typically addressed separately: the state-of-the-art
diacritics restoration methods do not tolerate other typos, but classical
spellcheckers also cannot deal adequately with all the diacritics missing. In
this work, we tackle both problems at once by employing the newly-developed
universal ByT5 byte-level seq2seq transformer model that requires no
language-specific model structures. For a comparison, we perform diacritics
restoration on benchmark datasets of 12 languages, with the addition of
Lithuanian. The experimental investigation proves that our approach is able to
achieve results (> 98%) comparable to the previous state-of-the-art, despite
being trained less and on fewer data. Our approach is also able to restore
diacritics in words not seen during training with > 76% accuracy. Our
simultaneous diacritics restoration and typos correction approach reaches > 94%
alpha-word accuracy on the 13 languages. It has no direct competitors and
strongly outperforms classical spell-checking or dictionary-based approaches.
We also demonstrate all the accuracies to further improve with more training.
Taken together, this shows the great real-world application potential of our
suggested methods to more data, languages, and error classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Implications of Topological Imbalance for Representation Learning on
  Biomedical Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Bonner, Ufuk Kirik, Ola Engkvist, <span class="highlight-author">Jian Tang</span>, Ian P Barrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adoption of recently developed methods from machine learning has given rise
to creation of drug-discovery knowledge graphs (KG) that utilize the
interconnected nature of the domain. Graph-based modelling of the data,
combined with KG embedding (KGE) methods, are promising as they provide a more
intuitive representation and are suitable for inference tasks such as
predicting missing links. One common application is to produce ranked lists of
genes for a given disease, where the rank is based on the perceived likelihood
of association between the gene and the disease. It is thus critical that these
predictions are not only pertinent but also biologically meaningful. However,
KGs can be biased either directly due to the underlying data sources that are
integrated or due to modeling choices in the construction of the graph, one
consequence of which is that certain entities can get topologically
overrepresented. We demonstrate the effect of these inherent structural
imbalances, resulting in densely-connected entities being highly ranked no
matter the context. We provide support for this observation across different
datasets, models as well as predictive tasks. Further, we present various graph
perturbation experiments which yield more support to the observation that KGE
models can be more influenced by the frequency of entities rather than any
biological information encoded within the relations. Our results highlight the
importance of data modeling choices, and emphasizes the need for practitioners
to be mindful of these issues when interpreting model outputs and during KG
composition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMCNet for Spatial-configuration-based Classification: A Summary of
  Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Farhadloo, Carl Molnar, Gaoxiang Luo, Yan Li, Shashi Shekhar, Rachel L. Maus, Svetomir N. Markovic, Raymond Moore, Alexey Leontovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of spatial-configuration-based classification is to build a
classifier to distinguish two classes (e.g., responder, non-responder) based on
the spatial arrangements (e.g., spatial interactions between different point
categories) given multi-category point data from two classes. This problem is
important for generating hypotheses in medical pathology towards discovering
new immunotherapies for cancer treatment as well as for other applications in
biomedical research and microbial ecology. This problem is challenging due to
an exponential number of category subsets which may vary in the strength of
spatial interactions. Most prior efforts on using human selected spatial
association measures may not be sufficient for capturing the relevant (e.g.,
surrounded by) spatial interactions which may be of biological significance. In
addition, the related deep neural networks are limited to category pairs and do
not explore larger subsets of point categories. To overcome these limitations,
we propose a Spatial-interaction Aware Multi-Category deep neural Network
(SAMCNet) architecture and contribute novel local reference frame
characterization and point pair prioritization layers for
spatial-configuration-based classification. Extensive experimental results on
multiple cancer datasets show that the proposed architecture provides higher
prediction accuracy over baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Kernel Thinning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.01593v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.01593v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raaz Dwivedi, Lester Mackey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a
probability distribution more effectively than independent sampling by
targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less
smooth square-root kernel. Here we provide four improvements. First, we show
that KT applied directly to the target RKHS yields tighter, dimension-free
guarantees for any kernel, any distribution, and any fixed function in the
RKHS. Second, we show that, for analytic kernels like Gaussian, inverse
multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD)
guarantees comparable to or better than those of square-root KT without making
explicit use of a square-root kernel. Third, we prove that KT with a fractional
power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth
kernels, like Laplace and Mat\'ern, that do not have square-roots. Fourth, we
establish that KT applied to a sum of the target and power kernels (a procedure
we call KT+) simultaneously inherits the improved MMD guarantees of power KT
and the tighter individual function guarantees of target KT. In our experiments
with target KT and KT+, we witness significant improvements in integration
error even in $100$ dimensions and when compressing challenging differential
equation posteriors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Priors in Bayesian Deep Learning: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.06868v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.06868v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Fortuin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the choice of prior is one of the most critical parts of the Bayesian
inference workflow, recent Bayesian deep learning models have often fallen back
on vague priors, such as standard Gaussians. In this review, we highlight the
importance of prior choices for Bayesian deep learning and present an overview
of different priors that have been proposed for (deep) Gaussian processes,
variational autoencoders, and Bayesian neural networks. We also outline
different methods of learning priors for these models from data. We hope to
motivate practitioners in Bayesian deep learning to think more carefully about
the prior specification for their models and to provide them with some
inspiration in this regard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Heterogeneous Electronic Health Records Systems via Text-Based
  Code Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.03625v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.03625v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyunghoon Hur, Jiyoung Lee, Jungwoo Oh, Wesley Price, Young-Hak Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Substantial increase in the use of Electronic Health Records (EHRs) has
opened new frontiers for predictive healthcare. However, while EHR systems are
nearly ubiquitous, they lack a unified code system for representing medical
concepts. Heterogeneous formats of EHR present a substantial barrier for the
training and deployment of state-of-the-art deep learning models at scale. To
overcome this problem, we introduce Description-based Embedding, DescEmb, a
code-agnostic description-based representation learning framework for
predictive modeling on EHR. DescEmb takes advantage of the flexibility of
neural language understanding models while maintaining a neutral approach that
can be combined with prior frameworks for task-specific representation learning
or predictive modeling. We tested our model's capacity on various experiments
including prediction tasks, transfer learning and pooled learning. DescEmb
shows higher performance in overall experiments compared to code-based
approach, opening the door to a text-based approach in predictive healthcare
research that is not constrained by EHR structure nor special domain knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHIL 2022. Main paper + supplementary material (21 pages,
  8 figures, 12 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lead-agnostic <span class="highlight-title">Self-supervised</span> Learning for Local and Global
  Representations of Electrocardiogram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungwoo Oh, Hyunseung Chung, Joon-myoung Kwon, Dong-gyun Hong, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, self-supervised learning methods have shown significant
improvement for pre-training with unlabeled data and have proven helpful for
electrocardiogram signals. However, most previous pre-training methods for
electrocardiogram focused on capturing only global contextual representations.
This inhibits the models from learning fruitful representation of
electrocardiogram, which results in poor performance on downstream tasks.
Additionally, they cannot fine-tune the model with an arbitrary set of
electrocardiogram leads unless the models were pre-trained on the same set of
leads. In this work, we propose an ECG pre-training method that learns both
local and global contextual representations for better generalizability and
performance on downstream tasks. In addition, we propose random lead masking as
an ECG-specific augmentation method to make our proposed model robust to an
arbitrary set of leads. Experimental results on two downstream tasks, cardiac
arrhythmia classification and patient identification, show that our proposed
approach outperforms other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHIL 2022 (16 pages, 3 figures, 4 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural network processing of holographic images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John S. Schreck, Gabrielle Gantos, Matthew Hayman, Aaron Bansemer, David John Gagne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HOLODEC, an airborne cloud particle imager, captures holographic images of a
fixed volume of cloud to characterize the types and sizes of cloud particles,
such as water droplets and ice crystals. Cloud particle properties include
position, diameter, and shape. We present a hologram processing algorithm,
HolodecML, that utilizes a neural segmentation model, GPUs, and computational
parallelization. HolodecML is trained using synthetically generated holograms
based on a model of the instrument, and predicts masks around particles found
within reconstructed images. From these masks, the position and size of the
detected particles can be characterized in three dimensions. In order to
successfully process real holograms, we find we must apply a series of image
corrupting transformations and noise to the synthetic images used in training.
  In this evaluation, HolodecML had comparable position and size estimation
performance to the standard processing method, but improved particle detection
by nearly 20\% on several thousand manually labeled HOLODEC images. However,
the improvement only occurred when image corruption was performed on the
simulated images during training, thereby mimicking non-ideal conditions in the
actual probe. The trained model also learned to differentiate artifacts and
other impurities in the HOLODEC images from the particles, even though no such
objects were present in the training data set, while the standard processing
method struggled to separate particles from artifacts. The novelty of the
training approach, which leveraged noise as a means for parameterizing
non-ideal aspects of the HOLODEC detector, could be applied in other domains
where the theoretical model is incapable of fully describing the real-world
operation of the instrument and accurate truth data required for supervised
learning cannot be obtained from real-world observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 15 figures. Submitted to Atmospheric Measurement Techniques</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Filters in Graph Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.10377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.10377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Apicella, Francesco Isgrò, Andrea Pollastro, Roberto Prevete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, we have witnessed the availability of an increasing
data generated from non-Euclidean domains, which are usually represented as
graphs with complex relationships, and Graph Neural Networks (GNN) have gained
a high interest because of their potential in processing graph-structured data.
In particular, there is a strong interest in exploring the possibilities in
performing convolution on graphs using an extension of the GNN architecture,
generally referred to as Graph Convolutional Neural Networks (ConvGNN).
Convolution on graphs has been achieved mainly in two forms: spectral and
spatial convolutions. Due to the higher flexibility in exploring and exploiting
the graph structure of data, there is recently an increasing interest in
investigating the possibilities that the spatial approach can offer. The idea
of finding a way to adapt the network behaviour to the inputs they process to
maximize the total performances has aroused much interest in the neural
networks literature over the years. This paper presents a novel method to adapt
the behaviour of a ConvGNN to the input proposing a method to perform spatial
convolution on graphs using input-specific filters, which are dynamically
generated from nodes feature vectors. The experimental assessment confirms the
capabilities of the proposed approach, which achieves satisfying results using
a low number of filters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Fraudster: Adversarial Attacks on Graph Neural Network Based
  Vertical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Guohan Huang, Haibin Zheng, Shanqing Yu, Wenrong Jiang, Chen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural network (GNN) has achieved great success on graph representation
learning. Challenged by large scale private data collected from user-side, GNN
may not be able to reflect the excellent performance, without rich features and
complete adjacent relationships. Addressing the problem, vertical federated
learning (VFL) is proposed to implement local data protection through training
a global model collaboratively. Consequently, for graph-structured data, it is
a natural idea to construct a GNN based VFL framework, denoted as GVFL.
However, GNN has been proved vulnerable to adversarial attacks. Whether the
vulnerability will be brought into the GVFL has not been studied. This is the
first study of adversarial attacks on GVFL. A novel adversarial attack method
is proposed, named Graph-Fraudster. It generates adversarial perturbations
based on the noise-added global node embeddings via the privacy leakage and the
gradient of pairwise node. Specifically, first, Graph-Fraudster steals the
global node embeddings and sets up a shadow model of the server for the attack
generator. Second, noise is added into node embeddings to confuse the shadow
model. At last, the gradient of pairwise node is used to generate attacks with
the guidance of noise-added node embeddings. Extensive experiments on five
benchmark datasets demonstrate that Graph-Fraudster achieves the
state-of-the-art attack performance compared with baselines in different GNN
based GVFLs. Furthermore, Graph-Fraudster can remain a threat to GVFL even if
two possible defense mechanisms are applied. Additionally, some suggestions are
put forward for the future work to improve the robustness of GVFL. The code and
datasets can be downloaded at https://github.com/hgh0545/Graph-Fraudster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Backpropagation through Time and Space: Learning Numerical Methods with
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Way, Dheeraj S. K. Kapilavai, Yiwei Fu, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Backpropagation Through Time and Space (BPTTS), a method for
training a recurrent spatio-temporal neural network, that is used in a
homogeneous multi-agent reinforcement learning (MARL) setting to learn
numerical methods for hyperbolic conservation laws. We treat the numerical
schemes underlying partial differential equations (PDEs) as a Partially
Observable Markov Game (POMG) in Reinforcement Learning (RL). Similar to
numerical solvers, our agent acts at each discrete location of a computational
space for efficient and generalizable learning. To learn higher-order spatial
methods by acting on local states, the agent must discern how its actions at a
given spatiotemporal location affect the future evolution of the state. The
manifestation of this non-stationarity is addressed by BPTTS, which allows for
the flow of gradients across both space and time. The learned numerical
policies are comparable to the SOTA numerics in two settings, the Burgers'
Equation and the Euler Equations, and generalize well to other simulation
set-ups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.06534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.06534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeeshan Akhtar, Ketan Rajawat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers stochastic convex optimization problems with two sets of
constraints: (a) deterministic constraints on the domain of the optimization
variable, which are difficult to project onto; and (b) deterministic or
stochastic constraints that admit efficient projection. Problems of this form
arise frequently in the context of semidefinite programming as well as when
various NP-hard problems are solved approximately via semidefinite relaxation.
Since projection onto the first set of constraints is difficult, it becomes
necessary to explore projection-free algorithms, such as the stochastic
Frank-Wolfe (FW) algorithm. On the other hand, the second set of constraints
cannot be handled in the same way, and must be incorporated as an indicator
function within the objective function, thereby complicating the application of
FW methods. Similar problems have been studied before; however, they suffer
from slow convergence rates. This work, equipped with momentum based gradient
tracking technique, guarantees fast convergence rates on par with the
best-known rates for problems without the second set of constraints.
Zeroth-order variants of the proposed algorithms are also developed and again
improve upon the state-of-the-art rate results. We further propose the novel
trimmed FW variants that enjoy the same convergence rates as their classical
counterparts, but are empirically shown to require significantly fewer calls to
the linear minimization oracle speeding up the overall algorithm. The efficacy
of the proposed algorithms is tested on relevant applications of sparse matrix
estimation, clustering via semidefinite relaxation, and uniform sparsest cut
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptive Hand Keypoint and Pixel Localization in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takehiko Ohkawa, Yu-Jhe Li, Qichen Fu, Ryosuke Furuta, Kris M. Kitani, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias and unfairness in machine learning models: a systematic literature
  <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Palma Pagano, Rafael Bessa Loureiro, Maira Matos Araujo, Fernanda Vitoria Nascimento Lisboa, Rodrigo Matos Peixoto, Guilherme Aragao de Sousa Guimaraes, Lucas Lisboa dos Santos, Gustavo Oliveira Ramos Cruz, Ewerton Lopes Silva de Oliveira, Marco Cruz, Ingrid Winkler, Erick Giovani Sperandio Nascimento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Dirichlet uncertainty for unsupervised out-of-distribution
  detection of eye fundus photographs in glaucoma screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Araújo, Guilherme Aresta, Hrvoje Bogunovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of automatic tools for early glaucoma diagnosis with color
fundus photographs can significantly reduce the impact of this disease.
However, current state-of-the-art solutions are not robust to real-world
scenarios, providing over-confident predictions for out-of-distribution cases.
With this in mind, we propose a model based on the Dirichlet distribution that
allows to obtain class-wise probabilities together with an uncertainty
estimation without exposure to out-of-distribution cases. We demonstrate our
approach on the AIROGS challenge. At the start of the final test phase (8 Feb.
2022), our method had the highest average score among all submissions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ISBI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift
  Estimation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joachim Nyborg, Charlotte Pelletier, Sébastien Lefèvre, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent developments of deep learning models that capture the complex
temporal patterns of crop phenology have greatly advanced crop classification
of Satellite Image Time Series (SITS). However, when applied to target regions
spatially different from the training region, these models perform poorly
without any target labels due to the temporal shift of crop phenology between
regions. To address this unsupervised cross-region adaptation setting, existing
methods learn domain-invariant features without any target supervision, but not
the temporal shift itself. As a consequence, these techniques provide only
limited benefits for SITS. In this paper, we propose TimeMatch, a new
unsupervised domain adaptation method for SITS that directly accounts for the
temporal shift. TimeMatch consists of two components: 1) temporal shift
estimation, which estimates the temporal shift of the unlabeled target region
with a source-trained model, and 2) TimeMatch learning, which combines temporal
shift estimation with semi-supervised learning to adapt a classifier to an
unlabeled target region. We also introduce an open-access dataset for
cross-region adaptation with SITS from four different regions in Europe. On
this dataset, we demonstrate that TimeMatch outperforms all competing methods
by 11% in F1-score across five different adaptation scenarios, setting a new
state-of-the-art for cross-region adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to the ISPRS Journal of Photogrammetry and Remote
  Sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning from Pixels in Environments with Combinatorially Hard Search
  Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Bagatella, Mirek Olšák, Michal Rolínek, Georg Martius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to form complex plans based on raw visual input is a litmus test
for current capabilities of artificial intelligence, as it requires a seamless
combination of visual processing and abstract algorithmic execution, two
traditionally separate areas of computer science. A recent surge of interest in
this field brought advances that yield good performance in tasks ranging from
arcade games to continuous control; these methods however do not come without
significant issues, such as limited generalization capabilities and
difficulties when dealing with combinatorially hard planning instances. Our
contribution is two-fold: (i) we present a method that learns to represent its
environment as a latent graph and leverages state reidentification to reduce
the complexity of finding a good policy from exponential to linear (ii) we
introduce a set of lightweight environments with an underlying discrete
combinatorial structure in which planning is challenging even for humans.
Moreover, we show that our methods achieves strong empirical generalization to
variations in the environment, even across highly disadvantaged regimes, such
as "one-shot" planning, or in an offline RL paradigm which only provides
low-quality trajectories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive <span class="highlight-title">Review</span> On Twin Support Vector Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.00336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.00336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Tanveer, T. Rajani, R. Rastogi, Y. H. Shao, M. A. Ganaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Twin support vector machine (TWSVM) and twin support vector regression (TSVR)
are newly emerging efficient machine learning techniques which offer promising
solutions for classification and regression challenges respectively. TWSVM is
based upon the idea to identify two nonparallel hyperplanes which classify the
data points to their respective classes. It requires to solve two small sized
quadratic programming problems (QPPs) in lieu of solving single large size QPP
in support vector machine (SVM) while TSVR is formulated on the lines of TWSVM
and requires to solve two SVM kind problems. Although there has been good
research progress on these techniques; there is limited literature on the
comparison of different variants of TSVR. Thus, this review presents a rigorous
analysis of recent research in TWSVM and TSVR simultaneously mentioning their
limitations and advantages. To begin with we first introduce the basic theory
of support vector machine, TWSVM and then focus on the various improvements and
applications of TWSVM, and then we introduce TSVR and its various enhancements.
Finally, we suggest future research and development prospects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-based and Feature-based Federated Learning for Unconstrained and
  Constrained Nonconvex Optimization via Mini-batch SSCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.06011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.06011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Cui, Yangchen Li, Chencheng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has become a hot research area in enabling the
collaborative training of machine learning models among multiple clients that
hold sensitive local data. Nevertheless, unconstrained federated optimization
has been studied mainly using stochastic gradient descent (SGD), which may
converge slowly, and constrained federated optimization, which is more
challenging, has not been investigated so far. This paper investigates
sample-based and feature-based federated optimization, respectively, and
considers both unconstrained and constrained nonconvex problems for each of
them. First, we propose FL algorithms using stochastic successive convex
approximation (SSCA) and mini-batch techniques. These algorithms can adequately
exploit the structures of the objective and constraint functions and
incrementally utilize samples. We show that the proposed FL algorithms converge
to stationary points and Karush-Kuhn-Tucker (KKT) points of the respective
unconstrained and constrained nonconvex problems, respectively. Next, we
provide algorithm examples with appealing computational complexity and
communication load per communication round. We show that the proposed algorithm
examples for unconstrained federated optimization are identical to FL
algorithms via momentum SGD and provide an analytical connection between SSCA
and momentum SGD. Finally, numerical experiments demonstrate the inherent
advantages of the proposed algorithms in convergence speeds, communication and
computation costs, and model specifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures. This work is submitted to IEEE Trans. Signal
  Process. (under major revision). arXiv admin note: substantial text overlap
  with arXiv:2103.09506</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Structured Neural Networks Through Manifold Identification and
  Variance Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zih-Syuan Huang, Ching-pei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an algorithm (RMDA) for training neural networks (NNs)
with a regularization term for promoting desired structures. RMDA does not
incur computation additional to proximal SGD with momentum, and achieves
variance reduction without requiring the objective function to be of the
finite-sum form. Through the tool of manifold identification from nonlinear
optimization, we prove that after a finite number of iterations, all iterates
of RMDA possess a desired structure identical to that induced by the
regularizer at the stationary point of asymptotic convergence, even in the
presence of engineering tricks like data augmentation and dropout that
complicate the training process. Experiments on training NNs with structured
sparsity confirm that variance reduction is necessary for such an
identification, and show that RMDA thus significantly outperforms existing
methods for this task. For unstructured sparsity, RMDA also outperforms a
state-of-the-art pruning method, validating the benefits of training structured
NNs through regularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transframer: Arbitrary Frame Prediction with Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, Peter Battaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a general-purpose framework for image modelling and vision tasks
based on probabilistic frame prediction. Our approach unifies a broad range of
tasks, from image segmentation, to novel view synthesis and video
interpolation. We pair this framework with an architecture we term Transframer,
which uses U-Net and Transformer components to condition on annotated context
frames, and outputs sequences of sparse, compressed image features. Transframer
is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can
generate coherent 30 second videos from a single image without any explicit
geometric information. A single generalist Transframer simultaneously produces
promising results on 8 tasks, including semantic segmentation, image
classification and optical flow prediction with no task-specific architectural
components, demonstrating that multi-task computer vision can be tackled using
probabilistic image models. Our approach can in principle be applied to a wide
range of applications that require learning the conditional structure of
annotated image-formatted data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Bayesian Learning Rule 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.04562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.04562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Emtiyaz Khan, Håvard Rue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that many machine-learning algorithms are specific instances of a
single algorithm called the Bayesian learning rule. The rule, derived from
Bayesian principles, yields a wide-range of algorithms from fields such as
optimization, deep learning, and graphical models. This includes classical
algorithms such as ridge regression, Newton's method, and Kalman filter, as
well as modern deep-learning algorithms such as stochastic-gradient descent,
RMSprop, and Dropout. The key idea in deriving such algorithms is to
approximate the posterior using candidate distributions estimated by using
natural gradients. Different candidate distributions result in different
algorithms and further approximations to natural gradients give rise to
variants of those algorithms. Our work not only unifies, generalizes, and
improves existing algorithms, but also helps us design new ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLSSVM: A (multi-)GPGPU-accelerated Least Squares Support Vector Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Van Craen, Marcel Breyer, Dirk Pflüger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms must be able to efficiently cope with massive
data sets. Therefore, they have to scale well on any modern system and be able
to exploit the computing power of accelerators independent of their vendor. In
the field of supervised learning, Support Vector Machines (SVMs) are widely
used. However, even modern and optimized implementations such as LIBSVM or
ThunderSVM do not scale well for large non-trivial dense data sets on
cutting-edge hardware: Most SVM implementations are based on Sequential Minimal
Optimization, an optimized though inherent sequential algorithm. Hence, they
are not well-suited for highly parallel GPUs. Furthermore, we are not aware of
a performance portable implementation that supports CPUs and GPUs from
different vendors.
  We have developed the PLSSVM library to solve both issues. First, we resort
to the formulation of the SVM as a least squares problem. Training an SVM then
boils down to solving a system of linear equations for which highly parallel
algorithms are known. Second, we provide a hardware independent yet efficient
implementation: PLSSVM uses different interchangeable backends--OpenMP, CUDA,
OpenCL, SYCL--supporting modern hardware from various vendors like NVIDIA, AMD,
or Intel on multiple GPUs. PLSSVM can be used as a drop-in replacement for
LIBSVM. We observe a speedup on CPUs of up to 10 compared to LIBSVM and on GPUs
of up to 14 compared to ThunderSVM. Our implementation scales on many-core CPUs
with a parallel speedup of 74.7 on up to 256 CPU threads and on multiple GPUs
with a parallel speedup of 3.71 on four GPUs.
  The code, utility scripts, and documentation are all available on GitHub:
https://github.com/SC-SGS/PLSSVM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on PDSEC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearest Neighbor Classifier with Margin Penalty for Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Cao, Zhiqiao Gao, Jie Hu, Mingchuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning becomes the mainstream in the field of natural language
processing, the need for suitable active learning method are becoming
unprecedented urgent. Active Learning (AL) methods based on nearest neighbor
classifier are proposed and demonstrated superior results. However, existing
nearest neighbor classifier are not suitable for classifying mutual exclusive
classes because inter-class discrepancy cannot be assured by nearest neighbor
classifiers. As a result, informative samples in the margin area can not be
discovered and AL performance are damaged. To this end, we propose a novel
Nearest neighbor Classifier with Margin penalty for Active Learning(NCMAL).
Firstly, mandatory margin penalty are added between classes, therefore both
inter-class discrepancy and intra-class compactness are both assured. Secondly,
a novel sample selection strategy are proposed to discover informative samples
within the margin area. To demonstrate the effectiveness of the methods, we
conduct extensive experiments on for datasets with other state-of-the-art
methods. The experimental results demonstrate that our method achieves better
results with fewer annotated samples than all baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UCB Momentum Q-learning: Correcting the bias without forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.01312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.01312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Menard, Omar Darwiche Domingues, Xuedong Shang, Michal Valko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm
for reinforcement learning in tabular and possibly stage-dependent, episodic
Markov decision process. UCBMQ is based on Q-learning where we add a momentum
term and rely on the principle of optimism in face of uncertainty to deal with
exploration. Our new technical ingredient of UCBMQ is the use of momentum to
correct the bias that Q-learning suffers while, at the same time, limiting the
impact it has on the second-order term of the regret. For UCBMQ, we are able to
guarantee a regret of at most $O(\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the
length of an episode, $S$ the number of states, $A$ the number of actions, $T$
the number of episodes and ignoring terms in poly-$\log(SAHT)$. Notably, UCBMQ
is the first algorithm that simultaneously matches the lower bound of
$\Omega(\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with
respect to the horizon $T$) that scales only linearly with the number of states
$S$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised machine learning approaches to the $q$-state Potts model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Tirelli, Danyella O. Carvalho, Lucas A. Oliveira, J. P. Lima, Natanael C. Costa, Raimundo R. dos Santos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper with study phase transitions of the $q$-state Potts model,
through a number of unsupervised machine learning techniques, namely Principal
Component Analysis (PCA), $k$-means clustering, Uniform Manifold Approximation
and Projection (UMAP), and Topological Data Analysis (TDA). Even though in all
cases we are able to retrieve the correct critical temperatures $T_c(q)$, for
$q = 3, 4$ and $5$, results show that non-linear methods as UMAP and TDA are
less dependent on finite size effects, while still being able to distinguish
between first and second order phase transitions. This study may be considered
as a benchmark for the use of different unsupervised machine learning
algorithms in the investigation of phase transitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added computation of critical exponents; exposition improved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and
  Practical Solutions <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.01098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.01098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leslie O'Bray, Max Horn, Bastian Rieck, Karsten Borgwardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph generative models are a highly active branch of machine learning. Given
the steady development of new models of ever-increasing complexity, it is
necessary to provide a principled way to evaluate and compare them. In this
paper, we enumerate the desirable criteria for such a comparison metric and
provide an overview of the status quo of graph generative model comparison in
use today, which predominantly relies on the maximum mean discrepancy (MMD). We
perform a systematic evaluation of MMD in the context of graph generative model
comparison, highlighting some of the challenges and pitfalls researchers
inadvertently may encounter. After conducting a thorough analysis of the
behaviour of MMD on synthetically-generated perturbed graphs as well as on
recently-proposed graph generative models, we are able to provide a suitable
procedure to mitigate these challenges and pitfalls. We aggregate our findings
into a list of practical recommendations for researchers to use when evaluating
graph generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Spotlight presentation at ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping conditional distributions for domain adaptation under
  generalized target shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Kirchmeyer, Alain Rakotomamonjy, Emmanuel de Bezenac, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of unsupervised domain adaptation (UDA) between a
source and a target domain under conditional and label shift a.k.a Generalized
Target Shift (GeTarS). Unlike simpler UDA settings, few works have addressed
this challenging problem. Recent approaches learn domain-invariant
representations, yet they have practical limitations and rely on strong
assumptions that may not hold in practice. In this paper, we explore a novel
and general approach to align pretrained representations, which circumvents
existing drawbacks. Instead of constraining representation invariance, it
learns an optimal transport map, implemented as a NN, which maps source
representations onto target ones. Our approach is flexible and scalable, it
preserves the problem's structure and it has strong theoretical guarantees
under mild assumptions. In particular, our solution is unique, matches
conditional distributions across domains, recovers target proportions and
explicitly controls the target generalization risk. Through an exhaustive
comparison on several datasets, we challenge the state-of-the-art in GeTarS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Asymptotic Analysis of Stochastic Approximation Algorithms for
  Streaming Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.07117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.07117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Godichon-Baggioni, Olivier Wintenberger, Nicklas Werge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the high-frequency data streams continuously generated,
real-time learning is becoming increasingly important. These data streams
should be processed sequentially with the property that the stream may change
over time. In this streaming setting, we propose techniques for minimizing a
convex objective through unbiased estimates of its gradients, commonly referred
to as stochastic approximation problems. Our methods rely on stochastic
approximation algorithms due to their computationally advantage as they only
use the previous iterate as a parameter estimate. The reasoning includes
iterate averaging that guarantees optimal statistical efficiency under
classical conditions. Our non-asymptotic analysis shows accelerated convergence
by selecting the learning rate according to the expected data streams. We show
that the average estimate converges optimally and robustly to any data stream
rate. In addition, noise reduction can be achieved by processing the data in a
specific pattern, which is advantageous for large-scale machine learning. These
theoretical results are illustrated for various data streams, showing the
effectiveness of the proposed algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Rotationally Invariant Neural Networks from PDEs and
  Variational Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Alt, Karl Schrader, Joachim Weickert, Pascal Peter, Matthias Augustin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial differential equation (PDE) models and their associated variational
energy formulations are often rotationally invariant by design. This ensures
that a rotation of the input results in a corresponding rotation of the output,
which is desirable in applications such as image analysis. Convolutional neural
networks (CNNs) do not share this property, and existing remedies are often
complex. The goal of our paper is to investigate how diffusion and variational
models achieve rotation invariance and transfer these ideas to neural networks.
As a core novelty we propose activation functions which couple network channels
by combining information from several oriented filters. This guarantees
rotation invariance within the basic building blocks of the networks while
still allowing for directional filtering. The resulting neural architectures
are inherently rotationally invariant. With only a few small filters, they can
achieve the same invariance as existing techniques which require a fine-grained
sampling of orientations. Our findings help to translate diffusion and
variational models into mathematically well-founded network architectures, and
provide novel concepts for model-based CNN design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS
  With Accurate Phoneme Duration Control <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchao He, Jian Luan, Yujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence expansion between encoder and decoder is a critical challenge in
sequence-to-sequence TTS. Attention-based methods achieve great naturalness but
suffer from unstable issues like missing and repeating phonemes, not to mention
accurate duration control. Duration-informed methods, on the contrary, seem to
easily adjust phoneme duration but show obvious degradation in speech
naturalness. This paper proposes PAMA-TTS to address the problem. It takes the
advantage of both flexible attention and explicit duration models. Based on the
monotonic attention mechanism, PAMA-TTS also leverages token duration and
relative position of a frame, especially countdown information, i.e. in how
many future frames the present phoneme will end. They help the attention to
move forward along the token sequence in a soft but reliable control.
Experimental results prove that PAMA-TTS achieves the highest naturalness,
while has on-par or even better duration controllability than the
duration-informed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2022. 5 pages, 4 figures, 3 tables. Audio samples
  are available at: https://pama-tts.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Coreset Selection for Rehearsal-based Continual Learning <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.01085v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.01085v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a
current dataset while selecting high-affinity samples to past tasks, which
directly inhibits catastrophic forgetting. We validate the effectiveness of our
coreset selection mechanism over various standard, imbalanced, and noisy
datasets against strong continual learning baselines, demonstrating that it
improves task adaptation and prevents catastrophic forgetting in a
sample-efficient manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.02635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.02635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adil Salim, Laurent Condat, Konstantin Mishchenko, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimizing the sum of three convex functions, where the first one
F is smooth, the second one is nonsmooth and proximable and the third one is
the composition of a nonsmooth proximable function with a linear operator L.
This template problem has many applications, for instance in image processing
and machine learning. First, we propose a new primal-dual algorithm, which we
call PDDY, for this problem. It is constructed by applying Davis-Yin splitting
to a monotone inclusion in a primal-dual product space, where the operators are
monotone under a specific metric depending on L. We show that three existing
algorithms (the two forms of the Condat-Vu algorithm and the PD3O algorithm)
have the same structure, so that PDDY is the fourth missing link in this
self-consistent class of primal-dual algorithms. This representation eases the
convergence analysis: it allows us to derive sublinear convergence rates in
general, and linear convergence results in presence of strong convexity.
Moreover, within our broad and flexible analysis framework, we propose new
stochastic generalizations of the algorithms, in which a variance-reduced
random estimate of the gradient of F is used, instead of the true gradient.
Furthermore, we obtain, as a special case of PDDY, a linearly converging
algorithm for the minimization of a strongly convex function F under a linear
constraint; we discuss its important application to decentralized optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adversarial Neural Networks for Domain Generalization: When It
  Works and How to Improve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.03924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.03924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Sicilia, Xingchen Zhao, Seong Jae Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theoretically, domain adaptation is a well-researched problem. Further, this
theory has been well-used in practice. In particular, we note the bound on
target error given by Ben-David et al. (2010) and the well-known
domain-aligning algorithm based on this work using Domain Adversarial Neural
Networks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple
variants of DANN have been proposed for the related problem of domain
generalization, but without much discussion of the original motivating bound.
In this paper, we investigate the validity of DANN in domain generalization
from this perspective. We investigate conditions under which application of
DANN makes sense and further consider DANN as a dynamic process during
training. Our investigation suggests that the application of DANN to domain
generalization may not be as straightforward as it seems. To address this, we
design an algorithmic extension to DANN in the domain generalization case. Our
experimentation validates both theory and algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robbing the Fed: Directly Obtaining Private Data in Federated Learning
  with Modified Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Fowl, Jonas Geiping, Wojtek Czaja, Micah Goldblum, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has quickly gained popularity with its promises of
increased user privacy and efficiency. Previous works have shown that federated
gradient updates contain information that can be used to approximately recover
user data in some situations. These previous attacks on user privacy have been
limited in scope and do not scale to gradient updates aggregated over even a
handful of data points, leaving some to conclude that data privacy is still
intact for realistic training regimes. In this work, we introduce a new threat
model based on minimal but malicious modifications of the shared model
architecture which enable the server to directly obtain a verbatim copy of user
data from gradient updates without solving difficult inverse problems. Even
user data aggregated over large batches -- where previous methods fail to
extract meaningful content -- can be reconstructed by these minimally modified
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BEAT: A Large-Scale Semantic and Emotional <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Dataset</span> for
  <span class="highlight-title">Conversation</span>al Gestures Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai<span class="highlight-author">yang Liu</span>, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accuracy and Fairness Trade-offs in Machine Learning: A Stochastic
  Multi-Objective Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.01132v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.01132v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyun Liu, Luis Nunes Vicente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the application of machine learning to real-life decision-making systems,
e.g., credit scoring and criminal justice, the prediction outcomes might
discriminate against people with sensitive attributes, leading to unfairness.
The commonly used strategy in fair machine learning is to include fairness as a
constraint or a penalization term in the minimization of the prediction loss,
which ultimately limits the information given to decision-makers. In this
paper, we introduce a new approach to handle fairness by formulating a
stochastic multi-objective optimization problem for which the corresponding
Pareto fronts uniquely and comprehensively define the accuracy-fairness
trade-offs. We have then applied a stochastic approximation-type method to
efficiently obtain well-spread and accurate Pareto fronts, and by doing so we
can handle training data arriving in a streaming way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalized Minimax Q-learning Algorithm for Two-Player Zero-Sum
  Stochastic Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1906.06659v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1906.06659v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghuram Bharadwaj Diddigi, Chandramouli Kamanchi, Shalabh Bhatnagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of two-player zero-sum games. This problem is
formulated as a min-max Markov game in the literature. The solution of this
game, which is the min-max payoff, starting from a given state is called the
min-max value of the state. In this work, we compute the solution of the
two-player zero-sum game utilizing the technique of successive relaxation that
has been successfully applied in the literature to compute a faster value
iteration algorithm in the context of Markov Decision Processes. We extend the
concept of successive relaxation to the setting of two-player zero-sum games.
We show that, under a special structure on the game, this technique facilitates
faster computation of the min-max value of the states. We then derive a
generalized minimax Q-learning algorithm that computes the optimal policy when
the model information is not known. Finally, we prove the convergence of the
proposed generalized minimax Q-learning algorithm utilizing stochastic
approximation techniques, under an assumption on the boundedness of iterates.
Through experiments, we demonstrate the effectiveness of our proposed
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Regret Minimization for Control of Non-stationary Linear
  Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.03772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.03772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Luo, Varun Gupta, Mladen Kolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of controlling a Linear Quadratic Regulator (LQR)
system over a finite horizon $T$ with fixed and known cost matrices $Q,R$, but
unknown and non-stationary dynamics $\{A_t, B_t\}$. The sequence of dynamics
matrices can be arbitrary, but with a total variation, $V_T$, assumed to be
$o(T)$ and unknown to the controller. Under the assumption that a sequence of
stabilizing, but potentially sub-optimal controllers is available for all $t$,
we present an algorithm that achieves the optimal dynamic regret of
$\tilde{\mathcal{O}}\left(V_T^{2/5}T^{3/5}\right)$. With piece-wise constant
dynamics, our algorithm achieves the optimal regret of
$\tilde{\mathcal{O}}(\sqrt{ST})$ where $S$ is the number of switches. The crux
of our algorithm is an adaptive non-stationarity detection strategy, which
builds on an approach recently developed for contextual Multi-armed Bandit
problems. We also argue that non-adaptive forgetting (e.g., restarting or using
sliding window learning with a static window size) may not be regret optimal
for the LQR problem, even when the window size is optimally tuned with the
knowledge of $V_T$. The main technical challenge in the analysis of our
algorithm is to prove that the ordinary least squares (OLS) estimator has a
small bias when the parameter to be estimated is non-stationary. Our analysis
also highlights that the key motif driving the regret is that the LQR problem
is in spirit a bandit problem with linear feedback and locally quadratic cost.
This motif is more universal than the LQR problem itself, and therefore we
believe our results should find wider application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> An Empirical Study of Training End-to-End <span class="highlight-title">Vision-and-Language</span>
  <span class="highlight-title">Transformer</span>s <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.02387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.02387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Yi Dou, Yichong Xu, <span class="highlight-author">Zhe Gan</span>, Jianfeng Wang, Shuohang Wang, <span class="highlight-author">Lijuan Wang</span>, Chenguang Zhu, Pengchuan Zhang, <span class="highlight-author">Lu Yuan</span>, Nanyun Peng, Zicheng Liu, Michael Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks often
degrades significantly. In this paper, we present METER, a Multimodal
End-to-end TransformER framework, through which we investigate how to design
and pre-train a fully transformer-based VL model in an end-to-end manner.
Specifically, we dissect the model designs along multiple dimensions: vision
encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,
DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),
architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training
objectives (e.g., masked image modeling). We conduct comprehensive experiments
and provide insights on how to train a performant VL transformer. METER
achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images
for pre-training, surpassing the state-of-the-art region-feature-based model by
1.04%, and outperforming the previous best fully transformer-based model by
1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy
of 80.54%. Code and pre-trained models are released at
https://github.com/zdou0830/METER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker Embedding-aware Neural Diarization: a Novel Framework for
  Overlapped Speech Diarization in the Meeting Scenario <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Du, Shiliang Zhang, Siqi Zheng, Zhijie Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we reformulate overlapped speech diarization as a single-label
prediction problem, which is always treated as a multi-label classification
task in previous studies. Specifically, the multiple labels of each frame are
encoded into a single label with the power set, which represents the possible
combinations of different speakers. Through this formulation, we propose the
speaker embedding-aware neural diarization (SEND) system. In SEND, the speech
encoder, speaker encoder, similarity scores, and post-processing network are
optimized to predict the power set encoded labels according to the similarities
between speech features and speaker embeddings. Experimental results show that
our method significantly outperforms the variational Bayesian hidden Markov
model-based clustering algorithm (VBx). Besides, the proposed method has two
benefits compared with the target-speaker voice activity detection (TSVAD).
First, SEND can achieve lower diarization error rates in the real meeting
scenario. Second, when the training data has a high overlap ratio, the learning
process of SEND is more stable than TSVAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INTERSPEECH 2022, 5 parges, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Contextualization for Video Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbin Hao, Hao Zhang, Chong-Wah Ngo, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning discriminative representation from the complex spatio-temporal
dynamic space is essential for video recognition. On top of those stylized
spatio-temporal computational units, further refining the learnt feature with
axial contexts is demonstrated to be promising in achieving this goal. However,
previous works generally focus on utilizing a single kind of contexts to
calibrate entire feature channels and could hardly apply to deal with diverse
video activities. The problem can be tackled by using pair-wise spatio-temporal
attentions to recompute feature response with cross-axis contexts at the
expense of heavy computations. In this paper, we propose an efficient feature
refinement method that decomposes the feature channels into several groups and
separately refines them with different axial contexts in parallel. We refer
this lightweight feature calibration as group contextualization (GC).
Specifically, we design a family of efficient element-wise calibrators, i.e.,
ECal-G/S/T/L, where their axial contexts are information dynamics aggregated
from other axes either globally or locally, to contextualize feature channel
groups. The GC module can be densely plugged into each residual layer of the
off-the-shelf video networks. With little computational overhead, consistent
improvement is observed when plugging in GC on different networks. By utilizing
calibrators to embed feature with four different kinds of contexts in parallel,
the learnt representation is expected to be more resilient to diverse types of
activities. On videos with rich temporal variations, empirically GC can boost
the performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the
state-of-the-art video networks. Code is available at
https://github.com/haoyanbin918/Group-Contextualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M5Product: Self-harmonized <span class="highlight-title">Contrastive Learning</span> for E-commercial
  <span class="highlight-title">Multi-modal</span> <span class="highlight-title">Pretrain</span>ing <span class="chip">CVPR2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.04275v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.04275v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Michael C. Kampffmeyer, Xiaoyong Wei, Minlong Lu, Yaowei Wang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the potential of multi-modal pre-training to learn highly
discriminative feature representations from complementary data modalities,
current progress is being slowed by the lack of large-scale modality-diverse
datasets. By leveraging the natural suitability of E-commerce, where different
modalities capture complementary semantic information, we contribute a
large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5
modalities (image, text, table, video, and audio), covers over 6,000 categories
and 5,000 attributes, and is 500 larger than the largest publicly available
dataset with a similar number of modalities. Furthermore, M5Product contains
incomplete modality pairs and noise while also having a long-tailed
distribution, resembling most real-world problems. We further propose
Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework
that integrates the different modalities into a unified model through an
adaptive feature fusion mechanism, where the importance of each modality is
learned directly from the modality embeddings and impacts the inter-modality
contrastive learning and masked tasks within a multi-modal transformer model.
We evaluate the current multi-modal pre-training state-of-the-art approaches
and benchmark their ability to learn from unlabeled data when faced with the
large number of modalities in the M5Product dataset. We conduct extensive
experiments on four downstream tasks and demonstrate the superiority of our
SCALE model, providing insights into the importance of dataset scale and
diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BEAT: A Large-Scale Semantic and Emotional <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Dataset</span> for
  <span class="highlight-title">Conversation</span>al Gestures Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai<span class="highlight-author">yang Liu</span>, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2022-03-26T05:24:11.468571477Z">
            <a href="https://github.com/AlongWY/ArxivDaily/actions">
                <img id="build-timestamp-badge"
                     src="https://img.shields.io/github/workflow/status/alongwy/arxivdaily/Rss%20Feed?label=2022-03-26 05:24:11 UTC&style=for-the-badge"
                alt="2022-03-26 05:24:11 UTC">
            </a>
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
