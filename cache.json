{"2022-03-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.10079v1","updated":"2022-03-18T17:47:58Z","published":"2022-03-18T17:47:58Z","title":"Simulating Bandit Learning from User Feedback for Extractive Question\n  Answering","summary":"  We study learning from user feedback for extractive question answering by\nsimulating feedback using supervised data. We cast the problem as contextual\nbandit learning, and analyze the characteristics of several learning scenarios\nwith focus on reducing data annotation. We show that systems initially trained\non a small number of examples can dramatically improve given feedback from\nusers on model-predicted answers, and that one can use existing datasets to\ndeploy systems in new domains without any annotation, but instead improving the\nsystem on-the-fly via user feedback.\n","authors":["Ge Gao","Eunsol Choi","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2203.10079v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.10053v1","updated":"2022-03-18T16:56:08Z","published":"2022-03-18T16:56:08Z","title":"RELIC: Retrieving Evidence for Literary Claims","summary":"  Humanities scholars commonly provide evidence for claims that they make about\na work of literature (e.g., a novel) in the form of quotations from the work.\nWe collect a large-scale dataset (RELiC) of 78K literary quotations and\nsurrounding critical analysis and use it to formulate the novel task of\nliterary evidence retrieval, in which models are given an excerpt of literary\nanalysis surrounding a masked quotation and asked to retrieve the quoted\npassage from the set of all passages in the work. Solving this retrieval task\nrequires a deep understanding of complex literary and linguistic phenomena,\nwhich proves challenging to methods that overwhelmingly rely on lexical and\nsemantic similarity matching. We implement a RoBERTa-based dense passage\nretriever for this task that outperforms existing pretrained information\nretrieval baselines; however, experiments and analysis by human domain experts\nindicate that there is substantial room for improvement over our dense\nretriever.\n","authors":["Katherine Thai","Yapei Chang","Kalpesh Krishna","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2203.10053v1.pdf","comment":"ACL 2022 camera ready (19 pages)"},{"id":"http://arxiv.org/abs/2201.13242v2","updated":"2022-03-18T16:27:49Z","published":"2022-01-31T13:52:51Z","title":"Correcting diacritics and typos with a ByT5 transformer model","summary":"  Due to the fast pace of life and online communications and the prevalence of\nEnglish and the QWERTY keyboard, people tend to forgo using diacritics, make\ntypographical errors (typos) when typing in other languages. Restoring\ndiacritics and correcting spelling is important for proper language use and the\ndisambiguation of texts for both humans and downstream algorithms. However,\nboth of these problems are typically addressed separately: the state-of-the-art\ndiacritics restoration methods do not tolerate other typos, but classical\nspellcheckers also cannot deal adequately with all the diacritics missing. In\nthis work, we tackle both problems at once by employing the newly-developed\nuniversal ByT5 byte-level seq2seq transformer model that requires no\nlanguage-specific model structures. For a comparison, we perform diacritics\nrestoration on benchmark datasets of 12 languages, with the addition of\nLithuanian. The experimental investigation proves that our approach is able to\nachieve results (> 98%) comparable to the previous state-of-the-art, despite\nbeing trained less and on fewer data. Our approach is also able to restore\ndiacritics in words not seen during training with > 76% accuracy. Our\nsimultaneous diacritics restoration and typos correction approach reaches > 94%\nalpha-word accuracy on the 13 languages. It has no direct competitors and\nstrongly outperforms classical spell-checking or dictionary-based approaches.\nWe also demonstrate all the accuracies to further improve with more training.\nTaken together, this shows the great real-world application potential of our\nsuggested methods to more data, languages, and error classes.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius","Jurgita Kapočiūtė-Dzikienė","Monika Briedienė","Tomas Krilavičius"],"pdf_url":"https://arxiv.org/pdf/2201.13242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.02035v2","updated":"2022-03-18T16:18:46Z","published":"2021-08-04T13:00:16Z","title":"Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt\n  Verbalizer for Text Classification","summary":"  Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.\n","authors":["Shengding Hu","Ning Ding","Huadong Wang","Zhiyuan Liu","Jingang Wang","Juanzi Li","Wei Wu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2108.02035v2.pdf","comment":"ACL 2022 main"},{"id":"http://arxiv.org/abs/2203.10024v1","updated":"2022-03-18T15:42:21Z","published":"2022-03-18T15:42:21Z","title":"Offensive Language Detection in Under-resourced Algerian Dialectal\n  Arabic Language","summary":"  This paper addresses the problem of detecting the offensive and abusive\ncontent in Facebook comments, where we focus on the Algerian dialectal Arabic\nwhich is one of under-resourced languages. The latter has a variety of dialects\nmixed with different languages (i.e. Berber, French and English). In addition,\nwe deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due\nto the scarcity of works on the same language, we have built a new corpus\nregrouping more than 8.7k texts manually annotated as normal, abusive and\noffensive. We have conducted a series of experiments using the state-of-the-art\nclassifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.\nThe results showed acceptable performances, but the problem requires further\ninvestigation on linguistic features to increase the identification accuracy.\n","authors":["Oussama Boucherit","Kheireddine Abainia"],"pdf_url":"https://arxiv.org/pdf/2203.10024v1.pdf","comment":"BigDML 2021"},{"id":"http://arxiv.org/abs/2203.10020v1","updated":"2022-03-18T15:36:57Z","published":"2022-03-18T15:36:57Z","title":"Challenges and Strategies in Cross-Cultural NLP","summary":"  Various efforts in the Natural Language Processing (NLP) community have been\nmade to accommodate linguistic diversity and serve speakers of many different\nlanguages. However, it is important to acknowledge that speakers and the\ncontent they produce and require, vary not just by language, but also by\nculture. Although language and culture are tightly linked, there are important\ndifferences. Analogous to cross-lingual and multilingual NLP, cross-cultural\nand multicultural NLP considers these differences in order to better serve\nusers of NLP systems. We propose a principled framework to frame these efforts,\nand survey existing and potential strategies.\n","authors":["Daniel Hershcovich","Stella Frank","Heather Lent","Miryam de Lhoneux","Mostafa Abdou","Stephanie Brandl","Emanuele Bugliarello","Laura Cabello Piqueras","Ilias Chalkidis","Ruixiang Cui","Constanza Fierro","Katerina Margatina","Phillip Rust","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2203.10020v1.pdf","comment":"ACL 2022 - Theme track"},{"id":"http://arxiv.org/abs/2203.10012v1","updated":"2022-03-18T15:21:11Z","published":"2022-03-18T15:21:11Z","title":"Report from the NSF Future Directions Workshop on Automatic Evaluation\n  of Dialog: Research Directions and Challenges","summary":"  This is a report on the NSF Future Directions Workshop on Automatic\nEvaluation of Dialog. The workshop explored the current state of the art along\nwith its limitations and suggested promising directions for future work in this\nimportant and very rapidly changing area of research.\n","authors":["Shikib Mehri","Jinho Choi","Luis Fernando D'Haro","Jan Deriu","Maxine Eskenazi","Milica Gasic","Kallirroi Georgila","Dilek Hakkani-Tur","Zekang Li","Verena Rieser","Samira Shaikh","David Traum","Yi-Ting Yeh","Zhou Yu","Yizhe Zhang","Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10012v1.pdf","comment":"Report from the NSF AED Workshop (http://dialrc.org/AED/)"},{"id":"http://arxiv.org/abs/2203.10010v1","updated":"2022-03-18T15:20:14Z","published":"2022-03-18T15:20:14Z","title":"CaMEL: Case Marker Extraction without Labels","summary":"  We introduce CaMEL (Case Marker Extraction without Labels), a novel and\nchallenging task in computational morphology that is especially relevant for\nlow-resource languages. We propose a first model for CaMEL that uses a\nmassively multilingual corpus to extract case markers in 83 languages based\nonly on a noun phrase chunker and an alignment system. To evaluate CaMEL, we\nautomatically construct a silver standard from UniMorph. The case markers\nextracted by our model can be used to detect and visualise similarities and\ndifferences between the case systems of different languages as well as to\nannotate fine-grained deep cases in languages in which they are not overtly\nmarked.\n","authors":["Leonie Weissweiler","Valentin Hofmann","Masoud Jalili Sabet","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2203.10010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10001v1","updated":"2022-03-18T15:01:32Z","published":"2022-03-18T15:01:32Z","title":"FORCE: A Framework of Rule-Based Conversational Recommender System","summary":"  The conversational recommender systems (CRSs) have received extensive\nattention in recent years. However, most of the existing works focus on various\ndeep learning models, which are largely limited by the requirement of\nlarge-scale human-annotated datasets. Such methods are not able to deal with\nthe cold-start scenarios in industrial products. To alleviate the problem, we\npropose FORCE, a Framework Of Rule-based Conversational Recommender system that\nhelps developers to quickly build CRS bots by simple configuration. We conduct\nexperiments on two datasets in different languages and domains to verify its\neffectiveness and usability.\n","authors":["Jun Quan","Ze Wei","Qiang Gan","Jingqi Yao","Jingyi Lu","Yuchen Dong","Yiming Liu","Yi Zeng","Chao Zhang","Yongzhi Li","Huang Hu","Yingying He","Yang Yang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.10001v1.pdf","comment":"AAAI 2022 (Demonstration Track)"},{"id":"http://arxiv.org/abs/2203.09994v1","updated":"2022-03-18T14:45:42Z","published":"2022-03-18T14:45:42Z","title":"Graph-Text Multi-Modal Pre-training for Medical Representation Learning","summary":"  As the volume of Electronic Health Records (EHR) sharply grows, there has\nbeen emerging interest in learning the representation of EHR for healthcare\napplications. Representation learning of EHR requires appropriate modeling of\nthe two dominant modalities in EHR: structured data and unstructured text. In\nthis paper, we present MedGTX, a pre-trained model for multi-modal\nrepresentation learning of the structured and textual EHR data. MedGTX uses a\nnovel graph encoder to exploit the graphical nature of structured EHR data, and\na text encoder to handle unstructured text, and a cross-modal encoder to learn\na joint representation space. We pre-train our model through four proxy tasks\non MIMIC-III, an open-source EHR data, and evaluate our model on two clinical\nbenchmarks and three novel downstream tasks which tackle real-world problems in\nEHR data. The results consistently show the effectiveness of pre-training the\nmodel for joint representation of both structured and unstructured information\nfrom EHR. Given the promising performance of MedGTX, we believe this work opens\na new door to jointly understanding the two fundamental modalities of EHR data.\n","authors":["Sungjin Park","Seongsu Bae","Jiho Kim","Tackeun Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2203.09994v1.pdf","comment":"To appear in Proceedings of the Conference on Health, Inference, and\n  Learning (CHIL 2022)"},{"id":"http://arxiv.org/abs/2203.09982v1","updated":"2022-03-18T14:18:12Z","published":"2022-03-18T14:18:12Z","title":"CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented\n  Cross-lingual Natural Language Understanding","summary":"  Task-oriented personal assistants enable people to interact with a host of\ndevices and services using natural language. One of the challenges of making\nneural dialogue systems available to more users is the lack of training data\nfor all but a few languages. Zero-shot methods try to solve this issue by\nacquiring task knowledge in a high-resource language such as English with the\naim of transferring it to the low-resource language(s). To this end, we\nintroduce CrossAligner, the principal method of a variety of effective\napproaches for zero-shot cross-lingual transfer based on learning alignment\nfrom unlabelled parallel data. We present a quantitative analysis of individual\nmethods as well as their weighted combinations, several of which exceed\nstate-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test\nsets and three benchmark multilingual datasets. A detailed qualitative error\nanalysis of the best methods shows that our fine-tuned language models can\nzero-shot transfer the task knowledge better than anticipated.\n","authors":["Milan Gritta","Ruoyu Hu","Ignacio Iacobacci"],"pdf_url":"https://arxiv.org/pdf/2203.09982v1.pdf","comment":"Long paper (multilingual track) to appear at ACL (Findings) 2022"},{"id":"http://arxiv.org/abs/2203.09975v1","updated":"2022-03-18T14:09:22Z","published":"2022-03-18T14:09:22Z","title":"BIOS: An Algorithmically Generated Biomedical Knowledge Graph","summary":"  Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for\nbiomedical and healthcare big data and artificial intelligence (AI),\nfacilitating natural language processing, model development, and data exchange.\nFor many decades, these knowledge graphs have been built via expert curation,\nwhich can no longer catch up with the speed of today's AI development, and a\ntransition to algorithmically generated BioMedKGs is necessary. In this work,\nwe introduce the Biomedical Informatics Ontology System (BIOS), the first large\nscale publicly available BioMedKG that is fully generated by machine learning\nalgorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in\ntwo languages, and 7.3 million relation triplets. We introduce the methodology\nfor developing BIOS, which covers curation of raw biomedical terms,\ncomputationally identifying synonymous terms and aggregating them to create\nconcept nodes, semantic type classification of the concepts, relation\nidentification, and biomedical machine translation. We provide statistics about\nthe current content of BIOS and perform preliminary assessment for term\nquality, synonym grouping, and relation extraction. Results suggest that\nmachine learning-based BioMedKG development is a totally viable solution for\nreplacing traditional expert curation.\n","authors":["Sheng Yu","Zheng Yuan","Jun Xia","Shengxuan Luo","Huaiyuan Ying","Sihang Zeng","Jingyi Ren","Hongyi Yuan","Zhengyun Zhao","Yucong Lin","Keming Lu","Jing Wang","Yutao Xie","Heung-Yeung Shum"],"pdf_url":"https://arxiv.org/pdf/2203.09975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09963v1","updated":"2022-03-18T13:59:02Z","published":"2022-03-18T13:59:02Z","title":"Towards Lithuanian grammatical error correction","summary":"  Everyone wants to write beautiful and correct text, yet the lack of language\nskills, experience, or hasty typing can result in errors. By employing the\nrecent advances in transformer architectures, we construct a grammatical error\ncorrection model for Lithuanian, the language rich in archaic features. We\ncompare subword and byte-level approaches and share our best trained model,\nachieving F$_{0.5}$=0.92, and accompanying code, in an online open-source\nrepository.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2203.09963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09946v1","updated":"2022-03-18T13:28:27Z","published":"2022-03-18T13:28:27Z","title":"Prompt-based Generative Approach towards Multi-Hierarchical Medical\n  Dialogue State Tracking","summary":"  The medical dialogue system is a promising application that can provide great\nconvenience for patients. The dialogue state tracking (DST) module in the\nmedical dialogue system which interprets utterances into the machine-readable\nstructure for downstream tasks is particularly challenging. Firstly, the states\nneed to be able to represent compound entities such as symptoms with their body\npart or diseases with degrees of severity to provide enough information for\ndecision support. Secondly, these named entities in the utterance might be\ndiscontinuous and scattered across sentences and speakers. These also make it\ndifficult to annotate a large corpus which is essential for most methods.\nTherefore, we first define a multi-hierarchical state structure. We annotate\nand publish a medical dialogue dataset in Chinese. To the best of our\nknowledge, there are no publicly available ones before. Then we propose a\nPrompt-based Generative Approach which can generate slot values with\nmulti-hierarchies incrementally using a top-down approach. A dialogue style\nprompt is also supplemented to utilize the large unlabeled dialogue corpus to\nalleviate the data scarcity problem. The experiments show that our approach\noutperforms other DST methods and is rather effective in the scenario with\nlittle data.\n","authors":["Jun Liu","Tong Ruan","Haofen Wang","Huanhuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09946v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2203.09936v1","updated":"2022-03-18T13:24:03Z","published":"2022-03-18T13:24:03Z","title":"Fake News Detection Using Majority Voting Technique","summary":"  Due to the evolution of the Web and social network platforms it becomes very\neasy to disseminate the information. Peoples are creating and sharing more\ninformation than ever before, which may be misleading, misinformation or fake\ninformation. Fake news detection is a crucial and challenging task due to the\nunstructured nature of the available information. In the recent years,\nresearchers have provided significant solutions to tackle with the problem of\nfake news detection, but due to its nature there are still many open issues. In\nthis paper, we have proposed majority voting approach to detect fake news\narticles. We have used different textual properties of fake and real news. We\nhave used publicly available fake news dataset, comprising of 20,800 news\narticles among which 10,387 are real and 10,413 are fake news labeled as binary\n0 and 1. For the evaluation of our approach, we have used commonly used machine\nlearning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random\nForest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the\naforementioned classifiers, we built a multi-model fake news detection system\nusing Majority Voting technique to achieve the more accurate results. The\nexperimental results show that, our proposed approach achieved accuracy of\n96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation\nconfirms that, Majority Voting technique achieved more acceptable results as\ncompare to individual learning technique.\n","authors":["Dharmaraj R. Patil"],"pdf_url":"https://arxiv.org/pdf/2203.09936v1.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.09904v1","updated":"2022-03-18T12:26:37Z","published":"2022-03-18T12:26:37Z","title":"Do Multilingual Language Models Capture Differing Moral Norms?","summary":"  Massively multilingual sentence representations are trained on large corpora\nof uncurated data, with a very imbalanced proportion of languages included in\nthe training. This may cause the models to grasp cultural values including\nmoral judgments from the high-resource languages and impose them on the\nlow-resource languages. The lack of data in certain languages can also lead to\ndeveloping random and thus potentially harmful beliefs. Both these issues can\nnegatively influence zero-shot cross-lingual model transfer and potentially\nlead to harmful outcomes. Therefore, we aim to (1) detect and quantify these\nissues by comparing different models in different languages, (2) develop\nmethods for improving undesirable properties of the models. Our initial\nexperiments using the multilingual model XLM-R show that indeed multilingual\nLMs capture moral norms, even with potentially higher human-agreement than\nmonolingual ones. However, it is not yet clear to what extent these moral norms\ndiffer between languages.\n","authors":["Katharina Hämmerl","Björn Deiseroth","Patrick Schramowski","Jindřich Libovický","Alexander Fraser","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2203.09904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09892v1","updated":"2022-03-18T12:04:09Z","published":"2022-03-18T12:04:09Z","title":"SCoT: Sense Clustering over Time: a tool for the analysis of lexical\n  change","summary":"  We present Sense Clustering over Time (SCoT), a novel network-based tool for\nanalysing lexical change. SCoT represents the meanings of a word as clusters of\nsimilar words. It visualises their formation, change, and demise. There are two\nmain approaches to the exploration of dynamic networks: the discrete one\ncompares a series of clustered graphs from separate points in time. The\ncontinuous one analyses the changes of one dynamic network over a time-span.\nSCoT offers a new hybrid solution. First, it aggregates time-stamped documents\ninto intervals and calculates one sense graph per discrete interval. Then, it\nmerges the static graphs to a new type of dynamic semantic neighbourhood graph\nover time. The resulting sense clusters offer uniquely detailed insights into\nlexical change over continuous intervals with model transparency and\nprovenance. SCoT has been successfully used in a European study on the changing\nmeaning of `crisis'.\n","authors":["Christian Haase","Saba Anwar","Seid Muhie Yimam","Alexander Friedrich","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2203.09892v1.pdf","comment":"Update of https://aclanthology.org/2021.eacl-demos.23/"},{"id":"http://arxiv.org/abs/2203.09866v1","updated":"2022-03-18T11:14:16Z","published":"2022-03-18T11:14:16Z","title":"Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias\n  in Speech Translation","summary":"  Gender bias is largely recognized as a problematic phenomenon affecting\nlanguage technologies, with recent studies underscoring that it might surface\ndifferently across languages. However, most of current evaluation practices\nadopt a word-level focus on a narrow set of occupational nouns under synthetic\nconditions. Such protocols overlook key features of grammatical gender\nlanguages, which are characterized by morphosyntactic chains of gender\nagreement, marked on a variety of lexical items and parts-of-speech (POS). To\novercome this limitation, we enrich the natural, gender-sensitive MuST-SHE\ncorpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS\nand agreement chains), and explore to what extent different lexical categories\nand agreement phenomena are impacted by gender skews. Focusing on speech\ntranslation, we conduct a multifaceted evaluation on three language directions\n(English-French/Italian/Spanish), with models trained on varying amounts of\ndata and different word segmentation techniques. By shedding light on model\nbehaviours, gender bias, and its detection at several levels of granularity,\nour findings emphasize the value of dedicated analyses beyond aggregated\noverall results.\n","authors":["Beatrice Savoldi","Marco Gaido","Luisa Bentivogli","Matteo Negri","Marco Turchi"],"pdf_url":"https://arxiv.org/pdf/2203.09866v1.pdf","comment":"Accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2104.13100v4","updated":"2022-03-18T10:28:57Z","published":"2021-04-27T10:50:47Z","title":"Shellcode_IA32: A Dataset for Automatic Shellcode Generation","summary":"  We take the first step to address the task of automatically generating\nshellcodes, i.e., small pieces of code used as a payload in the exploitation of\na software vulnerability, starting from natural language comments. We assemble\nand release a novel dataset (Shellcode_IA32), consisting of challenging but\ncommon assembly instructions with their natural language descriptions. We\nexperiment with standard methods in neural machine translation (NMT) to\nestablish baseline performance levels on this task.\n","authors":["Pietro Liguori","Erfan Al-Hossami","Domenico Cotroneo","Roberto Natella","Bojan Cukic","Samira Shaikh"],"pdf_url":"https://arxiv.org/pdf/2104.13100v4.pdf","comment":"Paper accepted to NLP4Prog Workshop 2021 co-located with ACL-IJCNLP\n  2021. Extended journal version of this work has been published in the\n  Automated Software Engineering journal, Volume 29, Article no. 30, March\n  2022, DOI: 10.1007/s10515-022-00331-3"},{"id":"http://arxiv.org/abs/2203.09825v1","updated":"2022-03-18T10:03:37Z","published":"2022-03-18T10:03:37Z","title":"AdaVocoder: Adaptive Vocoder for Custom Voice","summary":"  Custom voice is to construct a personal speech synthesis system by adapting\nthe source speech synthesis model to the target model through the target few\nrecordings. The solution to constructing a custom voice is to combine an\nadaptive acoustic model with a robust vocoder. However, training a robust\nvocoder usually requires a multi-speaker dataset, which should include various\nage groups and various timbres, so that the trained vocoder can be used for\nunseen speakers. Collecting such a multi-speaker dataset is difficult, and the\ndataset distribution always has a mismatch with the distribution of the target\nspeaker dataset. This paper proposes an adaptive vocoder for custom voice from\nanother novel perspective to solve the above problems. The adaptive vocoder\nmainly uses a cross-domain consistency loss to solve the overfitting problem\nencountered by the GAN-based neural vocoder in the transfer learning of\nfew-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.\nFirst, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,\nrespectively. Then, fine-tune it on the internal dataset VXI-children with few\nadaptation data. The empirical results show that a high-quality custom voice\nsystem can be built by combining a adaptive acoustic model with a adaptive\nvocoder.\n","authors":["Xin Yuan","Yongbing Feng","Mingming Ye","Cheng Tuo","Minghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09825v1.pdf","comment":"my paper was submitted to Insterspeech 2022"},{"id":"http://arxiv.org/abs/2109.11087v2","updated":"2022-03-18T09:30:34Z","published":"2021-09-23T00:46:47Z","title":"BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles","summary":"  A riddle is a question or statement with double or veiled meanings, followed\nby an unexpected answer. Solving riddle is a challenging task for both machine\nand human, testing the capability of understanding figurative, creative natural\nlanguage and reasoning with commonsense knowledge. We introduce BiRdQA, a\nbilingual multiple-choice question answering dataset with 6614 English riddles\nand 8751 Chinese riddles. For each riddle-answer pair, we provide four\ndistractors with additional information from Wikipedia. The distractors are\nautomatically generated at scale with minimal bias. Existing monolingual and\nmultilingual QA models fail to perform well on our dataset, indicating that\nthere is a long way to go before machine can beat human on solving tricky\nriddles. The dataset has been released to the community.\n","authors":["Yunxiang Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2109.11087v2.pdf","comment":"AAAI 2022"},{"id":"http://arxiv.org/abs/2203.09813v1","updated":"2022-03-18T09:19:14Z","published":"2022-03-18T09:19:14Z","title":"Are You Robert or RoBERTa? Deceiving Online Authorship Attribution\n  Models Using Neural Text Generators","summary":"  Recently, there has been a rise in the development of powerful pre-trained\nnatural language models, including GPT-2, Grover, and XLM. These models have\nshown state-of-the-art capabilities towards a variety of different NLP tasks,\nincluding question answering, content summarisation, and text generation.\nAlongside this, there have been many studies focused on online authorship\nattribution (AA). That is, the use of models to identify the authors of online\ntexts. Given the power of natural language models in generating convincing\ntexts, this paper examines the degree to which these language models can\ngenerate texts capable of deceiving online AA models. Experimenting with both\nblog and Twitter data, we utilise GPT-2 language models to generate texts using\nthe existing posts of online users. We then examine whether these AI-based text\ngenerators are capable of mimicking authorial style to such a degree that they\ncan deceive typical AA models. From this, we find that current AI-based text\ngenerators are able to successfully mimic authorship, showing capabilities\ntowards this on both datasets. Our findings, in turn, highlight the current\ncapacity of powerful natural language models to generate original online posts\ncapable of mimicking authorial style sufficiently to deceive popular AA\nmethods; a key finding given the proposed role of AA in real world applications\nsuch as spam-detection and forensic investigation.\n","authors":["Keenan Jones","Jason R. C. Nurse","Shujun Li"],"pdf_url":"https://arxiv.org/pdf/2203.09813v1.pdf","comment":"13 pages, 6 figures, 4 tables, Accepted for publication in the\n  proceedings of the sixteenth International AAAI Conference on Web and Social\n  Media (ICWSM-22)"},{"id":"http://arxiv.org/abs/2202.12226v2","updated":"2022-03-18T08:37:44Z","published":"2022-02-24T17:42:28Z","title":"Probing BERT's priors with serial reproduction chains","summary":"  Sampling is a promising bottom-up method for exposing what generative models\nhave learned about language, but it remains unclear how to generate\nrepresentative samples from popular masked language models (MLMs) like BERT.\nThe MLM objective yields a dependency network with no guarantee of consistent\nconditional distributions, posing a problem for naive approaches. Drawing from\ntheories of iterated learning in cognitive science, we explore the use of\nserial reproduction chains to sample from BERT's priors. In particular, we\nobserve that a unique and consistent estimator of the ground-truth joint\ndistribution is given by a Generative Stochastic Network (GSN) sampler, which\nrandomly selects which token to mask and reconstruct on each step. We show that\nthe lexical and syntactic statistics of sentences from GSN chains closely match\nthe ground-truth corpus distribution and perform better than other methods in a\nlarge corpus of naturalness judgments. Our findings establish a firmer\ntheoretical foundation for bottom-up probing and highlight richer deviations\nfrom human priors.\n","authors":["Takateru Yamakoshi","Thomas L. Griffiths","Robert D. Hawkins"],"pdf_url":"https://arxiv.org/pdf/2202.12226v2.pdf","comment":"Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2110.07159v2","updated":"2022-03-18T07:56:11Z","published":"2021-10-14T05:26:08Z","title":"Interpreting the Robustness of Neural NLP Models to Textual\n  Perturbations","summary":"  Modern Natural Language Processing (NLP) models are known to be sensitive to\ninput perturbations and their performance can decrease when applied to\nreal-world, noisy data. However, it is still unclear why models are less robust\nto some perturbations than others. In this work, we test the hypothesis that\nthe extent to which a model is affected by an unseen textual perturbation\n(robustness) can be explained by the learnability of the perturbation (defined\nas how well the model learns to identify the perturbation with a small amount\nof evidence). We further give a causal justification for the learnability\nmetric. We conduct extensive experiments with four prominent NLP models --\nTextRNN, BERT, RoBERTa and XLNet -- over eight types of textual perturbations\non three datasets. We show that a model which is better at identifying a\nperturbation (higher learnability) becomes worse at ignoring such a\nperturbation at test time (lower robustness), providing empirical support for\nour hypothesis.\n","authors":["Yunxiang Zhang","Liangming Pan","Samson Tan","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2110.07159v2.pdf","comment":"Accepted to Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2110.04486v2","updated":"2022-03-18T07:32:43Z","published":"2021-10-09T07:16:14Z","title":"PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS\n  With Accurate Phoneme Duration Control","summary":"  Sequence expansion between encoder and decoder is a critical challenge in\nsequence-to-sequence TTS. Attention-based methods achieve great naturalness but\nsuffer from unstable issues like missing and repeating phonemes, not to mention\naccurate duration control. Duration-informed methods, on the contrary, seem to\neasily adjust phoneme duration but show obvious degradation in speech\nnaturalness. This paper proposes PAMA-TTS to address the problem. It takes the\nadvantage of both flexible attention and explicit duration models. Based on the\nmonotonic attention mechanism, PAMA-TTS also leverages token duration and\nrelative position of a frame, especially countdown information, i.e. in how\nmany future frames the present phoneme will end. They help the attention to\nmove forward along the token sequence in a soft but reliable control.\nExperimental results prove that PAMA-TTS achieves the highest naturalness,\nwhile has on-par or even better duration controllability than the\nduration-informed model.\n","authors":["Yunchao He","Jian Luan","Yujun Wang"],"pdf_url":"https://arxiv.org/pdf/2110.04486v2.pdf","comment":"Accepted by ICASSP 2022. 5 pages, 4 figures, 3 tables. Audio samples\n  are available at: https://pama-tts.github.io/"},{"id":"http://arxiv.org/abs/2203.09770v1","updated":"2022-03-18T07:07:56Z","published":"2022-03-18T07:07:56Z","title":"Prototypical Verbalizer for Prompt-based Few-shot Tuning","summary":"  Prompt-based tuning for pre-trained language models (PLMs) has shown its\neffectiveness in few-shot learning. Typically, prompt-based tuning wraps the\ninput text into a cloze question. To make predictions, the model maps the\noutput words to labels via a verbalizer, which is either manually designed or\nautomatically built. However, manual verbalizers heavily depend on\ndomain-specific prior knowledge and human efforts, while finding appropriate\nlabel words automatically still remains challenging.In this work, we propose\nthe prototypical verbalizer (ProtoVerb) which is built directly from training\ndata. Specifically, ProtoVerb learns prototype vectors as verbalizers by\ncontrastive learning. In this way, the prototypes summarize training instances\nand are able to enclose rich class-level semantics. We conduct experiments on\nboth topic classification and entity typing tasks, and the results demonstrate\nthat ProtoVerb significantly outperforms current automatic verbalizers,\nespecially when training data is extremely scarce. More surprisingly, ProtoVerb\nconsistently boosts prompt-based tuning even on untuned PLMs, indicating an\nelegant non-tuning way to utilize PLMs. Our codes are avaliable at\nhttps://github.com/thunlp/OpenPrompt.\n","authors":["Ganqu Cui","Shengding Hu","Ning Ding","Longtao Huang","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2203.09770v1.pdf","comment":"11 pages. ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2203.03910v2","updated":"2022-03-18T06:12:36Z","published":"2022-03-08T08:08:45Z","title":"Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced\n  Training for Neural Machine Translation","summary":"  Neural networks tend to gradually forget the previously learned knowledge\nwhen learning multiple tasks sequentially from dynamic data distributions. This\nproblem is called \\textit{catastrophic forgetting}, which is a fundamental\nchallenge in the continual learning of neural networks. In this work, we\nobserve that catastrophic forgetting not only occurs in continual learning but\nalso affects the traditional static training. Neural networks, especially\nneural machine translation models, suffer from catastrophic forgetting even if\nthey learn from a static training set. To be specific, the final model pays\nimbalanced attention to training samples, where recently exposed samples\nattract more attention than earlier samples. The underlying cause is that\ntraining samples do not get balanced training in each model update, so we name\nthis problem \\textit{imbalanced training}. To alleviate this problem, we\npropose Complementary Online Knowledge Distillation (COKD), which uses\ndynamically updated teacher models trained on specific data orders to\niteratively provide complementary knowledge to the student model. Experimental\nresults on multiple machine translation tasks show that our method successfully\nalleviates the problem of imbalanced training and achieves substantial\nimprovements over strong baseline systems.\n","authors":["Chenze Shao","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2203.03910v2.pdf","comment":"ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2108.13655v2","updated":"2022-03-18T05:04:22Z","published":"2021-08-31T07:37:43Z","title":"MELM: Data Augmentation with Masked Entity Language Modeling for\n  Low-Resource NER","summary":"  Data augmentation is an effective solution to data scarcity in low-resource\nscenarios. However, when applied to token-level tasks such as NER, data\naugmentation methods often suffer from token-label misalignment, which leads to\nunsatsifactory performance. In this work, we propose Masked Entity Language\nModeling (MELM) as a novel data augmentation framework for low-resource NER. To\nalleviate the token-label misalignment issue, we explicitly inject NER labels\ninto sentence context, and thus the fine-tuned MELM is able to predict masked\nentity tokens by explicitly conditioning on their labels. Thereby, MELM\ngenerates high-quality augmented data with novel entities, which provides rich\nentity regularity knowledge and boosts NER performance. When training data from\nmultiple languages are available, we also integrate MELM with code-mixing for\nfurther improvement. We demonstrate the effectiveness of MELM on monolingual,\ncross-lingual and multilingual NER across various low-resource levels.\nExperimental results show that our MELM presents substantial improvement over\nthe baseline methods.\n","authors":["Ran Zhou","Xin Li","Ruidan He","Lidong Bing","Erik Cambria","Luo Si","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2108.13655v2.pdf","comment":"Accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2203.05297v3","updated":"2022-03-18T04:59:49Z","published":"2022-03-10T11:19:52Z","title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for\n  Conversational Gestures Synthesis","summary":"  Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n","authors":["Haiyang Liu","Zihao Zhu","Naoya Iwamoto","Yichen Peng","Zhengqing Li","You Zhou","Elif Bozkurt","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.05297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09742v1","updated":"2022-03-18T04:52:54Z","published":"2022-03-18T04:52:54Z","title":"GRS: Combining Generation and Revision in Unsupervised Sentence\n  Simplification","summary":"  We propose GRS: an unsupervised approach to sentence simplification that\ncombines text generation and text revision. We start with an iterative\nframework in which an input sentence is revised using explicit edit operations,\nand add paraphrasing as a new edit operation. This allows us to combine the\nadvantages of generative and revision-based approaches: paraphrasing captures\ncomplex edit operations, and the use of explicit edit operations in an\niterative manner provides controllability and interpretability. We demonstrate\nthese advantages of GRS compared to existing methods on the Newsela and ASSET\ndatasets.\n","authors":["Mohammad Dehghan","Dhruv Kumar","Lukasz Golab"],"pdf_url":"https://arxiv.org/pdf/2203.09742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09735v1","updated":"2022-03-18T04:23:20Z","published":"2022-03-18T04:23:20Z","title":"PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive\n  Weakly-Supervised Learning","summary":"  Weakly-supervised learning (WSL) has shown promising results in addressing\nlabel scarcity on many NLP tasks, but manually designing a comprehensive,\nhigh-quality labeling rule set is tedious and difficult. We study interactive\nweakly-supervised learning -- the problem of iteratively and automatically\ndiscovering novel labeling rules from data to improve the WSL model. Our\nproposed model, named PRBoost, achieves this goal via iterative prompt-based\nrule discovery and model boosting. It uses boosting to identify large-error\ninstances and then discovers candidate rules from them by prompting pre-trained\nLMs with rule templates. The candidate rules are judged by human experts, and\nthe accepted rules are used to generate complementary weak labels and\nstrengthen the current model. Experiments on four tasks show PRBoost\noutperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with\nfully supervised models. Our Implementation is available at\n\\url{https://github.com/rz-zhang/PRBoost}.\n","authors":["Rongzhi Zhang","Yue Yu","Pranav Shetty","Le Song","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09735v1.pdf","comment":"ACL 2022 (Main Conference). Code: https://github.com/rz-zhang/PRBoost"},{"id":"http://arxiv.org/abs/2111.02387v3","updated":"2022-03-18T03:29:10Z","published":"2021-11-03T17:55:36Z","title":"An Empirical Study of Training End-to-End Vision-and-Language\n  Transformers","summary":"  Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.\n","authors":["Zi-Yi Dou","Yichong Xu","Zhe Gan","Jianfeng Wang","Shuohang Wang","Lijuan Wang","Chenguang Zhu","Pengchuan Zhang","Lu Yuan","Nanyun Peng","Zicheng Liu","Michael Zeng"],"pdf_url":"https://arxiv.org/pdf/2111.02387v3.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09711v1","updated":"2022-03-18T03:11:35Z","published":"2022-03-18T03:11:35Z","title":"DEAM: Dialogue Coherence Evaluation using AMR-based Semantic\n  Manipulations","summary":"  Automatic evaluation metrics are essential for the rapid development of\nopen-domain dialogue systems as they facilitate hyper-parameter tuning and\ncomparison between models. Although recently proposed trainable\nconversation-level metrics have shown encouraging results, the quality of the\nmetrics is strongly dependent on the quality of training data. Prior works\nmainly resort to heuristic text-level manipulations (e.g. utterances shuffling)\nto bootstrap incoherent conversations (negative examples) from coherent\ndialogues (positive examples). Such approaches are insufficient to\nappropriately reflect the incoherence that occurs in interactions between\nadvanced dialogue models and humans. To tackle this problem, we propose DEAM, a\nDialogue coherence Evaluation metric that relies on Abstract Meaning\nRepresentation (AMR) to apply semantic-level Manipulations for incoherent\n(negative) data generation. AMRs naturally facilitate the injection of various\ntypes of incoherence sources, such as coreference inconsistency, irrelevancy,\ncontradictions, and decrease engagement, at the semantic level, thus resulting\nin more natural incoherent samples. Our experiments show that DEAM achieves\nhigher correlations with human judgments compared to baseline methods on\nseveral dialog datasets by significant margins. We also show that DEAM can\ndistinguish between coherent and incoherent dialogues generated by baseline\nmanipulations, whereas those baseline models cannot detect incoherent examples\ngenerated by DEAM. Our results demonstrate the potential of AMR-based semantic\nmanipulations for natural negative example generation.\n","authors":["Sarik Ghazarian","Nuan Wen","Aram Galstyan","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2203.09711v1.pdf","comment":"Association for Computational Linguistics (ACL 2022)"},{"id":"http://arxiv.org/abs/2203.09708v1","updated":"2022-03-18T02:57:32Z","published":"2022-03-18T02:57:32Z","title":"Improve few-shot voice cloning using multi-modal learning","summary":"  Recently, few-shot voice cloning has achieved a significant improvement.\nHowever, most models for few-shot voice cloning are single-modal, and\nmulti-modal few-shot voice cloning has been understudied. In this paper, we\npropose to use multi-modal learning to improve the few-shot voice cloning\nperformance. Inspired by the recent works on unsupervised speech\nrepresentation, the proposed multi-modal system is built by extending Tacotron2\nwith an unsupervised speech representation module. We evaluate our proposed\nsystem in two few-shot voice cloning scenarios, namely few-shot\ntext-to-speech(TTS) and voice conversion(VC). Experimental results demonstrate\nthat the proposed multi-modal learning can significantly improve the few-shot\nvoice cloning performance over their counterpart single-modal systems.\n","authors":["Haitong Zhang","Yue Lin"],"pdf_url":"https://arxiv.org/pdf/2203.09708v1.pdf","comment":"2022 IEEE International Conference on Acoustics, Speech and Signal\n  Processing"},{"id":"http://arxiv.org/abs/2203.09161v2","updated":"2022-03-18T02:18:54Z","published":"2022-03-17T08:30:30Z","title":"How Many Data Samples is an Additional Instruction Worth?","summary":"  Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state of the art task\nspecific models. Conventional approaches to improve model performance via\ncreating large datasets with lots of task instances or architectural/training\nchanges in model may not be feasible for non-expert users. However, they can\nwrite alternate instructions to represent an instruction task. Is\nInstruction-augumentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that\nthese significantly improve model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.\n","authors":["Ravsehaj Singh Puri","Swaroop Mishra","Mihir Parmar","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2203.09161v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2203.09690v1","updated":"2022-03-18T01:36:25Z","published":"2022-03-18T01:36:25Z","title":"A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech\n  Synthesis and Editing","summary":"  Recently, speech representation learning has improved many speech-related\ntasks such as speech recognition, speech classification, and speech-to-text\ntranslation. However, all the above tasks are in the direction of speech\nunderstanding, but for the inverse direction, speech synthesis, the potential\nof representation learning is yet to be realized, due to the challenging nature\nof generating high-quality speech. To address this problem, we propose our\nframework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which\nreconstructs masked acoustic signals with text input and acoustic-text\nalignment during training. In this way, the pretrained model can generate high\nquality of reconstructed spectrogram, which can be applied to the speech\nediting and unseen speaker TTS directly. Experiments show A$^3$T outperforms\nSOTA models on speech editing, and improves multi-speaker speech synthesis\nwithout the external speaker verification model.\n","authors":["He Bai","Renjie Zheng","Junkun Chen","Xintong Li","Mingbo Ma","Liang Huang"],"pdf_url":"https://arxiv.org/pdf/2203.09690v1.pdf","comment":"under review, 12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2203.09679v1","updated":"2022-03-18T01:13:21Z","published":"2022-03-18T01:13:21Z","title":"Modeling Intensification for Sign Language Generation: A Computational\n  Approach","summary":"  End-to-end sign language generation models do not accurately represent the\nprosody in sign language. A lack of temporal and spatial variations leads to\npoor-quality generated presentations that confuse human interpreters. In this\npaper, we aim to improve the prosody in generated sign languages by modeling\nintensification in a data-driven manner. We present different strategies\ngrounded in linguistics of sign language that inform how intensity modifiers\ncan be represented in gloss annotations. To employ our strategies, we first\nannotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,\nwith different levels of intensification. We then use a supervised intensity\ntagger to extend the annotated dataset and obtain labels for the remaining\nportion of it. This enhanced dataset is then used to train state-of-the-art\ntransformer models for sign language generation. We find that our efforts in\nintensification modeling yield better results when evaluated with automatic\nmetrics. Human evaluation also indicates a higher preference of the videos\ngenerated using our model.\n","authors":["Mert İnan","Yang Zhong","Sabit Hassan","Lorna Quandt","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2203.09679v1.pdf","comment":"15 pages, Findings of the Association for Computational Linguistics:\n  ACL 2022"},{"id":"http://arxiv.org/abs/2203.09673v1","updated":"2022-03-18T00:46:58Z","published":"2022-03-18T00:46:58Z","title":"Hate speech, Censorship, and Freedom of Speech: The Changing Policies of\n  Reddit","summary":"  This paper examines the shift in focus on content policies and user attitudes\non the social media platform Reddit. We do this by focusing on comments from\ngeneral Reddit users from five posts made by admins (moderators) on updates to\nReddit Content Policy. All five concern the nature of what kind of content is\nallowed to be posted on Reddit, and which measures will be taken against\ncontent that violates these policies. We use topic modeling to probe how the\ngeneral discourse for Redditors has changed around limitations on content, and\nlater, limitations on hate speech, or speech that incites violence against a\nparticular group. We show that there is a clear shift in both the contents and\nthe user attitudes that can be linked to contemporary societal upheaval as well\nas newly passed laws and regulations, and contribute to the wider discussion on\nhate speech moderation.\n","authors":["Elissa Nakajima Wickham","Emily Öhman"],"pdf_url":"https://arxiv.org/pdf/2203.09673v1.pdf","comment":"Submitted to Journal of Data Mining and Digital Humanities"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2203.10078v1","updated":"2022-03-18T17:47:29Z","published":"2022-03-18T17:47:29Z","title":"Bayesian Inversion for Nonlinear Imaging Models using Deep Generative\n  Priors","summary":"  Most modern imaging systems involve a computational reconstruction pipeline\nto infer the image of interest from acquired measurements. The Bayesian\nreconstruction framework relies on the characterization of the posterior\ndistribution, which depends on a model of the imaging system and prior\nknowledge on the image, for solving such inverse problems. Here, the choice of\nthe prior distribution is critical for obtaining high-quality estimates. In\nthis work, we use deep generative models to represent the prior distribution.\nWe develop a posterior sampling scheme for the class of nonlinear inverse\nproblems where the forward model has a neural-network-like structure. This\nclass includes most existing imaging modalities. We introduce the notion of\naugmented generative models in order to suitably handle quantitative image\nrecovery. We illustrate the advantages of our framework by applying it to two\nnonlinear imaging modalities-phase retrieval and optical diffraction\ntomography.\n","authors":["Pakshal Bohra","Thanh-an Pham","Jonathan Dong","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2203.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09219v2","updated":"2022-03-18T17:45:58Z","published":"2021-12-16T21:54:26Z","title":"All You Need is RAW: Defending Against Adversarial Attacks with Camera\n  Image Pipelines","summary":"  Existing neural networks for computer vision tasks are vulnerable to\nadversarial attacks: adding imperceptible perturbations to the input images can\nfool these methods to make a false prediction on an image that was correctly\npredicted without the perturbation. Various defense methods have proposed\nimage-to-image mapping methods, either including these perturbations in the\ntraining process or removing them in a preprocessing denoising step. In doing\nso, existing methods often ignore that the natural RGB images in today's\ndatasets are not captured but, in fact, recovered from RAW color filter array\ncaptures that are subject to various degradations in the capture. In this work,\nwe exploit this RAW data distribution as an empirical prior for adversarial\ndefense. Specifically, we proposed a model-agnostic adversarial defensive\nmethod, which maps the input RGB images to Bayer RAW space and back to output\nRGB using a learned camera image signal processing (ISP) pipeline to eliminate\npotential adversarial patterns. The proposed method acts as an off-the-shelf\npreprocessing module and, unlike model-specific adversarial training methods,\ndoes not require adversarial images to train. As a result, the method\ngeneralizes to unseen tasks without additional retraining. Experiments on\nlarge-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g.,\nclassification, semantic segmentation, object detection) validate that the\nmethod significantly outperforms existing methods across task domains.\n","authors":["Yuxuan Zhang","Bo Dong","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2112.09219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10073v1","updated":"2022-03-18T17:38:52Z","published":"2022-03-18T17:38:52Z","title":"Lunar Rover Localization Using Craters as Landmarks","summary":"  Onboard localization capabilities for planetary rovers to date have used\nrelative navigation, by integrating combinations of wheel odometry, visual\nodometry, and inertial measurements during each drive to track position\nrelative to the start of each drive. At the end of each drive, a\nground-in-the-loop (GITL) interaction is used to get a position update from\nhuman operators in a more global reference frame, by matching images or local\nmaps from onboard the rover to orbital reconnaissance images or maps of a large\nregion around the rover's current position. Autonomous rover drives are limited\nin distance so that accumulated relative navigation error does not risk the\npossibility of the rover driving into hazards known from orbital images.\nHowever, several rover mission concepts have recently been studied that require\nmuch longer drives between GITL cycles, particularly for the Moon. These\nconcepts require greater autonomy to minimize GITL cycles to enable such large\nrange; onboard global localization is a key element of such autonomy. Multiple\ntechniques have been studied in the past for onboard rover global localization,\nbut a satisfactory solution has not yet emerged. For the Moon, the ubiquitous\ncraters offer a new possibility, which involves mapping craters from orbit,\nthen recognizing crater landmarks with cameras and-or a lidar onboard the\nrover. This approach is applicable everywhere on the Moon, does not require\nhigh resolution stereo imaging from orbit as some other approaches do, and has\npotential to enable position knowledge with order of 5 to 10 m accuracy at all\ntimes. This paper describes our technical approach to crater-based lunar rover\nlocalization and presents initial results on crater detection using 3D point\ncloud data from onboard lidar or stereo cameras, as well as using shading cues\nin monocular onboard imagery.\n","authors":["Larry Matthies","Shreyansh Daftry","Scott Tepsuporn","Yang Cheng","Deegan Atha","R. Michael Swan","Sanjna Ravichandar","Masahiro Ono"],"pdf_url":"https://arxiv.org/pdf/2203.10073v1.pdf","comment":"IEEE Aerospace Conference, 2022"},{"id":"http://arxiv.org/abs/2203.10062v1","updated":"2022-03-18T17:11:05Z","published":"2022-03-18T17:11:05Z","title":"Imaging-based histological features are predictive of MET alterations in\n  Non-Small Cell Lung Cancer","summary":"  MET is a proto-oncogene whose somatic activation in non-small cell lung\ncancer leads to increased cell growth and tumor progression. The two major\nclasses of MET alterations are gene amplification and exon 14 deletion, both of\nwhich are therapeutic targets and detectable using existing molecular assays.\nHowever, existing tests are limited by their consumption of valuable tissue,\ncost and complexity that prevent widespread use. MET alterations could have an\neffect on cell morphology, and quantifying these associations could open new\navenues for research and development of morphology-based screening tools. Using\nH&E-stained whole slide images (WSIs), we investigated the association of\ndistinct cell-morphological features with MET amplifications and MET exon 14\ndeletions. We found that cell shape, color, grayscale intensity and\ntexture-based features from both tumor infiltrating lymphocytes and tumor cells\ndistinguished MET wild-type from MET amplified or MET exon 14 deletion cases.\nThe association of individual cell features with MET alterations suggested a\npredictive model could distinguish MET wild-type from MET amplification or MET\nexon 14 deletion. We therefore developed an L1-penalized logistic regression\nmodel, achieving a mean Area Under the Receiver Operating Characteristic Curve\n(ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent\nholdout test set. A sparse set of 43 features differentiated these classes,\nwhich included features similar to what was found in the univariate analysis as\nwell as the percent of tumor cells in the tissue. Our study demonstrates that\nMET alterations result in a detectable morphological signal in tumor cells and\nlymphocytes. These results suggest that development of low-cost predictive\nmodels based on H&E-stained WSIs may improve screening for MET altered tumors.\n","authors":["Rohan P. Joshi","Bo Osinski","Niha Beig","Lingdao Sha","Kshitij Ingale","Martin C. Stumpe"],"pdf_url":"https://arxiv.org/pdf/2203.10062v1.pdf","comment":"30 pages, 4 figures"},{"id":"http://arxiv.org/abs/2202.04239v2","updated":"2022-03-18T17:08:20Z","published":"2022-02-09T02:50:42Z","title":"A multiscale spatiotemporal approach for smallholder irrigation\n  detection","summary":"  In presenting an irrigation detection methodology that leverages multiscale\nsatellite imagery of vegetation abundance, this paper introduces a process to\nsupplement limited ground-collected labels and ensure classifier applicability\nin an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced\nVegetation Index (EVI) timeseries characterizes native vegetation phenologies\nat regional scale to provide the basis for a continuous phenology map that\nguides supplementary label collection over irrigated and non-irrigated\nagriculture. Subsequently, validated dry season greening and senescence cycles\nobserved in 10m Sentinel-2 imagery are used to train a suite of classifiers for\nautomated detection of potential smallholder irrigation. Strategies to improve\nmodel robustness are demonstrated, including a method of data augmentation that\nrandomly shifts training samples; and an assessment of classifier types that\nproduce the best performance in withheld target regions. The methodology is\napplied to detect smallholder irrigation in two states in the Ethiopian\nhighlands, Tigray and Amhara. Results show that a transformer-based neural\nnetwork architecture allows for the most robust prediction performance in\nwithheld regions, followed closely by a CatBoost random forest model. Over\nwithheld ground-collection survey labels, the transformer-based model achieves\n96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated\nsamples. Over a larger set of samples independently collected via the\nintroduced method of label supplementation, non-irrigated and irrigated labels\nare predicted with 98.3% and 95.5% accuracy, respectively. The detection model\nis then deployed over Tigray and Amhara, revealing crop rotation patterns and\nyear-over-year irrigated area change. Predictions suggest that irrigated area\nin these two states has decreased by approximately 40% from 2020 to 2021.\n","authors":["Terence Conlon","Christopher Small","Vijay Modi"],"pdf_url":"https://arxiv.org/pdf/2202.04239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10039v1","updated":"2022-03-18T16:26:53Z","published":"2022-03-18T16:26:53Z","title":"Multi-input segmentation of damaged brain in acute ischemic stroke\n  patients using slow fusion with skip connection","summary":"  Time is a fundamental factor during stroke treatments. A fast, automatic\napproach that segments the ischemic regions helps treatment decisions. In\nclinical use today, a set of color-coded parametric maps generated from\ncomputed tomography perfusion (CTP) images are investigated manually to decide\na treatment plan. We propose an automatic method based on a neural network\nusing a set of parametric maps to segment the two ischemic regions (core and\npenumbra) in patients affected by acute ischemic stroke. Our model is based on\na convolution-deconvolution bottleneck structure with multi-input and slow\nfusion. A loss function based on the focal Tversky index addresses the data\nimbalance issue. The proposed architecture demonstrates effective performance\nand results comparable to the ground truth annotated by neuroradiologists. A\nDice coefficient of 0.81 for penumbra and 0.52 for core over the large vessel\nocclusion test set is achieved. The full implementation is available at:\nhttps://git.io/JtFGb.\n","authors":["Luca Tomasetti","Mahdieh Khanmohammadi","Kjersti Engan","Liv Jorunn Høllesli","Kathinka Dæhli Kurz"],"pdf_url":"https://arxiv.org/pdf/2203.10039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.12219v2","updated":"2022-03-18T16:23:50Z","published":"2021-12-22T20:45:24Z","title":"SAMCNet for Spatial-configuration-based Classification: A Summary of\n  Results","summary":"  The goal of spatial-configuration-based classification is to build a\nclassifier to distinguish two classes (e.g., responder, non-responder) based on\nthe spatial arrangements (e.g., spatial interactions between different point\ncategories) given multi-category point data from two classes. This problem is\nimportant for generating hypotheses in medical pathology towards discovering\nnew immunotherapies for cancer treatment as well as for other applications in\nbiomedical research and microbial ecology. This problem is challenging due to\nan exponential number of category subsets which may vary in the strength of\nspatial interactions. Most prior efforts on using human selected spatial\nassociation measures may not be sufficient for capturing the relevant (e.g.,\nsurrounded by) spatial interactions which may be of biological significance. In\naddition, the related deep neural networks are limited to category pairs and do\nnot explore larger subsets of point categories. To overcome these limitations,\nwe propose a Spatial-interaction Aware Multi-Category deep neural Network\n(SAMCNet) architecture and contribute novel local reference frame\ncharacterization and point pair prioritization layers for\nspatial-configuration-based classification. Extensive experimental results on\nmultiple cancer datasets show that the proposed architecture provides higher\nprediction accuracy over baseline methods.\n","authors":["Majid Farhadloo","Carl Molnar","Gaoxiang Luo","Yan Li","Shashi Shekhar","Rachel L. Maus","Svetomir N. Markovic","Raymond Moore","Alexey Leontovich"],"pdf_url":"https://arxiv.org/pdf/2112.12219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10035v1","updated":"2022-03-18T16:08:22Z","published":"2022-03-18T16:08:22Z","title":"SHREC 2021: Classification in cryo-electron tomograms","summary":"  Cryo-electron tomography (cryo-ET) is an imaging technique that allows\nthree-dimensional visualization of macro-molecular assemblies under near-native\nconditions. Cryo-ET comes with a number of challenges, mainly low\nsignal-to-noise and inability to obtain images from all angles. Computational\nmethods are key to analyze cryo-electron tomograms.\n  To promote innovation in computational methods, we generate a novel simulated\ndataset to benchmark different methods of localization and classification of\nbiological macromolecules in tomograms. Our publicly available dataset contains\nten tomographic reconstructions of simulated cell-like volumes. Each volume\ncontains twelve different types of complexes, varying in size, function and\nstructure.\n  In this paper, we have evaluated seven different methods of finding and\nclassifying proteins. Seven research groups present results obtained with\nlearning-based methods and trained on the simulated dataset, as well as a\nbaseline template matching (TM), a traditional method widely used in cryo-ET\nresearch. We show that learning-based approaches can achieve notably better\nlocalization and classification performance than TM. We also experimentally\nconfirm that there is a negative relationship between particle size and\nperformance for all methods.\n","authors":["Ilja Gubins","Marten L. Chaillet","Gijs van der Schot","M. Cristina Trueba","Remco C. Veltkamp","Friedrich Förster","Xiao Wang","Daisuke Kihara","Emmanuel Moebel","Nguyen P. Nguyen","Tommi White","Filiz Bunyak","Giorgos Papoulias","Stavros Gerolymatos","Evangelia I. Zacharaki","Konstantinos Moustakas","Xiangrui Zeng","Sinuo Liu","Min Xu","Yaoyu Wang","Cheng Chen","Xuefeng Cui","Fa Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10035v1.pdf","comment":"Workshop version of the paper can be found here:\n  https://diglib.eg.org/handle/10.2312/3dor20211307"},{"id":"http://arxiv.org/abs/2203.10030v1","updated":"2022-03-18T16:02:27Z","published":"2022-03-18T16:02:27Z","title":"Nonnegative-Constrained Joint Collaborative Representation with Union\n  Dictionary for Hyperspectral Anomaly Detection","summary":"  Recently, many collaborative representation-based (CR) algorithms have been\nproposed for hyperspectral anomaly detection. CR-based detectors approximate\nthe image by a linear combination of background dictionaries and the\ncoefficient matrix, and derive the detection map by utilizing recovery\nresiduals. However, these CR-based detectors are often established on the\npremise of precise background features and strong image representation, which\nare very difficult to obtain. In addition, pursuing the coefficient matrix\nreinforced by the general $l_2$-min is very time consuming. To address these\nissues, a nonnegative-constrained joint collaborative representation model is\nproposed in this paper for the hyperspectral anomaly detection task. To extract\nreliable samples, a union dictionary consisting of background and anomaly\nsub-dictionaries is designed, where the background sub-dictionary is obtained\nat the superpixel level and the anomaly sub-dictionary is extracted by the\npre-detection process. And the coefficient matrix is jointly optimized by the\nFrobenius norm regularization with a nonnegative constraint and a sum-to-one\nconstraint. After the optimization process, the abnormal information is finally\nderived by calculating the residuals that exclude the assumed background\ninformation. To conduct comparable experiments, the proposed\nnonnegative-constrained joint collaborative representation (NJCR) model and its\nkernel version (KNJCR) are tested in four HSI data sets and achieve superior\nresults compared with other state-of-the-art detectors.\n","authors":["Shizhen Chang","Pedram Ghamisi"],"pdf_url":"https://arxiv.org/pdf/2203.10030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10026v1","updated":"2022-03-18T15:53:18Z","published":"2022-03-18T15:53:18Z","title":"Unbiased Subclass Regularization for Semi-Supervised Semantic\n  Segmentation","summary":"  Semi-supervised semantic segmentation learns from small amounts of labelled\nimages and large amounts of unlabelled images, which has witnessed impressive\nprogress with the recent advance of deep neural networks. However, it often\nsuffers from severe class-bias problem while exploring the unlabelled images,\nlargely due to the clear pixel-wise class imbalance in the labelled images.\nThis paper presents an unbiased subclass regularization network (USRN) that\nalleviates the class imbalance issue by learning class-unbiased segmentation\nfrom balanced subclass distributions. We build the balanced subclass\ndistributions by clustering pixels of each original class into multiple\nsubclasses of similar sizes, which provide class-balanced pseudo supervision to\nregularize the class-biased segmentation. In addition, we design an\nentropy-based gate mechanism to coordinate learning between the original\nclasses and the clustered subclasses which facilitates subclass regularization\neffectively by suppressing unconfident subclass predictions. Extensive\nexperiments over multiple public benchmarks show that USRN achieves superior\nperformance as compared with the state-of-the-art.\n","authors":["Dayan Guan","Jiaxing Huang","Aoran Xiao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2203.10026v1.pdf","comment":"Accepted to ICCV 2021. Code is available at\n  https://github.com/Dayan-Guan/USRN"},{"id":"http://arxiv.org/abs/2203.10016v1","updated":"2022-03-18T15:30:01Z","published":"2022-03-18T15:30:01Z","title":"ESS: Learning Event-based Semantic Segmentation from Still Images","summary":"  Retrieving accurate semantic information in challenging high dynamic range\n(HDR) and high-speed conditions remains an open challenge for image-based\nalgorithms due to severe image degradations. Event cameras promise to address\nthese challenges since they feature a much higher dynamic range and are\nresilient to motion blur. Nonetheless, semantic segmentation with event cameras\nis still in its infancy which is chiefly due to the novelty of the sensor, and\nthe lack of high-quality, labeled datasets. In this work, we introduce ESS,\nwhich tackles this problem by directly transferring the semantic segmentation\ntask from existing labeled image datasets to unlabeled events via unsupervised\ndomain adaptation (UDA). Compared to existing UDA methods, our approach aligns\nrecurrent, motion-invariant event embeddings with image embeddings. For this\nreason, our method neither requires video data nor per-pixel alignment between\nimages and events and, crucially, does not need to hallucinate motion from\nstill images. Additionally, to spur further research in event-based semantic\nsegmentation, we introduce DSEC-Semantic, the first large-scale event-based\ndataset with fine-grained labels. We show that using image labels alone, ESS\noutperforms existing UDA approaches, and when combined with event labels, it\neven outperforms state-of-the-art supervised approaches on both DDD17 and\nDSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount\nof existing labeled image datasets and paves the way for new and exciting\nresearch directions in new fields previously inaccessible for event cameras.\n","authors":["Zhaoning Sun","Nico Messikommer","Daniel Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2203.10016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10014v1","updated":"2022-03-18T15:26:05Z","published":"2022-03-18T15:26:05Z","title":"Parametric Scaling of Preprocessing assisted U-net Architecture for\n  Improvised Retinal Vessel Segmentation","summary":"  Extracting blood vessels from retinal fundus images plays a decisive role in\ndiagnosing the progression in pertinent diseases. In medical image analysis,\nvessel extraction is a semantic binary segmentation problem, where blood\nvasculature needs to be extracted from the background. Here, we present an\nimage enhancement technique based on the morphological preprocessing coupled\nwith a scaled U-net architecture. Despite a relatively less number of trainable\nnetwork parameters, the scaled version of U-net architecture provides better\nperformance compare to other methods in the domain. We validated the proposed\nmethod on retinal fundus images from the DRIVE database. A significant\nimprovement as compared to the other algorithms in the domain, in terms of the\narea under ROC curve (>0.9762) and classification accuracy (>95.47%) are\nevident from the results. Furthermore, the proposed method is resistant to the\ncentral vessel reflex while sensitive to detect blood vessels in the presence\nof background items viz. exudates, optic disc, and fovea.\n","authors":["Kundan Kumar","Sumanshu Agarwal"],"pdf_url":"https://arxiv.org/pdf/2203.10014v1.pdf","comment":"10 pages, 5 figures, ICAIHC-2022"},{"id":"http://arxiv.org/abs/2203.10009v1","updated":"2022-03-18T15:18:55Z","published":"2022-03-18T15:18:55Z","title":"Analyzing EEG Data with Machine and Deep Learning: A Benchmark","summary":"  Nowadays, machine and deep learning techniques are widely used in different\nareas, ranging from economics to biology. In general, these techniques can be\nused in two ways: trying to adapt well-known models and architectures to the\navailable data, or designing custom architectures. In both cases, to speed up\nthe research process, it is useful to know which type of models work best for a\nspecific problem and/or data type. By focusing on EEG signal analysis, and for\nthe first time in literature, in this paper a benchmark of machine and deep\nlearning for EEG signal classification is proposed. For our experiments we used\nthe four most widespread models, i.e., multilayer perceptron, convolutional\nneural network, long short-term memory, and gated recurrent unit, highlighting\nwhich one can be a good starting point for developing EEG classification\nmodels.\n","authors":["Danilo Avola","Marco Cascio","Luigi Cinque","Alessio Fagioli","Gian Luca Foresti","Marco Raoul Marini","Daniele Pannone"],"pdf_url":"https://arxiv.org/pdf/2203.10009v1.pdf","comment":"conference, 11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.10006v1","updated":"2022-03-18T15:14:13Z","published":"2022-03-18T15:14:13Z","title":"Ultra-low Latency Spiking Neural Networks with Spatio-Temporal\n  Compression and Synaptic Convolutional Block","summary":"  Spiking neural networks (SNNs), as one of the brain-inspired models, has\nspatio-temporal information processing capability, low power feature, and high\nbiological plausibility. The effective spatio-temporal feature makes it\nsuitable for event streams classification. However, neuromorphic datasets, such\nas N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events\ninto frames with a new higher temporal resolution for event stream\nclassification, which causes high training and inference latency. In this work,\nwe proposed a spatio-temporal compression method to aggregate individual events\ninto a few time steps of synaptic current to reduce the training and inference\nlatency. To keep the accuracy of SNNs under high compression ratios, we also\nproposed a synaptic convolutional block to balance the dramatic change between\nadjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with\nlearnable membrane time constant is introduced to increase its information\nprocessing capability. We evaluate the proposed method for event streams\nclassification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture\ndatasets. The experiment results show that our proposed method outperforms the\nstate-of-the-art accuracy on nearly all datasets, using fewer time steps.\n","authors":["Changqing Xu","Yi Liu","Yintang Yang"],"pdf_url":"https://arxiv.org/pdf/2203.10006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10005v1","updated":"2022-03-18T15:07:55Z","published":"2022-03-18T15:07:55Z","title":"Application of Top-hat Transformation for Enhanced Blood Vessel\n  Extraction","summary":"  In the medical domain, different computer-aided diagnosis systems have been\nproposed to extract blood vessels from retinal fundus images for the clinical\ntreatment of vascular diseases. Accurate extraction of blood vessels from the\nfundus images using a computer-generated method can help the clinician to\nproduce timely and accurate reports for the patient suffering from these\ndiseases. In this article, we integrate top-hat based preprocessing approach\nwith fine-tuned B-COSFIRE filter to achieve more accurate segregation of blood\nvessel pixels from the background. The use of top-hat transformation in the\npreprocessing stage enhances the efficacy of the algorithm to extract blood\nvessels in presence of structures like fovea, exudates, haemorrhages, etc.\nFurthermore, to reduce the false positives, small clusters of blood vessel\npixels are removed in the postprocessing stage. Further, we find that the\nproposed algorithm is more efficient as compared to various modern algorithms\nreported in the literature.\n","authors":["Tithi Parna Das","Sheetal Praharaj","Sarita Swain","Sumanshu Agarwal","Kundan Kumar"],"pdf_url":"https://arxiv.org/pdf/2203.10005v1.pdf","comment":"9 pages, 3 figures, ICAIHC-2022"},{"id":"http://arxiv.org/abs/2203.05340v4","updated":"2022-03-18T14:49:46Z","published":"2022-03-10T12:44:05Z","title":"Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing","summary":"  With diverse presentation attacks emerging continually, generalizable face\nanti-spoofing (FAS) has drawn growing attention. Most existing methods\nimplement domain generalization (DG) on the complete representations. However,\ndifferent image statistics may have unique properties for the FAS tasks. In\nthis work, we separate the complete representation into content and style ones.\nA novel Shuffled Style Assembly Network (SSAN) is proposed to extract and\nreassemble different content and style features for a stylized feature space.\nThen, to obtain a generalized representation, a contrastive learning strategy\nis developed to emphasize liveness-related style information while suppress the\ndomain-specific one. Finally, the representations of the correct assemblies are\nused to distinguish between living and spoofing during the inferring. On the\nother hand, despite the decent performance, there still exists a gap between\nacademia and industry, due to the difference in data quantity and distribution.\nThus, a new large-scale benchmark for FAS is built up to further evaluate the\nperformance of algorithms in reality. Both qualitative and quantitative results\non existing and proposed benchmarks demonstrate the effectiveness of our\nmethods. The codes will be available at https://github.com/wangzhuo2019/SSAN.\n","authors":["Zhuo Wang","Zezheng Wang","Zitong Yu","Weihong Deng","Jiahong Li","Tingting Gao","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.05340v4.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.09995v1","updated":"2022-03-18T14:47:33Z","published":"2022-03-18T14:47:33Z","title":"Elastica Models for Color Image Regularization","summary":"  One classical approach to regularize color is to tream them as two\ndimensional surfaces embedded in a five dimensional spatial-chromatic space. In\nthis case, a natural regularization term arises as the image surface area.\nChoosing the chromatic coordinates as dominating over the spatial ones, the\nimage spatial coordinates could be thought of as a paramterization of the image\nsurface manifold in a three dimensional color space. Minimizing the area of the\nimage manifold leads to the Beltrami flow or mean curvature flow of the image\nsurface in the 3D color space, while minimizing the elastica of the image\nsurface yields an additional interesting regularization. Recently, the authors\nproposed a color elastica model, which minimizes both the surface area and\nelastica of the image manifold. In this paper, we propose to modify the color\nelastica and introduce two new models for color image regularization. The\nrevised measures are motivated by the relations between the color elastica\nmodel, Euler's elastica model and the total variation model for gray level\nimages. Compared to our previous color elastica model, the new models are\ndirect extensions of Euler's elastica model to color images. The proposed\nmodels are nonlinear and challenging to minimize. To overcome this difficulty,\ntwo operator-splitting methods are suggested. Specifically, nonlinearities are\ndecoupled by introducing new vector- and matrix-valued variables. Then, the\nminimization problems are converted to solving initial value problems which are\ntime-discretized by operator splitting. Each subproblem, after splitting\neither, has a closed-form solution or can be solved efficiently. The\neffectiveness and advantages of the proposed models are demonstrated by\ncomprehensive experiments. The benefits of incorporating the elastica of the\nimage surface as regularization terms compared to common alternatives are\nempirically validated.\n","authors":["Hao Liu","Xue-Cheng Tai","Ron Kimmel","Roland Glowinski"],"pdf_url":"https://arxiv.org/pdf/2203.09995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09992v1","updated":"2022-03-18T14:39:42Z","published":"2022-03-18T14:39:42Z","title":"Diffusion and Volume Maximization-Based Clustering of Highly Mixed\n  Hyperspectral Images","summary":"  Hyperspectral images of a scene or object are a rich data source, often\nencoding a hundred or more spectral bands of reflectance at each pixel. Despite\nbeing very high-dimensional, these images typically encode latent\nlow-dimensional structure that can be exploited for material discrimination.\nHowever, due to an inherent trade-off between spectral and spatial resolution,\nmany hyperspectral images are generated at a coarse spatial scale, and single\npixels may correspond to spatial regions containing multiple materials. This\narticle introduces the \\emph{Diffusion and Volume maximization-based Image\nClustering} (\\emph{D-VIC}) algorithm for unsupervised material discrimination.\nD-VIC locates cluster modes -- high-density, high-purity pixels in the\nhyperspectral image that are far in diffusion distance (a data-dependent\ndistance metric) from other high-density, high-purity pixels -- and assigns\nthese pixels unique labels, as these points are meant to exemplify underlying\nmaterial structure. Non-modal pixels are labeled according to their diffusion\ndistance nearest neighbor of higher density and purity that is already labeled.\nBy directly incorporating pixel purity into its modal and non-modal labeling,\nD-VIC upweights pixels that correspond to a spatial region containing just a\nsingle material, yielding more interpretable clusterings. D-VIC is shown to\noutperform baseline and comparable state-of-the-art methods in extensive\nnumerical experiments on a range of hyperspectral images, implying that it is\nwell-equipped for material discrimination and clustering of these data.\n","authors":["Sam L. Polk","Kangning Cui","Robert J. Plemmons","James M. Murphy"],"pdf_url":"https://arxiv.org/pdf/2203.09992v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2203.09986v1","updated":"2022-03-18T14:23:49Z","published":"2022-03-18T14:23:49Z","title":"GiNGR: Generalized Iterative Non-Rigid Point Cloud and Surface\n  Registration Using Gaussian Process Regression","summary":"  In this paper, we unify popular non-rigid registration methods for point sets\nand surfaces under our general framework, GiNGR. GiNGR builds upon Gaussian\nProcess Morphable Models (GPMM) and hence separates modeling the deformation\nprior from model adaptation for registration. In addition, it provides\nexplainable hyperparameters, multi-resolution registration, trivial inclusion\nof expert annotation, and the ability to use and combine analytical and\nstatistical deformation priors. But more importantly, the reformulation allows\nfor a direct comparison of registration methods. Instead of using a general\nsolver in the optimization step, we show how Gaussian process regression (GPR)\niteratively can warp a reference onto a target, leading to smooth deformations\nfollowing the prior for any dense, sparse, or partial estimated correspondences\nin a principled way. We show how the popular CPD and ICP algorithms can be\ndirectly explained with GiNGR. Furthermore, we show how existing algorithms in\nthe GiNGR framework can perform probabilistic registration to obtain a\ndistribution of different registrations instead of a single best registration.\nThis can be used to analyze the uncertainty e.g. when registering partial\nobservations. GiNGR is publicly available and fully modular to allow for\ndomain-specific prior construction.\n","authors":["Dennis Madsen","Jonathan Aellen","Andreas Morel-Forster","Thomas Vetter","Marcel Lüthi"],"pdf_url":"https://arxiv.org/pdf/2203.09986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09974v1","updated":"2022-03-18T14:08:20Z","published":"2022-03-18T14:08:20Z","title":"SynthStrip: Skull-Stripping for Any Brain Image","summary":"  The removal of non-brain signal from magnetic resonance imaging (MRI) data,\nknown as skull-stripping, is an integral component of many neuroimage analysis\nstreams. Despite their abundance, popular classical skull-stripping methods are\nusually tailored to images with specific acquisition properties, namely\nnear-isotropic resolution and T1-weighted (T1w) MRI contrast, which are\nprevalent in research settings. As a result, existing tools tend to adapt\npoorly to other image types, such as stacks of thick slices acquired with fast\nspin-echo (FSE) MRI that are common in the clinic. While learning-based\napproaches for brain extraction have gained traction in recent years, these\nmethods face a similar burden, as they are only effective for image types seen\nduring the training procedure. To achieve robust skull-stripping across a\nlandscape of protocols, we introduce SynthStrip, a rapid, learning-based\nbrain-extraction tool. By leveraging anatomical segmentations to generate an\nentirely synthetic training dataset with anatomies, intensity distributions,\nand artifacts that far exceed the realistic range of medical images, SynthStrip\nlearns to successfully generalize to a variety of real acquired brain images,\nremoving the need for training data with target contrasts. We demonstrate the\nefficacy of SynthStrip for a diverse set of image acquisitions and resolutions\nacross subject populations, ranging from newborn to adult. We show substantial\nimprovements in accuracy over popular skull-stripping baselines - all with a\nsingle trained model. Our method and labeled evaluation data are available at\nhttps://w3id.org/synthstrip.\n","authors":["Andrew Hoopes","Jocelyn S. Mora","Adrian V. Dalca","Bruce Fischl","Malte Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2203.09974v1.pdf","comment":"18 pages, 8 figures, 7 tables, skull stripping, brain extraction,\n  image synthesis, MRI contrast agnosticism, deep learning"},{"id":"http://arxiv.org/abs/2203.09957v1","updated":"2022-03-18T13:49:25Z","published":"2022-03-18T13:49:25Z","title":"Enhancement of Novel View Synthesis Using Omnidirectional Image\n  Completion","summary":"  We present a method for synthesizing novel views from a single 360-degree\nimage based on the neural radiance field (NeRF) . Prior studies rely on the\nneighborhood interpolation capability of multi-layer perceptrons to complete\nmissing regions caused by occlusion and zooming, and this leads to artifacts.\nIn the proposed method, the input image is reprojected to 360-degree images at\nother camera positions, the missing regions of the reprojected images are\ncompleted by a self-supervised trained generative model, and the completed\nimages are utilized to train the NeRF. Because multiple completed images\ncontain inconsistencies in 3D, we introduce a method to train NeRF while\ndynamically selecting a sparse set of completed images, to reduce the\ndiscrimination error of the synthesized views with real images. Experiments\nindicate that the proposed method can synthesize plausible novel views while\npreserving the features of the scene for both artificial and real-world data.\n","authors":["Takayuki Hara","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2203.09957v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2203.08344v2","updated":"2022-03-18T13:35:04Z","published":"2022-03-16T01:32:21Z","title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild","summary":"  We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n","authors":["Takehiko Ohkawa","Yu-Jhe Li","Qichen Fu","Ryosuke Furuta","Kris M. Kitani","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2203.08344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09928v1","updated":"2022-03-18T13:11:54Z","published":"2022-03-18T13:11:54Z","title":"Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on\n  Synthetic Images","summary":"  Most recent style-transfer techniques based on generative architectures are\nable to obtain synthetic multimedia contents, or commonly called deepfakes,\nwith almost no artifacts. Researchers already demonstrated that synthetic\nimages contain patterns that can determine not only if it is a deepfake but\nalso the generative architecture employed to create the image data itself.\nThese traces can be exploited to study problems that have never been addressed\nin the context of deepfakes. To this aim, in this paper a first approach to\ninvestigate the image ballistics on deepfake images subject to style-transfer\nmanipulations is proposed. Specifically, this paper describes a study on\ndetecting how many times a digital image has been processed by a generative\narchitecture for style transfer. Moreover, in order to address and study\naccurately forensic ballistics on deepfake images, some mathematical properties\nof style-transfer operations were investigated.\n","authors":["Luca Guarnera","Oliver Giudice","Sebastiano Battiato"],"pdf_url":"https://arxiv.org/pdf/2203.09928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00843v2","updated":"2022-03-18T12:45:21Z","published":"2022-03-02T03:35:37Z","title":"X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D\n  Dense Captioning","summary":"  3D dense captioning aims to describe individual objects by natural language\nin 3D scenes, where 3D scenes are usually represented as RGB-D scans or point\nclouds. However, only exploiting single modal information, e.g., point cloud,\nprevious approaches fail to produce faithful descriptions. Though aggregating\n2D features into point clouds may be beneficial, it introduces an extra\ncomputational burden, especially in inference phases. In this study, we\ninvestigate a cross-modal knowledge transfer using Transformer for 3D dense\ncaptioning, X-Trans2Cap, to effectively boost the performance of single-modal\n3D caption through knowledge distillation using a teacher-student framework. In\npractice, during the training phase, the teacher network exploits auxiliary 2D\nmodality and guides the student network that only takes point clouds as input\nthrough the feature consistency constraints. Owing to the well-designed\ncross-modal feature fusion module and the feature alignment in the training\nphase, X-Trans2Cap acquires rich appearance information embedded in 2D images\nwith ease. Thus, a more faithful caption can be generated only using point\nclouds during the inference. Qualitative and quantitative results confirm that\nX-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,\nabout +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,\nrespectively.\n","authors":["Zhihao Yuan","Xu Yan","Yinghong Liao","Yao Guo","Guanbin Li","Zhen Li","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2203.00843v2.pdf","comment":"To appear in CVPR2022"},{"id":"http://arxiv.org/abs/2203.09913v1","updated":"2022-03-18T12:44:44Z","published":"2022-03-18T12:44:44Z","title":"Convolutional Simultaneous Sparse Approximation with Applications to\n  RGB-NIR Image Fusion","summary":"  Simultaneous sparse approximation (SSA) seeks to represent a set of dependent\nsignals using sparse vectors with identical supports. The SSA model has been\nused in various signal and image processing applications involving multiple\ncorrelated input signals. In this paper, we propose algorithms for\nconvolutional SSA (CSSA) based on the alternating direction method of\nmultipliers. Specifically, we address the CSSA problem with different sparsity\nstructures and the convolutional feature learning problem in multimodal\ndata/signals based on the SSA model. We evaluate the proposed algorithms by\napplying them to multimodal and multifocus image fusion problems.\n","authors":["Farshad G. Veshki","Sergiy A. Vorobyov"],"pdf_url":"https://arxiv.org/pdf/2203.09913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09910v1","updated":"2022-03-18T12:39:31Z","published":"2022-03-18T12:39:31Z","title":"Fourier Document Restoration for Robust Document Dewarping and\n  Recognition","summary":"  State-of-the-art document dewarping techniques learn to predict 3-dimensional\ninformation of documents which are prone to errors while dealing with documents\nwith irregular distortions or large variations in depth. This paper presents\nFDRNet, a Fourier Document Restoration Network that can restore documents with\ndifferent distortions and improve document recognition in a reliable and\nsimpler manner. FDRNet focuses on high-frequency components in the Fourier\nspace that capture most structural information but are largely free of\ndegradation in appearance. It dewarps documents by a flexible Thin-Plate Spline\ntransformation which can handle various deformations effectively without\nrequiring deformation annotations in training. These features allow FDRNet to\nlearn from a small amount of simply labeled training images, and the learned\nmodel can dewarp documents with complex geometric distortion and recognize the\nrestored texts accurately. To facilitate document restoration research, we\ncreate a benchmark dataset consisting of over one thousand camera documents\nwith different types of geometric and photometric distortion. Extensive\nexperiments show that FDRNet outperforms the state-of-the-art by large margins\non both dewarping and text recognition tasks. In addition, FDRNet requires a\nsmall amount of simply labeled training data and is easy to deploy.\n","authors":["Chuhui Xue","Zichen Tian","Fangneng Zhan","Shijian Lu","Song Bai"],"pdf_url":"https://arxiv.org/pdf/2203.09910v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.07697v3","updated":"2022-03-18T12:37:25Z","published":"2022-03-15T07:30:27Z","title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose\n  Estimation","summary":"  In this paper, we present a novel Distribution-Aware Single-stage (DAS) model\nfor tackling the challenging multi-person 3D pose estimation problem. Different\nfrom existing top-down and bottom-up methods, the proposed DAS model\nsimultaneously localizes person positions and their corresponding body joints\nin the 3D camera space in a one-pass manner. This leads to a simplified\npipeline with enhanced efficiency. In addition, DAS learns the true\ndistribution of body joints for the regression of their positions, rather than\nmaking a simple Laplacian or Gaussian assumption as previous works. This\nprovides valuable priors for model prediction and thus boosts the\nregression-based scheme to achieve competitive performance with volumetric-base\nones. Moreover, DAS exploits a recursive update strategy for progressively\napproaching to regression target, alleviating the optimization difficulty and\nfurther lifting the regression performance. DAS is implemented with a fully\nConvolutional Neural Network and end-to-end learnable. Comprehensive\nexperiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior\nefficiency of the proposed DAS model, specifically 1.5x speedup over previous\nbest model, and its stat-of-the-art accuracy for multi-person 3D pose\nestimation.\n","authors":["Zitian Wang","Xuecheng Nie","Xiaochao Qu","Yunpeng Chen","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2203.07697v3.pdf","comment":"To appear in CVPR 2022. Code will be released"},{"id":"http://arxiv.org/abs/2203.09905v1","updated":"2022-03-18T12:29:06Z","published":"2022-03-18T12:29:06Z","title":"Learning Affordance Grounding from Exocentric Images","summary":"  Affordance grounding, a task to ground (i.e., localize) action possibility\nregion in objects, which faces the challenge of establishing an explicit link\nwith object parts due to the diversity of interactive affordance. Human has the\nability that transform the various exocentric interactions to invariant\negocentric affordance so as to counter the impact of interactive diversity. To\nempower an agent with such ability, this paper proposes a task of affordance\ngrounding from exocentric view, i.e., given exocentric human-object interaction\nand egocentric object images, learning the affordance knowledge of the object\nand transferring it to the egocentric image using only the affordance label as\nsupervision. To this end, we devise a cross-view knowledge transfer framework\nthat extracts affordance-specific features from exocentric interactions and\nenhances the perception of affordance regions by preserving affordance\ncorrelation. Specifically, an Affordance Invariance Mining module is devised to\nextract specific clues by minimizing the intra-class differences originated\nfrom interaction habits in exocentric images. Besides, an Affordance\nCo-relation Preserving strategy is presented to perceive and localize\naffordance by aligning the co-relation matrix of predicted results between the\ntwo views. Particularly, an affordance grounding dataset named AGD20K is\nconstructed by collecting and labeling over 20K images from 36 affordance\ncategories. Experimental results demonstrate that our method outperforms the\nrepresentative models in terms of objective metrics and visual quality. Code:\ngithub.com/lhc1224/Cross-View-AG.\n","authors":["Hongchen Luo","Wei Zhai","Jing Zhang","Yang Cao","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2203.09905v1.pdf","comment":"CVPR2022"},{"id":"http://arxiv.org/abs/2202.12634v2","updated":"2022-03-18T12:14:34Z","published":"2022-02-25T11:51:45Z","title":"Deep Dirichlet uncertainty for unsupervised out-of-distribution\n  detection of eye fundus photographs in glaucoma screening","summary":"  The development of automatic tools for early glaucoma diagnosis with color\nfundus photographs can significantly reduce the impact of this disease.\nHowever, current state-of-the-art solutions are not robust to real-world\nscenarios, providing over-confident predictions for out-of-distribution cases.\nWith this in mind, we propose a model based on the Dirichlet distribution that\nallows to obtain class-wise probabilities together with an uncertainty\nestimation without exposure to out-of-distribution cases. We demonstrate our\napproach on the AIROGS challenge. At the start of the final test phase (8 Feb.\n2022), our method had the highest average score among all submissions.\n","authors":["Teresa Araújo","Guilherme Aresta","Hrvoje Bogunovic"],"pdf_url":"https://arxiv.org/pdf/2202.12634v2.pdf","comment":"Submitted to ISBI 2022"},{"id":"http://arxiv.org/abs/2203.09887v1","updated":"2022-03-18T11:50:25Z","published":"2022-03-18T11:50:25Z","title":"CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric\n  Guidance","summary":"  Transformers have gained much attention by outperforming convolutional neural\nnetworks in many 2D vision tasks. However, they are known to have\ngeneralization problems and rely on massive-scale pre-training and\nsophisticated training techniques. When applying to 3D tasks, the irregular\ndata structure and limited data scale add to the difficulty of transformer's\napplication. We propose CodedVTR (Codebook-based Voxel TRansformer), which\nimproves data efficiency and generalization ability for 3D sparse voxel\ntransformers. On the one hand, we propose the codebook-based attention that\nprojects an attention space into its subspace represented by the combination of\n\"prototypes\" in a learnable codebook. It regularizes attention learning and\nimproves generalization. On the other hand, we propose geometry-aware\nself-attention that utilizes geometric information (geometric pattern, density)\nto guide attention learning. CodedVTR could be embedded into existing sparse\nconvolution-based methods, and bring consistent performance improvements for\nindoor and outdoor 3D semantic segmentation tasks\n","authors":["Tianchen Zhao","Niansong Zhang","Xuefei Ning","He Wang","Li Yi","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2203.09887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.02682v2","updated":"2022-03-18T11:47:26Z","published":"2021-11-04T08:32:59Z","title":"TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift\n  Estimation","summary":"  The recent developments of deep learning models that capture the complex\ntemporal patterns of crop phenology have greatly advanced crop classification\nof Satellite Image Time Series (SITS). However, when applied to target regions\nspatially different from the training region, these models perform poorly\nwithout any target labels due to the temporal shift of crop phenology between\nregions. To address this unsupervised cross-region adaptation setting, existing\nmethods learn domain-invariant features without any target supervision, but not\nthe temporal shift itself. As a consequence, these techniques provide only\nlimited benefits for SITS. In this paper, we propose TimeMatch, a new\nunsupervised domain adaptation method for SITS that directly accounts for the\ntemporal shift. TimeMatch consists of two components: 1) temporal shift\nestimation, which estimates the temporal shift of the unlabeled target region\nwith a source-trained model, and 2) TimeMatch learning, which combines temporal\nshift estimation with semi-supervised learning to adapt a classifier to an\nunlabeled target region. We also introduce an open-access dataset for\ncross-region adaptation with SITS from four different regions in Europe. On\nthis dataset, we demonstrate that TimeMatch outperforms all competing methods\nby 11% in F1-score across five different adaptation scenarios, setting a new\nstate-of-the-art for cross-region adaptation.\n","authors":["Joachim Nyborg","Charlotte Pelletier","Sébastien Lefèvre","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2111.02682v2.pdf","comment":"Preprint submitted to the ISPRS Journal of Photogrammetry and Remote\n  Sensing"},{"id":"http://arxiv.org/abs/2203.09446v2","updated":"2022-03-18T11:10:19Z","published":"2022-03-17T17:06:00Z","title":"Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D\n  MRI Scans with Geometric Deep Neural Networks","summary":"  The reconstruction of cortical surfaces from brain magnetic resonance imaging\n(MRI) scans is essential for quantitative analyses of cortical thickness and\nsulcal morphology. Although traditional and deep learning-based algorithmic\npipelines exist for this purpose, they have two major drawbacks: lengthy\nruntimes of multiple hours (traditional) or intricate post-processing, such as\nmesh extraction and topology correction (deep learning-based). In this work, we\naddress both of these issues and propose Vox2Cortex, a deep learning-based\nalgorithm that directly yields topologically correct, three-dimensional meshes\nof the boundaries of the cortex. Vox2Cortex leverages convolutional and graph\nconvolutional neural networks to deform an initial template to the densely\nfolded geometry of the cortex represented by an input MRI scan. We show in\nextensive experiments on three brain MRI datasets that our meshes are as\naccurate as the ones reconstructed by state-of-the-art methods in the field,\nwithout the need for time- and resource-intensive post-processing. To\naccurately reconstruct the tightly folded cortex, we work with meshes\ncontaining about 168,000 vertices at test time, scaling deep explicit\nreconstruction methods to a new level.\n","authors":["Fabian Bongratz","Anne-Marie Rickmann","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2203.09446v2.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09860v1","updated":"2022-03-18T11:02:18Z","published":"2022-03-18T11:02:18Z","title":"Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification","summary":"  Deep learning models were frequently reported to learn from shortcuts like\ndataset biases. As deep learning is playing an increasingly important role in\nthe modern healthcare system, it is of great need to combat shortcut learning\nin medical data as well as develop unbiased and trustworthy models. In this\npaper, we study the problem of developing debiased chest X-ray diagnosis models\nfrom the biased training data without knowing exactly the bias labels. We start\nwith the observations that the imbalance of bias distribution is one of the key\nreasons causing shortcut learning, and the dataset biases are preferred by the\nmodel if they were easier to be learned than the intended features. Based on\nthese observations, we propose a novel algorithm, pseudo bias-balanced\nlearning, which first captures and predicts per-sample bias labels via\ngeneralized cross entropy loss and then trains a debiased model using pseudo\nbias labels and bias-balanced softmax function. To our best knowledge, we are\npioneered in tackling dataset biases in medical images without explicit\nlabeling on the bias attributes. We constructed several chest X-ray datasets\nwith various dataset bias situations and demonstrated with extensive\nexperiments that our proposed method achieved consistent improvements over\nother state-of-the-art approaches.\n","authors":["Luyang Luo","Dunyuan Xu","Hao Chen","Tien-Tsin Wong","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2203.09860v1.pdf","comment":"Debias; Shortcut Learning; Chest X-ray"},{"id":"http://arxiv.org/abs/2203.09855v1","updated":"2022-03-18T10:48:22Z","published":"2022-03-18T10:48:22Z","title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion","summary":"  In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\nCodes and pre-trained models are available at\nhttps://github.com/anonymoustbd/MMMPT.\n","authors":["Zhiqiang Yan","Xiang Li","Kun Wang","Zhenyu Zhang","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2203.09855v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2203.09494v2","updated":"2022-03-18T10:34:43Z","published":"2022-03-17T17:48:32Z","title":"Transframer: Arbitrary Frame Prediction with Generative Models","summary":"  We present a general-purpose framework for image modelling and vision tasks\nbased on probabilistic frame prediction. Our approach unifies a broad range of\ntasks, from image segmentation, to novel view synthesis and video\ninterpolation. We pair this framework with an architecture we term Transframer,\nwhich uses U-Net and Transformer components to condition on annotated context\nframes, and outputs sequences of sparse, compressed image features. Transframer\nis the state-of-the-art on a variety of video generation benchmarks, is\ncompetitive with the strongest models on few-shot view synthesis, and can\ngenerate coherent 30 second videos from a single image without any explicit\ngeometric information. A single generalist Transframer simultaneously produces\npromising results on 8 tasks, including semantic segmentation, image\nclassification and optical flow prediction with no task-specific architectural\ncomponents, demonstrating that multi-task computer vision can be tackled using\nprobabilistic image models. Our approach can in principle be applied to a wide\nrange of applications that require learning the conditional structure of\nannotated image-formatted data.\n","authors":["Charlie Nash","João Carreira","Jacob Walker","Iain Barr","Andrew Jaegle","Mateusz Malinowski","Peter Battaglia"],"pdf_url":"https://arxiv.org/pdf/2203.09494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09845v1","updated":"2022-03-18T10:33:40Z","published":"2022-03-18T10:33:40Z","title":"Location-Free Camouflage Generation Network","summary":"  Camouflage is a common visual phenomenon, which refers to hiding the\nforeground objects into the background images, making them briefly invisible to\nthe human eye. Previous work has typically been implemented by an iterative\noptimization process. However, these methods struggle in 1) efficiently\ngenerating camouflage images using foreground and background with arbitrary\nstructure; 2) camouflaging foreground objects to regions with multiple\nappearances (e.g. the junction of the vegetation and the mountains), which\nlimit their practical application. To address these problems, this paper\nproposes a novel Location-free Camouflage Generation Network (LCG-Net) that\nfuse high-level features of foreground and background image, and generate\nresult by one inference. Specifically, a Position-aligned Structure Fusion\n(PSF) module is devised to guide structure feature fusion based on the\npoint-to-point structure similarity of foreground and background, and introduce\nlocal appearance features point-by-point. To retain the necessary identifiable\nfeatures, a new immerse loss is adopted under our pipeline, while a background\npatch appearance loss is utilized to ensure that the hidden objects look\ncontinuous and natural at regions with multiple appearances. Experiments show\nthat our method has results as satisfactory as state-of-the-art in the\nsingle-appearance regions and are less likely to be completely invisible, but\nfar exceed the quality of the state-of-the-art in the multi-appearance regions.\nMoreover, our method is hundreds of times faster than previous methods.\nBenefitting from the unique advantages of our method, we provide some\ndownstream applications for camouflage generation, which show its potential.\nThe related code and dataset will be released at\nhttps://github.com/Tale17/LCG-Net.\n","authors":["Yangyang Li","Wei Zhai","Yang Cao","Zheng-jun Zha"],"pdf_url":"https://arxiv.org/pdf/2203.09845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09836v1","updated":"2022-03-18T10:20:21Z","published":"2022-03-18T10:20:21Z","title":"Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation","summary":"  Most recent 6D object pose estimation methods, including unsupervised ones,\nrequire many real training images. Unfortunately, for some applications, such\nas those in space or deep under water, acquiring real images, even unannotated,\nis virtually impossible. In this paper, we propose a method that can be trained\nsolely on synthetic images, or optionally using a few additional real ones.\nGiven a rough pose estimate obtained from a first network, it uses a second\nnetwork to predict a dense 2D correspondence field between the image rendered\nusing the rough pose and the real image and infers the required pose\ncorrection. This approach is much less sensitive to the domain shift between\nsynthetic and real images than state-of-the-art methods. It performs on par\nwith methods that require annotated real images for training when not using\nany, and outperforms them considerably when using as few as twenty real images.\n","authors":["Yinlin Hu","Pascal Fua","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2203.09836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09831v1","updated":"2022-03-18T10:15:02Z","published":"2022-03-18T10:15:02Z","title":"DTA: Physical Camouflage Attacks using Differentiable Transformation\n  Network","summary":"  To perform adversarial attacks in the physical world, many studies have\nproposed adversarial camouflage, a method to hide a target object by applying\ncamouflage patterns on 3D object surfaces. For obtaining optimal physical\nadversarial camouflage, previous studies have utilized the so-called neural\nrenderer, as it supports differentiability. However, existing neural renderers\ncannot fully represent various real-world transformations due to a lack of\ncontrol of scene parameters compared to the legacy photo-realistic renderers.\nIn this paper, we propose the Differentiable Transformation Attack (DTA), a\nframework for generating a robust physical adversarial pattern on a target\nobject to camouflage it against object detection models with a wide range of\ntransformations. It utilizes our novel Differentiable Transformation Network\n(DTN), which learns the expected transformation of a rendered object when the\ntexture is changed while preserving the original properties of the target\nobject. Using our attack framework, an adversary can gain both the advantages\nof the legacy photo-realistic renderers including various physical-world\ntransformations and the benefit of white-box access by offering\ndifferentiability. Our experiments show that our camouflaged 3D vehicles can\nsuccessfully evade state-of-the-art object detection models in the\nphoto-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our\ndemonstration on a scaled Tesla Model 3 proves the applicability and\ntransferability of our method to the real world.\n","authors":["Naufal Suryanto","Yongsu Kim","Hyoeun Kang","Harashta Tatimma Larasati","Youngyeo Yun","Thi-Thu-Huong Le","Hunmin Yang","Se-Yoon Oh","Howon Kim"],"pdf_url":"https://arxiv.org/pdf/2203.09831v1.pdf","comment":"Accepted for CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09830v1","updated":"2022-03-18T10:14:35Z","published":"2022-03-18T10:14:35Z","title":"Laneformer: Object-aware Row-Column Transformers for Lane Detection","summary":"  We present Laneformer, a conceptually simple yet powerful transformer-based\narchitecture tailored for lane detection that is a long-standing research topic\nfor visual perception in autonomous driving. The dominant paradigms rely on\npurely CNN-based architectures which often fail in incorporating relations of\nlong-range lane points and global contexts induced by surrounding objects\n(e.g., pedestrians, vehicles). Inspired by recent advances of the transformer\nencoder-decoder architecture in various vision tasks, we move forwards to\ndesign a new end-to-end Laneformer architecture that revolutionizes the\nconventional transformers into better capturing the shape and semantic\ncharacteristics of lanes, with minimal overhead in latency. First, coupling\nwith deformable pixel-wise self-attention in the encoder, Laneformer presents\ntwo new row and column self-attention operations to efficiently mine point\ncontext along with the lane shapes. Second, motivated by the appearing objects\nwould affect the decision of predicting lane segments, Laneformer further\nincludes the detected object instances as extra inputs of multi-head attention\nblocks in the encoder and decoder to facilitate the lane point detection by\nsensing semantic contexts. Specifically, the bounding box locations of objects\nare added into Key module to provide interaction with each pixel and query\nwhile the ROI-aligned features are inserted into Value module. Extensive\nexperiments demonstrate our Laneformer achieves state-of-the-art performances\non CULane benchmark, in terms of 77.1% F1 score. We hope our simple and\neffective Laneformer will serve as a strong baseline for future research in\nself-attention models for lane detection.\n","authors":["Jianhua Han","Xiajun Deng","Xinyue Cai","Zhen Yang","Hang Xu","Chunjing Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2203.09830v1.pdf","comment":"AAAI2022"},{"id":"http://arxiv.org/abs/2203.09824v1","updated":"2022-03-18T10:03:07Z","published":"2022-03-18T10:03:07Z","title":"Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?","summary":"  This work digs into a root question in human perception: can face geometry be\ngleaned from one's voices? Previous works that study this question only adopt\ndevelopments in image synthesis and convert voices into face images to show\ncorrelations, but working on the image domain unavoidably involves predicting\nattributes that voices cannot hint, including facial textures, hairstyles, and\nbackgrounds. We instead investigate the ability to reconstruct 3D faces to\nconcentrate on only geometry, which is much more physiologically grounded. We\npropose our analysis framework, Cross-Modal Perceptionist, under both\nsupervised and unsupervised learning. First, we construct a dataset,\nVoxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,\nmaking supervised learning possible. Second, we use a knowledge distillation\nmechanism to study whether face geometry can still be gleaned from voices\nwithout paired voices and 3D face data under limited availability of 3D face\nscans. We break down the core question into four parts and perform visual and\nnumerical analyses as responses to the core question. Our findings echo those\nin physiology and neuroscience about the correlation between voices and facial\nstructures. The work provides future human-centric cross-modal learning with\nexplainable foundations. See our project page:\nhttps://choyingw.github.io/works/Voice2Mesh/index.html\n","authors":["Cho-Ying Wu","Chin-Cheng Hsu","Ulrich Neumann"],"pdf_url":"https://arxiv.org/pdf/2203.09824v1.pdf","comment":"Accepted to CVPR 2022. Project page:\n  https://choyingw.github.io/works/Voice2Mesh/index.html. This version\n  supersedes arXiv:2104.10299"},{"id":"http://arxiv.org/abs/2106.05517v4","updated":"2022-03-18T09:45:04Z","published":"2021-06-10T06:16:00Z","title":"Learning to Affiliate: Mutual Centralized Learning for Few-shot\n  Classification","summary":"  Few-shot learning (FSL) aims to learn a classifier that can be easily adapted\nto accommodate new tasks not seen during training, given only a few examples.\nTo handle the limited-data problem in few-shot regimes, recent methods tend to\ncollectively use a set of local features to densely represent an image instead\nof using a mixed global feature. They generally explore a unidirectional\nquery-to-support paradigm in FSL, e.g., find the nearest/optimal support\nfeature for each query feature and aggregate these local matches for a joint\nclassification. In this paper, we propose a new method Mutual Centralized\nLearning (MCL) to fully affiliate the two disjoint sets of dense features in a\nbidirectional paradigm. We associate each local feature with a particle that\ncan bidirectionally random walk in a discrete feature space by the\naffiliations. To estimate the class probability, we propose the features'\naccessibility that measures the expected number of visits to the support\nfeatures of that class in a Markov process. We relate our method to learning a\ncentrality on an affiliation network and demonstrate its capability to be\nplugged in existing methods by highlighting centralized local features.\nExperiments show that our method achieves the state-of-the-art on both\nminiImageNet and tieredImageNet.\n","authors":["Yang Liu","Weifeng Zhang","Chao Xiang","Tu Zheng","Deng Cai","Xiaofei He"],"pdf_url":"https://arxiv.org/pdf/2106.05517v4.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09812v1","updated":"2022-03-18T09:16:48Z","published":"2022-03-18T09:16:48Z","title":"Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared\n  Control on the Hannes Prosthesis","summary":"  We consider the task of object grasping with a prosthetic hand capable of\nmultiple grasp types. In this setting, communicating the intended grasp type\noften requires a high user cognitive load which can be reduced adopting shared\nautonomy frameworks. Among these, so-called eye-in-hand systems automatically\ncontrol the hand aperture and pre-shaping before the grasp, based on visual\ninput coming from a camera on the wrist. In this work, we present an\neye-in-hand learning-based approach for hand pre-shape classification from RGB\nsequences. In order to reduce the need for tedious data collection sessions for\ntraining the system, we devise a pipeline for rendering synthetic visual\nsequences of hand trajectories for the purpose. We tackle the peculiarity of\nthe eye-in-hand setting by means of a model for the human arm trajectories,\nwith domain randomization over relevant visual elements. We develop a\nsensorized setup to acquire real human grasping sequences for benchmarking and\nshow that, compared on practical use cases, models trained with our synthetic\ndataset achieve better generalization performance than models trained on real\ndata. We finally integrate our model on the Hannes prosthetic hand and show its\npractical effectiveness. Our code, real and synthetic datasets will be released\nupon acceptance.\n","authors":["Federico Vasile","Elisa Maiettini","Giulia Pasquale","Astrid Florio","Nicolò Boccardo","Lorenzo Natale"],"pdf_url":"https://arxiv.org/pdf/2203.09812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09811v1","updated":"2022-03-18T09:14:13Z","published":"2022-03-18T09:14:13Z","title":"Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased\n  Scene Graph Generation","summary":"  Scene Graph Generation, which generally follows a regular encoder-decoder\npipeline, aims to first encode the visual contents within the given image and\nthen parse them into a compact summary graph. Existing SGG approaches generally\nnot only neglect the insufficient modality fusion between vision and language,\nbut also fail to provide informative predicates due to the biased relationship\npredictions, leading SGG far from practical. Towards this end, in this paper,\nwe first present a novel Stacked Hybrid-Attention network, which facilitates\nthe intra-modal refinement as well as the inter-modal interaction, to serve as\nthe encoder. We then devise an innovative Group Collaborative Learning strategy\nto optimize the decoder. Particularly, based upon the observation that the\nrecognition capability of one classifier is limited towards an extremely\nunbalanced dataset, we first deploy a group of classifiers that are expert in\ndistinguishing different subsets of classes, and then cooperatively optimize\nthem from two aspects to promote the unbiased SGG. Experiments conducted on VG\nand GQA datasets demonstrate that, we not only establish a new state-of-the-art\nin the unbiased metric, but also nearly double the performance compared with\ntwo baselines.\n","authors":["Xingning Dong","Tian Gan","Xuemeng Song","Jianlong Wu","Yuan Cheng","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2203.09811v1.pdf","comment":"Accepted by CVPR 2022, the code is available at\n  https://github.com/dongxingning/SHA-GCL-for-SGG"},{"id":"http://arxiv.org/abs/2203.09803v1","updated":"2022-03-18T09:05:51Z","published":"2022-03-18T09:05:51Z","title":"Learning Consistency from High-quality Pseudo-labels for Weakly\n  Supervised Object Localization","summary":"  Pseudo-supervised learning methods have been shown to be effective for weakly\nsupervised object localization tasks. However, the effectiveness depends on the\npowerful regularization ability of deep neural networks. Based on the\nassumption that the localization network should have similar location\npredictions on different versions of the same image, we propose a two-stage\napproach to learn more consistent localization. In the first stage, we propose\na mask-based pseudo label generator algorithm, and use the pseudo-supervised\nlearning method to initialize an object localization network. In the second\nstage, we propose a simple and effective method for evaluating the confidence\nof pseudo-labels based on classification discrimination, and by learning\nconsistency from high-quality pseudo-labels, we further refine the localization\nnetwork to get better localization performance. Experimental results show that\nour proposed approach achieves excellent performance in three benchmark\ndatasets including CUB-200-2011, ImageNet-1k and Tiny-ImageNet, which\ndemonstrates its effectiveness.\n","authors":["Kangbo Sun","Jie Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.09803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.10224v4","updated":"2022-03-18T08:45:29Z","published":"2021-07-21T17:23:06Z","title":"CycleMLP: A MLP-like Architecture for Dense Prediction","summary":"  This paper presents a simple MLP-like architecture, CycleMLP, which is a\nversatile backbone for visual recognition and dense predictions. As compared to\nmodern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose\narchitectures are correlated to image size and thus are infeasible in object\ndetection and segmentation, CycleMLP has two advantages compared to modern\napproaches. (1) It can cope with various image sizes. (2) It achieves linear\ncomputational complexity to image size by using local windows. In contrast,\nprevious MLPs have $O(N^2)$ computations due to fully spatial connections. We\nbuild a family of models which surpass existing MLPs and even state-of-the-art\nTransformer-based models, e.g., Swin Transformer, while using fewer parameters\nand FLOPs. We expand the MLP-like models' applicability, making them a\nversatile backbone for dense prediction tasks. CycleMLP achieves competitive\nresults on object detection, instance segmentation, and semantic segmentation.\nIn particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K\ndataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot\nrobustness on ImageNet-C dataset. Code is available at\nhttps://github.com/ShoufaChen/CycleMLP.\n","authors":["Shoufa Chen","Enze Xie","Chongjian Ge","Runjian Chen","Ding Liang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2107.10224v4.pdf","comment":"ICLR 2022 (Oral). Camera-ready Code:\n  https://github.com/ShoufaChen/CycleMLP"},{"id":"http://arxiv.org/abs/2203.09795v1","updated":"2022-03-18T08:23:03Z","published":"2022-03-18T08:23:03Z","title":"Three things everyone should know about Vision Transformers","summary":"  After their initial success in natural language processing, transformer\narchitectures have rapidly gained traction in computer vision, providing\nstate-of-the-art results for tasks such as image classification, detection,\nsegmentation, and video analysis. We offer three insights based on simple and\neasy to implement variants of vision transformers. (1) The residual layers of\nvision transformers, which are usually processed sequentially, can to some\nextent be processed efficiently in parallel without noticeably affecting the\naccuracy. (2) Fine-tuning the weights of the attention layers is sufficient to\nadapt vision transformers to a higher resolution and to other classification\ntasks. This saves compute, reduces the peak memory consumption at fine-tuning\ntime, and allows sharing the majority of weights across tasks. (3) Adding\nMLP-based patch pre-processing layers improves Bert-like self-supervised\ntraining based on patch masking. We evaluate the impact of these design choices\nusing the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test\nset. Transfer performance is measured across six smaller datasets.\n","authors":["Hugo Touvron","Matthieu Cord","Alaaeldin El-Nouby","Jakob Verbeek","Hervé Jégou"],"pdf_url":"https://arxiv.org/pdf/2203.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08049v3","updated":"2022-03-18T08:20:55Z","published":"2022-03-15T16:43:40Z","title":"On Hyperbolic Embeddings in 2D Object Detection","summary":"  Object detection, for the most part, has been formulated in the euclidean\nspace, where euclidean or spherical geodesic distances measure the similarity\nof an image region to an object class prototype. In this work, we study whether\na hyperbolic geometry better matches the underlying structure of the object\nclassification space. We incorporate a hyperbolic classifier in two-stage,\nkeypoint-based, and transformer-based object detection architectures and\nevaluate them on large-scale, long-tailed, and zero-shot object detection\nbenchmarks. In our extensive experimental evaluations, we observe categorical\nclass hierarchies emerging in the structure of the classification space,\nresulting in lower classification errors and boosting the overall object\ndetection performance.\n","authors":["Christopher Lang","Alexander Braun","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2203.08049v3.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2107.06307v4","updated":"2022-03-18T08:15:56Z","published":"2021-07-13T18:06:46Z","title":"HDMapNet: An Online HD Map Construction and Evaluation Framework","summary":"  Constructing HD semantic maps is a central component of autonomous driving.\nHowever, traditional pipelines require a vast amount of human efforts and\nresources in annotating and maintaining the semantics in the map, which limits\nits scalability. In this paper, we introduce the problem of HD semantic map\nlearning, which dynamically constructs the local semantics based on onboard\nsensor observations. Meanwhile, we introduce a semantic map learning method,\ndubbed HDMapNet. HDMapNet encodes image features from surrounding cameras\nand/or point clouds from LiDAR, and predicts vectorized map elements in the\nbird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all\nsettings, it performs better than baseline methods. Of note, our camera-LiDAR\nfusion-based HDMapNet outperforms existing methods by more than 50% in all\nmetrics. In addition, we develop semantic-level and instance-level metrics to\nevaluate the map learning performance. Finally, we showcase our method is\ncapable of predicting a locally consistent map. By introducing the method and\nmetrics, we invite the community to study this novel map learning problem.\n","authors":["Qi Li","Yue Wang","Yilun Wang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2107.06307v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09790v1","updated":"2022-03-18T08:13:56Z","published":"2022-03-18T08:13:56Z","title":"Towards Robust 2D Convolution for Reliable Visual Recognition","summary":"  2D convolution (Conv2d), which is responsible for extracting features from\nthe input image, is one of the key modules of a convolutional neural network\n(CNN). However, Conv2d is vulnerable to image corruptions and adversarial\nsamples. It is an important yet rarely investigated problem that whether we can\ndesign a more robust alternative of Conv2d for more reliable feature\nextraction. In this paper, inspired by the recently developed learnable sparse\ntransform that learns to convert the CNN features into a compact and sparse\nlatent space, we design a novel building block, denoted by RConv-MK, to\nstrengthen the robustness of extracted convolutional features. Our method\nleverages a set of learnable kernels of different sizes to extract features at\ndifferent frequencies and employs a normalized soft thresholding operator to\nadaptively remove noises and trivial features at different corruption levels.\nExtensive experiments on clean images, corrupted images as well as adversarial\nsamples validate the effectiveness of the proposed robust module for reliable\nvisual recognition. The source codes are enclosed in the submission.\n","authors":["Lida Li","Shuai Li","Kun Wang","Xiangchu Feng","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09780v1","updated":"2022-03-18T07:56:35Z","published":"2022-03-18T07:56:35Z","title":"Sparse Fuse Dense: Towards High Quality 3D Detection with Depth\n  Completion","summary":"  Current LiDAR-only 3D detection methods inevitably suffer from the sparsity\nof point clouds. Many multi-modal methods are proposed to alleviate this issue,\nwhile different representations of images and point clouds make it difficult to\nfuse them, resulting in suboptimal performance. In this paper, we present a\nnovel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo\npoint clouds generated from depth completion to tackle the issues mentioned\nabove. Different from prior works, we propose a new RoI fusion strategy 3D-GAF\n(3D Grid-wise Attentive Fusion) to make fuller use of information from\ndifferent types of point clouds. Specifically, 3D-GAF fuses 3D RoI features\nfrom the couple of point clouds in a grid-wise attentive way, which is more\nfine-grained and more precise. In addition, we propose a SynAugment\n(Synchronized Augmentation) to enable our multi-modal framework to utilize all\ndata augmentation approaches tailored to LiDAR-only methods. Lastly, we\ncustomize an effective and efficient feature extractor CPConv (Color Point\nConvolution) for pseudo point clouds. It can explore 2D image features and 3D\ngeometric features of pseudo point clouds simultaneously. Our method holds the\nhighest entry on the KITTI car 3D object detection leaderboard, demonstrating\nthe effectiveness of our SFD. Code will be made publicly available.\n","authors":["Xiaopei Wu","Liang Peng","Honghui Yang","Liang Xie","Chenxi Huang","Chengqi Deng","Haifeng Liu","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2203.09780v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2103.10702v3","updated":"2022-03-18T07:47:51Z","published":"2021-03-19T09:31:08Z","title":"ClawCraneNet: Leveraging Object-level Relation for Text-based Video\n  Segmentation","summary":"  Text-based video segmentation is a challenging task that segments out the\nnatural language referred objects in videos. It essentially requires semantic\ncomprehension and fine-grained video understanding. Existing methods introduce\nlanguage representation into segmentation models in a bottom-up manner, which\nmerely conducts vision-language interaction within local receptive fields of\nConvNets. We argue that such interaction is not fulfilled since the model can\nbarely construct region-level relationships given partial observations, which\nis contrary to the description logic of natural language/referring expressions.\nIn fact, people usually describe a target object using relations with other\nobjects, which may not be easily understood without seeing the whole video. To\naddress the issue, we introduce a novel top-down approach by imitating how we\nhuman segment an object with the language guidance. We first figure out all\ncandidate objects in videos and then choose the refereed one by parsing\nrelations among those high-level objects. Three kinds of object-level relations\nare investigated for precise relationship understanding, i.e., positional\nrelation, text-guided semantic relation, and temporal relation. Extensive\nexperiments on A2D Sentences and J-HMDB Sentences show our method outperforms\nstate-of-the-art methods by a large margin. Qualitative results also show our\nresults are more explainable.\n","authors":["Chen Liang","Yu Wu","Yawei Luo","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2103.10702v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09777v1","updated":"2022-03-18T07:43:03Z","published":"2022-03-18T07:43:03Z","title":"Transferable Class-Modelling for Decentralized Source Attribution of\n  GAN-Generated Images","summary":"  GAN-generated deepfakes as a genre of digital images are gaining ground as\nboth catalysts of artistic expression and malicious forms of deception,\ntherefore demanding systems to enforce and accredit their ethical use. Existing\ntechniques for the source attribution of synthetic images identify subtle\nintrinsic fingerprints using multiclass classification neural nets limited in\nfunctionality and scalability. Hence, we redefine the deepfake detection and\nsource attribution problems as a series of related binary classification tasks.\nWe leverage transfer learning to rapidly adapt forgery detection networks for\nmultiple independent attribution problems, by proposing a semi-decentralized\nmodular design to solve them simultaneously and efficiently. Class activation\nmapping is also demonstrated as an effective means of feature localization for\nmodel interpretation. Our models are determined via experimentation to be\ncompetitive with current benchmarks, and capable of decent performance on human\nportraits in ideal conditions. Decentralized fingerprint-based attribution is\nfound to retain validity in the presence of novel sources, but is more\nsusceptible to type II errors that intensify with image perturbations and\nattributive uncertainty. We describe both our conceptual framework and model\nprototypes for further enhancement when investigating the technical limits of\nreactive deepfake attribution.\n","authors":["Brandon B. G. Khoo","Chern Hong Lim","Raphael C. -W. Phan"],"pdf_url":"https://arxiv.org/pdf/2203.09777v1.pdf","comment":"21 pages, 8 figures. Code:\n  https://github.com/quarxilon/Generator_Attribution"},{"id":"http://arxiv.org/abs/2203.09775v1","updated":"2022-03-18T07:41:48Z","published":"2022-03-18T07:41:48Z","title":"ContrastMask: Contrastive Learning to Segment Every Thing","summary":"  Partially-supervised instance segmentation is a task which requests\nsegmenting objects from novel unseen categories via learning on limited seen\ncategories with annotated masks thus eliminating demands of heavy annotation\nburden. The key to addressing this task is to build an effective class-agnostic\nmask segmentation model. Unlike previous methods that learn such models only on\nseen categories, in this paper, we propose a new method, named ContrastMask,\nwhich learns a mask segmentation model on both seen and unseen categories under\na unified pixel-level contrastive learning framework. In this framework,\nannotated masks of seen categories and pseudo masks of unseen categories serve\nas a prior for contrastive learning, where features from the mask regions\n(foreground) are pulled together, and are contrasted against those from the\nbackground, and vice versa. Through this framework, feature discrimination\nbetween foreground and background is largely improved, facilitating learning of\nthe class-agnostic mask segmentation model. Exhaustive experiments on the COCO\ndataset demonstrate the superiority of our method, which outperforms previous\nstate-of-the-arts.\n","authors":["Xuehui Wang","Kai Zhao","Ruixin Zhang","Shouhong Ding","Yan Wang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2203.09775v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09773v1","updated":"2022-03-18T07:35:26Z","published":"2022-03-18T07:35:26Z","title":"Local-Global Context Aware Transformer for Language-Guided Video\n  Segmentation","summary":"  We explore the task of language-guided video segmentation (LVS). Previous\nalgorithms mostly adopt 3D CNNs to learn video representation, struggling to\ncapture long-term context and easily suffering from visual-linguistic\nmisalignment. In light of this, we present Locater (local-global context aware\nTransformer), which augments the Transformer architecture with a finite memory\nso as to query the entire video with the language expression in an efficient\nmanner. The memory is designed to involve two components -- one for\npersistently preserving global video content, and one for dynamically gathering\nlocal temporal context and segmentation history. Based on the memorized\nlocal-global context and the particular content of each frame, Locater\nholistically and flexibly comprehends the expression as an adaptive query\nvector for each frame. The vector is used to query the corresponding frame for\nmask generation. The memory also allows Locater to process videos with linear\ntime complexity and constant size memory, while Transformer-style\nself-attention computation scales quadratically with sequence length. To\nthoroughly examine the visual grounding capability of LVS models, we contribute\na new LVS dataset, A2D-S+, which is built upon A2D-S dataset but poses\nincreased challenges in disambiguating among similar objects. Experiments on\nthree LVS datasets and our A2D-S+ show that Locater outperforms previous\nstate-of-the-arts. Further, our Locater based solution achieved the 1st place\nin the Referring Video Object Segmentation Track of the 3rd Large-scale Video\nObject Segmentation Challenge. Our code and dataset are available at:\nhttps://github.com/leonnnop/Locater\n","authors":["Chen Liang","Wenguan Wang","Tianfei Zhou","Jiaxu Miao","Yawei Luo","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2203.09773v1.pdf","comment":"Code, data: https://github.com/leonnnop/Locater"},{"id":"http://arxiv.org/abs/2203.09772v1","updated":"2022-03-18T07:31:41Z","published":"2022-03-18T07:31:41Z","title":"Completing Partial Point Clouds with Outliers by Collaborative\n  Completion and Segmentation","summary":"  Most existing point cloud completion methods are only applicable to partial\npoint clouds without any noises and outliers, which does not always hold in\npractice. We propose in this paper an end-to-end network, named CS-Net, to\ncomplete the point clouds contaminated by noises or containing outliers. In our\nCS-Net, the completion and segmentation modules work collaboratively to promote\neach other, benefited from our specifically designed cascaded structure. With\nthe help of segmentation, more clean point cloud is fed into the completion\nmodule. We design a novel completion decoder which harnesses the labels\nobtained by segmentation together with FPS to purify the point cloud and\nleverages KNN-grouping for better generation. The completion and segmentation\nmodules work alternately share the useful information from each other to\ngradually improve the quality of prediction. To train our network, we build a\ndataset to simulate the real case where incomplete point clouds contain\noutliers. Our comprehensive experiments and comparisons against\nstate-of-the-art completion methods demonstrate our superiority. We also\ncompare with the scheme of segmentation followed by completion and their\nend-to-end fusion, which also proves our efficacy.\n","authors":["Changfeng Ma","Yang Yang","Jie Guo","Chongjun Wang","Yanwen Guo"],"pdf_url":"https://arxiv.org/pdf/2203.09772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09771v1","updated":"2022-03-18T07:28:14Z","published":"2022-03-18T07:28:14Z","title":"Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach\n  to Continuous Image Transition","summary":"  Video frame interpolation (VFI) aims to improve the temporal resolution of a\nvideo sequence. Most of the existing deep learning based VFI methods adopt\noff-the-shelf optical flow algorithms to estimate the bidirectional flows and\ninterpolate the missing frames accordingly. Though having achieved a great\nsuccess, these methods require much human experience to tune the bidirectional\nflows and often generate unpleasant results when the estimated flows are not\naccurate. In this work, we rethink the VFI problem and formulate it as a\ncontinuous image transition (CIT) task, whose key issue is to transition an\nimage from one space to another space continuously. More specifically, we learn\nto implicitly decouple the images into a translatable flow space and a\nnon-translatable feature space. The former depicts the translatable states\nbetween the given images, while the later aims to reconstruct the intermediate\nfeatures that cannot be directly translated. In this way, we can easily perform\nimage interpolation in the flow space and intermediate image synthesis in the\nfeature space, obtaining a CIT model. The proposed space decoupled learning\n(SDL) approach is simple to implement, while it provides an effective framework\nto a variety of CIT problems beyond VFI, such as style transfer and image\nmorphing. Our extensive experiments on a variety of CIT tasks demonstrate the\nsuperiority of SDL to existing methods. The source code and models can be found\nat \\url{https://github.com/yangxy/SDL}.\n","authors":["Tao Yang","Peiran Ren","Xuansong Xie","Xiansheng Hua","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.01085v4","updated":"2022-03-18T07:21:44Z","published":"2021-06-02T11:39:25Z","title":"Online Coreset Selection for Rehearsal-based Continual Learning","summary":"  A dataset is a shred of crucial evidence to describe a task. However, each\ndata point in the dataset does not have the same potential, as some of the data\npoints can be more representative or informative than others. This unequal\nimportance among the data points may have a large impact in rehearsal-based\ncontinual learning, where we store a subset of the training examples (coreset)\nto be replayed later to alleviate catastrophic forgetting. In continual\nlearning, the quality of the samples stored in the coreset directly affects the\nmodel's effectiveness and efficiency. The coreset selection problem becomes\neven more important under realistic settings, such as imbalanced continual\nlearning or noisy data scenarios. To tackle this problem, we propose Online\nCoreset Selection (OCS), a simple yet effective method that selects the most\nrepresentative and informative coreset at each iteration and trains them in an\nonline manner. Our proposed method maximizes the model's adaptation to a\ncurrent dataset while selecting high-affinity samples to past tasks, which\ndirectly inhibits catastrophic forgetting. We validate the effectiveness of our\ncoreset selection mechanism over various standard, imbalanced, and noisy\ndatasets against strong continual learning baselines, demonstrating that it\nimproves task adaptation and prevents catastrophic forgetting in a\nsample-efficient manner.\n","authors":["Jaehong Yoon","Divyam Madaan","Eunho Yang","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2106.01085v4.pdf","comment":"ICLR 2022"},{"id":"http://arxiv.org/abs/2109.04275v4","updated":"2022-03-18T06:56:04Z","published":"2021-09-09T13:50:22Z","title":"M5Product: Self-harmonized Contrastive Learning for E-commercial\n  Multi-modal Pretraining","summary":"  Despite the potential of multi-modal pre-training to learn highly\ndiscriminative feature representations from complementary data modalities,\ncurrent progress is being slowed by the lack of large-scale modality-diverse\ndatasets. By leveraging the natural suitability of E-commerce, where different\nmodalities capture complementary semantic information, we contribute a\nlarge-scale multi-modal pre-training dataset M5Product. The dataset comprises 5\nmodalities (image, text, table, video, and audio), covers over 6,000 categories\nand 5,000 attributes, and is 500 larger than the largest publicly available\ndataset with a similar number of modalities. Furthermore, M5Product contains\nincomplete modality pairs and noise while also having a long-tailed\ndistribution, resembling most real-world problems. We further propose\nSelf-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework\nthat integrates the different modalities into a unified model through an\nadaptive feature fusion mechanism, where the importance of each modality is\nlearned directly from the modality embeddings and impacts the inter-modality\ncontrastive learning and masked tasks within a multi-modal transformer model.\nWe evaluate the current multi-modal pre-training state-of-the-art approaches\nand benchmark their ability to learn from unlabeled data when faced with the\nlarge number of modalities in the M5Product dataset. We conduct extensive\nexperiments on four downstream tasks and demonstrate the superiority of our\nSCALE model, providing insights into the importance of dataset scale and\ndiversity.\n","authors":["Xiao Dong","Xunlin Zhan","Yangxin Wu","Yunchao Wei","Michael C. Kampffmeyer","Xiaoyong Wei","Minlong Lu","Yaowei Wang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2109.04275v4.pdf","comment":"CVPR2022"},{"id":"http://arxiv.org/abs/2203.09364v2","updated":"2022-03-18T06:55:19Z","published":"2022-03-17T14:51:11Z","title":"Interacting Attention Graph for Single Image Two-Hand Reconstruction","summary":"  Graph convolutional network (GCN) has achieved great success in single hand\nreconstruction task, while interacting two-hand reconstruction by GCN remains\nunexplored. In this paper, we present Interacting Attention Graph Hand\n(IntagHand), the first graph convolution based network that reconstructs two\ninteracting hands from a single RGB image. To solve occlusion and interaction\nchallenges of two-hand reconstruction, we introduce two novel attention based\nmodules in each upsampling step of the original GCN. The first module is the\npyramid image feature attention (PIFA) module, which utilizes multiresolution\nfeatures to implicitly obtain vertex-to-image alignment. The second module is\nthe cross hand attention (CHA) module that encodes the coherence of interacting\nhands by building dense cross-attention between two hand vertices. As a result,\nour model outperforms all existing two-hand reconstruction methods by a large\nmargin on InterHand2.6M benchmark. Moreover, ablation studies verify the\neffectiveness of both PIFA and CHA modules for improving the reconstruction\naccuracy. Results on in-the-wild images and live video streams further\ndemonstrate the generalization ability of our network. Our code is available at\nhttps://github.com/Dw1010/IntagHand.\n","authors":["Mengcheng Li","Liang An","Hongwen Zhang","Lianpeng Wu","Feng Chen","Tao Yu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2203.09364v2.pdf","comment":"To appear in CVPR 2022. Project page:\n  http://www.liuyebin.com/IntagHand/Intaghand.html"},{"id":"http://arxiv.org/abs/2101.06910v3","updated":"2022-03-18T06:55:01Z","published":"2021-01-18T07:30:51Z","title":"A Simple Mutual Information based Registration Method for\n  Thermal-Optical Image Pairs applied on a Novel Dataset","summary":"  While thermal optical registered datasets are becoming widely available, most\nof these works are based on image pairs which are pre-registered. However,\nthermal imagers where these images are registered by default are quite\nexpensive. We present in this work, a thermal image registration technique\nwhich is computationally lightweight, and can be employed regardless of the\nresolution of the images captured. We use 2 different thermal imagers to create\na completely new database and introduce it as a part of this work as well. The\nimages captured are based on 5 different classes and encompass subjects like\nthe Prayagraj Kumbh Mela, one of the largest public fairs in the world,\ncaptured over a period of 2 years.\n","authors":["Suranjan Goswami","Satish Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2101.06910v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03373v3","updated":"2022-03-18T06:47:05Z","published":"2022-03-07T13:22:25Z","title":"Adversarial Texture for Fooling Person Detectors in the Physical World","summary":"  Nowadays, cameras equipped with AI systems can capture and analyze images to\ndetect people automatically. However, the AI system can make mistakes when\nreceiving deliberately designed patterns in the real world, i.e., physical\nadversarial examples. Prior works have shown that it is possible to print\nadversarial patches on clothes to evade DNN-based person detectors. However,\nthese adversarial examples could have catastrophic drops in the attack success\nrate when the viewing angle (i.e., the camera's angle towards the object)\nchanges. To perform a multi-angle attack, we propose Adversarial Texture\n(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people\nwearing such clothes can hide from person detectors from different viewing\nangles. We propose a generative method, named Toroidal-Cropping-based\nExpandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive\nstructures. We printed several pieces of cloth with AdvTexure and then made\nT-shirts, skirts, and dresses in the physical world. Experiments showed that\nthese clothes could fool person detectors in the physical world.\n","authors":["Zhanhao Hu","Siyuan Huang","Xiaopei Zhu","Xiaolin Hu","Fuchun Sun","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.03373v3.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2102.03924v2","updated":"2022-03-18T06:19:28Z","published":"2021-02-07T21:33:41Z","title":"Domain Adversarial Neural Networks for Domain Generalization: When It\n  Works and How to Improve","summary":"  Theoretically, domain adaptation is a well-researched problem. Further, this\ntheory has been well-used in practice. In particular, we note the bound on\ntarget error given by Ben-David et al. (2010) and the well-known\ndomain-aligning algorithm based on this work using Domain Adversarial Neural\nNetworks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple\nvariants of DANN have been proposed for the related problem of domain\ngeneralization, but without much discussion of the original motivating bound.\nIn this paper, we investigate the validity of DANN in domain generalization\nfrom this perspective. We investigate conditions under which application of\nDANN makes sense and further consider DANN as a dynamic process during\ntraining. Our investigation suggests that the application of DANN to domain\ngeneralization may not be as straightforward as it seems. To address this, we\ndesign an algorithmic extension to DANN in the domain generalization case. Our\nexperimentation validates both theory and algorithm.\n","authors":["Anthony Sicilia","Xingchen Zhao","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2102.03924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01583v2","updated":"2022-03-18T06:10:15Z","published":"2022-03-03T09:23:51Z","title":"Towards Universal Backward-Compatible Representation Learning","summary":"  Conventional model upgrades for visual search systems require offline refresh\nof gallery features by feeding gallery images into new models (dubbed as\n\"backfill\"), which is time-consuming and expensive, especially in large-scale\napplications. The task of backward-compatible representation learning is\ntherefore introduced to support backfill-free model upgrades, where the new\nquery features are interoperable with the old gallery features. Despite the\nsuccess, previous works only investigated a close-set training scenario (i.e.,\nthe new training set shares the same classes as the old one), and are limited\nby more realistic and challenging open-set scenarios. To this end, we first\nintroduce a new problem of universal backward-compatible representation\nlearning, covering all possible data split in model upgrades. We further\npropose a simple yet effective method, dubbed as Universal Backward-Compatible\nTraining (UniBCT) with a novel structural prototype refinement algorithm, to\nlearn compatible representations in all kinds of model upgrading benchmarks in\na unified manner. Comprehensive experiments on the large-scale face recognition\ndatasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.\n","authors":["Binjie Zhang","Yixiao Ge","Yantao Shen","Shupeng Su","Fanzi Wu","Chun Yuan","Xuyuan Xu","Yexin Wang","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2203.01583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09756v1","updated":"2022-03-18T06:06:06Z","published":"2022-03-18T06:06:06Z","title":"AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack","summary":"  Deep neural networks (DNNs) have been proven to be vulnerable to adversarial\nexamples. A special branch of adversarial examples, namely sparse adversarial\nexamples, can fool the target DNNs by perturbing only a few pixels. However,\nmany existing sparse adversarial attacks use heuristic methods to select the\npixels to be perturbed, and regard the pixel selection and the adversarial\nattack as two separate steps. From the perspective of neural network pruning,\nwe propose a novel end-to-end sparse adversarial attack method, namely\nAutoAdversary, which can find the most important pixels automatically by\nintegrating the pixel selection into the adversarial attack. Specifically, our\nmethod utilizes a trainable neural network to generate a binary mask for the\npixel selection. After jointly optimizing the adversarial perturbation and the\nneural network, only the pixels corresponding to the value 1 in the mask are\nperturbed. Experiments demonstrate the superiority of our proposed method over\nseveral state-of-the-art methods. Furthermore, since AutoAdversary does not\nrequire a heuristic pixel selection process, it does not slow down excessively\nas other methods when the image size increases.\n","authors":["Jinqiao Li","Xiaotao Liu","Jian Zhao","Furao Shen"],"pdf_url":"https://arxiv.org/pdf/2203.09756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.03814v4","updated":"2022-03-18T05:31:21Z","published":"2021-09-08T17:59:12Z","title":"Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with\n  Transformers","summary":"  Panoptic segmentation involves a combination of joint semantic segmentation\nand instance segmentation, where image contents are divided into two types:\nthings and stuff. We present Panoptic SegFormer, a general framework for\npanoptic segmentation with transformers. It contains three innovative\ncomponents: an efficient deeply-supervised mask decoder, a query decoupling\nstrategy, and an improved post-processing method. We also use Deformable DETR\nto efficiently process multi-scale features, which is a fast and efficient\nversion of DETR. Specifically, we supervise the attention modules in the mask\ndecoder in a layer-wise manner. This deep supervision strategy lets the\nattention modules quickly focus on meaningful semantic regions. It improves\nperformance and reduces the number of required training epochs by half compared\nto Deformable DETR. Our query decoupling strategy decouples the\nresponsibilities of the query set and avoids mutual interference between things\nand stuff. In addition, our post-processing strategy improves performance\nwithout additional costs by jointly considering classification and segmentation\nqualities to resolve conflicting mask overlaps. Our approach increases the\naccuracy 6.2\\% PQ over the baseline DETR model. Panoptic SegFormer achieves\nstate-of-the-art results on COCO test-dev with 56.2\\% PQ. It also shows\nstronger zero-shot robustness over existing methods. The code is released at\n\\url{https://github.com/zhiqi-li/Panoptic-SegFormer}.\n","authors":["Zhiqi Li","Wenhai Wang","Enze Xie","Zhiding Yu","Anima Anandkumar","Jose M. Alvarez","Ping Luo","Tong Lu"],"pdf_url":"https://arxiv.org/pdf/2109.03814v4.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09749v1","updated":"2022-03-18T05:17:00Z","published":"2022-03-18T05:17:00Z","title":"Robot peels banana with goal-conditioned dual-action deep imitation\n  learning","summary":"  A long-horizon dexterous robot manipulation task of deformable objects, such\nas banana peeling, is problematic because of difficulties in object modeling\nand a lack of knowledge about stable and dexterous manipulation skills. This\npaper presents a goal-conditioned dual-action deep imitation learning (DIL)\nwhich can learn dexterous manipulation skills using human demonstration data.\nPrevious DIL methods map the current sensory input and reactive action, which\neasily fails because of compounding errors in imitation learning caused by\nrecurrent computation of actions. The proposed method predicts reactive action\nwhen the precise manipulation of the target object is required (local action)\nand generates the entire trajectory when the precise manipulation is not\nrequired. This dual-action formulation effectively prevents compounding error\nwith the trajectory-based global action while respond to unexpected changes in\nthe target object with the reactive local action. Furthermore, in this\nformulation, both global/local actions are conditioned by a goal state which is\ndefined as the last step of each subtask, for robust policy prediction. The\nproposed method was tested in the real dual-arm robot and successfully\naccomplished the banana peeling task.\n","authors":["Heecheol Kim","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2203.09749v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2203.09208v2","updated":"2022-03-18T05:10:12Z","published":"2022-03-17T09:59:26Z","title":"Neural Compression-Based Feature Learning for Video Restoration","summary":"  How to efficiently utilize the temporal features is crucial, yet challenging,\nfor video restoration. The temporal features usually contain various noisy and\nuncorrelated information, and they may interfere with the restoration of the\ncurrent frame. This paper proposes learning noise-robust feature\nrepresentations to help video restoration. We are inspired by that the neural\ncodec is a natural denoiser. In neural codec, the noisy and uncorrelated\ncontents which are hard to predict but cost lots of bits are more inclined to\nbe discarded for bitrate saving. Therefore, we design a neural compression\nmodule to filter the noise and keep the most useful information in features for\nvideo restoration. To achieve robustness to noise, our compression module\nadopts a spatial channel-wise quantization mechanism to adaptively determine\nthe quantization step size for each position in the latent. Experiments show\nthat our method can significantly boost the performance on video denoising,\nwhere we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs.\nMeanwhile, our method also obtains SOTA results on video deraining and\ndehazing.\n","authors":["Cong Huang","Jiahao Li","Bin Li","Dong Liu","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2203.09208v2.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2112.12612v2","updated":"2022-03-18T05:00:11Z","published":"2021-12-17T22:33:23Z","title":"Towards Disturbance-Free Visual Mobile Manipulation","summary":"  Deep reinforcement learning has shown promising results on an abundance of\nrobotic tasks in simulation, including visual navigation and manipulation.\nPrior work generally aims to build embodied agents that solve their assigned\ntasks as quickly as possible, while largely ignoring the problems caused by\ncollision with objects during interaction. This lack of prioritization is\nunderstandable: there is no inherent cost in breaking virtual objects. As a\nresult, \"well-trained\" agents frequently collide with objects before achieving\ntheir primary goals, a behavior that would be catastrophic in the real world.\nIn this paper, we study the problem of training agents to complete the task of\nvisual mobile manipulation in the ManipulaTHOR environment while avoiding\nunnecessary collision (disturbance) with objects. We formulate disturbance\navoidance as a penalty term in the reward function, but find that directly\ntraining with such penalized rewards often results in agents being unable to\nescape poor local optima. Instead, we propose a two-stage training curriculum\nwhere an agent is first allowed to freely explore and build basic competencies\nwithout penalization, after which a disturbance penalty is introduced to refine\nthe agent's behavior. Results on testing scenes show that our curriculum not\nonly avoids these poor local optima, but also leads to 10% absolute gains in\nsuccess rate without disturbance, compared to our state-of-the-art baselines.\nMoreover, we propose a novel disturbance-prediction auxiliary task accelerating\nlearning and further improving success rates.\n","authors":["Tianwei Ni","Kiana Ehsani","Luca Weihs","Jordi Salvador"],"pdf_url":"https://arxiv.org/pdf/2112.12612v2.pdf","comment":"Project site: https://sites.google.com/view/disturb-free"},{"id":"http://arxiv.org/abs/2203.05297v3","updated":"2022-03-18T04:59:49Z","published":"2022-03-10T11:19:52Z","title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for\n  Conversational Gestures Synthesis","summary":"  Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n","authors":["Haiyang Liu","Zihao Zhu","Naoya Iwamoto","Yichen Peng","Zhengqing Li","You Zhou","Elif Bozkurt","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.05297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09744v1","updated":"2022-03-18T04:56:20Z","published":"2022-03-18T04:56:20Z","title":"Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic\n  Segmentation","summary":"  Domain adaptive semantic segmentation aims to learn a model with the\nsupervision of source domain data, and produce satisfactory dense predictions\non unlabeled target domain. One popular solution to this challenging task is\nself-training, which selects high-scoring predictions on target samples as\npseudo labels for training. However, the produced pseudo labels often contain\nmuch noise because the model is biased to source domain as well as majority\ncategories. To address the above issues, we propose to directly explore the\nintrinsic pixel distributions of target domain data, instead of heavily relying\non the source domain. Specifically, we simultaneously cluster pixels and\nrectify pseudo labels with the obtained cluster assignments. This process is\ndone in an online fashion so that pseudo labels could co-evolve with the\nsegmentation model without extra training rounds. To overcome the class\nimbalance problem on long-tailed categories, we employ a distribution alignment\ntechnique to enforce the marginal class distribution of cluster assignments to\nbe close to that of pseudo labels. The proposed method, namely Class-balanced\nPixel-level Self-Labeling (CPSL), improves the segmentation performance on\ntarget domain over state-of-the-arts by a large margin, especially on\nlong-tailed categories.\n","authors":["Ruihuang Li","Shuai Li","Chenhang He","Yabin Zhang","Xu Jia","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09744v1.pdf","comment":"This paper has been accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2011.11961v4","updated":"2022-03-18T04:49:53Z","published":"2020-11-24T08:38:36Z","title":"MODNet: Real-Time Trimap-Free Portrait Matting via Objective\n  Decomposition","summary":"  Existing portrait matting methods either require auxiliary inputs that are\ncostly to obtain or involve multiple stages that are computationally expensive,\nmaking them less suitable for real-time applications. In this work, we present\na light-weight matting objective decomposition network (MODNet) for portrait\nmatting in real-time with a single input image. The key idea behind our\nefficient design is by optimizing a series of sub-objectives simultaneously via\nexplicit constraints. In addition, MODNet includes two novel techniques for\nimproving model efficiency and robustness. First, an Efficient Atrous Spatial\nPyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for\nsemantic estimation. Second, a self-supervised sub-objectives consistency (SOC)\nstrategy is proposed to adapt MODNet to real-world data to address the domain\nshift problem common to trimap-free methods. MODNet is easy to be trained in an\nend-to-end manner. It is much faster than contemporaneous methods and runs at\n67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms\nprior trimap-free methods by a large margin on both Adobe Matting Dataset and a\ncarefully designed photographic portrait matting (PPM-100) benchmark proposed\nby us. Further, MODNet achieves remarkable results on daily photos and videos.\nOur code and models are available at https://github.com/ZHKKKe/MODNet, and the\nPPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.\n","authors":["Zhanghan Ke","Jiayu Sun","Kaican Li","Qiong Yan","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2011.11961v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07182v2","updated":"2022-03-18T04:41:55Z","published":"2022-03-14T15:23:04Z","title":"NeILF: Neural Incident Light Field for Physically-based Material\n  Estimation","summary":"  We present a differentiable rendering framework for material and lighting\nestimation from multi-view images and a reconstructed geometry. In the\nframework, we represent scene lightings as the Neural Incident Light Field\n(NeILF) and material properties as the surface BRDF modelled by multi-layer\nperceptrons. Compared with recent approaches that approximate scene lightings\nas the 2D environment map, NeILF is a fully 5D light field that is capable of\nmodelling illuminations of any static scenes. In addition, occlusions and\nindirect lights can be handled naturally by the NeILF representation without\nrequiring multiple bounces of ray tracing, making it possible to estimate\nmaterial properties even for scenes with complex lightings and geometries. We\nalso propose a smoothness regularization and a Lambertian assumption to reduce\nthe material-lighting ambiguity during the optimization. Our method strictly\nfollows the physically-based rendering equation, and jointly optimizes material\nand lighting through the differentiable rendering process. We have intensively\nevaluated the proposed method on our in-house synthetic dataset, the DTU MVS\ndataset, and real-world BlendedMVS scenes. Our method is able to outperform\nprevious methods by a significant margin in terms of novel view rendering\nquality, setting a new state-of-the-art for image-based material and lighting\nestimation.\n","authors":["Yao Yao","Jingyang Zhang","Jingbo Liu","Yihang Qu","Tian Fang","David McKinnon","Yanghai Tsin","Long Quan"],"pdf_url":"https://arxiv.org/pdf/2203.07182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09739v1","updated":"2022-03-18T04:38:18Z","published":"2022-03-18T04:38:18Z","title":"Do Deep Networks Transfer Invariances Across Classes?","summary":"  To generalize well, classifiers must learn to be invariant to nuisance\ntransformations that do not alter an input's class. Many problems have\n\"class-agnostic\" nuisance transformations that apply similarly to all classes,\nsuch as lighting and background changes for image classification. Neural\nnetworks can learn these invariances given sufficient data, but many real-world\ndatasets are heavily class imbalanced and contain only a few examples for most\nof the classes. We therefore pose the question: how well do neural networks\ntransfer class-agnostic invariances learned from the large classes to the small\nones? Through careful experimentation, we observe that invariance to\nclass-agnostic transformations is still heavily dependent on class size, with\nthe networks being much less invariant on smaller classes. This result holds\neven when using data balancing techniques, and suggests poor invariance\ntransfer across classes. Our results provide one explanation for why\nclassifiers generalize poorly on unbalanced and long-tailed distributions.\nBased on this analysis, we show how a generative approach for learning the\nnuisance transformations can help transfer invariances across classes and\nimprove performance on a set of imbalanced image classification benchmarks.\nSource code for our experiments is available at\nhttps://github.com/AllanYangZhou/generative-invariance-transfer.\n","authors":["Allan Zhou","Fahim Tajwar","Alexander Robey","Tom Knowles","George J. Pappas","Hamed Hassani","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2203.09739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09737v1","updated":"2022-03-18T04:28:58Z","published":"2022-03-18T04:28:58Z","title":"Semi-Supervised Learning with Mutual Distillation for Monocular Depth\n  Estimation","summary":"  We propose a semi-supervised learning framework for monocular depth\nestimation. Compared to existing semi-supervised learning methods, which\ninherit limitations of both sparse supervised and unsupervised loss functions,\nwe achieve the complementary advantages of both loss functions, by building two\nseparate network branches for each loss and distilling each other through the\nmutual distillation loss function. We also present to apply different data\naugmentation to each branch, which improves the robustness. We conduct\nexperiments to demonstrate the effectiveness of our framework over the latest\nmethods and provide extensive ablation studies.\n","authors":["Jongbeom Baek","Gyeongnyeon Kim","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2203.09737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09736v1","updated":"2022-03-18T04:23:25Z","published":"2022-03-18T04:23:25Z","title":"Series Photo Selection via Multi-view Graph Learning","summary":"  Series photo selection (SPS) is an important branch of the image aesthetics\nquality assessment, which focuses on finding the best one from a series of\nnearly identical photos. While a great progress has been observed, most of the\nexisting SPS approaches concentrate solely on extracting features from the\noriginal image, neglecting that multiple views, e.g, saturation level, color\nhistogram and depth of field of the image, will be of benefit to successfully\nreflecting the subtle aesthetic changes. Taken multi-view into consideration,\nwe leverage a graph neural network to construct the relationships between\nmulti-view features. Besides, multiple views are aggregated with an\nadaptive-weight self-attention module to verify the significance of each view.\nFinally, a siamese network is proposed to select the best one from a series of\nnearly identical photos. Experimental results demonstrate that our model\naccomplish the highest success rates compared with competitive methods.\n","authors":["Jin Huang","Lu Zhang","Yongshun Gong","Jian Zhang","Xiushan Nie","Yilong Yin"],"pdf_url":"https://arxiv.org/pdf/2203.09736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09733v1","updated":"2022-03-18T04:20:36Z","published":"2022-03-18T04:20:36Z","title":"Distortion-Tolerant Monocular Depth Estimation On Omnidirectional Images\n  Using Dual-cubemap","summary":"  Estimating the depth of omnidirectional images is more challenging than that\nof normal field-of-view (NFoV) images because the varying distortion can\nsignificantly twist an object's shape. The existing methods suffer from\ntroublesome distortion while estimating the depth of omnidirectional images,\nleading to inferior performance. To reduce the negative impact of the\ndistortion influence, we propose a distortion-tolerant omnidirectional depth\nestimation algorithm using a dual-cubemap. It comprises two modules:\nDual-Cubemap Depth Estimation (DCDE) module and Boundary Revision (BR) module.\nIn DCDE module, we present a rotation-based dual-cubemap model to estimate the\naccurate NFoV depth, reducing the distortion at the cost of boundary\ndiscontinuity on omnidirectional depths. Then a boundary revision module is\ndesigned to smooth the discontinuous boundaries, which contributes to the\nprecise and visually continuous omnidirectional depths. Extensive experiments\ndemonstrate the superiority of our method over other state-of-the-art\nsolutions.\n","authors":["Zhijie Shen","Chunyu Lin","Lang Nie","Kang Liao","Yao zhao"],"pdf_url":"https://arxiv.org/pdf/2203.09733v1.pdf","comment":"Accepted by ICME2021, poster"},{"id":"http://arxiv.org/abs/2109.07648v3","updated":"2022-03-18T04:14:32Z","published":"2021-09-16T01:01:55Z","title":"METEOR:A Dense, Heterogeneous, and Unstructured Traffic Dataset With\n  Rare Behaviors","summary":"  We present a new traffic dataset, METEOR, which captures traffic patterns and\nmulti-agent driving behaviors in unstructured scenarios. METEOR consists of\nmore than 1000 one-minute videos, over 2 million annotated frames with bounding\nboxes and GPS trajectories for 16 unique agent categories, and more than 13\nmillion bounding boxes for traffic agents. METEOR is a dataset for rare and\ninteresting, multi-agent driving behaviors that are grouped into traffic\nviolations, atypical interactions, and diverse scenarios. Every video in METEOR\nis tagged using a diverse range of factors corresponding to weather, time of\nthe day, road conditions, and traffic density. We use METEOR to benchmark\nperception methods for object detection and multi-agent behavior prediction.\nOur key finding is that state-of-the-art models for object detection and\nbehavior prediction, which otherwise succeed on existing datasets such as\nWaymo, fail on the METEOR dataset. METEOR marks the first step towards the\ndevelopment of more sophisticated perception models for dense, heterogeneous,\nand unstructured scenarios.\n","authors":["Rohan Chandra","Xijun Wang","Mridul Mahajan","Rahul Kala","Rishitha Palugulla","Chandrababu Naidu","Alok Jain","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2109.07648v3.pdf","comment":"Under review at IROS 2022"},{"id":"http://arxiv.org/abs/2203.09730v1","updated":"2022-03-18T04:04:54Z","published":"2022-03-18T04:04:54Z","title":"A Dual Weighting Label Assignment Scheme for Object Detection","summary":"  Label assignment (LA), which aims to assign each training sample a positive\n(pos) and a negative (neg) loss weight, plays an important role in object\ndetection. Existing LA methods mostly focus on the design of pos weighting\nfunction, while the neg weight is directly derived from the pos weight. Such a\nmechanism limits the learning capacity of detectors. In this paper, we explore\na new weighting paradigm, termed dual weighting (DW), to specify pos and neg\nweights separately. We first identify the key influential factors of pos/neg\nweights by analyzing the evaluation metrics in object detection, and then\ndesign the pos and neg weighting functions based on them. Specifically, the pos\nweight of a sample is determined by the consistency degree between its\nclassification and localization scores, while the neg weight is decomposed into\ntwo terms: the probability that it is a neg sample and its importance\nconditioned on being a neg sample. Such a weighting strategy offers greater\nflexibility to distinguish between important and less important samples,\nresulting in a more effective object detector. Equipped with the proposed DW\nmethod, a single FCOS-ResNet-50 detector can reach 41.5% mAP on COCO under 1x\nschedule, outperforming other existing LA methods. It consistently improves the\nbaselines on COCO by a large margin under various backbones without bells and\nwhistles. Code is available at https://github.com/strongwolf/DW.\n","authors":["Shuai Li","Chenhang He","Ruihuang Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09730v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.09729v1","updated":"2022-03-18T04:04:45Z","published":"2022-03-18T04:04:45Z","title":"REALY: Rethinking the Evaluation of 3D Face Reconstruction","summary":"  The evaluation of 3D face reconstruction results typically relies on a rigid\nshape alignment between the estimated 3D model and the ground-truth scan. We\nobserve that aligning two shapes with different reference points can largely\naffect the evaluation results. This poses difficulties for precisely diagnosing\nand improving a 3D face reconstruction method. In this paper, we propose a\nnovel evaluation approach with a new benchmark REALY, consists of 100 globally\naligned face scans with accurate facial keypoints, high-quality region masks,\nand topology-consistent meshes. Our approach performs region-wise shape\nalignment and leads to more accurate, bidirectional correspondences during\ncomputing the shape errors. The fine-grained, region-wise evaluation results\nprovide us detailed understandings about the performance of state-of-the-art 3D\nface reconstruction methods. For example, our experiments on single-image based\nreconstruction methods reveal that DECA performs the best on nose regions,\nwhile GANFit performs better on cheek regions. Besides, a new and high-quality\n3DMM basis, HIFI3D++, is further derived using the same procedure as we\nconstruct REALY to align and retopologize several 3D face datasets. We will\nrelease REALY, HIFI3D++, and our new evaluation pipeline at\nhttps://realy3dface.com.\n","authors":["Zenghao Chai","Haoxian Zhang","Jing Ren","Di Kang","Zhengzhuo Xu","Xuefei Zhe","Chun Yuan","Linchao Bao"],"pdf_url":"https://arxiv.org/pdf/2203.09729v1.pdf","comment":"Project page: https://realy3dface.com"},{"id":"http://arxiv.org/abs/2112.03530v3","updated":"2022-03-18T03:49:05Z","published":"2021-12-07T06:59:06Z","title":"A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud\n  Completion","summary":"  3D point cloud is an important 3D representation for capturing real world 3D\nobjects. However, real-scanned 3D point clouds are often incomplete, and it is\nimportant to recover complete point clouds for downstream applications. Most\nexisting point cloud completion methods use Chamfer Distance (CD) loss for\ntraining. The CD loss estimates correspondences between two point clouds by\nsearching nearest neighbors, which does not capture the overall point density\ndistribution on the generated shape, and therefore likely leads to non-uniform\npoint cloud generation. To tackle this problem, we propose a novel Point\nDiffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of\na Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The\nCGNet uses a conditional generative model called the denoising diffusion\nprobabilistic model (DDPM) to generate a coarse completion conditioned on the\npartial observation. DDPM establishes a one-to-one pointwise mapping between\nthe generated point cloud and the uniform ground truth, and then optimizes the\nmean squared error loss to realize uniform generation. The RFNet refines the\ncoarse output of the CGNet and further improves quality of the completed point\ncloud. Furthermore, we develop a novel dual-path architecture for both\nnetworks. The architecture can (1) effectively and efficiently extract\nmulti-level features from partially observed point clouds to guide completion,\nand (2) accurately manipulate spatial locations of 3D points to obtain smooth\nsurfaces and sharp details. Extensive experimental results on various benchmark\ndatasets show that our PDR paradigm outperforms previous state-of-the-art\nmethods for point cloud completion. Remarkably, with the help of the RFNet, we\ncan accelerate the iterative generation process of the DDPM by up to 50 times\nwithout much performance drop.\n","authors":["Zhaoyang Lyu","Zhifeng Kong","Xudong Xu","Liang Pan","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2112.03530v3.pdf","comment":"Accepted to ICLR 2022. Code is released at\n  https://github.com/ZhaoyangLyu/Point_Diffusion_Refinement"},{"id":"http://arxiv.org/abs/2203.09724v1","updated":"2022-03-18T03:41:36Z","published":"2022-03-18T03:41:36Z","title":"Rethinking the optimization process for self-supervised model-driven MRI\n  reconstruction","summary":"  Recovering high-quality images from undersampled measurements is critical for\naccelerated MRI reconstruction. Recently, various supervised deep\nlearning-based MRI reconstruction methods have been developed. Despite the\nachieved promising performances, these methods require fully sampled reference\ndata, the acquisition of which is resource-intensive and time-consuming.\nSelf-supervised learning has emerged as a promising solution to alleviate the\nreliance on fully sampled datasets. However, existing self-supervised methods\nsuffer from reconstruction errors due to the insufficient constraint enforced\non the non-sampled data points and the error accumulation happened alongside\nthe iterative image reconstruction process for model-driven deep learning\nreconstrutions. To address these challenges, we propose K2Calibrate, a K-space\nadaptation strategy for self-supervised model-driven MR reconstruction\noptimization. By iteratively calibrating the learned measurements, K2Calibrate\ncan reduce the network's reconstruction deterioration caused by statistically\ndependent noise. Extensive experiments have been conducted on the open-source\ndataset FastMRI, and K2Calibrate achieves better results than five\nstate-of-the-art methods. The proposed K2Calibrate is plug-and-play and can be\neasily integrated with different model-driven deep learning reconstruction\nmethods.\n","authors":["Weijian Huang","Cheng Li","Wenxin Fan","Yongjin Zhou","Qiegen Liu","Hairong Zheng","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.09724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09721v1","updated":"2022-03-18T03:37:14Z","published":"2022-03-18T03:37:14Z","title":"Deterministic Bridge Regression for Compressive Classification","summary":"  Pattern classification with compact representation is an important component\nin machine intelligence. In this work, an analytic bridge solution is proposed\nfor compressive classification. The proposal has been based upon solving a\npenalized error formulation utilizing an approximated $\\ell_p$-norm. The\nsolution comes in a primal form for over-determined systems and in a dual form\nfor under-determined systems. While the primal form is suitable for problems of\nlow dimension with large data samples, the dual form is suitable for problems\nof high dimension but with a small number of data samples. The solution has\nalso been extended for problems with multiple classification outputs. Numerical\nstudies based on simulated and real-world data validated the effectiveness of\nthe proposed solution.\n","authors":["Kar-Ann Toh","Giuseppe Molteni","Zhiping Lin"],"pdf_url":"https://arxiv.org/pdf/2203.09721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.11325v5","updated":"2022-03-18T03:29:48Z","published":"2021-12-21T16:16:30Z","title":"iSegFormer: Interactive Segmentation via Transformers with Application\n  to 3D Knee MR Images","summary":"  We propose iSegFormer, a memory-efficient transformer that combines a Swin\ntransformer with a lightweight multilayer perceptron (MLP) decoder. With the\nefficient Swin transformer blocks for hierarchical self-attention and the\nsimple MLP decoder for aggregating both local and global attention, iSegFormer\nlearns powerful representations while achieving high computational\nefficiencies. Specifically, we apply iSegFormer to interactive 3D medical image\nsegmentation.\n","authors":["Qin Liu","Zhenlin Xu","Yining Jiao","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2112.11325v5.pdf","comment":"11 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2111.02387v3","updated":"2022-03-18T03:29:10Z","published":"2021-11-03T17:55:36Z","title":"An Empirical Study of Training End-to-End Vision-and-Language\n  Transformers","summary":"  Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.\n","authors":["Zi-Yi Dou","Yichong Xu","Zhe Gan","Jianfeng Wang","Shuohang Wang","Lijuan Wang","Chenguang Zhu","Pengchuan Zhang","Lu Yuan","Nanyun Peng","Zicheng Liu","Michael Zeng"],"pdf_url":"https://arxiv.org/pdf/2111.02387v3.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09388v2","updated":"2022-03-18T03:07:32Z","published":"2022-03-17T15:28:29Z","title":"A Text Attention Network for Spatial Deformation Robust Scene Text Image\n  Super-resolution","summary":"  Scene text image super-resolution aims to increase the resolution and\nreadability of the text in low-resolution images. Though significant\nimprovement has been achieved by deep convolutional neural networks (CNNs), it\nremains difficult to reconstruct high-resolution images for spatially deformed\ntexts, especially rotated and curve-shaped ones. This is because the current\nCNN-based methods adopt locality-based operations, which are not effective to\ndeal with the variation caused by deformations. In this paper, we propose a CNN\nbased Text ATTention network (TATT) to address this problem. The semantics of\nthe text are firstly extracted by a text recognition module as text prior\ninformation. Then we design a novel transformer-based module, which leverages\nglobal attention mechanism, to exert the semantic guidance of text prior to the\ntext reconstruction process. In addition, we propose a text structure\nconsistency loss to refine the visual appearance by imposing structural\nconsistency on the reconstructions of regular and deformed texts. Experiments\non the benchmark TextZoom dataset show that the proposed TATT not only achieves\nstate-of-the-art performance in terms of PSNR/SSIM metrics, but also\nsignificantly improves the recognition accuracy in the downstream text\nrecognition task, particularly for text instances with multi-orientation and\ncurved shapes. Code is available at https://github.com/mjq11302010044/TATT.\n","authors":["Jianqi Ma","Zhetong Liang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09388v2.pdf","comment":"Accepted to CVPR2022"},{"id":"http://arxiv.org/abs/2203.09704v1","updated":"2022-03-18T02:34:59Z","published":"2022-03-18T02:34:59Z","title":"VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial\n  Attention","summary":"  Detecting objects from LiDAR point clouds is of tremendous significance in\nautonomous driving. In spite of good progress, accurate and reliable 3D\ndetection is yet to be achieved due to the sparsity and irregularity of LiDAR\npoint clouds. Among existing strategies, multi-view methods have shown great\npromise by leveraging the more comprehensive information from both bird's eye\nview (BEV) and range view (RV). These multi-view methods either refine the\nproposals predicted from single view via fused features, or fuse the features\nwithout considering the global spatial context; their performance is limited\nconsequently. In this paper, we propose to adaptively fuse multi-view features\nin a global spatial context via Dual Cross-VIew SpaTial Attention (VISTA). The\nproposed VISTA is a novel plug-and-play fusion module, wherein the multi-layer\nperceptron widely adopted in standard attention modules is replaced with a\nconvolutional one. Thanks to the learned attention mechanism, VISTA can produce\nfused features of high quality for prediction of proposals. We decouple the\nclassification and regression tasks in VISTA, and an additional constraint of\nattention variance is applied that enables the attention module to focus on\nspecific targets instead of generic points. We conduct thorough experiments on\nthe benchmarks of nuScenes and Waymo; results confirm the efficacy of our\ndesigns. At the time of submission, our method achieves 63.0% in overall mAP\nand 69.8% in NDS on the nuScenes benchmark, outperforming all published methods\nby up to 24% in safety-crucial categories such as cyclist. The source code in\nPyTorch is available at https://github.com/Gorilla-Lab-SCUT/VISTA\n","authors":["Shengheng Deng","Zhihao Liang","Lin Sun","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2203.09704v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.09161v2","updated":"2022-03-18T02:18:54Z","published":"2022-03-17T08:30:30Z","title":"How Many Data Samples is an Additional Instruction Worth?","summary":"  Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state of the art task\nspecific models. Conventional approaches to improve model performance via\ncreating large datasets with lots of task instances or architectural/training\nchanges in model may not be feasible for non-expert users. However, they can\nwrite alternate instructions to represent an instruction task. Is\nInstruction-augumentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that\nthese significantly improve model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.\n","authors":["Ravsehaj Singh Puri","Swaroop Mishra","Mihir Parmar","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2203.09161v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2203.04564v2","updated":"2022-03-18T02:18:20Z","published":"2022-03-09T07:55:45Z","title":"Region-Aware Face Swapping","summary":"  This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to\nachieve identity-consistent harmonious high-resolution face generation in a\nlocal-global manner: \\textbf{1)} Local Facial Region-Aware (FRA) branch\naugments local identity-relevant features by introducing the Transformer to\neffectively model misaligned cross-scale semantic interaction. \\textbf{2)}\nGlobal Source Feature-Adaptive (SFA) branch further complements global\nidentity-relevant cues for generating identity-consistent swapped faces.\nBesides, we propose a \\textit{Face Mask Predictor} (FMP) module incorporated\nwith StyleGAN2 to predict identity-relevant soft facial masks in an\nunsupervised manner that is more practical for generating harmonious\nhigh-resolution faces. Abundant experiments qualitatively and quantitatively\ndemonstrate the superiority of our method for generating more\nidentity-consistent high-resolution swapped faces over SOTA methods, \\eg,\nobtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\\uparrow$.\n","authors":["Chao Xu","Jiangning Zhang","Miao Hua","Qian He","Zili Yi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2203.04564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09694v1","updated":"2022-03-18T01:49:40Z","published":"2022-03-18T01:49:40Z","title":"Group Contextualization for Video Recognition","summary":"  Learning discriminative representation from the complex spatio-temporal\ndynamic space is essential for video recognition. On top of those stylized\nspatio-temporal computational units, further refining the learnt feature with\naxial contexts is demonstrated to be promising in achieving this goal. However,\nprevious works generally focus on utilizing a single kind of contexts to\ncalibrate entire feature channels and could hardly apply to deal with diverse\nvideo activities. The problem can be tackled by using pair-wise spatio-temporal\nattentions to recompute feature response with cross-axis contexts at the\nexpense of heavy computations. In this paper, we propose an efficient feature\nrefinement method that decomposes the feature channels into several groups and\nseparately refines them with different axial contexts in parallel. We refer\nthis lightweight feature calibration as group contextualization (GC).\nSpecifically, we design a family of efficient element-wise calibrators, i.e.,\nECal-G/S/T/L, where their axial contexts are information dynamics aggregated\nfrom other axes either globally or locally, to contextualize feature channel\ngroups. The GC module can be densely plugged into each residual layer of the\noff-the-shelf video networks. With little computational overhead, consistent\nimprovement is observed when plugging in GC on different networks. By utilizing\ncalibrators to embed feature with four different kinds of contexts in parallel,\nthe learnt representation is expected to be more resilient to diverse types of\nactivities. On videos with rich temporal variations, empirically GC can boost\nthe performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the\nstate-of-the-art video networks. Code is available at\nhttps://github.com/haoyanbin918/Group-Contextualization.\n","authors":["Yanbin Hao","Hao Zhang","Chong-Wah Ngo","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2203.09694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09692v1","updated":"2022-03-18T01:42:59Z","published":"2022-03-18T01:42:59Z","title":"Facial Geometric Detail Recovery via Implicit Representation","summary":"  Learning a dense 3D model with fine-scale details from a single facial image\nis highly challenging and ill-posed. To address this problem, many approaches\nfit smooth geometries through facial prior while learning details as additional\ndisplacement maps or personalized basis. However, these techniques typically\nrequire vast datasets of paired multi-view data or 3D scans, whereas such\ndatasets are scarce and expensive. To alleviate heavy data dependency, we\npresent a robust texture-guided geometric detail recovery approach using only a\nsingle in-the-wild facial image. More specifically, our method combines\nhigh-quality texture completion with the powerful expressiveness of implicit\nsurfaces. Initially, we inpaint occluded facial parts, generate complete\ntextures, and build an accurate multi-view dataset of the same subject. In\norder to estimate the detailed geometry, we define an implicit signed distance\nfunction and employ a physically-based implicit renderer to reconstruct fine\ngeometric details from the generated multi-view images. Our method not only\nrecovers accurate facial details but also decomposes normals, albedos, and\nshading parts in a self-supervised way. Finally, we register the implicit shape\ndetails to a 3D Morphable Model template, which can be used in traditional\nmodeling and rendering pipelines. Extensive experiments demonstrate that the\nproposed approach can reconstruct impressive facial details from a single\nimage, especially when compared with state-of-the-art methods trained on large\ndatasets.\n","authors":["Xingyu Ren","Alexandros Lattas","Baris Gecer","Jiankang Deng","Chao Ma","Xiaokang Yang","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2203.09692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09679v1","updated":"2022-03-18T01:13:21Z","published":"2022-03-18T01:13:21Z","title":"Modeling Intensification for Sign Language Generation: A Computational\n  Approach","summary":"  End-to-end sign language generation models do not accurately represent the\nprosody in sign language. A lack of temporal and spatial variations leads to\npoor-quality generated presentations that confuse human interpreters. In this\npaper, we aim to improve the prosody in generated sign languages by modeling\nintensification in a data-driven manner. We present different strategies\ngrounded in linguistics of sign language that inform how intensity modifiers\ncan be represented in gloss annotations. To employ our strategies, we first\nannotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,\nwith different levels of intensification. We then use a supervised intensity\ntagger to extend the annotated dataset and obtain labels for the remaining\nportion of it. This enhanced dataset is then used to train state-of-the-art\ntransformer models for sign language generation. We find that our efforts in\nintensification modeling yield better results when evaluated with automatic\nmetrics. Human evaluation also indicates a higher preference of the videos\ngenerated using our model.\n","authors":["Mert İnan","Yang Zhong","Sabit Hassan","Lorna Quandt","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2203.09679v1.pdf","comment":"15 pages, Findings of the Association for Computational Linguistics:\n  ACL 2022"},{"id":"http://arxiv.org/abs/2108.05081v3","updated":"2022-03-18T00:56:50Z","published":"2021-08-11T07:52:59Z","title":"Cervical Optical Coherence Tomography Image Classification Based on\n  Contrastive Self-Supervised Texture Learning","summary":"  Background: Cervical cancer seriously affects the health of the female\nreproductive system. Optical coherence tomography (OCT) emerged as a\nnon-invasive, high-resolution imaging technology for cervical disease\ndetection. However, OCT image annotation is knowledge-intensive and\ntime-consuming, which impedes the training process of deep-learning-based\nclassification models. Purpose: This study aims to develop a computer-aided\ndiagnosis (CADx) approach to classifying in-vivo cervical OCT images based on\nself-supervised learning. Methods: In addition to high-level semantic features\nextracted by a convolutional neural network (CNN), the proposed CADx approach\nleverages unlabeled cervical OCT images' texture features learned by\ncontrastive texture learning. We conducted ten-fold cross-validation on the OCT\nimage dataset from a multi-center clinical study on 733 patients from China.\nResults: In a binary classification task for detecting high-risk diseases,\nincluding high-grade squamous intraepithelial lesion and cervical cancer, our\nmethod achieved an area-under-the-curve value of 0.9798 plus or minus 0.0157\nwith a sensitivity of 91.17 plus or minus 4.99% and a specificity of 93.96 plus\nor minus 4.72% for OCT image patches; also, it outperformed two out of four\nmedical experts on the test set. Furthermore, our method achieved a 91.53%\nsensitivity and 97.37% specificity on an external validation dataset containing\n287 3D OCT volumes from 118 Chinese patients in a new hospital using a\ncross-shaped threshold voting strategy. Conclusions: The proposed\ncontrastive-learning-based CADx method outperformed the end-to-end CNN models\nand provided better interpretability based on texture features, which holds\ngreat potential to be used in the clinical protocol of \"see-and-treat.\"\n","authors":["Kaiyi Chen","Qingbin Wang","Yutao Ma"],"pdf_url":"https://arxiv.org/pdf/2108.05081v3.pdf","comment":"22 pages, 7 figures, and 7 tables"},{"id":"http://arxiv.org/abs/2203.09674v1","updated":"2022-03-18T00:47:32Z","published":"2022-03-18T00:47:32Z","title":"A workflow for segmenting soil and plant X-ray CT images with deep\n  learning in Googles Colaboratory","summary":"  X-ray micro-computed tomography (X-ray microCT) has enabled the\ncharacterization of the properties and processes that take place in plants and\nsoils at the micron scale. Despite the widespread use of this advanced\ntechnique, major limitations in both hardware and software limit the speed and\naccuracy of image processing and data analysis. Recent advances in machine\nlearning, specifically the application of convolutional neural networks to\nimage analysis, have enabled rapid and accurate segmentation of image data.\nYet, challenges remain in applying convolutional neural networks to the\nanalysis of environmentally and agriculturally relevant images. Specifically,\nthere is a disconnect between the computer scientists and engineers, who build\nthese AI/ML tools, and the potential end users in agricultural research, who\nmay be unsure of how to apply these tools in their work. Additionally, the\ncomputing resources required for training and applying deep learning models are\nunique, more common to computer gaming systems or graphics design work, than to\ntraditional computational systems. To navigate these challenges, we developed a\nmodular workflow for applying convolutional neural networks to X-ray microCT\nimages, using low-cost resources in Googles Colaboratory web application. Here\nwe present the results of the workflow, illustrating how parameters can be\noptimized to achieve best results using example scans from walnut leaves,\nalmond flower buds, and a soil aggregate. We expect that this framework will\naccelerate the adoption and use of emerging deep learning techniques within the\nplant and soil sciences.\n","authors":["Devin A. Rippner","Pranav Raja","J. Mason Earles","Alexander Buchko","Mina Momayyezi","Fiona Duong","Dilworth Parkinson","Elizabeth Forrestel","Ken Shackel","Andrew J. McElrone"],"pdf_url":"https://arxiv.org/pdf/2203.09674v1.pdf","comment":"31 pages, 7 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2201.13242v2","updated":"2022-03-18T16:27:49Z","published":"2022-01-31T13:52:51Z","title":"Correcting diacritics and typos with a ByT5 transformer model","summary":"  Due to the fast pace of life and online communications and the prevalence of\nEnglish and the QWERTY keyboard, people tend to forgo using diacritics, make\ntypographical errors (typos) when typing in other languages. Restoring\ndiacritics and correcting spelling is important for proper language use and the\ndisambiguation of texts for both humans and downstream algorithms. However,\nboth of these problems are typically addressed separately: the state-of-the-art\ndiacritics restoration methods do not tolerate other typos, but classical\nspellcheckers also cannot deal adequately with all the diacritics missing. In\nthis work, we tackle both problems at once by employing the newly-developed\nuniversal ByT5 byte-level seq2seq transformer model that requires no\nlanguage-specific model structures. For a comparison, we perform diacritics\nrestoration on benchmark datasets of 12 languages, with the addition of\nLithuanian. The experimental investigation proves that our approach is able to\nachieve results (> 98%) comparable to the previous state-of-the-art, despite\nbeing trained less and on fewer data. Our approach is also able to restore\ndiacritics in words not seen during training with > 76% accuracy. Our\nsimultaneous diacritics restoration and typos correction approach reaches > 94%\nalpha-word accuracy on the 13 languages. It has no direct competitors and\nstrongly outperforms classical spell-checking or dictionary-based approaches.\nWe also demonstrate all the accuracies to further improve with more training.\nTaken together, this shows the great real-world application potential of our\nsuggested methods to more data, languages, and error classes.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius","Jurgita Kapočiūtė-Dzikienė","Monika Briedienė","Tomas Krilavičius"],"pdf_url":"https://arxiv.org/pdf/2201.13242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10024v1","updated":"2022-03-18T15:42:21Z","published":"2022-03-18T15:42:21Z","title":"Offensive Language Detection in Under-resourced Algerian Dialectal\n  Arabic Language","summary":"  This paper addresses the problem of detecting the offensive and abusive\ncontent in Facebook comments, where we focus on the Algerian dialectal Arabic\nwhich is one of under-resourced languages. The latter has a variety of dialects\nmixed with different languages (i.e. Berber, French and English). In addition,\nwe deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due\nto the scarcity of works on the same language, we have built a new corpus\nregrouping more than 8.7k texts manually annotated as normal, abusive and\noffensive. We have conducted a series of experiments using the state-of-the-art\nclassifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.\nThe results showed acceptable performances, but the problem requires further\ninvestigation on linguistic features to increase the identification accuracy.\n","authors":["Oussama Boucherit","Kheireddine Abainia"],"pdf_url":"https://arxiv.org/pdf/2203.10024v1.pdf","comment":"BigDML 2021"},{"id":"http://arxiv.org/abs/2202.10326v2","updated":"2022-03-18T15:34:52Z","published":"2022-02-17T11:51:32Z","title":"A Deep Learning Approach for Repairing Missing Activity Labels in Event\n  Logs for Process Mining","summary":"  Process mining is a relatively new subject that builds a bridge between\ntraditional process modeling and data mining. Process discovery is one of the\nmost critical parts of process mining, which aims at discovering process models\nautomatically from event logs. The performance of existing process discovery\nalgorithms can be affected when there are missing activity labels in event\nlogs. Several methods have been proposed to repair missing activity labels, but\ntheir accuracy can drop when a large number of activity labels are missing. In\nthis paper, we propose an LSTM-based prediction model to predict the missing\nactivity labels in event logs. The proposed model takes both the prefix and\nsuffix sequences of the events with missing activity labels as input.\nAdditional attributes of event logs are also utilized to improve the\nperformance. Our evaluation of several publicly available datasets shows that\nthe proposed method performed consistently better than existing methods in\nterms of repairing missing activity labels in event logs.\n","authors":["Yang Lu","Qifan Chen","Simon K. Poon"],"pdf_url":"https://arxiv.org/pdf/2202.10326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10001v1","updated":"2022-03-18T15:01:32Z","published":"2022-03-18T15:01:32Z","title":"FORCE: A Framework of Rule-Based Conversational Recommender System","summary":"  The conversational recommender systems (CRSs) have received extensive\nattention in recent years. However, most of the existing works focus on various\ndeep learning models, which are largely limited by the requirement of\nlarge-scale human-annotated datasets. Such methods are not able to deal with\nthe cold-start scenarios in industrial products. To alleviate the problem, we\npropose FORCE, a Framework Of Rule-based Conversational Recommender system that\nhelps developers to quickly build CRS bots by simple configuration. We conduct\nexperiments on two datasets in different languages and domains to verify its\neffectiveness and usability.\n","authors":["Jun Quan","Ze Wei","Qiang Gan","Jingqi Yao","Jingyi Lu","Yuchen Dong","Yiming Liu","Yi Zeng","Chao Zhang","Yongzhi Li","Huang Hu","Yingying He","Yang Yang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.10001v1.pdf","comment":"AAAI 2022 (Demonstration Track)"},{"id":"http://arxiv.org/abs/2203.09963v1","updated":"2022-03-18T13:59:02Z","published":"2022-03-18T13:59:02Z","title":"Towards Lithuanian grammatical error correction","summary":"  Everyone wants to write beautiful and correct text, yet the lack of language\nskills, experience, or hasty typing can result in errors. By employing the\nrecent advances in transformer architectures, we construct a grammatical error\ncorrection model for Lithuanian, the language rich in archaic features. We\ncompare subword and byte-level approaches and share our best trained model,\nachieving F$_{0.5}$=0.92, and accompanying code, in an online open-source\nrepository.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2203.09963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09174v2","updated":"2022-03-18T10:21:07Z","published":"2022-03-17T08:51:56Z","title":"Nearest Neighbor Classifier with Margin Penalty for Active Learning","summary":"  As deep learning becomes the mainstream in the field of natural language\nprocessing, the need for suitable active learning method are becoming\nunprecedented urgent. Active Learning (AL) methods based on nearest neighbor\nclassifier are proposed and demonstrated superior results. However, existing\nnearest neighbor classifier are not suitable for classifying mutual exclusive\nclasses because inter-class discrepancy cannot be assured by nearest neighbor\nclassifiers. As a result, informative samples in the margin area can not be\ndiscovered and AL performance are damaged. To this end, we propose a novel\nNearest neighbor Classifier with Margin penalty for Active Learning(NCMAL).\nFirstly, mandatory margin penalty are added between classes, therefore both\ninter-class discrepancy and intra-class compactness are both assured. Secondly,\na novel sample selection strategy are proposed to discover informative samples\nwithin the margin area. To demonstrate the effectiveness of the methods, we\nconduct extensive experiments on for datasets with other state-of-the-art\nmethods. The experimental results demonstrate that our method achieves better\nresults with fewer annotated samples than all baseline methods.\n","authors":["Yuan Cao","Zhiqiao Gao","Jie Hu","Mingchuan Yang"],"pdf_url":"https://arxiv.org/pdf/2203.09174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09802v1","updated":"2022-03-18T08:54:18Z","published":"2022-03-18T08:54:18Z","title":"Secondary complementary balancing compressive imaging with a free-space\n  balanced amplified photodetector","summary":"  Single-pixel imaging (SPI) has attracted widespread attention because it\ngenerally uses a non-pixelated photodetector and a digital micromirror device\n(DMD) to acquire the object image. Since the modulated patterns seen from two\nreflection directions of the DMD are naturally complementary, one can apply\ncomplementary balanced measurements to greatly improve the measurement\nsignal-to-noise ratio and reconstruction quality. However, the balance between\ntwo reflection arms significantly determines the quality of differential\nmeasurements. In this work, we propose and demonstrate a simple secondary\ncomplementary balancing mechanism to minimize the impact of the imbalance on\nthe imaging system. In our SPI setup, we used a silicon free-space balanced\namplified photodetector with 5 mm active diameter which could directly output\nthe difference between two optical input signals in two reflection arms. Both\nsimulation and experimental results have demonstrated that the use of secondary\ncomplementary balancing can result in a better cancellation of direct current\ncomponents of measurements and a better image restoration quality.\n","authors":["Wen-Kai Yu","Ying Yang","Jin-Rui Liu","Ning Wei","Shuo-Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2203.09802v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2102.06008v2","updated":"2022-03-18T20:46:54Z","published":"2021-02-11T13:54:10Z","title":"Cross-Domain Multi-Task Learning for Sequential Sentence Classification\n  in Research Papers","summary":"  Sequential sentence classification deals with the categorisation of sentences\nbased on their content and context. Applied to scientific texts, it enables the\nautomatic structuring of research papers and the improvement of academic search\nengines. However, previous work has not investigated the potential of transfer\nlearning for sentence classification across different scientific domains and\nthe issue of different text structure of full papers and abstracts. In this\npaper, we derive seven related research questions and present several\ncontributions to address them: First, we suggest a novel uniform deep learning\narchitecture and multi-task learning for cross-domain sequential sentence\nclassification in scientific texts. Second, we tailor two common transfer\nlearning methods, sequential transfer learning and multi-task learning, to deal\nwith the challenges of the given task. Semantic relatedness of tasks is a\nprerequisite for successful transfer learning of neural models. Consequently,\nour third contribution is an approach to semi-automatically identify\nsemantically related classes from different annotation schemes and we present\nan analysis of four annotation schemes. Comprehensive experimental results\nindicate that models, which are trained on datasets from different scientific\ndomains, benefit from one another when using the proposed multi-task learning\narchitecture. We also report comparisons with several state-of-the-art\napproaches. Our approach outperforms the state of the art on full paper\ndatasets significantly while being on par for datasets consisting of abstracts.\n","authors":["Arthur Brack","Anett Hoppe","Pascal Buschermöhle","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2102.06008v2.pdf","comment":"Accepted for publication in ACM/IEEE Joint Conference on Digital\n  Libraries (JCDL), 2022"},{"id":"http://arxiv.org/abs/2203.12598v1","updated":"2022-03-18T18:08:57Z","published":"2022-03-18T18:08:57Z","title":"Learning Personalized Item-to-Item Recommendation Metric via Implicit\n  Feedback","summary":"  This paper studies the item-to-item recommendation problem in recommender\nsystems from a new perspective of metric learning via implicit feedback. We\ndevelop and investigate a personalizable deep metric model that captures both\nthe internal contents of items and how they were interacted with by users.\nThere are two key challenges in learning such model. First, there is no\nexplicit similarity annotation, which deviates from the assumption of most\nmetric learning methods. Second, these approaches ignore the fact that items\nare often represented by multiple sources of meta data and different users use\ndifferent combinations of these sources to form their own notion of similarity.\n  To address these challenges, we develop a new metric representation embedded\nas kernel parameters of a probabilistic model. This helps express the\ncorrelation between items that a user has interacted with, which can be used to\npredict user interaction with new items. Our approach hinges on the intuition\nthat similar items induce similar interactions from the same user, thus fitting\na metric-parameterized model to predict an implicit feedback signal could\nindirectly guide it towards finding the most suitable metric for each user. To\nthis end, we also analyze how and when the proposed method is effective from a\ntheoretical lens. Its empirical effectiveness is also demonstrated on several\nreal-world datasets.\n","authors":["Trong Nghia Hoang","Anoop Deoras","Tong Zhao","Jin Li","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2203.12598v1.pdf","comment":"AISTATS-22"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2202.09297v2","updated":"2022-03-18T17:59:04Z","published":"2022-02-18T16:58:40Z","title":"tinyMAN: Lightweight Energy Manager using Reinforcement Learning for\n  Energy Harvesting Wearable IoT Devices","summary":"  Advances in low-power electronics and machine learning techniques lead to\nmany novel wearable IoT devices. These devices have limited battery capacity\nand computational power. Thus, energy harvesting from ambient sources is a\npromising solution to power these low-energy wearable devices. They need to\nmanage the harvested energy optimally to achieve energy-neutral operation,\nwhich eliminates recharging requirements. Optimal energy management is a\nchallenging task due to the dynamic nature of the harvested energy and the\nbattery energy constraints of the target device. To address this challenge, we\npresent a reinforcement learning-based energy management framework, tinyMAN,\nfor resource-constrained wearable IoT devices. The framework maximizes the\nutilization of the target device under dynamic energy harvesting patterns and\nbattery constraints. Moreover, tinyMAN does not rely on forecasts of the\nharvested energy which makes it a prediction-free approach. We deployed tinyMAN\non a wearable device prototype using TensorFlow Lite for Micro thanks to its\nsmall memory footprint of less than 100 KB. Our evaluations show that tinyMAN\nachieves less than 2.36 ms and 27.75 $\\mu$J while maintaining up to 45% higher\nutility compared to prior approaches.\n","authors":["Toygun Basaklar","Yigit Tuncel","Umit Y. Ogras"],"pdf_url":"https://arxiv.org/pdf/2202.09297v2.pdf","comment":"7 pages, 4 figures, accepted as \"Full Paper\" for the 2022 tinyML\n  Research Symposium"},{"id":"http://arxiv.org/abs/2203.10087v1","updated":"2022-03-18T17:55:39Z","published":"2022-03-18T17:55:39Z","title":"But that's not why: Inference adjustment by interactive prototype\n  deselection","summary":"  Despite significant advances in machine learning, decision-making of\nartificial agents is still not perfect and often requires post-hoc human\ninterventions. If the prediction of a model relies on unreasonable factors it\nis desirable to remove their effect. Deep interactive prototype adjustment\nenables the user to give hints and correct the model's reasoning. In this\npaper, we demonstrate that prototypical-part models are well suited for this\ntask as their prediction is based on prototypical image patches that can be\ninterpreted semantically by the user. It shows that even correct\nclassifications can rely on unreasonable prototypes that result from\nconfounding variables in a dataset. Hence, we propose simple yet effective\ninteraction schemes for inference adjustment: The user is consulted\ninteractively to identify faulty prototypes. Non-object prototypes can be\nremoved by prototype masking or a custom mode of deselection training.\nInteractive prototype rejection allows machine learning na\\\"{i}ve users to\nadjust the logic of reasoning without compromising the accuracy.\n","authors":["Michael Gerstenberger","Sebastian Lapuschkin","Peter Eisert","Sebastian Bosse"],"pdf_url":"https://arxiv.org/pdf/2203.10087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10085v1","updated":"2022-03-18T17:51:20Z","published":"2022-03-18T17:51:20Z","title":"I Know Therefore I Score: Label-Free Crafting of Scoring Functions using\n  Constraints Based on Domain Expertise","summary":"  Several real-life applications require crafting concise, quantitative scoring\nfunctions (also called rating systems) from measured observations. For example,\nan effectiveness score needs to be created for advertising campaigns using a\nnumber of engagement metrics. Experts often need to create such scoring\nfunctions in the absence of labelled data, where the scores need to reflect\nbusiness insights and rules as understood by the domain experts. Without a way\nto capture these inputs systematically, this becomes a time-consuming process\ninvolving trial and error. In this paper, we introduce a label-free practical\napproach to learn a scoring function from multi-dimensional numerical data. The\napproach incorporates insights and business rules from domain experts in the\nform of easily observable and specifiable constraints, which are used as weak\nsupervision by a machine learning model. We convert such constraints into loss\nfunctions that are optimized simultaneously while learning the scoring\nfunction. We examine the efficacy of the approach using a synthetic dataset as\nwell as four real-life datasets, and also compare how it performs vis-a-vis\nsupervised learning models.\n","authors":["Ragja Palakkadavath","Sarath Sivaprasad","Shirish Karande","Niranjan Pedanekar"],"pdf_url":"https://arxiv.org/pdf/2203.10085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10073v1","updated":"2022-03-18T17:38:52Z","published":"2022-03-18T17:38:52Z","title":"Lunar Rover Localization Using Craters as Landmarks","summary":"  Onboard localization capabilities for planetary rovers to date have used\nrelative navigation, by integrating combinations of wheel odometry, visual\nodometry, and inertial measurements during each drive to track position\nrelative to the start of each drive. At the end of each drive, a\nground-in-the-loop (GITL) interaction is used to get a position update from\nhuman operators in a more global reference frame, by matching images or local\nmaps from onboard the rover to orbital reconnaissance images or maps of a large\nregion around the rover's current position. Autonomous rover drives are limited\nin distance so that accumulated relative navigation error does not risk the\npossibility of the rover driving into hazards known from orbital images.\nHowever, several rover mission concepts have recently been studied that require\nmuch longer drives between GITL cycles, particularly for the Moon. These\nconcepts require greater autonomy to minimize GITL cycles to enable such large\nrange; onboard global localization is a key element of such autonomy. Multiple\ntechniques have been studied in the past for onboard rover global localization,\nbut a satisfactory solution has not yet emerged. For the Moon, the ubiquitous\ncraters offer a new possibility, which involves mapping craters from orbit,\nthen recognizing crater landmarks with cameras and-or a lidar onboard the\nrover. This approach is applicable everywhere on the Moon, does not require\nhigh resolution stereo imaging from orbit as some other approaches do, and has\npotential to enable position knowledge with order of 5 to 10 m accuracy at all\ntimes. This paper describes our technical approach to crater-based lunar rover\nlocalization and presents initial results on crater detection using 3D point\ncloud data from onboard lidar or stereo cameras, as well as using shading cues\nin monocular onboard imagery.\n","authors":["Larry Matthies","Shreyansh Daftry","Scott Tepsuporn","Yang Cheng","Deegan Atha","R. Michael Swan","Sanjna Ravichandar","Masahiro Ono"],"pdf_url":"https://arxiv.org/pdf/2203.10073v1.pdf","comment":"IEEE Aerospace Conference, 2022"},{"id":"http://arxiv.org/abs/2009.03717v2","updated":"2022-03-18T17:13:26Z","published":"2020-09-08T13:11:07Z","title":"Hierarchical Message-Passing Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have become a prominent approach to machine\nlearning with graphs and have been increasingly applied in a multitude of\ndomains. Nevertheless, since most existing GNN models are based on flat\nmessage-passing mechanisms, two limitations need to be tackled: (i) they are\ncostly in encoding long-range information spanning the graph structure; (ii)\nthey are failing to encode features in the high-order neighbourhood in the\ngraphs as they only perform information aggregation across the observed edges\nin the original graph. To deal with these two issues, we propose a novel\nHierarchical Message-passing Graph Neural Networks framework. The key idea is\ngenerating a hierarchical structure that re-organises all nodes in a flat graph\ninto multi-level super graphs, along with innovative intra- and inter-level\npropagation manners. The derived hierarchy creates shortcuts connecting\nfar-away nodes so that informative long-range interactions can be efficiently\naccessed via message passing and incorporates meso- and macro-level semantics\ninto the learned node representations. We present the first model to implement\nthis framework, termed Hierarchical Community-aware Graph Neural Network\n(HC-GNN), with the assistance of a hierarchical community detection algorithm.\nThe theoretical analysis illustrates HC-GNN's remarkable capacity in capturing\nlong-range information without introducing heavy additional computation\ncomplexity. Empirical experiments conducted on 9 datasets under transductive,\ninductive, and few-shot settings exhibit that HC-GNN can outperform\nstate-of-the-art GNN models in network analysis tasks, including node\nclassification, link prediction, and community detection. Moreover, the model\nanalysis further demonstrates HC-GNN's robustness facing graph sparsity and the\nflexibility in incorporating different GNN encoders.\n","authors":["Zhiqiang Zhong","Cheng-Te Li","Jun Pang"],"pdf_url":"https://arxiv.org/pdf/2009.03717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.04239v2","updated":"2022-03-18T17:08:20Z","published":"2022-02-09T02:50:42Z","title":"A multiscale spatiotemporal approach for smallholder irrigation\n  detection","summary":"  In presenting an irrigation detection methodology that leverages multiscale\nsatellite imagery of vegetation abundance, this paper introduces a process to\nsupplement limited ground-collected labels and ensure classifier applicability\nin an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced\nVegetation Index (EVI) timeseries characterizes native vegetation phenologies\nat regional scale to provide the basis for a continuous phenology map that\nguides supplementary label collection over irrigated and non-irrigated\nagriculture. Subsequently, validated dry season greening and senescence cycles\nobserved in 10m Sentinel-2 imagery are used to train a suite of classifiers for\nautomated detection of potential smallholder irrigation. Strategies to improve\nmodel robustness are demonstrated, including a method of data augmentation that\nrandomly shifts training samples; and an assessment of classifier types that\nproduce the best performance in withheld target regions. The methodology is\napplied to detect smallholder irrigation in two states in the Ethiopian\nhighlands, Tigray and Amhara. Results show that a transformer-based neural\nnetwork architecture allows for the most robust prediction performance in\nwithheld regions, followed closely by a CatBoost random forest model. Over\nwithheld ground-collection survey labels, the transformer-based model achieves\n96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated\nsamples. Over a larger set of samples independently collected via the\nintroduced method of label supplementation, non-irrigated and irrigated labels\nare predicted with 98.3% and 95.5% accuracy, respectively. The detection model\nis then deployed over Tigray and Amhara, revealing crop rotation patterns and\nyear-over-year irrigated area change. Predictions suggest that irrigated area\nin these two states has decreased by approximately 40% from 2020 to 2021.\n","authors":["Terence Conlon","Christopher Small","Vijay Modi"],"pdf_url":"https://arxiv.org/pdf/2202.04239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12683v2","updated":"2022-03-18T17:07:59Z","published":"2022-01-29T23:31:25Z","title":"A Priori Denoising Strategies for Sparse Identification of Nonlinear\n  Dynamical Systems: A Comparative Study","summary":"  In recent years, identification of nonlinear dynamical systems from data has\nbecome increasingly popular. Sparse regression approaches, such as Sparse\nIdentification of Nonlinear Dynamics (SINDy), fostered the development of novel\ngoverning equation identification algorithms assuming the state variables are\nknown a priori and the governing equations lend themselves to sparse, linear\nexpansions in a (nonlinear) basis of the state variables. In the context of the\nidentification of governing equations of nonlinear dynamical systems, one faces\nthe problem of identifiability of model parameters when state measurements are\ncorrupted by noise. Measurement noise affects the stability of the recovery\nprocess yielding incorrect sparsity patterns and inaccurate estimation of\ncoefficients of the governing equations. In this work, we investigate and\ncompare the performance of several local and global smoothing techniques to a\npriori denoise the state measurements and numerically estimate the state\ntime-derivatives to improve the accuracy and robustness of two sparse\nregression methods to recover governing equations: Sequentially Thresholded\nLeast Squares (STLS) and Weighted Basis Pursuit Denoising (WBPDN) algorithms.\nWe empirically show that, in general, global methods, which use the entire\nmeasurement data set, outperform local methods, which employ a neighboring data\nsubset around a local point. We additionally compare Generalized Cross\nValidation (GCV) and Pareto curve criteria as model selection techniques to\nautomatically estimate near optimal tuning parameters, and conclude that Pareto\ncurves yield better results. The performance of the denoising strategies and\nsparse regression methods is empirically evaluated through well-known benchmark\nproblems of nonlinear dynamical systems.\n","authors":["Alexandre Cortiella","Kwang-Chun Park","Alireza Doostan"],"pdf_url":"https://arxiv.org/pdf/2201.12683v2.pdf","comment":"39 pages, 24 figures, 7 tables"},{"id":"http://arxiv.org/abs/2203.10050v1","updated":"2022-03-18T16:50:38Z","published":"2022-03-18T16:50:38Z","title":"SURF: Semi-supervised Reward Learning with Data Augmentation for\n  Feedback-efficient Preference-based Reinforcement Learning","summary":"  Preference-based reinforcement learning (RL) has shown potential for teaching\nagents to perform the target tasks without a costly, pre-defined reward\nfunction by learning the reward with a supervisor's preference between the two\nagent behaviors. However, preference-based learning often requires a large\namount of human feedback, making it difficult to apply this approach to various\napplications. This data-efficiency problem, on the other hand, has been\ntypically addressed by using unlabeled samples or data augmentation techniques\nin the context of supervised learning. Motivated by the recent success of these\napproaches, we present SURF, a semi-supervised reward learning framework that\nutilizes a large amount of unlabeled samples with data augmentation. In order\nto leverage unlabeled samples for reward learning, we infer pseudo-labels of\nthe unlabeled samples based on the confidence of the preference predictor. To\nfurther improve the label-efficiency of reward learning, we introduce a new\ndata augmentation that temporally crops consecutive subsequences from the\noriginal behaviors. Our experiments demonstrate that our approach significantly\nimproves the feedback-efficiency of the state-of-the-art preference-based\nmethod on a variety of locomotion and robotic manipulation tasks.\n","authors":["Jongjin Park","Younggyo Seo","Jinwoo Shin","Honglak Lee","Pieter Abbeel","Kimin Lee"],"pdf_url":"https://arxiv.org/pdf/2203.10050v1.pdf","comment":"Accepted to ICLR 2022"},{"id":"http://arxiv.org/abs/2203.10045v1","updated":"2022-03-18T16:40:30Z","published":"2022-03-18T16:40:30Z","title":"Risk-Sensitive Bayesian Games for Multi-Agent Reinforcement Learning\n  under Policy Uncertainty","summary":"  In stochastic games with incomplete information, the uncertainty is evoked by\nthe lack of knowledge about a player's own and the other players' types, i.e.\nthe utility function and the policy space, and also the inherent stochasticity\nof different players' interactions. In existing literature, the risk in\nstochastic games has been studied in terms of the inherent uncertainty evoked\nby the variability of transitions and actions. In this work, we instead focus\non the risk associated with the \\textit{uncertainty over types}. We contrast\nthis with the multi-agent reinforcement learning framework where the other\nagents have fixed stationary policies and investigate risk-sensitiveness due to\nthe uncertainty about the other agents' adaptive policies. We propose\nrisk-sensitive versions of existing algorithms proposed for risk-neutral\nstochastic games, such as Iterated Best Response (IBR), Fictitious Play (FP)\nand a general multi-objective gradient approach using dual ascent (DAPG). Our\nexperimental analysis shows that risk-sensitive DAPG performs better than\ncompeting algorithms for both social welfare and general-sum stochastic games.\n","authors":["Hannes Eriksson","Debabrota Basu","Mina Alibeigi","Christos Dimitrakakis"],"pdf_url":"https://arxiv.org/pdf/2203.10045v1.pdf","comment":"5 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2203.10044v1","updated":"2022-03-18T16:38:30Z","published":"2022-03-18T16:38:30Z","title":"Bayesian Low-rank Matrix Completion with Dual-graph Embedding: Prior\n  Analysis and Tuning-free Inference","summary":"  Recently, there is a revival of interest in low-rank matrix completion-based\nunsupervised learning through the lens of dual-graph regularization, which has\nsignificantly improved the performance of multidisciplinary machine learning\ntasks such as recommendation systems, genotype imputation and image inpainting.\nWhile the dual-graph regularization contributes a major part of the success,\ncomputational costly hyper-parameter tunning is usually involved. To circumvent\nsuch a drawback and improve the completion performance, we propose a novel\nBayesian learning algorithm that automatically learns the hyper-parameters\nassociated with dual-graph regularization, and at the same time, guarantees the\nlow-rankness of matrix completion. Notably, a novel prior is devised to promote\nthe low-rankness of the matrix and encode the dual-graph information\nsimultaneously, which is more challenging than the single-graph counterpart. A\nnontrivial conditional conjugacy between the proposed priors and likelihood\nfunction is then explored such that an efficient algorithm is derived under\nvariational inference framework. Extensive experiments using synthetic and\nreal-world datasets demonstrate the state-of-the-art performance of the\nproposed learning algorithm for various data analysis tasks.\n","authors":["Yangge Chen","Lei Cheng","Yik-Chung Wu"],"pdf_url":"https://arxiv.org/pdf/2203.10044v1.pdf","comment":"30 pages, 14 figures"},{"id":"http://arxiv.org/abs/2201.13242v2","updated":"2022-03-18T16:27:49Z","published":"2022-01-31T13:52:51Z","title":"Correcting diacritics and typos with a ByT5 transformer model","summary":"  Due to the fast pace of life and online communications and the prevalence of\nEnglish and the QWERTY keyboard, people tend to forgo using diacritics, make\ntypographical errors (typos) when typing in other languages. Restoring\ndiacritics and correcting spelling is important for proper language use and the\ndisambiguation of texts for both humans and downstream algorithms. However,\nboth of these problems are typically addressed separately: the state-of-the-art\ndiacritics restoration methods do not tolerate other typos, but classical\nspellcheckers also cannot deal adequately with all the diacritics missing. In\nthis work, we tackle both problems at once by employing the newly-developed\nuniversal ByT5 byte-level seq2seq transformer model that requires no\nlanguage-specific model structures. For a comparison, we perform diacritics\nrestoration on benchmark datasets of 12 languages, with the addition of\nLithuanian. The experimental investigation proves that our approach is able to\nachieve results (> 98%) comparable to the previous state-of-the-art, despite\nbeing trained less and on fewer data. Our approach is also able to restore\ndiacritics in words not seen during training with > 76% accuracy. Our\nsimultaneous diacritics restoration and typos correction approach reaches > 94%\nalpha-word accuracy on the 13 languages. It has no direct competitors and\nstrongly outperforms classical spell-checking or dictionary-based approaches.\nWe also demonstrate all the accuracies to further improve with more training.\nTaken together, this shows the great real-world application potential of our\nsuggested methods to more data, languages, and error classes.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius","Jurgita Kapočiūtė-Dzikienė","Monika Briedienė","Tomas Krilavičius"],"pdf_url":"https://arxiv.org/pdf/2201.13242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.06567v2","updated":"2022-03-18T16:27:48Z","published":"2021-12-13T11:20:36Z","title":"Implications of Topological Imbalance for Representation Learning on\n  Biomedical Knowledge Graphs","summary":"  Adoption of recently developed methods from machine learning has given rise\nto creation of drug-discovery knowledge graphs (KG) that utilize the\ninterconnected nature of the domain. Graph-based modelling of the data,\ncombined with KG embedding (KGE) methods, are promising as they provide a more\nintuitive representation and are suitable for inference tasks such as\npredicting missing links. One common application is to produce ranked lists of\ngenes for a given disease, where the rank is based on the perceived likelihood\nof association between the gene and the disease. It is thus critical that these\npredictions are not only pertinent but also biologically meaningful. However,\nKGs can be biased either directly due to the underlying data sources that are\nintegrated or due to modeling choices in the construction of the graph, one\nconsequence of which is that certain entities can get topologically\noverrepresented. We demonstrate the effect of these inherent structural\nimbalances, resulting in densely-connected entities being highly ranked no\nmatter the context. We provide support for this observation across different\ndatasets, models as well as predictive tasks. Further, we present various graph\nperturbation experiments which yield more support to the observation that KGE\nmodels can be more influenced by the frequency of entities rather than any\nbiological information encoded within the relations. Our results highlight the\nimportance of data modeling choices, and emphasizes the need for practitioners\nto be mindful of these issues when interpreting model outputs and during KG\ncomposition.\n","authors":["Stephen Bonner","Ufuk Kirik","Ola Engkvist","Jian Tang","Ian P Barrett"],"pdf_url":"https://arxiv.org/pdf/2112.06567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.12219v2","updated":"2022-03-18T16:23:50Z","published":"2021-12-22T20:45:24Z","title":"SAMCNet for Spatial-configuration-based Classification: A Summary of\n  Results","summary":"  The goal of spatial-configuration-based classification is to build a\nclassifier to distinguish two classes (e.g., responder, non-responder) based on\nthe spatial arrangements (e.g., spatial interactions between different point\ncategories) given multi-category point data from two classes. This problem is\nimportant for generating hypotheses in medical pathology towards discovering\nnew immunotherapies for cancer treatment as well as for other applications in\nbiomedical research and microbial ecology. This problem is challenging due to\nan exponential number of category subsets which may vary in the strength of\nspatial interactions. Most prior efforts on using human selected spatial\nassociation measures may not be sufficient for capturing the relevant (e.g.,\nsurrounded by) spatial interactions which may be of biological significance. In\naddition, the related deep neural networks are limited to category pairs and do\nnot explore larger subsets of point categories. To overcome these limitations,\nwe propose a Spatial-interaction Aware Multi-Category deep neural Network\n(SAMCNet) architecture and contribute novel local reference frame\ncharacterization and point pair prioritization layers for\nspatial-configuration-based classification. Extensive experimental results on\nmultiple cancer datasets show that the proposed architecture provides higher\nprediction accuracy over baseline methods.\n","authors":["Majid Farhadloo","Carl Molnar","Gaoxiang Luo","Yan Li","Shashi Shekhar","Rachel L. Maus","Svetomir N. Markovic","Raymond Moore","Alexey Leontovich"],"pdf_url":"https://arxiv.org/pdf/2112.12219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10036v1","updated":"2022-03-18T16:09:53Z","published":"2022-03-18T16:09:53Z","title":"On the Generalization Mystery in Deep Learning","summary":"  The generalization mystery in deep learning is the following: Why do\nover-parameterized neural networks trained with gradient descent (GD)\ngeneralize well on real datasets even though they are capable of fitting random\ndatasets of comparable size? Furthermore, from among all solutions that fit the\ntraining data, how does GD find one that generalizes well (when such a\nwell-generalizing solution exists)?\n  We argue that the answer to both questions lies in the interaction of the\ngradients of different examples during training. Intuitively, if the\nper-example gradients are well-aligned, that is, if they are coherent, then one\nmay expect GD to be (algorithmically) stable, and hence generalize well. We\nformalize this argument with an easy to compute and interpretable metric for\ncoherence, and show that the metric takes on very different values on real and\nrandom datasets for several common vision networks. The theory also explains a\nnumber of other phenomena in deep learning, such as why some examples are\nreliably learned earlier than others, why early stopping works, and why it is\npossible to learn from noisy labels. Moreover, since the theory provides a\ncausal explanation of how GD finds a well-generalizing solution when one\nexists, it motivates a class of simple modifications to GD that attenuate\nmemorization and improve generalization.\n  Generalization in deep learning is an extremely broad phenomenon, and\ntherefore, it requires an equally general explanation. We conclude with a\nsurvey of alternative lines of attack on this problem, and argue that the\nproposed approach is the most viable one on this basis.\n","authors":["Satrajit Chatterjee","Piotr Zielinski"],"pdf_url":"https://arxiv.org/pdf/2203.10036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10033v1","updated":"2022-03-18T16:03:27Z","published":"2022-03-18T16:03:27Z","title":"Skill-based Multi-objective Reinforcement Learning of Industrial Robot\n  Tasks with Planning and Knowledge Integration","summary":"  In modern industrial settings with small batch sizes it should be easy to set\nup a robot system for a new task. Strategies exist, e.g. the use of skills, but\nwhen it comes to handling forces and torques, these systems often fall short.\nWe introduce an approach that provides a combination of task-level planning\nwith targeted learning of scenario-specific parameters for skill-based systems.\nWe propose the following pipeline: (1) the user provides a task goal in the\nplanning language PDDL, (2) a plan (i.e., a sequence of skills) is generated\nand the learnable parameters of the skills are automatically identified. An\noperator then chooses (3) reward functions and hyperparameters for the learning\nprocess. Two aspects of our methodology are critical: (a) learning is tightly\nintegrated with a knowledge framework to support symbolic planning and to\nprovide priors for learning, (b) using multi-objective optimization. This can\nhelp to balance key performance indicators (KPIs) such as safety and task\nperformance since they can often affect each other. We adopt a multi-objective\nBayesian optimization approach and learn entirely in simulation. We demonstrate\nthe efficacy and versatility of our approach by learning skill parameters for\ntwo different contact-rich tasks. We show their successful execution on a real\n7-DOF KUKA-iiwa manipulator and outperform the manual parameterization by human\nrobot operators.\n","authors":["Matthias Mayr","Faseeh Ahmad","Konstantinos Chatzilygeroudis","Luigi Nardi","Volker Krueger"],"pdf_url":"https://arxiv.org/pdf/2203.10033v1.pdf","comment":"8 pages, 6 figures, submitted to 2022 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2110.01593v4","updated":"2022-03-18T15:47:49Z","published":"2021-10-04T17:41:53Z","title":"Generalized Kernel Thinning","summary":"  The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a\nprobability distribution more effectively than independent sampling by\ntargeting a reproducing kernel Hilbert space (RKHS) and leveraging a less\nsmooth square-root kernel. Here we provide four improvements. First, we show\nthat KT applied directly to the target RKHS yields tighter, dimension-free\nguarantees for any kernel, any distribution, and any fixed function in the\nRKHS. Second, we show that, for analytic kernels like Gaussian, inverse\nmultiquadric, and sinc, target KT admits maximum mean discrepancy (MMD)\nguarantees comparable to or better than those of square-root KT without making\nexplicit use of a square-root kernel. Third, we prove that KT with a fractional\npower kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth\nkernels, like Laplace and Mat\\'ern, that do not have square-roots. Fourth, we\nestablish that KT applied to a sum of the target and power kernels (a procedure\nwe call KT+) simultaneously inherits the improved MMD guarantees of power KT\nand the tighter individual function guarantees of target KT. In our experiments\nwith target KT and KT+, we witness significant improvements in integration\nerror even in $100$ dimensions and when compressing challenging differential\nequation posteriors.\n","authors":["Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2110.01593v4.pdf","comment":"Published in ICLR 2022"},{"id":"http://arxiv.org/abs/2203.10014v1","updated":"2022-03-18T15:26:05Z","published":"2022-03-18T15:26:05Z","title":"Parametric Scaling of Preprocessing assisted U-net Architecture for\n  Improvised Retinal Vessel Segmentation","summary":"  Extracting blood vessels from retinal fundus images plays a decisive role in\ndiagnosing the progression in pertinent diseases. In medical image analysis,\nvessel extraction is a semantic binary segmentation problem, where blood\nvasculature needs to be extracted from the background. Here, we present an\nimage enhancement technique based on the morphological preprocessing coupled\nwith a scaled U-net architecture. Despite a relatively less number of trainable\nnetwork parameters, the scaled version of U-net architecture provides better\nperformance compare to other methods in the domain. We validated the proposed\nmethod on retinal fundus images from the DRIVE database. A significant\nimprovement as compared to the other algorithms in the domain, in terms of the\narea under ROC curve (>0.9762) and classification accuracy (>95.47%) are\nevident from the results. Furthermore, the proposed method is resistant to the\ncentral vessel reflex while sensitive to detect blood vessels in the presence\nof background items viz. exudates, optic disc, and fovea.\n","authors":["Kundan Kumar","Sumanshu Agarwal"],"pdf_url":"https://arxiv.org/pdf/2203.10014v1.pdf","comment":"10 pages, 5 figures, ICAIHC-2022"},{"id":"http://arxiv.org/abs/2203.10012v1","updated":"2022-03-18T15:21:11Z","published":"2022-03-18T15:21:11Z","title":"Report from the NSF Future Directions Workshop on Automatic Evaluation\n  of Dialog: Research Directions and Challenges","summary":"  This is a report on the NSF Future Directions Workshop on Automatic\nEvaluation of Dialog. The workshop explored the current state of the art along\nwith its limitations and suggested promising directions for future work in this\nimportant and very rapidly changing area of research.\n","authors":["Shikib Mehri","Jinho Choi","Luis Fernando D'Haro","Jan Deriu","Maxine Eskenazi","Milica Gasic","Kallirroi Georgila","Dilek Hakkani-Tur","Zekang Li","Verena Rieser","Samira Shaikh","David Traum","Yi-Ting Yeh","Zhou Yu","Yizhe Zhang","Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10012v1.pdf","comment":"Report from the NSF AED Workshop (http://dialrc.org/AED/)"},{"id":"http://arxiv.org/abs/2203.10009v1","updated":"2022-03-18T15:18:55Z","published":"2022-03-18T15:18:55Z","title":"Analyzing EEG Data with Machine and Deep Learning: A Benchmark","summary":"  Nowadays, machine and deep learning techniques are widely used in different\nareas, ranging from economics to biology. In general, these techniques can be\nused in two ways: trying to adapt well-known models and architectures to the\navailable data, or designing custom architectures. In both cases, to speed up\nthe research process, it is useful to know which type of models work best for a\nspecific problem and/or data type. By focusing on EEG signal analysis, and for\nthe first time in literature, in this paper a benchmark of machine and deep\nlearning for EEG signal classification is proposed. For our experiments we used\nthe four most widespread models, i.e., multilayer perceptron, convolutional\nneural network, long short-term memory, and gated recurrent unit, highlighting\nwhich one can be a good starting point for developing EEG classification\nmodels.\n","authors":["Danilo Avola","Marco Cascio","Luigi Cinque","Alessio Fagioli","Gian Luca Foresti","Marco Raoul Marini","Daniele Pannone"],"pdf_url":"https://arxiv.org/pdf/2203.10009v1.pdf","comment":"conference, 11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2105.06868v3","updated":"2022-03-18T15:17:30Z","published":"2021-05-14T14:53:30Z","title":"Priors in Bayesian Deep Learning: A Review","summary":"  While the choice of prior is one of the most critical parts of the Bayesian\ninference workflow, recent Bayesian deep learning models have often fallen back\non vague priors, such as standard Gaussians. In this review, we highlight the\nimportance of prior choices for Bayesian deep learning and present an overview\nof different priors that have been proposed for (deep) Gaussian processes,\nvariational autoencoders, and Bayesian neural networks. We also outline\ndifferent methods of learning priors for these models from data. We hope to\nmotivate practitioners in Bayesian deep learning to think more carefully about\nthe prior specification for their models and to provide them with some\ninspiration in this regard.\n","authors":["Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2105.06868v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.03625v3","updated":"2022-03-18T15:16:42Z","published":"2021-08-08T12:47:42Z","title":"Unifying Heterogeneous Electronic Health Records Systems via Text-Based\n  Code Embedding","summary":"  Substantial increase in the use of Electronic Health Records (EHRs) has\nopened new frontiers for predictive healthcare. However, while EHR systems are\nnearly ubiquitous, they lack a unified code system for representing medical\nconcepts. Heterogeneous formats of EHR present a substantial barrier for the\ntraining and deployment of state-of-the-art deep learning models at scale. To\novercome this problem, we introduce Description-based Embedding, DescEmb, a\ncode-agnostic description-based representation learning framework for\npredictive modeling on EHR. DescEmb takes advantage of the flexibility of\nneural language understanding models while maintaining a neutral approach that\ncan be combined with prior frameworks for task-specific representation learning\nor predictive modeling. We tested our model's capacity on various experiments\nincluding prediction tasks, transfer learning and pooled learning. DescEmb\nshows higher performance in overall experiments compared to code-based\napproach, opening the door to a text-based approach in predictive healthcare\nresearch that is not constrained by EHR structure nor special domain knowledge.\n","authors":["Kyunghoon Hur","Jiyoung Lee","Jungwoo Oh","Wesley Price","Young-Hak Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2108.03625v3.pdf","comment":"Accepted at CHIL 2022. Main paper + supplementary material (21 pages,\n  8 figures, 12 tables)"},{"id":"http://arxiv.org/abs/2203.06889v2","updated":"2022-03-18T15:15:53Z","published":"2022-03-14T07:10:39Z","title":"Lead-agnostic Self-supervised Learning for Local and Global\n  Representations of Electrocardiogram","summary":"  In recent years, self-supervised learning methods have shown significant\nimprovement for pre-training with unlabeled data and have proven helpful for\nelectrocardiogram signals. However, most previous pre-training methods for\nelectrocardiogram focused on capturing only global contextual representations.\nThis inhibits the models from learning fruitful representation of\nelectrocardiogram, which results in poor performance on downstream tasks.\nAdditionally, they cannot fine-tune the model with an arbitrary set of\nelectrocardiogram leads unless the models were pre-trained on the same set of\nleads. In this work, we propose an ECG pre-training method that learns both\nlocal and global contextual representations for better generalizability and\nperformance on downstream tasks. In addition, we propose random lead masking as\nan ECG-specific augmentation method to make our proposed model robust to an\narbitrary set of leads. Experimental results on two downstream tasks, cardiac\narrhythmia classification and patient identification, show that our proposed\napproach outperforms other state-of-the-art methods.\n","authors":["Jungwoo Oh","Hyunseung Chung","Joon-myoung Kwon","Dong-gyun Hong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2203.06889v2.pdf","comment":"Accepted at CHIL 2022 (16 pages, 3 figures, 4 tables)"},{"id":"http://arxiv.org/abs/2203.10006v1","updated":"2022-03-18T15:14:13Z","published":"2022-03-18T15:14:13Z","title":"Ultra-low Latency Spiking Neural Networks with Spatio-Temporal\n  Compression and Synaptic Convolutional Block","summary":"  Spiking neural networks (SNNs), as one of the brain-inspired models, has\nspatio-temporal information processing capability, low power feature, and high\nbiological plausibility. The effective spatio-temporal feature makes it\nsuitable for event streams classification. However, neuromorphic datasets, such\nas N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events\ninto frames with a new higher temporal resolution for event stream\nclassification, which causes high training and inference latency. In this work,\nwe proposed a spatio-temporal compression method to aggregate individual events\ninto a few time steps of synaptic current to reduce the training and inference\nlatency. To keep the accuracy of SNNs under high compression ratios, we also\nproposed a synaptic convolutional block to balance the dramatic change between\nadjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with\nlearnable membrane time constant is introduced to increase its information\nprocessing capability. We evaluate the proposed method for event streams\nclassification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture\ndatasets. The experiment results show that our proposed method outperforms the\nstate-of-the-art accuracy on nearly all datasets, using fewer time steps.\n","authors":["Changqing Xu","Yi Liu","Yintang Yang"],"pdf_url":"https://arxiv.org/pdf/2203.10006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08898v2","updated":"2022-03-18T15:08:42Z","published":"2022-03-16T19:20:37Z","title":"Neural network processing of holographic images","summary":"  HOLODEC, an airborne cloud particle imager, captures holographic images of a\nfixed volume of cloud to characterize the types and sizes of cloud particles,\nsuch as water droplets and ice crystals. Cloud particle properties include\nposition, diameter, and shape. We present a hologram processing algorithm,\nHolodecML, that utilizes a neural segmentation model, GPUs, and computational\nparallelization. HolodecML is trained using synthetically generated holograms\nbased on a model of the instrument, and predicts masks around particles found\nwithin reconstructed images. From these masks, the position and size of the\ndetected particles can be characterized in three dimensions. In order to\nsuccessfully process real holograms, we find we must apply a series of image\ncorrupting transformations and noise to the synthetic images used in training.\n  In this evaluation, HolodecML had comparable position and size estimation\nperformance to the standard processing method, but improved particle detection\nby nearly 20\\% on several thousand manually labeled HOLODEC images. However,\nthe improvement only occurred when image corruption was performed on the\nsimulated images during training, thereby mimicking non-ideal conditions in the\nactual probe. The trained model also learned to differentiate artifacts and\nother impurities in the HOLODEC images from the particles, even though no such\nobjects were present in the training data set, while the standard processing\nmethod struggled to separate particles from artifacts. The novelty of the\ntraining approach, which leveraged noise as a means for parameterizing\nnon-ideal aspects of the HOLODEC detector, could be applied in other domains\nwhere the theoretical model is incapable of fully describing the real-world\noperation of the instrument and accurate truth data required for supervised\nlearning cannot be obtained from real-world observations.\n","authors":["John S. Schreck","Gabrielle Gantos","Matthew Hayman","Aaron Bansemer","David John Gagne"],"pdf_url":"https://arxiv.org/pdf/2203.08898v2.pdf","comment":"38 pages, 15 figures. Submitted to Atmospheric Measurement Techniques"},{"id":"http://arxiv.org/abs/2203.10005v1","updated":"2022-03-18T15:07:55Z","published":"2022-03-18T15:07:55Z","title":"Application of Top-hat Transformation for Enhanced Blood Vessel\n  Extraction","summary":"  In the medical domain, different computer-aided diagnosis systems have been\nproposed to extract blood vessels from retinal fundus images for the clinical\ntreatment of vascular diseases. Accurate extraction of blood vessels from the\nfundus images using a computer-generated method can help the clinician to\nproduce timely and accurate reports for the patient suffering from these\ndiseases. In this article, we integrate top-hat based preprocessing approach\nwith fine-tuned B-COSFIRE filter to achieve more accurate segregation of blood\nvessel pixels from the background. The use of top-hat transformation in the\npreprocessing stage enhances the efficacy of the algorithm to extract blood\nvessels in presence of structures like fovea, exudates, haemorrhages, etc.\nFurthermore, to reduce the false positives, small clusters of blood vessel\npixels are removed in the postprocessing stage. Further, we find that the\nproposed algorithm is more efficient as compared to various modern algorithms\nreported in the literature.\n","authors":["Tithi Parna Das","Sheetal Praharaj","Sarita Swain","Sumanshu Agarwal","Kundan Kumar"],"pdf_url":"https://arxiv.org/pdf/2203.10005v1.pdf","comment":"9 pages, 3 figures, ICAIHC-2022"},{"id":"http://arxiv.org/abs/2105.10377v3","updated":"2022-03-18T15:03:46Z","published":"2021-05-21T14:36:39Z","title":"Adaptive Filters in Graph Convolutional Neural Networks","summary":"  Over the last few years, we have witnessed the availability of an increasing\ndata generated from non-Euclidean domains, which are usually represented as\ngraphs with complex relationships, and Graph Neural Networks (GNN) have gained\na high interest because of their potential in processing graph-structured data.\nIn particular, there is a strong interest in exploring the possibilities in\nperforming convolution on graphs using an extension of the GNN architecture,\ngenerally referred to as Graph Convolutional Neural Networks (ConvGNN).\nConvolution on graphs has been achieved mainly in two forms: spectral and\nspatial convolutions. Due to the higher flexibility in exploring and exploiting\nthe graph structure of data, there is recently an increasing interest in\ninvestigating the possibilities that the spatial approach can offer. The idea\nof finding a way to adapt the network behaviour to the inputs they process to\nmaximize the total performances has aroused much interest in the neural\nnetworks literature over the years. This paper presents a novel method to adapt\nthe behaviour of a ConvGNN to the input proposing a method to perform spatial\nconvolution on graphs using input-specific filters, which are dynamically\ngenerated from nodes feature vectors. The experimental assessment confirms the\ncapabilities of the proposed approach, which achieves satisfying results using\na low number of filters.\n","authors":["Andrea Apicella","Francesco Isgrò","Andrea Pollastro","Roberto Prevete"],"pdf_url":"https://arxiv.org/pdf/2105.10377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10001v1","updated":"2022-03-18T15:01:32Z","published":"2022-03-18T15:01:32Z","title":"FORCE: A Framework of Rule-Based Conversational Recommender System","summary":"  The conversational recommender systems (CRSs) have received extensive\nattention in recent years. However, most of the existing works focus on various\ndeep learning models, which are largely limited by the requirement of\nlarge-scale human-annotated datasets. Such methods are not able to deal with\nthe cold-start scenarios in industrial products. To alleviate the problem, we\npropose FORCE, a Framework Of Rule-based Conversational Recommender system that\nhelps developers to quickly build CRS bots by simple configuration. We conduct\nexperiments on two datasets in different languages and domains to verify its\neffectiveness and usability.\n","authors":["Jun Quan","Ze Wei","Qiang Gan","Jingqi Yao","Jingyi Lu","Yuchen Dong","Yiming Liu","Yi Zeng","Chao Zhang","Yongzhi Li","Huang Hu","Yingying He","Yang Yang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.10001v1.pdf","comment":"AAAI 2022 (Demonstration Track)"},{"id":"http://arxiv.org/abs/2203.09994v1","updated":"2022-03-18T14:45:42Z","published":"2022-03-18T14:45:42Z","title":"Graph-Text Multi-Modal Pre-training for Medical Representation Learning","summary":"  As the volume of Electronic Health Records (EHR) sharply grows, there has\nbeen emerging interest in learning the representation of EHR for healthcare\napplications. Representation learning of EHR requires appropriate modeling of\nthe two dominant modalities in EHR: structured data and unstructured text. In\nthis paper, we present MedGTX, a pre-trained model for multi-modal\nrepresentation learning of the structured and textual EHR data. MedGTX uses a\nnovel graph encoder to exploit the graphical nature of structured EHR data, and\na text encoder to handle unstructured text, and a cross-modal encoder to learn\na joint representation space. We pre-train our model through four proxy tasks\non MIMIC-III, an open-source EHR data, and evaluate our model on two clinical\nbenchmarks and three novel downstream tasks which tackle real-world problems in\nEHR data. The results consistently show the effectiveness of pre-training the\nmodel for joint representation of both structured and unstructured information\nfrom EHR. Given the promising performance of MedGTX, we believe this work opens\na new door to jointly understanding the two fundamental modalities of EHR data.\n","authors":["Sungjin Park","Seongsu Bae","Jiho Kim","Tackeun Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2203.09994v1.pdf","comment":"To appear in Proceedings of the Conference on Health, Inference, and\n  Learning (CHIL 2022)"},{"id":"http://arxiv.org/abs/2203.09992v1","updated":"2022-03-18T14:39:42Z","published":"2022-03-18T14:39:42Z","title":"Diffusion and Volume Maximization-Based Clustering of Highly Mixed\n  Hyperspectral Images","summary":"  Hyperspectral images of a scene or object are a rich data source, often\nencoding a hundred or more spectral bands of reflectance at each pixel. Despite\nbeing very high-dimensional, these images typically encode latent\nlow-dimensional structure that can be exploited for material discrimination.\nHowever, due to an inherent trade-off between spectral and spatial resolution,\nmany hyperspectral images are generated at a coarse spatial scale, and single\npixels may correspond to spatial regions containing multiple materials. This\narticle introduces the \\emph{Diffusion and Volume maximization-based Image\nClustering} (\\emph{D-VIC}) algorithm for unsupervised material discrimination.\nD-VIC locates cluster modes -- high-density, high-purity pixels in the\nhyperspectral image that are far in diffusion distance (a data-dependent\ndistance metric) from other high-density, high-purity pixels -- and assigns\nthese pixels unique labels, as these points are meant to exemplify underlying\nmaterial structure. Non-modal pixels are labeled according to their diffusion\ndistance nearest neighbor of higher density and purity that is already labeled.\nBy directly incorporating pixel purity into its modal and non-modal labeling,\nD-VIC upweights pixels that correspond to a spatial region containing just a\nsingle material, yielding more interpretable clusterings. D-VIC is shown to\noutperform baseline and comparable state-of-the-art methods in extensive\nnumerical experiments on a range of hyperspectral images, implying that it is\nwell-equipped for material discrimination and clustering of these data.\n","authors":["Sam L. Polk","Kangning Cui","Robert J. Plemmons","James M. Murphy"],"pdf_url":"https://arxiv.org/pdf/2203.09992v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2110.06468v2","updated":"2022-03-18T14:39:26Z","published":"2021-10-13T03:06:02Z","title":"Graph-Fraudster: Adversarial Attacks on Graph Neural Network Based\n  Vertical Federated Learning","summary":"  Graph neural network (GNN) has achieved great success on graph representation\nlearning. Challenged by large scale private data collected from user-side, GNN\nmay not be able to reflect the excellent performance, without rich features and\ncomplete adjacent relationships. Addressing the problem, vertical federated\nlearning (VFL) is proposed to implement local data protection through training\na global model collaboratively. Consequently, for graph-structured data, it is\na natural idea to construct a GNN based VFL framework, denoted as GVFL.\nHowever, GNN has been proved vulnerable to adversarial attacks. Whether the\nvulnerability will be brought into the GVFL has not been studied. This is the\nfirst study of adversarial attacks on GVFL. A novel adversarial attack method\nis proposed, named Graph-Fraudster. It generates adversarial perturbations\nbased on the noise-added global node embeddings via the privacy leakage and the\ngradient of pairwise node. Specifically, first, Graph-Fraudster steals the\nglobal node embeddings and sets up a shadow model of the server for the attack\ngenerator. Second, noise is added into node embeddings to confuse the shadow\nmodel. At last, the gradient of pairwise node is used to generate attacks with\nthe guidance of noise-added node embeddings. Extensive experiments on five\nbenchmark datasets demonstrate that Graph-Fraudster achieves the\nstate-of-the-art attack performance compared with baselines in different GNN\nbased GVFLs. Furthermore, Graph-Fraudster can remain a threat to GVFL even if\ntwo possible defense mechanisms are applied. Additionally, some suggestions are\nput forward for the future work to improve the robustness of GVFL. The code and\ndatasets can be downloaded at https://github.com/hgh0545/Graph-Fraudster.\n","authors":["Jinyin Chen","Guohan Huang","Haibin Zheng","Shanqing Yu","Wenrong Jiang","Chen Cui"],"pdf_url":"https://arxiv.org/pdf/2110.06468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08937v2","updated":"2022-03-18T14:36:36Z","published":"2022-03-16T20:50:24Z","title":"Backpropagation through Time and Space: Learning Numerical Methods with\n  Multi-Agent Reinforcement Learning","summary":"  We introduce Backpropagation Through Time and Space (BPTTS), a method for\ntraining a recurrent spatio-temporal neural network, that is used in a\nhomogeneous multi-agent reinforcement learning (MARL) setting to learn\nnumerical methods for hyperbolic conservation laws. We treat the numerical\nschemes underlying partial differential equations (PDEs) as a Partially\nObservable Markov Game (POMG) in Reinforcement Learning (RL). Similar to\nnumerical solvers, our agent acts at each discrete location of a computational\nspace for efficient and generalizable learning. To learn higher-order spatial\nmethods by acting on local states, the agent must discern how its actions at a\ngiven spatiotemporal location affect the future evolution of the state. The\nmanifestation of this non-stationarity is addressed by BPTTS, which allows for\nthe flow of gradients across both space and time. The learned numerical\npolicies are comparable to the SOTA numerics in two settings, the Burgers'\nEquation and the Euler Equations, and generalize well to other simulation\nset-ups.\n","authors":["Elliot Way","Dheeraj S. K. Kapilavai","Yiwei Fu","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2203.08937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.06534v2","updated":"2022-03-18T14:23:08Z","published":"2021-07-14T08:01:30Z","title":"Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained\n  Optimization","summary":"  This paper considers stochastic convex optimization problems with two sets of\nconstraints: (a) deterministic constraints on the domain of the optimization\nvariable, which are difficult to project onto; and (b) deterministic or\nstochastic constraints that admit efficient projection. Problems of this form\narise frequently in the context of semidefinite programming as well as when\nvarious NP-hard problems are solved approximately via semidefinite relaxation.\nSince projection onto the first set of constraints is difficult, it becomes\nnecessary to explore projection-free algorithms, such as the stochastic\nFrank-Wolfe (FW) algorithm. On the other hand, the second set of constraints\ncannot be handled in the same way, and must be incorporated as an indicator\nfunction within the objective function, thereby complicating the application of\nFW methods. Similar problems have been studied before; however, they suffer\nfrom slow convergence rates. This work, equipped with momentum based gradient\ntracking technique, guarantees fast convergence rates on par with the\nbest-known rates for problems without the second set of constraints.\nZeroth-order variants of the proposed algorithms are also developed and again\nimprove upon the state-of-the-art rate results. We further propose the novel\ntrimmed FW variants that enjoy the same convergence rates as their classical\ncounterparts, but are empirically shown to require significantly fewer calls to\nthe linear minimization oracle speeding up the overall algorithm. The efficacy\nof the proposed algorithms is tested on relevant applications of sparse matrix\nestimation, clustering via semidefinite relaxation, and uniform sparsest cut\nproblem.\n","authors":["Zeeshan Akhtar","Ketan Rajawat"],"pdf_url":"https://arxiv.org/pdf/2107.06534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09981v1","updated":"2022-03-18T14:17:48Z","published":"2022-03-18T14:17:48Z","title":"Image Storage on Synthetic DNA Using Autoencoders","summary":"  Over the past years, the ever-growing trend on data storage demand, more\nspecifically for \"cold\" data (rarely accessed data), has motivated research for\nalternative systems of data storage. Because of its biochemical\ncharacteristics, synthetic DNA molecules are now considered as serious\ncandidates for this new kind of storage. This paper presents some results on\nlossy image compression methods based on convolutional autoencoders adapted to\nDNA data storage.\n  The model architectures presented here have been designed to efficiently\ncompress images, encode them into a quaternary code, and finally store them\ninto synthetic DNA molecules. This work also aims at making the compression\nmodels better fit the problematics that we encounter when storing data into\nDNA, namely the fact that the DNA writing, storing and reading methods are\nerror prone processes. The main take away of this kind of compressive\nautoencoder is our quantization and the robustness to substitution errors\nthanks to the noise model that we use during training.\n","authors":["Xavier Pic","Marc Antonini"],"pdf_url":"https://arxiv.org/pdf/2203.09981v1.pdf","comment":"Submitted to ICIP 2022"},{"id":"http://arxiv.org/abs/2203.09978v1","updated":"2022-03-18T14:12:54Z","published":"2022-03-18T14:12:54Z","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series\n  Tasks","summary":"  Machine learning models often fail to generalize well under distributional\nshifts. Understanding and overcoming these failures have led to a research\nfield of Out-of-Distribution (OOD) generalization. Despite being extensively\nstudied for static computer vision tasks, OOD generalization has been\nunderexplored for time series tasks. To shine light on this gap, we present\nWOODS: eight challenging open-source time series benchmarks covering a diverse\nrange of data modalities, such as videos, brain recordings, and sensor signals.\nWe revise the existing OOD generalization algorithms for time series tasks and\nevaluate them using our systematic framework. Our experiments show a large room\nfor improvement for empirical risk minimization and OOD generalization\nalgorithms on our datasets, thus underscoring the new challenges posed by time\nseries tasks. Code and documentation are available at\nhttps://woods-benchmarks.github.io .\n","authors":["Jean-Christophe Gagnon-Audet","Kartik Ahuja","Mohammad-Javad Darvishi-Bayazi","Guillaume Dumas","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2203.09978v1.pdf","comment":"43 pages, 20 figures"},{"id":"http://arxiv.org/abs/2203.09975v1","updated":"2022-03-18T14:09:22Z","published":"2022-03-18T14:09:22Z","title":"BIOS: An Algorithmically Generated Biomedical Knowledge Graph","summary":"  Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for\nbiomedical and healthcare big data and artificial intelligence (AI),\nfacilitating natural language processing, model development, and data exchange.\nFor many decades, these knowledge graphs have been built via expert curation,\nwhich can no longer catch up with the speed of today's AI development, and a\ntransition to algorithmically generated BioMedKGs is necessary. In this work,\nwe introduce the Biomedical Informatics Ontology System (BIOS), the first large\nscale publicly available BioMedKG that is fully generated by machine learning\nalgorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in\ntwo languages, and 7.3 million relation triplets. We introduce the methodology\nfor developing BIOS, which covers curation of raw biomedical terms,\ncomputationally identifying synonymous terms and aggregating them to create\nconcept nodes, semantic type classification of the concepts, relation\nidentification, and biomedical machine translation. We provide statistics about\nthe current content of BIOS and perform preliminary assessment for term\nquality, synonym grouping, and relation extraction. Results suggest that\nmachine learning-based BioMedKG development is a totally viable solution for\nreplacing traditional expert curation.\n","authors":["Sheng Yu","Zheng Yuan","Jun Xia","Shengxuan Luo","Huaiyuan Ying","Sihang Zeng","Jingyi Ren","Hongyi Yuan","Zhengyun Zhao","Yucong Lin","Keming Lu","Jing Wang","Yutao Xie","Heung-Yeung Shum"],"pdf_url":"https://arxiv.org/pdf/2203.09975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09963v1","updated":"2022-03-18T13:59:02Z","published":"2022-03-18T13:59:02Z","title":"Towards Lithuanian grammatical error correction","summary":"  Everyone wants to write beautiful and correct text, yet the lack of language\nskills, experience, or hasty typing can result in errors. By employing the\nrecent advances in transformer architectures, we construct a grammatical error\ncorrection model for Lithuanian, the language rich in archaic features. We\ncompare subword and byte-level approaches and share our best trained model,\nachieving F$_{0.5}$=0.92, and accompanying code, in an online open-source\nrepository.\n","authors":["Lukas Stankevičius","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2203.09963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09962v1","updated":"2022-03-18T13:57:17Z","published":"2022-03-18T13:57:17Z","title":"SS-SAM : Stochastic Scheduled Sharpness-Aware Minimization for\n  Efficiently Training Deep Neural Networks","summary":"  By driving optimizers to converge to flat minima, sharpness-aware\nminimization (SAM) has shown the power to improve the model generalization.\nHowever, SAM requires to perform two forward-backward propagations for one\nparameter update, which largely burdens the practical computation. In this\npaper, we propose a novel and efficient training scheme, called Stochastic\nScheduled SAM (SS-SAM). Specifically, in SS-SAM, the optimizer is arranged by a\npredefined scheduling function to perform a random trial at each update step,\nwhich would randomly select to perform the SGD optimization or the SAM\noptimization. In this way, the overall count of propagation pair could be\nlargely reduced. Then, we empirically investigate four typical types of\nscheduling functions, and demonstrates the computational efficiency and their\nimpact on model performance respectively. We show that with proper scheduling\nfunctions, models could be trained to achieve comparable or even better\nperformance with much lower computation cost compared to models trained with\nonly SAM training scheme.\n","authors":["Yang Zhao","Hao Zhang","Xiuyuan Hu"],"pdf_url":"https://arxiv.org/pdf/2203.09962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08344v2","updated":"2022-03-18T13:35:04Z","published":"2022-03-16T01:32:21Z","title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild","summary":"  We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n","authors":["Takehiko Ohkawa","Yu-Jhe Li","Qichen Fu","Ryosuke Furuta","Kris M. Kitani","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2203.08344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09940v1","updated":"2022-03-18T13:25:18Z","published":"2022-03-18T13:25:18Z","title":"Defending Variational Autoencoders from Adversarial Attacks with MCMC","summary":"  Variational autoencoders (VAEs) are deep generative models used in various\ndomains. VAEs can generate complex objects and provide meaningful latent\nrepresentations, which can be further used in downstream tasks such as\nclassification. As previous work has shown, one can easily fool VAEs to produce\nunexpected latent representations and reconstructions for a visually slightly\nmodified input. Here, we examine several objective functions for adversarial\nattacks construction, suggest metrics assess the model robustness, and propose\na solution to alleviate the effect of an attack. Our method utilizes the Markov\nChain Monte Carlo (MCMC) technique in the inference step and is motivated by\nour theoretical analysis. Thus, we do not incorporate any additional costs\nduring training or we do not decrease the performance on non-attacked inputs.\nWe validate our approach on a variety of datasets (MNIST, Fashion MNIST, Color\nMNIST, CelebA) and VAE configurations ($\\beta$-VAE, NVAE, TC-VAE) and show that\nit consistently improves the model robustness to adversarial attacks.\n","authors":["Anna Kuzina","Max Welling","Jakub M. Tomczak"],"pdf_url":"https://arxiv.org/pdf/2203.09940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09914v1","updated":"2022-03-18T12:47:49Z","published":"2022-03-18T12:47:49Z","title":"Comparing SONN Types for Efficient Robot Motion Planning in the\n  Configuration Space","summary":"  Motion planning in the configuration space (C-space) induces benefits, such\nas smooth trajectories. It becomes more complex as the degrees of freedom (DOF)\nincrease. This is due to the direct relation between the dimensionality of the\nsearch space and the DOF. Self-organizing neural networks (SONN) and their\nfamous candidate, the Self-Organizing Map, have been proven to be useful tools\nfor C-space reduction while preserving its underlying topology, as presented in\n[29]. In this work, we extend our previous study with additional models and\nadapt the approach from human motion data towards robots' kinematics. The\nevaluation includes the best performant models from [29] and three additional\nSONN architectures, representing the consequent continuation of this previous\nwork. Generated Trajectories, planned with the different SONN models, were\nsuccessfully tested in a robot simulation.\n","authors":["Lea Steffen","Tobias Weyer","Katharina Glueck","Stefan Ulbrich","Arne Roennau","Rüdiger Dillmann"],"pdf_url":"https://arxiv.org/pdf/2203.09914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09913v1","updated":"2022-03-18T12:44:44Z","published":"2022-03-18T12:44:44Z","title":"Convolutional Simultaneous Sparse Approximation with Applications to\n  RGB-NIR Image Fusion","summary":"  Simultaneous sparse approximation (SSA) seeks to represent a set of dependent\nsignals using sparse vectors with identical supports. The SSA model has been\nused in various signal and image processing applications involving multiple\ncorrelated input signals. In this paper, we propose algorithms for\nconvolutional SSA (CSSA) based on the alternating direction method of\nmultipliers. Specifically, we address the CSSA problem with different sparsity\nstructures and the convolutional feature learning problem in multimodal\ndata/signals based on the SSA model. We evaluate the proposed algorithms by\napplying them to multimodal and multifocus image fusion problems.\n","authors":["Farshad G. Veshki","Sergiy A. Vorobyov"],"pdf_url":"https://arxiv.org/pdf/2203.09913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09911v1","updated":"2022-03-18T12:39:35Z","published":"2022-03-18T12:39:35Z","title":"Why we need biased AI -- How including cognitive and ethical machine\n  biases can enhance AI systems","summary":"  This paper stresses the importance of biases in the field of artificial\nintelligence (AI) in two regards. First, in order to foster efficient\nalgorithmic decision-making in complex, unstable, and uncertain real-world\nenvironments, we argue for the structurewise implementation of human cognitive\nbiases in learning algorithms. Secondly, we argue that in order to achieve\nethical machine behavior, filter mechanisms have to be applied for selecting\nbiased training stimuli that represent social or behavioral traits that are\nethically desirable. We use insights from cognitive science as well as ethics\nand apply them to the AI field, combining theoretical considerations with seven\ncase studies depicting tangible bias implementation scenarios. Ultimately, this\npaper is the first tentative step to explicitly pursue the idea of a\nre-evaluation of the ethical significance of machine biases, as well as putting\nthe idea forth to implement cognitive biases into machines.\n","authors":["Sarah Fabi","Thilo Hagendorff"],"pdf_url":"https://arxiv.org/pdf/2203.09911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.08176v2","updated":"2022-03-18T12:26:15Z","published":"2022-02-16T16:27:00Z","title":"Bias and unfairness in machine learning models: a systematic literature\n  review","summary":"  One of the difficulties of artificial intelligence is to ensure that model\ndecisions are fair and free of bias. In research, datasets, metrics,\ntechniques, and tools are applied to detect and mitigate algorithmic unfairness\nand bias. This study aims to examine existing knowledge on bias and unfairness\nin Machine Learning models, identifying mitigation methods, fairness metrics,\nand supporting tools. A Systematic Literature Review found 40 eligible articles\npublished between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and\nGoogle Scholar knowledge bases. The results show numerous bias and unfairness\ndetection and mitigation approaches for ML technologies, with clearly defined\nmetrics in the literature, and varied metrics can be highlighted. We recommend\nfurther research to define the techniques and metrics that should be employed\nin each case to standardize and ensure the impartiality of the machine learning\nmodel, thus, allowing the most appropriate metric to detect bias and unfairness\nin a given context.\n","authors":["Tiago Palma Pagano","Rafael Bessa Loureiro","Maira Matos Araujo","Fernanda Vitoria Nascimento Lisboa","Rodrigo Matos Peixoto","Guilherme Aragao de Sousa Guimaraes","Lucas Lisboa dos Santos","Gustavo Oliveira Ramos Cruz","Ewerton Lopes Silva de Oliveira","Marco Cruz","Ingrid Winkler","Erick Giovani Sperandio Nascimento"],"pdf_url":"https://arxiv.org/pdf/2202.08176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12634v2","updated":"2022-03-18T12:14:34Z","published":"2022-02-25T11:51:45Z","title":"Deep Dirichlet uncertainty for unsupervised out-of-distribution\n  detection of eye fundus photographs in glaucoma screening","summary":"  The development of automatic tools for early glaucoma diagnosis with color\nfundus photographs can significantly reduce the impact of this disease.\nHowever, current state-of-the-art solutions are not robust to real-world\nscenarios, providing over-confident predictions for out-of-distribution cases.\nWith this in mind, we propose a model based on the Dirichlet distribution that\nallows to obtain class-wise probabilities together with an uncertainty\nestimation without exposure to out-of-distribution cases. We demonstrate our\napproach on the AIROGS challenge. At the start of the final test phase (8 Feb.\n2022), our method had the highest average score among all submissions.\n","authors":["Teresa Araújo","Guilherme Aresta","Hrvoje Bogunovic"],"pdf_url":"https://arxiv.org/pdf/2202.12634v2.pdf","comment":"Submitted to ISBI 2022"},{"id":"http://arxiv.org/abs/2203.09893v1","updated":"2022-03-18T12:07:36Z","published":"2022-03-18T12:07:36Z","title":"A Lightweight Instrument-Agnostic Model for Polyphonic Note\n  Transcription and Multipitch Estimation","summary":"  Automatic Music Transcription (AMT) has been recognized as a key enabling\ntechnology with a wide range of applications. Given the task's complexity, best\nresults have typically been reported for systems focusing on specific settings,\ne.g. instrument-specific systems tend to yield improved results over\ninstrument-agnostic methods. Similarly, higher accuracy can be obtained when\nonly estimating frame-wise $f_0$ values and neglecting the harder note event\ndetection. Despite their high accuracy, such specialized systems often cannot\nbe deployed in the real-world. Storage and network constraints prohibit the use\nof multiple specialized models, while memory and run-time constraints limit\ntheir complexity. In this paper, we propose a lightweight neural network for\nmusical instrument transcription, which supports polyphonic outputs and\ngeneralizes to a wide variety of instruments (including vocals). Our model is\ntrained to jointly predict frame-wise onsets, multipitch and note activations,\nand we experimentally show that this multi-output structure improves the\nresulting frame-level note accuracy. Despite its simplicity, benchmark results\nshow our system's note estimation to be substantially better than a comparable\nbaseline, and its frame-level accuracy to be only marginally below those of\nspecialized state-of-the-art AMT systems. With this work we hope to encourage\nthe community to further investigate low-resource, instrument-agnostic AMT\nsystems.\n","authors":["Rachel M. Bittner","Juan José Bosch","David Rubinstein","Gabriel Meseguer-Brocal","Sebastian Ewert"],"pdf_url":"https://arxiv.org/pdf/2203.09893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09888v1","updated":"2022-03-18T11:51:59Z","published":"2022-03-18T11:51:59Z","title":"Hypergraph Modeling via Spectral Embedding Connection: Hypergraph Cut,\n  Weighted Kernel $k$-means, and Heat Kernel","summary":"  We propose a theoretical framework of multi-way similarity to model\nreal-valued data into hypergraphs for clustering via spectral embedding. For\ngraph cut based spectral clustering, it is common to model real-valued data\ninto graph by modeling pairwise similarities using kernel function. This is\nbecause the kernel function has a theoretical connection to the graph cut. For\nproblems where using multi-way similarities are more suitable than pairwise\nones, it is natural to model as a hypergraph, which is generalization of a\ngraph. However, although the hypergraph cut is well-studied, there is not yet\nestablished a hypergraph cut based framework to model multi-way similarity. In\nthis paper, we formulate multi-way similarities by exploiting the theoretical\nfoundation of kernel function. We show a theoretical connection between our\nformulation and hypergraph cut in two ways, generalizing both weighted kernel\n$k$-means and the heat kernel, by which we justify our formulation. We also\nprovide a fast algorithm for spectral clustering. Our algorithm empirically\nshows better performance than existing graph and other heuristic modeling\nmethods.\n","authors":["Shota Saito"],"pdf_url":"https://arxiv.org/pdf/2203.09888v1.pdf","comment":"Extended version of our AAAI-22 paper"},{"id":"http://arxiv.org/abs/2111.02682v2","updated":"2022-03-18T11:47:26Z","published":"2021-11-04T08:32:59Z","title":"TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift\n  Estimation","summary":"  The recent developments of deep learning models that capture the complex\ntemporal patterns of crop phenology have greatly advanced crop classification\nof Satellite Image Time Series (SITS). However, when applied to target regions\nspatially different from the training region, these models perform poorly\nwithout any target labels due to the temporal shift of crop phenology between\nregions. To address this unsupervised cross-region adaptation setting, existing\nmethods learn domain-invariant features without any target supervision, but not\nthe temporal shift itself. As a consequence, these techniques provide only\nlimited benefits for SITS. In this paper, we propose TimeMatch, a new\nunsupervised domain adaptation method for SITS that directly accounts for the\ntemporal shift. TimeMatch consists of two components: 1) temporal shift\nestimation, which estimates the temporal shift of the unlabeled target region\nwith a source-trained model, and 2) TimeMatch learning, which combines temporal\nshift estimation with semi-supervised learning to adapt a classifier to an\nunlabeled target region. We also introduce an open-access dataset for\ncross-region adaptation with SITS from four different regions in Europe. On\nthis dataset, we demonstrate that TimeMatch outperforms all competing methods\nby 11% in F1-score across five different adaptation scenarios, setting a new\nstate-of-the-art for cross-region adaptation.\n","authors":["Joachim Nyborg","Charlotte Pelletier","Sébastien Lefèvre","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2111.02682v2.pdf","comment":"Preprint submitted to the ISPRS Journal of Photogrammetry and Remote\n  Sensing"},{"id":"http://arxiv.org/abs/2110.06149v2","updated":"2022-03-18T11:46:45Z","published":"2021-10-12T16:38:08Z","title":"Planning from Pixels in Environments with Combinatorially Hard Search\n  Spaces","summary":"  The ability to form complex plans based on raw visual input is a litmus test\nfor current capabilities of artificial intelligence, as it requires a seamless\ncombination of visual processing and abstract algorithmic execution, two\ntraditionally separate areas of computer science. A recent surge of interest in\nthis field brought advances that yield good performance in tasks ranging from\narcade games to continuous control; these methods however do not come without\nsignificant issues, such as limited generalization capabilities and\ndifficulties when dealing with combinatorially hard planning instances. Our\ncontribution is two-fold: (i) we present a method that learns to represent its\nenvironment as a latent graph and leverages state reidentification to reduce\nthe complexity of finding a good policy from exponential to linear (ii) we\nintroduce a set of lightweight environments with an underlying discrete\ncombinatorial structure in which planning is challenging even for humans.\nMoreover, we show that our methods achieves strong empirical generalization to\nvariations in the environment, even across highly disadvantaged regimes, such\nas \"one-shot\" planning, or in an offline RL paradigm which only provides\nlow-quality trajectories.\n","authors":["Marco Bagatella","Mirek Olšák","Michal Rolínek","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2110.06149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09880v1","updated":"2022-03-18T11:45:01Z","published":"2022-03-18T11:45:01Z","title":"Identification of Hypokinetic Dysarthria Using Acoustic Analysis of Poem\n  Recitation","summary":"  Up to 90 % of patients with Parkinson's disease (PD) suffer from hypokinetic\ndysarthria (HD). In this work, we analysed the power of conventional speech\nfeatures quantifying imprecise articulation, dysprosody, speech dysfluency and\nspeech quality deterioration extracted from a specialized poem recitation task\nto discriminate dysarthric and healthy speech. For this purpose, 152 speakers\n(53 healthy speakers, 99 PD patients) were examined. Only mildly strong\ncorrelation between speech features and clinical status of the speakers was\nobserved. In the case of univariate classification analysis, sensitivity of\n62.63% (imprecise articulation), 61.62% (dysprosody), 71.72% (speech\ndysfluency) and 59.60% (speech quality deterioration) was achieved.\nMultivariate classification analysis improved the classification performance.\nSensitivity of 83.42% using only two features describing imprecise articulation\nand speech quality deterioration in HD was achieved. We showed the promising\npotential of the selected speech features and especially the use of poem\nrecitation task to quantify and identify HD in PD.\n","authors":["Jan Mucha","Zoltan Galaz","Jiri Mekyska","Tomas Kiska","Vojtech Zvoncak","Zdenek Smekal","Ilona Eliasova","Martina Mrackova","Milena Kostalova","Irena Rektorova","Marcos Faundez-Zanuy","Jesus B. Alonso-Hernandez"],"pdf_url":"https://arxiv.org/pdf/2203.09880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09879v1","updated":"2022-03-18T11:43:12Z","published":"2022-03-18T11:43:12Z","title":"Class-wise Classifier Design Capable of Continual Learning using\n  Adaptive Resonance Theory-based Topological Clustering","summary":"  This paper proposes a supervised classification algorithm capable of\ncontinual learning by utilizing an Adaptive Resonance Theory (ART)-based\ngrowing self-organizing clustering algorithm. The ART-based clustering\nalgorithm is theoretically capable of continual learning, and the proposed\nalgorithm independently applies it to each class of training data for\ngenerating classifiers. Whenever an additional training data set from a new\nclass is given, a new ART-based clustering will be defined in a different\nlearning space. Thanks to the above-mentioned features, the proposed algorithm\nrealizes continual learning capability. Simulation experiments showed that the\nproposed algorithm has superior classification performance compared with\nstate-of-the-art clustering-based classification algorithms capable of\ncontinual learning.\n","authors":["Naoki Masuyama","Itsuki Tsubota","Yusuke Nojima","Hisao Ishibuchi"],"pdf_url":"https://arxiv.org/pdf/2203.09879v1.pdf","comment":"This paper is currently under review. arXiv admin note: substantial\n  text overlap with arXiv:2201.10713"},{"id":"http://arxiv.org/abs/2105.00336v3","updated":"2022-03-18T11:26:09Z","published":"2021-05-01T19:48:45Z","title":"Comprehensive Review On Twin Support Vector Machines","summary":"  Twin support vector machine (TWSVM) and twin support vector regression (TSVR)\nare newly emerging efficient machine learning techniques which offer promising\nsolutions for classification and regression challenges respectively. TWSVM is\nbased upon the idea to identify two nonparallel hyperplanes which classify the\ndata points to their respective classes. It requires to solve two small sized\nquadratic programming problems (QPPs) in lieu of solving single large size QPP\nin support vector machine (SVM) while TSVR is formulated on the lines of TWSVM\nand requires to solve two SVM kind problems. Although there has been good\nresearch progress on these techniques; there is limited literature on the\ncomparison of different variants of TSVR. Thus, this review presents a rigorous\nanalysis of recent research in TWSVM and TSVR simultaneously mentioning their\nlimitations and advantages. To begin with we first introduce the basic theory\nof support vector machine, TWSVM and then focus on the various improvements and\napplications of TWSVM, and then we introduce TSVR and its various enhancements.\nFinally, we suggest future research and development prospects.\n","authors":["M. Tanveer","T. Rajani","R. Rastogi","Y. H. Shao","M. A. Ganaie"],"pdf_url":"https://arxiv.org/pdf/2105.00336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.06011v2","updated":"2022-03-18T11:18:07Z","published":"2021-04-13T08:23:46Z","title":"Sample-based and Feature-based Federated Learning for Unconstrained and\n  Constrained Nonconvex Optimization via Mini-batch SSCA","summary":"  Federated learning (FL) has become a hot research area in enabling the\ncollaborative training of machine learning models among multiple clients that\nhold sensitive local data. Nevertheless, unconstrained federated optimization\nhas been studied mainly using stochastic gradient descent (SGD), which may\nconverge slowly, and constrained federated optimization, which is more\nchallenging, has not been investigated so far. This paper investigates\nsample-based and feature-based federated optimization, respectively, and\nconsiders both unconstrained and constrained nonconvex problems for each of\nthem. First, we propose FL algorithms using stochastic successive convex\napproximation (SSCA) and mini-batch techniques. These algorithms can adequately\nexploit the structures of the objective and constraint functions and\nincrementally utilize samples. We show that the proposed FL algorithms converge\nto stationary points and Karush-Kuhn-Tucker (KKT) points of the respective\nunconstrained and constrained nonconvex problems, respectively. Next, we\nprovide algorithm examples with appealing computational complexity and\ncommunication load per communication round. We show that the proposed algorithm\nexamples for unconstrained federated optimization are identical to FL\nalgorithms via momentum SGD and provide an analytical connection between SSCA\nand momentum SGD. Finally, numerical experiments demonstrate the inherent\nadvantages of the proposed algorithms in convergence speeds, communication and\ncomputation costs, and model specifications.\n","authors":["Ying Cui","Yangchen Li","Chencheng Ye"],"pdf_url":"https://arxiv.org/pdf/2104.06011v2.pdf","comment":"18 pages, 4 figures. This work is submitted to IEEE Trans. Signal\n  Process. (under major revision). arXiv admin note: substantial text overlap\n  with arXiv:2103.09506"},{"id":"http://arxiv.org/abs/2203.09862v1","updated":"2022-03-18T11:04:50Z","published":"2022-03-18T11:04:50Z","title":"Finite-sample analysis of identification of switched linear systems with\n  arbitrary or restricted switching","summary":"  This work aims to derive a data-independent finite-sample error bound for the\nleast-squares (LS) estimation error of switched linear systems when the state\nand the switching signal are measured. While the existing finite-sample bounds\nfor linear system identification extend to the problem under consideration, the\nGramian of the switched system, an essential term in the error bound, depends\non the measured switching signal. Therefore, data-independent bounds on the\nspectrum of the Gramian are developed for globally asymptotically and\nmarginally stable switched systems when the switching is arbitrary or subject\nto an average dwell time constraint. Combining the bounds on the spectrum of\nthe Gramian and the preliminary error bound extended from linear system\nidentification leads to the error bound for the LS estimate of the switched\nsystem.\n","authors":["Shengling Shi","Othmane Mazhar","Bart De Schutter"],"pdf_url":"https://arxiv.org/pdf/2203.09862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09852v1","updated":"2022-03-18T10:44:11Z","published":"2022-03-18T10:44:11Z","title":"Decision-Making under Miscalibration","summary":"  ML-based predictions are used to inform consequential decisions about\nindividuals. How should we use predictions (e.g., risk of heart attack) to\ninform downstream binary classification decisions (e.g., undergoing a medical\nprocedure)? When the risk estimates are perfectly calibrated, the answer is\nwell understood: a classification problem's cost structure induces an optimal\ntreatment threshold $j^{\\star}$. In practice, however, some amount of\nmiscalibration is unavoidable, raising a fundamental question: how should one\nuse potentially miscalibrated predictions to inform binary decisions? We\nformalize a natural (distribution-free) solution concept: given anticipated\nmiscalibration of $\\alpha$, we propose using the threshold $j$ that minimizes\nthe worst-case regret over all $\\alpha$-miscalibrated predictors, where the\nregret is the difference in clinical utility between using the threshold in\nquestion and using the optimal threshold in hindsight. We provide closed form\nexpressions for $j$ when miscalibration is measured using both expected and\nmaximum calibration error, which reveal that it indeed differs from $j^{\\star}$\n(the optimal threshold under perfect calibration). We validate our theoretical\nfindings on real data, demonstrating that there are natural cases in which\nmaking decisions using $j$ improves the clinical utility.\n","authors":["Guy N. Rothblum","Gal Yona"],"pdf_url":"https://arxiv.org/pdf/2203.09852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09849v1","updated":"2022-03-18T10:37:20Z","published":"2022-03-18T10:37:20Z","title":"Neural Predictor for Black-Box Adversarial Attacks on Speech Recognition","summary":"  Recent works have revealed the vulnerability of automatic speech recognition\n(ASR) models to adversarial examples (AEs), i.e., small perturbations that\ncause an error in the transcription of the audio signal. Studying audio\nadversarial attacks is therefore the first step towards robust ASR. Despite the\nsignificant progress made in attacking audio examples, the black-box attack\nremains challenging because only the hard-label information of transcriptions\nis provided. Due to this limited information, existing black-box methods often\nrequire an excessive number of queries to attack a single audio example. In\nthis paper, we introduce NP-Attack, a neural predictor-based method, which\nprogressively evolves the search towards a small adversarial perturbation.\nGiven a perturbation direction, our neural predictor directly estimates the\nsmallest perturbation that causes a mistranscription. In particular, it enables\nNP-Attack to accurately learn promising perturbation directions via\ngradient-based optimization. Experimental results show that NP-Attack achieves\ncompetitive results with other state-of-the-art black-box adversarial attacks\nwhile requiring a significantly smaller number of queries. The code of\nNP-Attack is available online.\n","authors":["Marie Biolková","Bac Nguyen"],"pdf_url":"https://arxiv.org/pdf/2203.09849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09848v1","updated":"2022-03-18T10:37:19Z","published":"2022-03-18T10:37:19Z","title":"Gender classification by means of online uppercase handwriting: A\n  text-dependent allographic approach","summary":"  This paper presents a gender classification schema based on online\nhandwriting. Using samples acquired with a digital tablet that captures the\ndynamics of the writing, it classifies the writer as a male or a female. The\nmethod proposed is allographic, regarding strokes as the structural units of\nhandwriting. Strokes performed while the writing device is not exerting any\npressure on the writing surface, pen-up (in-air) strokes, are also taken into\naccount. The method is also text-dependent meaning that training and testing is\ndone with exactly the same text. Text-dependency allows classification be\nperformed with very small amounts of text. Experimentation, performed with\nsamples from the BiosecurID database, yields results that fall in the range of\nthe classification averages expected from human judges. With only four\nrepetitions of a single uppercase word, the average rate of well classified\nwriters is 68%; with sixteen words, the rate rises to an average 72.6%.\nStatistical analysis reveals that the aforementioned rates are highly\nsignificant. In order to explore the classification potential of the pen-up\nstrokes, these are also considered. Although in this case results are not\nconclusive, an outstanding average of 74% of well classified writers is\nobtained when information from pen-up strokes is combined with information from\npen-down ones.\n","authors":["Enric Sesa-Nogueras","Marcos Faundez-Zanuy","Josep Roure-Alcobé"],"pdf_url":"https://arxiv.org/pdf/2203.09848v1.pdf","comment":"25 pages, published in Cogn Comput 8, pages 15 to 29, year 2016"},{"id":"http://arxiv.org/abs/2112.02612v3","updated":"2022-03-18T10:36:17Z","published":"2021-12-05T16:23:53Z","title":"Training Structured Neural Networks Through Manifold Identification and\n  Variance Reduction","summary":"  This paper proposes an algorithm (RMDA) for training neural networks (NNs)\nwith a regularization term for promoting desired structures. RMDA does not\nincur computation additional to proximal SGD with momentum, and achieves\nvariance reduction without requiring the objective function to be of the\nfinite-sum form. Through the tool of manifold identification from nonlinear\noptimization, we prove that after a finite number of iterations, all iterates\nof RMDA possess a desired structure identical to that induced by the\nregularizer at the stationary point of asymptotic convergence, even in the\npresence of engineering tricks like data augmentation and dropout that\ncomplicate the training process. Experiments on training NNs with structured\nsparsity confirm that variance reduction is necessary for such an\nidentification, and show that RMDA thus significantly outperforms existing\nmethods for this task. For unstructured sparsity, RMDA also outperforms a\nstate-of-the-art pruning method, validating the benefits of training structured\nNNs through regularization.\n","authors":["Zih-Syuan Huang","Ching-pei Lee"],"pdf_url":"https://arxiv.org/pdf/2112.02612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09494v2","updated":"2022-03-18T10:34:43Z","published":"2022-03-17T17:48:32Z","title":"Transframer: Arbitrary Frame Prediction with Generative Models","summary":"  We present a general-purpose framework for image modelling and vision tasks\nbased on probabilistic frame prediction. Our approach unifies a broad range of\ntasks, from image segmentation, to novel view synthesis and video\ninterpolation. We pair this framework with an architecture we term Transframer,\nwhich uses U-Net and Transformer components to condition on annotated context\nframes, and outputs sequences of sparse, compressed image features. Transframer\nis the state-of-the-art on a variety of video generation benchmarks, is\ncompetitive with the strongest models on few-shot view synthesis, and can\ngenerate coherent 30 second videos from a single image without any explicit\ngeometric information. A single generalist Transframer simultaneously produces\npromising results on 8 tasks, including semantic segmentation, image\nclassification and optical flow prediction with no task-specific architectural\ncomponents, demonstrating that multi-task computer vision can be tackled using\nprobabilistic image models. Our approach can in principle be applied to a wide\nrange of applications that require learning the conditional structure of\nannotated image-formatted data.\n","authors":["Charlie Nash","João Carreira","Jacob Walker","Iain Barr","Andrew Jaegle","Mateusz Malinowski","Peter Battaglia"],"pdf_url":"https://arxiv.org/pdf/2203.09494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.04562v2","updated":"2022-03-18T10:30:25Z","published":"2021-07-09T17:28:55Z","title":"The Bayesian Learning Rule","summary":"  We show that many machine-learning algorithms are specific instances of a\nsingle algorithm called the Bayesian learning rule. The rule, derived from\nBayesian principles, yields a wide-range of algorithms from fields such as\noptimization, deep learning, and graphical models. This includes classical\nalgorithms such as ridge regression, Newton's method, and Kalman filter, as\nwell as modern deep-learning algorithms such as stochastic-gradient descent,\nRMSprop, and Dropout. The key idea in deriving such algorithms is to\napproximate the posterior using candidate distributions estimated by using\nnatural gradients. Different candidate distributions result in different\nalgorithms and further approximations to natural gradients give rise to\nvariants of those algorithms. Our work not only unifies, generalizes, and\nimproves existing algorithms, but also helps us design new ones.\n","authors":["Mohammad Emtiyaz Khan","Håvard Rue"],"pdf_url":"https://arxiv.org/pdf/2107.04562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12674v2","updated":"2022-03-18T10:26:53Z","published":"2022-02-25T13:24:23Z","title":"PLSSVM: A (multi-)GPGPU-accelerated Least Squares Support Vector Machine","summary":"  Machine learning algorithms must be able to efficiently cope with massive\ndata sets. Therefore, they have to scale well on any modern system and be able\nto exploit the computing power of accelerators independent of their vendor. In\nthe field of supervised learning, Support Vector Machines (SVMs) are widely\nused. However, even modern and optimized implementations such as LIBSVM or\nThunderSVM do not scale well for large non-trivial dense data sets on\ncutting-edge hardware: Most SVM implementations are based on Sequential Minimal\nOptimization, an optimized though inherent sequential algorithm. Hence, they\nare not well-suited for highly parallel GPUs. Furthermore, we are not aware of\na performance portable implementation that supports CPUs and GPUs from\ndifferent vendors.\n  We have developed the PLSSVM library to solve both issues. First, we resort\nto the formulation of the SVM as a least squares problem. Training an SVM then\nboils down to solving a system of linear equations for which highly parallel\nalgorithms are known. Second, we provide a hardware independent yet efficient\nimplementation: PLSSVM uses different interchangeable backends--OpenMP, CUDA,\nOpenCL, SYCL--supporting modern hardware from various vendors like NVIDIA, AMD,\nor Intel on multiple GPUs. PLSSVM can be used as a drop-in replacement for\nLIBSVM. We observe a speedup on CPUs of up to 10 compared to LIBSVM and on GPUs\nof up to 14 compared to ThunderSVM. Our implementation scales on many-core CPUs\nwith a parallel speedup of 74.7 on up to 256 CPU threads and on multiple GPUs\nwith a parallel speedup of 3.71 on four GPUs.\n  The code, utility scripts, and documentation are all available on GitHub:\nhttps://github.com/SC-SGS/PLSSVM.\n","authors":["Alexander Van Craen","Marcel Breyer","Dirk Pflüger"],"pdf_url":"https://arxiv.org/pdf/2202.12674v2.pdf","comment":"Accepted on PDSEC 2022"},{"id":"http://arxiv.org/abs/2203.09174v2","updated":"2022-03-18T10:21:07Z","published":"2022-03-17T08:51:56Z","title":"Nearest Neighbor Classifier with Margin Penalty for Active Learning","summary":"  As deep learning becomes the mainstream in the field of natural language\nprocessing, the need for suitable active learning method are becoming\nunprecedented urgent. Active Learning (AL) methods based on nearest neighbor\nclassifier are proposed and demonstrated superior results. However, existing\nnearest neighbor classifier are not suitable for classifying mutual exclusive\nclasses because inter-class discrepancy cannot be assured by nearest neighbor\nclassifiers. As a result, informative samples in the margin area can not be\ndiscovered and AL performance are damaged. To this end, we propose a novel\nNearest neighbor Classifier with Margin penalty for Active Learning(NCMAL).\nFirstly, mandatory margin penalty are added between classes, therefore both\ninter-class discrepancy and intra-class compactness are both assured. Secondly,\na novel sample selection strategy are proposed to discover informative samples\nwithin the margin area. To demonstrate the effectiveness of the methods, we\nconduct extensive experiments on for datasets with other state-of-the-art\nmethods. The experimental results demonstrate that our method achieves better\nresults with fewer annotated samples than all baseline methods.\n","authors":["Yuan Cao","Zhiqiao Gao","Jie Hu","Mingchuan Yang"],"pdf_url":"https://arxiv.org/pdf/2203.09174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09829v1","updated":"2022-03-18T10:12:24Z","published":"2022-03-18T10:12:24Z","title":"Towards Representative Subset Selection for Self-Supervised Speech\n  Recognition","summary":"  Self-supervised speech recognition models require considerable labeled\ntraining data for learning high-fidelity representations for Automatic Speech\nRecognition (ASR), which hinders their application to low-resource languages.\nWe consider the task of identifying an optimal subset of training data to\nfine-tune self-supervised speech models for ASR. We make a surprising\nobservation that active learning strategies for sampling harder-to-learn\nexamples do not perform better than random subset selection for fine-tuning\nself-supervised ASR. We then present the COWERAGE algorithm for better subset\nselection in self-supervised ASR which is based on our finding that ensuring\nthe coverage of examples based on training WER in the early training epochs\nleads to better generalization performance. Extensive experiments on the\nwav2vec 2.0 model and TIMIT dataset show the effectiveness of COWERAGE, with up\nto 27% absolute WER improvement over active learning methods. We also report\nthe connection between training WER and the phonemic cover and demonstrate that\nour algorithm ensures inclusion of phonemically diverse examples.\n","authors":["Abdul Hameed Azeemi","Ihsan Ayyub Qazi","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2203.09829v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2103.01312v2","updated":"2022-03-18T10:06:17Z","published":"2021-03-01T21:08:48Z","title":"UCB Momentum Q-learning: Correcting the bias without forgetting","summary":"  We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm\nfor reinforcement learning in tabular and possibly stage-dependent, episodic\nMarkov decision process. UCBMQ is based on Q-learning where we add a momentum\nterm and rely on the principle of optimism in face of uncertainty to deal with\nexploration. Our new technical ingredient of UCBMQ is the use of momentum to\ncorrect the bias that Q-learning suffers while, at the same time, limiting the\nimpact it has on the second-order term of the regret. For UCBMQ, we are able to\nguarantee a regret of at most $O(\\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the\nlength of an episode, $S$ the number of states, $A$ the number of actions, $T$\nthe number of episodes and ignoring terms in poly-$\\log(SAHT)$. Notably, UCBMQ\nis the first algorithm that simultaneously matches the lower bound of\n$\\Omega(\\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with\nrespect to the horizon $T$) that scales only linearly with the number of states\n$S$.\n","authors":["Pierre Menard","Omar Darwiche Domingues","Xuedong Shang","Michal Valko"],"pdf_url":"https://arxiv.org/pdf/2103.01312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09824v1","updated":"2022-03-18T10:03:07Z","published":"2022-03-18T10:03:07Z","title":"Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?","summary":"  This work digs into a root question in human perception: can face geometry be\ngleaned from one's voices? Previous works that study this question only adopt\ndevelopments in image synthesis and convert voices into face images to show\ncorrelations, but working on the image domain unavoidably involves predicting\nattributes that voices cannot hint, including facial textures, hairstyles, and\nbackgrounds. We instead investigate the ability to reconstruct 3D faces to\nconcentrate on only geometry, which is much more physiologically grounded. We\npropose our analysis framework, Cross-Modal Perceptionist, under both\nsupervised and unsupervised learning. First, we construct a dataset,\nVoxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,\nmaking supervised learning possible. Second, we use a knowledge distillation\nmechanism to study whether face geometry can still be gleaned from voices\nwithout paired voices and 3D face data under limited availability of 3D face\nscans. We break down the core question into four parts and perform visual and\nnumerical analyses as responses to the core question. Our findings echo those\nin physiology and neuroscience about the correlation between voices and facial\nstructures. The work provides future human-centric cross-modal learning with\nexplainable foundations. See our project page:\nhttps://choyingw.github.io/works/Voice2Mesh/index.html\n","authors":["Cho-Ying Wu","Chin-Cheng Hsu","Ulrich Neumann"],"pdf_url":"https://arxiv.org/pdf/2203.09824v1.pdf","comment":"Accepted to CVPR 2022. Project page:\n  https://choyingw.github.io/works/Voice2Mesh/index.html. This version\n  supersedes arXiv:2104.10299"},{"id":"http://arxiv.org/abs/2112.06735v2","updated":"2022-03-18T09:51:37Z","published":"2021-12-13T15:37:13Z","title":"Unsupervised machine learning approaches to the $q$-state Potts model","summary":"  In this paper with study phase transitions of the $q$-state Potts model,\nthrough a number of unsupervised machine learning techniques, namely Principal\nComponent Analysis (PCA), $k$-means clustering, Uniform Manifold Approximation\nand Projection (UMAP), and Topological Data Analysis (TDA). Even though in all\ncases we are able to retrieve the correct critical temperatures $T_c(q)$, for\n$q = 3, 4$ and $5$, results show that non-linear methods as UMAP and TDA are\nless dependent on finite size effects, while still being able to distinguish\nbetween first and second order phase transitions. This study may be considered\nas a benchmark for the use of different unsupervised machine learning\nalgorithms in the investigation of phase transitions.\n","authors":["Andrea Tirelli","Danyella O. Carvalho","Lucas A. Oliveira","J. P. Lima","Natanael C. Costa","Raimundo R. dos Santos"],"pdf_url":"https://arxiv.org/pdf/2112.06735v2.pdf","comment":"Added computation of critical exponents; exposition improved"},{"id":"http://arxiv.org/abs/2203.09813v1","updated":"2022-03-18T09:19:14Z","published":"2022-03-18T09:19:14Z","title":"Are You Robert or RoBERTa? Deceiving Online Authorship Attribution\n  Models Using Neural Text Generators","summary":"  Recently, there has been a rise in the development of powerful pre-trained\nnatural language models, including GPT-2, Grover, and XLM. These models have\nshown state-of-the-art capabilities towards a variety of different NLP tasks,\nincluding question answering, content summarisation, and text generation.\nAlongside this, there have been many studies focused on online authorship\nattribution (AA). That is, the use of models to identify the authors of online\ntexts. Given the power of natural language models in generating convincing\ntexts, this paper examines the degree to which these language models can\ngenerate texts capable of deceiving online AA models. Experimenting with both\nblog and Twitter data, we utilise GPT-2 language models to generate texts using\nthe existing posts of online users. We then examine whether these AI-based text\ngenerators are capable of mimicking authorial style to such a degree that they\ncan deceive typical AA models. From this, we find that current AI-based text\ngenerators are able to successfully mimic authorship, showing capabilities\ntowards this on both datasets. Our findings, in turn, highlight the current\ncapacity of powerful natural language models to generate original online posts\ncapable of mimicking authorial style sufficiently to deceive popular AA\nmethods; a key finding given the proposed role of AA in real world applications\nsuch as spam-detection and forensic investigation.\n","authors":["Keenan Jones","Jason R. C. Nurse","Shujun Li"],"pdf_url":"https://arxiv.org/pdf/2203.09813v1.pdf","comment":"13 pages, 6 figures, 4 tables, Accepted for publication in the\n  proceedings of the sixteenth International AAAI Conference on Web and Social\n  Media (ICWSM-22)"},{"id":"http://arxiv.org/abs/2203.09810v1","updated":"2022-03-18T09:13:57Z","published":"2022-03-18T09:13:57Z","title":"Dencentralized learning in the presence of low-rank noise","summary":"  Observations collected by agents in a network may be unreliable due to\nobservation noise or interference. This paper proposes a distributed algorithm\nthat allows each node to improve the reliability of its own observation by\nrelying solely on local computations and interactions with immediate neighbors,\nassuming that the field (graph signal) monitored by the network lies in a\nlow-dimensional subspace and that a low-rank noise is present in addition to\nthe usual full-rank noise. While oblique projections can be used to project\nmeasurements onto a low-rank subspace along a direction that is oblique to the\nsubspace, the resulting solution is not distributed. Starting from the\ncentralized solution, we propose an algorithm that performs the oblique\nprojection of the overall set of observations onto the signal subspace in an\niterative and distributed manner. We then show how the oblique projection\nframework can be extended to handle distributed learning and adaptation\nproblems over networks.\n","authors":["Roula Nassif","Virginia Bordignon","Stefan Vlaski","Ali H. Sayed"],"pdf_url":"https://arxiv.org/pdf/2203.09810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09809v1","updated":"2022-03-18T09:13:13Z","published":"2022-03-18T09:13:13Z","title":"Proximal Policy Optimization with Adaptive Threshold for Symmetric\n  Relative Density Ratio","summary":"  Deep reinforcement learning (DRL) is one of the promising approaches for\nintroducing robots into complicated environments. The recent remarkable\nprogress of DRL stands on regularization of policy, which allows the policy to\nimprove stably and efficiently. A popular method, so-called proximal policy\noptimization (PPO), and its variants constrain density ratio of the latest and\nbaseline policies when the density ratio exceeds a given threshold. This\nthreshold can be designed relatively intuitively, and in fact its recommended\nvalue range has been suggested. However, the density ratio is asymmetric for\nits center, and the possible error scale from its center, which should be close\nto the threshold, would depend on how the baseline policy is given. In order to\nmaximize the values of regularization of policy, this paper proposes a new PPO\nderived using relative Pearson (RPE) divergence, therefore so-called PPO-RPE,\nto design the threshold adaptively. In PPO-RPE, the relative density ratio,\nwhich can be formed with symmetry, replaces the raw density ratio. Thanks to\nthis symmetry, its error scale from center can easily be estimated, hence, the\nthreshold can be adapted for the estimated error scale. From three simple\nbenchmark simulations, the importance of algorithm-dependent threshold design\nis revealed. By simulating additional four locomotion tasks, it is verified\nthat the proposed method statistically contributes to task accomplishment by\nappropriately restricting the policy updates.\n","authors":["Taisuke Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2203.09809v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2106.01098v3","updated":"2022-03-18T09:02:56Z","published":"2021-06-02T12:04:29Z","title":"Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and\n  Practical Solutions","summary":"  Graph generative models are a highly active branch of machine learning. Given\nthe steady development of new models of ever-increasing complexity, it is\nnecessary to provide a principled way to evaluate and compare them. In this\npaper, we enumerate the desirable criteria for such a comparison metric and\nprovide an overview of the status quo of graph generative model comparison in\nuse today, which predominantly relies on the maximum mean discrepancy (MMD). We\nperform a systematic evaluation of MMD in the context of graph generative model\ncomparison, highlighting some of the challenges and pitfalls researchers\ninadvertently may encounter. After conducting a thorough analysis of the\nbehaviour of MMD on synthetically-generated perturbed graphs as well as on\nrecently-proposed graph generative models, we are able to provide a suitable\nprocedure to mitigate these challenges and pitfalls. We aggregate our findings\ninto a list of practical recommendations for researchers to use when evaluating\ngraph generative models.\n","authors":["Leslie O'Bray","Max Horn","Bastian Rieck","Karsten Borgwardt"],"pdf_url":"https://arxiv.org/pdf/2106.01098v3.pdf","comment":"Accepted as a Spotlight presentation at ICLR 2022"},{"id":"http://arxiv.org/abs/2110.15057v2","updated":"2022-03-18T08:35:23Z","published":"2021-10-26T14:25:07Z","title":"Mapping conditional distributions for domain adaptation under\n  generalized target shift","summary":"  We consider the problem of unsupervised domain adaptation (UDA) between a\nsource and a target domain under conditional and label shift a.k.a Generalized\nTarget Shift (GeTarS). Unlike simpler UDA settings, few works have addressed\nthis challenging problem. Recent approaches learn domain-invariant\nrepresentations, yet they have practical limitations and rely on strong\nassumptions that may not hold in practice. In this paper, we explore a novel\nand general approach to align pretrained representations, which circumvents\nexisting drawbacks. Instead of constraining representation invariance, it\nlearns an optimal transport map, implemented as a NN, which maps source\nrepresentations onto target ones. Our approach is flexible and scalable, it\npreserves the problem's structure and it has strong theoretical guarantees\nunder mild assumptions. In particular, our solution is unique, matches\nconditional distributions across domains, recovers target proportions and\nexplicitly controls the target generalization risk. Through an exhaustive\ncomparison on several datasets, we challenge the state-of-the-art in GeTarS.\n","authors":["Matthieu Kirchmeyer","Alain Rakotomamonjy","Emmanuel de Bezenac","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2110.15057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.07117v4","updated":"2022-03-18T08:27:04Z","published":"2021-09-15T06:58:23Z","title":"Non-Asymptotic Analysis of Stochastic Approximation Algorithms for\n  Streaming Data","summary":"  Motivated by the high-frequency data streams continuously generated,\nreal-time learning is becoming increasingly important. These data streams\nshould be processed sequentially with the property that the stream may change\nover time. In this streaming setting, we propose techniques for minimizing a\nconvex objective through unbiased estimates of its gradients, commonly referred\nto as stochastic approximation problems. Our methods rely on stochastic\napproximation algorithms due to their computationally advantage as they only\nuse the previous iterate as a parameter estimate. The reasoning includes\niterate averaging that guarantees optimal statistical efficiency under\nclassical conditions. Our non-asymptotic analysis shows accelerated convergence\nby selecting the learning rate according to the expected data streams. We show\nthat the average estimate converges optimally and robustly to any data stream\nrate. In addition, noise reduction can be achieved by processing the data in a\nspecific pattern, which is advantageous for large-scale machine learning. These\ntheoretical results are illustrated for various data streams, showing the\neffectiveness of the proposed algorithms.\n","authors":["Antoine Godichon-Baggioni","Olivier Wintenberger","Nicklas Werge"],"pdf_url":"https://arxiv.org/pdf/2109.07117v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09792v1","updated":"2022-03-18T08:18:03Z","published":"2022-03-18T08:18:03Z","title":"AdIoTack: Quantifying and Refining Resilience of Decision Tree Ensemble\n  Inference Models against Adversarial Volumetric Attacks on IoT Networks","summary":"  Machine Learning-based techniques have shown success in cyber intelligence.\nHowever, they are increasingly becoming targets of sophisticated data-driven\nadversarial attacks resulting in misprediction, eroding their ability to detect\nthreats on network devices. In this paper, we present AdIoTack, a system that\nhighlights vulnerabilities of decision trees against adversarial attacks,\nhelping cybersecurity teams quantify and refine the resilience of their trained\nmodels for monitoring IoT networks. To assess the model for the worst-case\nscenario, AdIoTack performs white-box adversarial learning to launch successful\nvolumetric attacks that decision tree ensemble models cannot flag. Our first\ncontribution is to develop a white-box algorithm that takes a trained decision\ntree ensemble model and the profile of an intended network-based attack on a\nvictim class as inputs. It then automatically generates recipes that specify\ncertain packets on top of the indented attack packets (less than 15% overhead)\nthat together can bypass the inference model unnoticed. We ensure that the\ngenerated attack instances are feasible for launching on IP networks and\neffective in their volumetric impact. Our second contribution develops a method\nto monitor the network behavior of connected devices actively, inject\nadversarial traffic (when feasible) on behalf of a victim IoT device, and\nsuccessfully launch the intended attack. Our third contribution prototypes\nAdIoTack and validates its efficacy on a testbed consisting of a handful of\nreal IoT devices monitored by a trained inference model. We demonstrate how the\nmodel detects all non-adversarial volumetric attacks on IoT devices while\nmissing many adversarial ones. The fourth contribution develops systematic\nmethods for applying patches to trained decision tree ensemble models,\nimproving their resilience against adversarial volumetric attacks.\n","authors":["Arman Pashamokhtari","Gustavo Batista","Hassan Habibi Gharakheili"],"pdf_url":"https://arxiv.org/pdf/2203.09792v1.pdf","comment":"15 pages, 16 figures, 4 tables"},{"id":"http://arxiv.org/abs/2108.13993v2","updated":"2022-03-18T08:11:31Z","published":"2021-08-31T17:34:40Z","title":"Designing Rotationally Invariant Neural Networks from PDEs and\n  Variational Methods","summary":"  Partial differential equation (PDE) models and their associated variational\nenergy formulations are often rotationally invariant by design. This ensures\nthat a rotation of the input results in a corresponding rotation of the output,\nwhich is desirable in applications such as image analysis. Convolutional neural\nnetworks (CNNs) do not share this property, and existing remedies are often\ncomplex. The goal of our paper is to investigate how diffusion and variational\nmodels achieve rotation invariance and transfer these ideas to neural networks.\nAs a core novelty we propose activation functions which couple network channels\nby combining information from several oriented filters. This guarantees\nrotation invariance within the basic building blocks of the networks while\nstill allowing for directional filtering. The resulting neural architectures\nare inherently rotationally invariant. With only a few small filters, they can\nachieve the same invariance as existing techniques which require a fine-grained\nsampling of orientations. Our findings help to translate diffusion and\nvariational models into mathematically well-founded network architectures, and\nprovide novel concepts for model-based CNN design.\n","authors":["Tobias Alt","Karl Schrader","Joachim Weickert","Pascal Peter","Matthias Augustin"],"pdf_url":"https://arxiv.org/pdf/2108.13993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09789v1","updated":"2022-03-18T08:10:02Z","published":"2022-03-18T08:10:02Z","title":"Constitutive model characterization and discovery using physics-informed\n  deep learning","summary":"  Classically, the mechanical response of materials is described through\nconstitutive models, often in the form of constrained ordinary differential\nequations. These models have a very limited number of parameters, yet, they are\nextremely efficient in reproducing complex responses observed in experiments.\nAdditionally, in their discretized form, they are computationally very\nefficient, often resulting in a simple algebraic relation, and therefore they\nhave been extensively used within large-scale explicit and implicit finite\nelement models. However, it is very challenging to formulate new constitutive\nmodels, particularly for materials with complex microstructures such as\ncomposites. A recent trend in constitutive modeling leverages complex neural\nnetwork architectures to construct complex material responses where a\nconstitutive model does not yet exist. Whilst very accurate, they suffer from\ntwo deficiencies. First, they are interpolation models and often do poorly in\nextrapolation. Second, due to their complex architecture and numerous\nparameters, they are inefficient to be used as a constitutive model within a\nlarge-scale finite element model. In this study, we propose a novel approach\nbased on the physics-informed learning machines for the characterization and\ndiscovery of constitutive models. Unlike data-driven constitutive models, we\nleverage foundations of elastoplasticity theory as regularization terms in the\ntotal loss function to find parametric constitutive models that are also\ntheoretically sound. We demonstrate that our proposed framework can efficiently\nidentify the underlying constitutive model describing different datasets from\nthe von Mises family.\n","authors":["Ehsan Haghighat","Sahar Abouali","Reza Vaziri"],"pdf_url":"https://arxiv.org/pdf/2203.09789v1.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.09783v1","updated":"2022-03-18T08:01:04Z","published":"2022-03-18T08:01:04Z","title":"ISDE : Independence Structure Density Estimation","summary":"  Density estimation appears as a subroutine in many learning procedures, so it\nis of interest to have efficient methods for it to perform in practical\nsituations. Multidimensional density estimation suffers from the curse of\ndimensionality. A solution to this problem is to add a structural hypothesis\nthrough an undirected graphical model on the underlying distribution. We\npropose ISDE (Independence Structure Density Estimation), an algorithm designed\nto estimate a density and an undirected graphical model from a particular\nfamily of graphs corresponding to Independence Structure (IS), a situation\nwhere we can separate features into independent groups. ISDE works for\nmoderately high-dimensional data (up to a few dozen features), and it is\nuseable in parametric and nonparametric situations. Existing methods on\nnonparametric graphical model estimation focus on multidimensional dependencies\nonly through pairwise ones: ISDE does not suffer from this restriction and can\naddress structures not yet covered by available algorithms. In this paper, we\npresent the existing theory about IS, explain the construction of our algorithm\nand prove its effectiveness. This is done on synthetic data both\nquantitatively, through measures of density estimation performance under\nKullback-Leibler loss, and qualitatively, in terms of capability to recover IS.\nBy applying ISDE on mass cytometry datasets, we also show how it performs both\nquantitatively and qualitatively on real-world datasets. Then we provide\ninformation about running time.\n","authors":["Louis Pujol"],"pdf_url":"https://arxiv.org/pdf/2203.09783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09777v1","updated":"2022-03-18T07:43:03Z","published":"2022-03-18T07:43:03Z","title":"Transferable Class-Modelling for Decentralized Source Attribution of\n  GAN-Generated Images","summary":"  GAN-generated deepfakes as a genre of digital images are gaining ground as\nboth catalysts of artistic expression and malicious forms of deception,\ntherefore demanding systems to enforce and accredit their ethical use. Existing\ntechniques for the source attribution of synthetic images identify subtle\nintrinsic fingerprints using multiclass classification neural nets limited in\nfunctionality and scalability. Hence, we redefine the deepfake detection and\nsource attribution problems as a series of related binary classification tasks.\nWe leverage transfer learning to rapidly adapt forgery detection networks for\nmultiple independent attribution problems, by proposing a semi-decentralized\nmodular design to solve them simultaneously and efficiently. Class activation\nmapping is also demonstrated as an effective means of feature localization for\nmodel interpretation. Our models are determined via experimentation to be\ncompetitive with current benchmarks, and capable of decent performance on human\nportraits in ideal conditions. Decentralized fingerprint-based attribution is\nfound to retain validity in the presence of novel sources, but is more\nsusceptible to type II errors that intensify with image perturbations and\nattributive uncertainty. We describe both our conceptual framework and model\nprototypes for further enhancement when investigating the technical limits of\nreactive deepfake attribution.\n","authors":["Brandon B. G. Khoo","Chern Hong Lim","Raphael C. -W. Phan"],"pdf_url":"https://arxiv.org/pdf/2203.09777v1.pdf","comment":"21 pages, 8 figures. Code:\n  https://github.com/quarxilon/Generator_Attribution"},{"id":"http://arxiv.org/abs/2110.04486v2","updated":"2022-03-18T07:32:43Z","published":"2021-10-09T07:16:14Z","title":"PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS\n  With Accurate Phoneme Duration Control","summary":"  Sequence expansion between encoder and decoder is a critical challenge in\nsequence-to-sequence TTS. Attention-based methods achieve great naturalness but\nsuffer from unstable issues like missing and repeating phonemes, not to mention\naccurate duration control. Duration-informed methods, on the contrary, seem to\neasily adjust phoneme duration but show obvious degradation in speech\nnaturalness. This paper proposes PAMA-TTS to address the problem. It takes the\nadvantage of both flexible attention and explicit duration models. Based on the\nmonotonic attention mechanism, PAMA-TTS also leverages token duration and\nrelative position of a frame, especially countdown information, i.e. in how\nmany future frames the present phoneme will end. They help the attention to\nmove forward along the token sequence in a soft but reliable control.\nExperimental results prove that PAMA-TTS achieves the highest naturalness,\nwhile has on-par or even better duration controllability than the\nduration-informed model.\n","authors":["Yunchao He","Jian Luan","Yujun Wang"],"pdf_url":"https://arxiv.org/pdf/2110.04486v2.pdf","comment":"Accepted by ICASSP 2022. 5 pages, 4 figures, 3 tables. Audio samples\n  are available at: https://pama-tts.github.io/"},{"id":"http://arxiv.org/abs/2106.01085v4","updated":"2022-03-18T07:21:44Z","published":"2021-06-02T11:39:25Z","title":"Online Coreset Selection for Rehearsal-based Continual Learning","summary":"  A dataset is a shred of crucial evidence to describe a task. However, each\ndata point in the dataset does not have the same potential, as some of the data\npoints can be more representative or informative than others. This unequal\nimportance among the data points may have a large impact in rehearsal-based\ncontinual learning, where we store a subset of the training examples (coreset)\nto be replayed later to alleviate catastrophic forgetting. In continual\nlearning, the quality of the samples stored in the coreset directly affects the\nmodel's effectiveness and efficiency. The coreset selection problem becomes\neven more important under realistic settings, such as imbalanced continual\nlearning or noisy data scenarios. To tackle this problem, we propose Online\nCoreset Selection (OCS), a simple yet effective method that selects the most\nrepresentative and informative coreset at each iteration and trains them in an\nonline manner. Our proposed method maximizes the model's adaptation to a\ncurrent dataset while selecting high-affinity samples to past tasks, which\ndirectly inhibits catastrophic forgetting. We validate the effectiveness of our\ncoreset selection mechanism over various standard, imbalanced, and noisy\ndatasets against strong continual learning baselines, demonstrating that it\nimproves task adaptation and prevents catastrophic forgetting in a\nsample-efficient manner.\n","authors":["Jaehong Yoon","Divyam Madaan","Eunho Yang","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2106.01085v4.pdf","comment":"ICLR 2022"},{"id":"http://arxiv.org/abs/2203.09770v1","updated":"2022-03-18T07:07:56Z","published":"2022-03-18T07:07:56Z","title":"Prototypical Verbalizer for Prompt-based Few-shot Tuning","summary":"  Prompt-based tuning for pre-trained language models (PLMs) has shown its\neffectiveness in few-shot learning. Typically, prompt-based tuning wraps the\ninput text into a cloze question. To make predictions, the model maps the\noutput words to labels via a verbalizer, which is either manually designed or\nautomatically built. However, manual verbalizers heavily depend on\ndomain-specific prior knowledge and human efforts, while finding appropriate\nlabel words automatically still remains challenging.In this work, we propose\nthe prototypical verbalizer (ProtoVerb) which is built directly from training\ndata. Specifically, ProtoVerb learns prototype vectors as verbalizers by\ncontrastive learning. In this way, the prototypes summarize training instances\nand are able to enclose rich class-level semantics. We conduct experiments on\nboth topic classification and entity typing tasks, and the results demonstrate\nthat ProtoVerb significantly outperforms current automatic verbalizers,\nespecially when training data is extremely scarce. More surprisingly, ProtoVerb\nconsistently boosts prompt-based tuning even on untuned PLMs, indicating an\nelegant non-tuning way to utilize PLMs. Our codes are avaliable at\nhttps://github.com/thunlp/OpenPrompt.\n","authors":["Ganqu Cui","Shengding Hu","Ning Ding","Longtao Huang","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2203.09770v1.pdf","comment":"11 pages. ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2203.09767v1","updated":"2022-03-18T06:40:39Z","published":"2022-03-18T06:40:39Z","title":"Speaker Embedding-aware Neural Diarization: a Novel Framework for\n  Overlapped Speech Diarization in the Meeting Scenario","summary":"  In this paper, we reformulate overlapped speech diarization as a single-label\nprediction problem, which is always treated as a multi-label classification\ntask in previous studies. Specifically, the multiple labels of each frame are\nencoded into a single label with the power set, which represents the possible\ncombinations of different speakers. Through this formulation, we propose the\nspeaker embedding-aware neural diarization (SEND) system. In SEND, the speech\nencoder, speaker encoder, similarity scores, and post-processing network are\noptimized to predict the power set encoded labels according to the similarities\nbetween speech features and speaker embeddings. Experimental results show that\nour method significantly outperforms the variational Bayesian hidden Markov\nmodel-based clustering algorithm (VBx). Besides, the proposed method has two\nbenefits compared with the target-speaker voice activity detection (TSVAD).\nFirst, SEND can achieve lower diarization error rates in the real meeting\nscenario. Second, when the training data has a high overlap ratio, the learning\nprocess of SEND is more stable than TSVAD.\n","authors":["Zhihao Du","Shiliang Zhang","Siqi Zheng","Zhijie Yan"],"pdf_url":"https://arxiv.org/pdf/2203.09767v1.pdf","comment":"Submitted to INTERSPEECH 2022, 5 parges, 1 figure"},{"id":"http://arxiv.org/abs/2004.02635v3","updated":"2022-03-18T06:24:05Z","published":"2020-04-03T10:48:01Z","title":"Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms","summary":"  We consider minimizing the sum of three convex functions, where the first one\nF is smooth, the second one is nonsmooth and proximable and the third one is\nthe composition of a nonsmooth proximable function with a linear operator L.\nThis template problem has many applications, for instance in image processing\nand machine learning. First, we propose a new primal-dual algorithm, which we\ncall PDDY, for this problem. It is constructed by applying Davis-Yin splitting\nto a monotone inclusion in a primal-dual product space, where the operators are\nmonotone under a specific metric depending on L. We show that three existing\nalgorithms (the two forms of the Condat-Vu algorithm and the PD3O algorithm)\nhave the same structure, so that PDDY is the fourth missing link in this\nself-consistent class of primal-dual algorithms. This representation eases the\nconvergence analysis: it allows us to derive sublinear convergence rates in\ngeneral, and linear convergence results in presence of strong convexity.\nMoreover, within our broad and flexible analysis framework, we propose new\nstochastic generalizations of the algorithms, in which a variance-reduced\nrandom estimate of the gradient of F is used, instead of the true gradient.\nFurthermore, we obtain, as a special case of PDDY, a linearly converging\nalgorithm for the minimization of a strongly convex function F under a linear\nconstraint; we discuss its important application to decentralized optimization.\n","authors":["Adil Salim","Laurent Condat","Konstantin Mishchenko","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2004.02635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.03924v2","updated":"2022-03-18T06:19:28Z","published":"2021-02-07T21:33:41Z","title":"Domain Adversarial Neural Networks for Domain Generalization: When It\n  Works and How to Improve","summary":"  Theoretically, domain adaptation is a well-researched problem. Further, this\ntheory has been well-used in practice. In particular, we note the bound on\ntarget error given by Ben-David et al. (2010) and the well-known\ndomain-aligning algorithm based on this work using Domain Adversarial Neural\nNetworks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple\nvariants of DANN have been proposed for the related problem of domain\ngeneralization, but without much discussion of the original motivating bound.\nIn this paper, we investigate the validity of DANN in domain generalization\nfrom this perspective. We investigate conditions under which application of\nDANN makes sense and further consider DANN as a dynamic process during\ntraining. Our investigation suggests that the application of DANN to domain\ngeneralization may not be as straightforward as it seems. To address this, we\ndesign an algorithmic extension to DANN in the domain generalization case. Our\nexperimentation validates both theory and algorithm.\n","authors":["Anthony Sicilia","Xingchen Zhao","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2102.03924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09756v1","updated":"2022-03-18T06:06:06Z","published":"2022-03-18T06:06:06Z","title":"AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack","summary":"  Deep neural networks (DNNs) have been proven to be vulnerable to adversarial\nexamples. A special branch of adversarial examples, namely sparse adversarial\nexamples, can fool the target DNNs by perturbing only a few pixels. However,\nmany existing sparse adversarial attacks use heuristic methods to select the\npixels to be perturbed, and regard the pixel selection and the adversarial\nattack as two separate steps. From the perspective of neural network pruning,\nwe propose a novel end-to-end sparse adversarial attack method, namely\nAutoAdversary, which can find the most important pixels automatically by\nintegrating the pixel selection into the adversarial attack. Specifically, our\nmethod utilizes a trainable neural network to generate a binary mask for the\npixel selection. After jointly optimizing the adversarial perturbation and the\nneural network, only the pixels corresponding to the value 1 in the mask are\nperturbed. Experiments demonstrate the superiority of our proposed method over\nseveral state-of-the-art methods. Furthermore, since AutoAdversary does not\nrequire a heuristic pixel selection process, it does not slow down excessively\nas other methods when the image size increases.\n","authors":["Jinqiao Li","Xiaotao Liu","Jian Zhao","Furao Shen"],"pdf_url":"https://arxiv.org/pdf/2203.09756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.13057v2","updated":"2022-03-18T05:52:49Z","published":"2021-10-25T15:52:06Z","title":"Robbing the Fed: Directly Obtaining Private Data in Federated Learning\n  with Modified Models","summary":"  Federated learning has quickly gained popularity with its promises of\nincreased user privacy and efficiency. Previous works have shown that federated\ngradient updates contain information that can be used to approximately recover\nuser data in some situations. These previous attacks on user privacy have been\nlimited in scope and do not scale to gradient updates aggregated over even a\nhandful of data points, leaving some to conclude that data privacy is still\nintact for realistic training regimes. In this work, we introduce a new threat\nmodel based on minimal but malicious modifications of the shared model\narchitecture which enable the server to directly obtain a verbatim copy of user\ndata from gradient updates without solving difficult inverse problems. Even\nuser data aggregated over large batches -- where previous methods fail to\nextract meaningful content -- can be reconstructed by these minimally modified\nmodels.\n","authors":["Liam Fowl","Jonas Geiping","Wojtek Czaja","Micah Goldblum","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2110.13057v2.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2203.09755v1","updated":"2022-03-18T05:49:13Z","published":"2022-03-18T05:49:13Z","title":"Distributed Sketching for Randomized Optimization: Exact\n  Characterization, Concentration and Lower Bounds","summary":"  We consider distributed optimization methods for problems where forming the\nHessian is computationally challenging and communication is a significant\nbottleneck. We leverage randomized sketches for reducing the problem dimensions\nas well as preserving privacy and improving straggler resilience in\nasynchronous distributed systems. We derive novel approximation guarantees for\nclassical sketching methods and establish tight concentration results that\nserve as both upper and lower bounds on the error. We then extend our analysis\nto the accuracy of parameter averaging for distributed sketches. Furthermore,\nwe develop unbiased parameter averaging methods for randomized second order\noptimization for regularized problems that employ sketching of the Hessian.\nExisting works do not take the bias of the estimators into consideration, which\nlimits their application to massively parallel computation. We provide\nclosed-form formulas for regularization parameters and step sizes that provably\nminimize the bias for sketched Newton directions. Additionally, we demonstrate\nthe implications of our theoretical findings via large scale experiments on a\nserverless cloud computing platform.\n","authors":["Burak Bartan","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2203.09755v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2002.06540"},{"id":"http://arxiv.org/abs/2203.09751v1","updated":"2022-03-18T05:25:35Z","published":"2022-03-18T05:25:35Z","title":"Look-Ahead Acquisition Functions for Bernoulli Level Set Estimation","summary":"  Level set estimation (LSE) is the problem of identifying regions where an\nunknown function takes values above or below a specified threshold. Active\nsampling strategies for efficient LSE have primarily been studied in\ncontinuous-valued functions. Motivated by applications in human psychophysics\nwhere common experimental designs produce binary responses, we study LSE active\nsampling with Bernoulli outcomes. With Gaussian process classification\nsurrogate models, the look-ahead model posteriors used by state-of-the-art\ncontinuous-output methods are intractable. However, we derive analytic\nexpressions for look-ahead posteriors of sublevel set membership, and show how\nthese lead to analytic expressions for a class of look-ahead LSE acquisition\nfunctions, including information-based methods. Benchmark experiments show the\nimportance of considering the global look-ahead impact on the entire posterior.\nWe demonstrate a clear benefit to using this new class of acquisition functions\non benchmark problems, and on a challenging real-world task of estimating a\nhigh-dimensional contrast sensitivity function.\n","authors":["Benjamin Letham","Phillip Guan","Chase Tymms","Eytan Bakshy","Michael Shvartsman"],"pdf_url":"https://arxiv.org/pdf/2203.09751v1.pdf","comment":"In: Proceedings of the 25th International Conference on Artificial\n  Intelligence and Statistics, AISTATS"},{"id":"http://arxiv.org/abs/2203.05297v3","updated":"2022-03-18T04:59:49Z","published":"2022-03-10T11:19:52Z","title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for\n  Conversational Gestures Synthesis","summary":"  Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n","authors":["Haiyang Liu","Zihao Zhu","Naoya Iwamoto","Yichen Peng","Zhengqing Li","You Zhou","Elif Bozkurt","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.05297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09747v1","updated":"2022-03-18T04:58:34Z","published":"2022-03-18T04:58:34Z","title":"Efficient Split-Mix Federated Learning for On-Demand and In-Situ\n  Customization","summary":"  Federated learning (FL) provides a distributed learning framework for\nmultiple participants to collaborate learning without sharing raw data. In many\npractical FL scenarios, participants have heterogeneous resources due to\ndisparities in hardware and inference dynamics that require quickly loading\nmodels of different sizes and levels of robustness. The heterogeneity and\ndynamics together impose significant challenges to existing FL approaches and\nthus greatly limit FL's applicability. In this paper, we propose a novel\nSplit-Mix FL strategy for heterogeneous participants that, once training is\ndone, provides in-situ customization of model sizes and robustness.\nSpecifically, we achieve customization by learning a set of base sub-networks\nof different sizes and robustness levels, which are later aggregated on-demand\naccording to inference requirements. This split-mix strategy achieves\ncustomization with high efficiency in communication, storage, and inference.\nExtensive experiments demonstrate that our method provides better in-situ\ncustomization than the existing heterogeneous-architecture FL methods. Codes\nand pre-trained models are available: https://github.com/illidanlab/SplitMix.\n","authors":["Junyuan Hong","Haotao Wang","Zhangyang Wang","Jiayu Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.09747v1.pdf","comment":"published at ICLR2022"},{"id":"http://arxiv.org/abs/2203.09746v1","updated":"2022-03-18T04:58:18Z","published":"2022-03-18T04:58:18Z","title":"Soft Smoothness for Audio Inpainting Using a Latent Matrix Model in\n  Delay-embedded Space","summary":"  Here, we propose a new reconstruction method of smooth time-series signals. A\nkey concept of this study is not considering the model in signal space, but in\ndelay-embedded space. In other words, we indirectly represent a time-series\nsignal as an output of inverse delay-embedding of a matrix, and the matrix is\nconstrained. Based on the model under inverse delay-embedding, we propose to\nconstrain the matrix to be rank-1 with smooth factor vectors. The proposed\nmodel is closely related to the convolutional model, and quadratic variation\n(QV) regularization. Especially, the proposed method can be characterized as a\ngeneralization of QV regularization. In addition, we show that the proposed\nmethod provides the softer smoothness than QV regularization. Experiments of\naudio inpainting and declipping are conducted to show its advantages in\ncomparison with several existing interpolation methods and sparse modeling.\n","authors":["Tatsuya Yokota"],"pdf_url":"https://arxiv.org/pdf/2203.09746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09739v1","updated":"2022-03-18T04:38:18Z","published":"2022-03-18T04:38:18Z","title":"Do Deep Networks Transfer Invariances Across Classes?","summary":"  To generalize well, classifiers must learn to be invariant to nuisance\ntransformations that do not alter an input's class. Many problems have\n\"class-agnostic\" nuisance transformations that apply similarly to all classes,\nsuch as lighting and background changes for image classification. Neural\nnetworks can learn these invariances given sufficient data, but many real-world\ndatasets are heavily class imbalanced and contain only a few examples for most\nof the classes. We therefore pose the question: how well do neural networks\ntransfer class-agnostic invariances learned from the large classes to the small\nones? Through careful experimentation, we observe that invariance to\nclass-agnostic transformations is still heavily dependent on class size, with\nthe networks being much less invariant on smaller classes. This result holds\neven when using data balancing techniques, and suggests poor invariance\ntransfer across classes. Our results provide one explanation for why\nclassifiers generalize poorly on unbalanced and long-tailed distributions.\nBased on this analysis, we show how a generative approach for learning the\nnuisance transformations can help transfer invariances across classes and\nimprove performance on a set of imbalanced image classification benchmarks.\nSource code for our experiments is available at\nhttps://github.com/AllanYangZhou/generative-invariance-transfer.\n","authors":["Allan Zhou","Fahim Tajwar","Alexander Robey","Tom Knowles","George J. Pappas","Hamed Hassani","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2203.09739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.01132v3","updated":"2022-03-18T04:34:04Z","published":"2020-08-03T18:51:24Z","title":"Accuracy and Fairness Trade-offs in Machine Learning: A Stochastic\n  Multi-Objective Approach","summary":"  In the application of machine learning to real-life decision-making systems,\ne.g., credit scoring and criminal justice, the prediction outcomes might\ndiscriminate against people with sensitive attributes, leading to unfairness.\nThe commonly used strategy in fair machine learning is to include fairness as a\nconstraint or a penalization term in the minimization of the prediction loss,\nwhich ultimately limits the information given to decision-makers. In this\npaper, we introduce a new approach to handle fairness by formulating a\nstochastic multi-objective optimization problem for which the corresponding\nPareto fronts uniquely and comprehensively define the accuracy-fairness\ntrade-offs. We have then applied a stochastic approximation-type method to\nefficiently obtain well-spread and accurate Pareto fronts, and by doing so we\ncan handle training data arriving in a streaming way.\n","authors":["Suyun Liu","Luis Nunes Vicente"],"pdf_url":"https://arxiv.org/pdf/2008.01132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/1906.06659v7","updated":"2022-03-18T04:24:09Z","published":"2019-06-16T07:26:21Z","title":"A Generalized Minimax Q-learning Algorithm for Two-Player Zero-Sum\n  Stochastic Games","summary":"  We consider the problem of two-player zero-sum games. This problem is\nformulated as a min-max Markov game in the literature. The solution of this\ngame, which is the min-max payoff, starting from a given state is called the\nmin-max value of the state. In this work, we compute the solution of the\ntwo-player zero-sum game utilizing the technique of successive relaxation that\nhas been successfully applied in the literature to compute a faster value\niteration algorithm in the context of Markov Decision Processes. We extend the\nconcept of successive relaxation to the setting of two-player zero-sum games.\nWe show that, under a special structure on the game, this technique facilitates\nfaster computation of the min-max value of the states. We then derive a\ngeneralized minimax Q-learning algorithm that computes the optimal policy when\nthe model information is not known. Finally, we prove the convergence of the\nproposed generalized minimax Q-learning algorithm utilizing stochastic\napproximation techniques, under an assumption on the boundedness of iterates.\nThrough experiments, we demonstrate the effectiveness of our proposed\nalgorithm.\n","authors":["Raghuram Bharadwaj Diddigi","Chandramouli Kamanchi","Shalabh Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/1906.06659v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09735v1","updated":"2022-03-18T04:23:20Z","published":"2022-03-18T04:23:20Z","title":"PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive\n  Weakly-Supervised Learning","summary":"  Weakly-supervised learning (WSL) has shown promising results in addressing\nlabel scarcity on many NLP tasks, but manually designing a comprehensive,\nhigh-quality labeling rule set is tedious and difficult. We study interactive\nweakly-supervised learning -- the problem of iteratively and automatically\ndiscovering novel labeling rules from data to improve the WSL model. Our\nproposed model, named PRBoost, achieves this goal via iterative prompt-based\nrule discovery and model boosting. It uses boosting to identify large-error\ninstances and then discovers candidate rules from them by prompting pre-trained\nLMs with rule templates. The candidate rules are judged by human experts, and\nthe accepted rules are used to generate complementary weak labels and\nstrengthen the current model. Experiments on four tasks show PRBoost\noutperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with\nfully supervised models. Our Implementation is available at\n\\url{https://github.com/rz-zhang/PRBoost}.\n","authors":["Rongzhi Zhang","Yue Yu","Pranav Shetty","Le Song","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.09735v1.pdf","comment":"ACL 2022 (Main Conference). Code: https://github.com/rz-zhang/PRBoost"},{"id":"http://arxiv.org/abs/2111.03772v2","updated":"2022-03-18T04:09:09Z","published":"2021-11-06T01:30:51Z","title":"Dynamic Regret Minimization for Control of Non-stationary Linear\n  Dynamical Systems","summary":"  We consider the problem of controlling a Linear Quadratic Regulator (LQR)\nsystem over a finite horizon $T$ with fixed and known cost matrices $Q,R$, but\nunknown and non-stationary dynamics $\\{A_t, B_t\\}$. The sequence of dynamics\nmatrices can be arbitrary, but with a total variation, $V_T$, assumed to be\n$o(T)$ and unknown to the controller. Under the assumption that a sequence of\nstabilizing, but potentially sub-optimal controllers is available for all $t$,\nwe present an algorithm that achieves the optimal dynamic regret of\n$\\tilde{\\mathcal{O}}\\left(V_T^{2/5}T^{3/5}\\right)$. With piece-wise constant\ndynamics, our algorithm achieves the optimal regret of\n$\\tilde{\\mathcal{O}}(\\sqrt{ST})$ where $S$ is the number of switches. The crux\nof our algorithm is an adaptive non-stationarity detection strategy, which\nbuilds on an approach recently developed for contextual Multi-armed Bandit\nproblems. We also argue that non-adaptive forgetting (e.g., restarting or using\nsliding window learning with a static window size) may not be regret optimal\nfor the LQR problem, even when the window size is optimally tuned with the\nknowledge of $V_T$. The main technical challenge in the analysis of our\nalgorithm is to prove that the ordinary least squares (OLS) estimator has a\nsmall bias when the parameter to be estimated is non-stationary. Our analysis\nalso highlights that the key motif driving the regret is that the LQR problem\nis in spirit a bandit problem with linear feedback and locally quadratic cost.\nThis motif is more universal than the LQR problem itself, and therefore we\nbelieve our results should find wider application.\n","authors":["Yuwei Luo","Varun Gupta","Mladen Kolar"],"pdf_url":"https://arxiv.org/pdf/2111.03772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09727v1","updated":"2022-03-18T03:52:18Z","published":"2022-03-18T03:52:18Z","title":"DEFORM: A Practical, Universal Deep Beamforming System","summary":"  We introduce, design, and evaluate a set of universal receiver beamforming\ntechniques. Our approach and system DEFORM, a Deep Learning (DL) based RX\nbeamforming achieves significant gain for multi antenna RF receivers while\nbeing agnostic to the transmitted signal features (e.g., modulation or\nbandwidth). It is well known that combining coherent RF signals from multiple\nantennas results in a beamforming gain proportional to the number of receiving\nelements. However in practice, this approach heavily relies on explicit channel\nestimation techniques, which are link specific and require significant\ncommunication overhead to be transmitted to the receiver. DEFORM addresses this\nchallenge by leveraging Convolutional Neural Network to estimate the channel\ncharacteristics in particular the relative phase to antenna elements. It is\nspecifically designed to address the unique features of wireless signals\ncomplex samples, such as the ambiguous $2\\pi$ phase discontinuity and the high\nsensitivity of the link Bit Error Rate. The channel prediction is subsequently\nused in the Maximum Ratio Combining algorithm to achieve an optimal combination\nof the received signals. While being trained on a fixed, basic RF settings, we\nshow that DEFORM DL model is universal, achieving up to 3 dB of SNR gain for a\ntwo antenna receiver in extensive experiments demonstrating various settings of\nmodulations, bandwidths, and channels. The universality of DEFORM is\ndemonstrated through joint beamforming relaying of LoRa (Chirp Spread Spectrum\nmodulation) and ZigBee signals, achieving significant improvements to Packet\nLoss/Delivery Rates relatively to conventional Amplify and Forward (LoRa PLR\nreduced by 23 times and ZigBee PDR increased by 8 times).\n","authors":["Hai N. Nguyen","Guevara Noubir"],"pdf_url":"https://arxiv.org/pdf/2203.09727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09721v1","updated":"2022-03-18T03:37:14Z","published":"2022-03-18T03:37:14Z","title":"Deterministic Bridge Regression for Compressive Classification","summary":"  Pattern classification with compact representation is an important component\nin machine intelligence. In this work, an analytic bridge solution is proposed\nfor compressive classification. The proposal has been based upon solving a\npenalized error formulation utilizing an approximated $\\ell_p$-norm. The\nsolution comes in a primal form for over-determined systems and in a dual form\nfor under-determined systems. While the primal form is suitable for problems of\nlow dimension with large data samples, the dual form is suitable for problems\nof high dimension but with a small number of data samples. The solution has\nalso been extended for problems with multiple classification outputs. Numerical\nstudies based on simulated and real-world data validated the effectiveness of\nthe proposed solution.\n","authors":["Kar-Ann Toh","Giuseppe Molteni","Zhiping Lin"],"pdf_url":"https://arxiv.org/pdf/2203.09721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09717v1","updated":"2022-03-18T03:30:57Z","published":"2022-03-18T03:30:57Z","title":"Towards an AI-Driven Universal Anti-Jamming Solution with Convolutional\n  Interference Cancellation Network","summary":"  Wireless links are increasingly used to deliver critical services, while\nintentional interference (jamming) remains a very serious threat to such\nservices. In this paper, we are concerned with the design and evaluation of a\nuniversal anti-jamming building block, that is agnostic to the specifics of the\ncommunication link and can therefore be combined with existing technologies. We\nbelieve that such a block should not require explicit probes, sounding,\ntraining sequences, channel estimation, or even the cooperation of the\ntransmitter. To meet these requirements, we propose an approach that relies on\nadvances in Machine Learning, and the promises of neural accelerators and\nsoftware defined radios. We identify and address multiple challenges, resulting\nin a convolutional neural network architecture and models for a multi-antenna\nsystem to infer the existence of interference, the number of interfering\nemissions and their respective phases. This information is continuously fed\ninto an algorithm that cancels the interfering signal. We develop a two-antenna\nprototype system and evaluate our jamming cancellation approach in various\nenvironment settings and modulation schemes using Software Defined Radio\nplatforms. We demonstrate that the receiving node equipped with our approach\ncan detect a jammer with over 99% of accuracy and achieve a Bit Error Rate\n(BER) as low as $10^{-6}$ even when the jammer power is nearly two orders of\nmagnitude (18 dB) higher than the legitimate signal, and without requiring\nmodifications to the link modulation. In non-adversarial settings, our approach\ncan have other advantages such as detecting and mitigating collisions.\n","authors":["Hai N. Nguyen","Guevara Noubir"],"pdf_url":"https://arxiv.org/pdf/2203.09717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.02387v3","updated":"2022-03-18T03:29:10Z","published":"2021-11-03T17:55:36Z","title":"An Empirical Study of Training End-to-End Vision-and-Language\n  Transformers","summary":"  Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.\n","authors":["Zi-Yi Dou","Yichong Xu","Zhe Gan","Jianfeng Wang","Shuohang Wang","Lijuan Wang","Chenguang Zhu","Pengchuan Zhang","Lu Yuan","Nanyun Peng","Zicheng Liu","Michael Zeng"],"pdf_url":"https://arxiv.org/pdf/2111.02387v3.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.09710v1","updated":"2022-03-18T03:09:24Z","published":"2022-03-18T03:09:24Z","title":"Learning Stabilizable Deep Dynamics Models","summary":"  When neural networks are used to model dynamics, properties such as stability\nof the dynamics are generally not guaranteed. In contrast, there is a recent\nmethod for learning the dynamics of autonomous systems that guarantees global\nexponential stability using neural networks. In this paper, we propose a new\nmethod for learning the dynamics of input-affine control systems. An important\nfeature is that a stabilizing controller and control Lyapunov function of the\nlearned model are obtained as well. Moreover, the proposed method can also be\napplied to solving Hamilton-Jacobi inequalities. The usefulness of the proposed\nmethod is examined through numerical examples.\n","authors":["Kenji Kashima","Ryota Yoshiuchi","Yu Kawano"],"pdf_url":"https://arxiv.org/pdf/2203.09710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09702v1","updated":"2022-03-18T02:32:05Z","published":"2022-03-18T02:32:05Z","title":"Federated Learning for Privacy Preservation in Smart Healthcare Systems:\n  A Comprehensive Survey","summary":"  Recent advances in electronic devices and communication infrastructure have\nrevolutionized the traditional healthcare system into a smart healthcare system\nby using IoMT devices. However, due to the centralized training approach of\nartificial intelligence (AI), the use of mobile and wearable IoMT devices\nraises privacy concerns with respect to the information that has been\ncommunicated between hospitals and end users. The information conveyed by the\nIoMT devices is highly confidential and can be exposed to adversaries. In this\nregard, federated learning (FL), a distributive AI paradigm has opened up new\nopportunities for privacy-preservation in IoMT without accessing the\nconfidential data of the participants. Further, FL provides privacy to end\nusers as only gradients are shared during training. For these specific\nproperties of FL, in this paper we present privacy related issues in IoMT.\nAfterwards, we present the role of FL in IoMT networks for privacy preservation\nand introduce some advanced FL architectures incorporating deep reinforcement\nlearning (DRL), digital twin, and generative adversarial networks (GANs) for\ndetecting privacy threats. Subsequently, we present some practical\nopportunities of FL in smart healthcare systems. At the end, we conclude this\nsurvey by providing open research challenges for FL that can be used in future\nsmart healthcare systems\n","authors":["Mansoor Ali","Faisal Naeem","Muhammad Tariq","Geroges Kaddoum"],"pdf_url":"https://arxiv.org/pdf/2203.09702v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2109.04275v4","updated":"2022-03-18T06:56:04Z","published":"2021-09-09T13:50:22Z","title":"M5Product: Self-harmonized Contrastive Learning for E-commercial\n  Multi-modal Pretraining","summary":"  Despite the potential of multi-modal pre-training to learn highly\ndiscriminative feature representations from complementary data modalities,\ncurrent progress is being slowed by the lack of large-scale modality-diverse\ndatasets. By leveraging the natural suitability of E-commerce, where different\nmodalities capture complementary semantic information, we contribute a\nlarge-scale multi-modal pre-training dataset M5Product. The dataset comprises 5\nmodalities (image, text, table, video, and audio), covers over 6,000 categories\nand 5,000 attributes, and is 500 larger than the largest publicly available\ndataset with a similar number of modalities. Furthermore, M5Product contains\nincomplete modality pairs and noise while also having a long-tailed\ndistribution, resembling most real-world problems. We further propose\nSelf-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework\nthat integrates the different modalities into a unified model through an\nadaptive feature fusion mechanism, where the importance of each modality is\nlearned directly from the modality embeddings and impacts the inter-modality\ncontrastive learning and masked tasks within a multi-modal transformer model.\nWe evaluate the current multi-modal pre-training state-of-the-art approaches\nand benchmark their ability to learn from unlabeled data when faced with the\nlarge number of modalities in the M5Product dataset. We conduct extensive\nexperiments on four downstream tasks and demonstrate the superiority of our\nSCALE model, providing insights into the importance of dataset scale and\ndiversity.\n","authors":["Xiao Dong","Xunlin Zhan","Yangxin Wu","Yunchao Wei","Michael C. Kampffmeyer","Xiaoyong Wei","Minlong Lu","Yaowei Wang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2109.04275v4.pdf","comment":"CVPR2022"},{"id":"http://arxiv.org/abs/2203.09767v1","updated":"2022-03-18T06:40:39Z","published":"2022-03-18T06:40:39Z","title":"Speaker Embedding-aware Neural Diarization: a Novel Framework for\n  Overlapped Speech Diarization in the Meeting Scenario","summary":"  In this paper, we reformulate overlapped speech diarization as a single-label\nprediction problem, which is always treated as a multi-label classification\ntask in previous studies. Specifically, the multiple labels of each frame are\nencoded into a single label with the power set, which represents the possible\ncombinations of different speakers. Through this formulation, we propose the\nspeaker embedding-aware neural diarization (SEND) system. In SEND, the speech\nencoder, speaker encoder, similarity scores, and post-processing network are\noptimized to predict the power set encoded labels according to the similarities\nbetween speech features and speaker embeddings. Experimental results show that\nour method significantly outperforms the variational Bayesian hidden Markov\nmodel-based clustering algorithm (VBx). Besides, the proposed method has two\nbenefits compared with the target-speaker voice activity detection (TSVAD).\nFirst, SEND can achieve lower diarization error rates in the real meeting\nscenario. Second, when the training data has a high overlap ratio, the learning\nprocess of SEND is more stable than TSVAD.\n","authors":["Zhihao Du","Shiliang Zhang","Siqi Zheng","Zhijie Yan"],"pdf_url":"https://arxiv.org/pdf/2203.09767v1.pdf","comment":"Submitted to INTERSPEECH 2022, 5 parges, 1 figure"},{"id":"http://arxiv.org/abs/2203.05297v3","updated":"2022-03-18T04:59:49Z","published":"2022-03-10T11:19:52Z","title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for\n  Conversational Gestures Synthesis","summary":"  Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n","authors":["Haiyang Liu","Zihao Zhu","Naoya Iwamoto","Yichen Peng","Zhengqing Li","You Zhou","Elif Bozkurt","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.05297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09694v1","updated":"2022-03-18T01:49:40Z","published":"2022-03-18T01:49:40Z","title":"Group Contextualization for Video Recognition","summary":"  Learning discriminative representation from the complex spatio-temporal\ndynamic space is essential for video recognition. On top of those stylized\nspatio-temporal computational units, further refining the learnt feature with\naxial contexts is demonstrated to be promising in achieving this goal. However,\nprevious works generally focus on utilizing a single kind of contexts to\ncalibrate entire feature channels and could hardly apply to deal with diverse\nvideo activities. The problem can be tackled by using pair-wise spatio-temporal\nattentions to recompute feature response with cross-axis contexts at the\nexpense of heavy computations. In this paper, we propose an efficient feature\nrefinement method that decomposes the feature channels into several groups and\nseparately refines them with different axial contexts in parallel. We refer\nthis lightweight feature calibration as group contextualization (GC).\nSpecifically, we design a family of efficient element-wise calibrators, i.e.,\nECal-G/S/T/L, where their axial contexts are information dynamics aggregated\nfrom other axes either globally or locally, to contextualize feature channel\ngroups. The GC module can be densely plugged into each residual layer of the\noff-the-shelf video networks. With little computational overhead, consistent\nimprovement is observed when plugging in GC on different networks. By utilizing\ncalibrators to embed feature with four different kinds of contexts in parallel,\nthe learnt representation is expected to be more resilient to diverse types of\nactivities. On videos with rich temporal variations, empirically GC can boost\nthe performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the\nstate-of-the-art video networks. Code is available at\nhttps://github.com/haoyanbin918/Group-Contextualization.\n","authors":["Yanbin Hao","Hao Zhang","Chong-Wah Ngo","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2203.09694v1.pdf","comment":null}]},"2022-03-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.11187v1","updated":"2022-03-21T17:57:58Z","published":"2022-03-21T17:57:58Z","title":"Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning","summary":"  We study the challenge of learning causal reasoning over procedural text to\nanswer \"What if...\" questions when external commonsense knowledge is required.\nWe propose a novel multi-hop graph reasoning model to 1) efficiently extract a\ncommonsense subgraph with the most relevant information from a large knowledge\ngraph; 2) predict the causal answer by reasoning over the representations\nobtained from the commonsense subgraph and the contextual interactions between\nthe questions and context. We evaluate our model on WIQA benchmark and achieve\nstate-of-the-art performance compared to the recent models.\n","authors":["Chen Zheng","Parisa Kordjamshidi"],"pdf_url":"https://arxiv.org/pdf/2203.11187v1.pdf","comment":"Accepted by ACL 2022 findings short paper"},{"id":"http://arxiv.org/abs/2110.04541v3","updated":"2022-03-21T17:57:13Z","published":"2021-10-09T11:05:16Z","title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining\n  Example Design","summary":"  Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for pretraining\nexample design indicates new training schemes for self-improving\nrepresentations.\n","authors":["Yoav Levine","Noam Wies","Daniel Jannai","Dan Navon","Yedid Hoshen","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2110.04541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.08678v2","updated":"2022-03-21T17:49:05Z","published":"2021-09-17T17:58:28Z","title":"RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base\n  Question Answering","summary":"  Existing KBQA approaches, despite achieving strong performance on i.i.d. test\ndata, often struggle in generalizing to questions involving unseen KB schema\nitems. Prior ranking-based approaches have shown some success in\ngeneralization, but suffer from the coverage issue. We present RnG-KBQA, a\nRank-and-Generate approach for KBQA, which remedies the coverage issue with a\ngeneration model while preserving a strong generalization capability. Our\napproach first uses a contrastive ranker to rank a set of candidate logical\nforms obtained by searching over the knowledge graph. It then introduces a\ntailored generation model conditioned on the question and the top-ranked\ncandidates to compose the final logical form. We achieve new state-of-the-art\nresults on GrailQA and WebQSP datasets. In particular, our method surpasses the\nprior state-of-the-art by a large margin on the GrailQA leaderboard. In\naddition, RnG-KBQA outperforms all prior approaches on the popular WebQSP\nbenchmark, even including the ones that use the oracle entity linking. The\nexperimental results demonstrate the effectiveness of the interplay between\nranking and generation, which leads to the superior performance of our proposed\napproach across all settings with especially strong improvements in zero-shot\ngeneralization.\n","authors":["Xi Ye","Semih Yavuz","Kazuma Hashimoto","Yingbo Zhou","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2109.08678v2.pdf","comment":"ACL 2022 Camera-ready"},{"id":"http://arxiv.org/abs/2203.11171v1","updated":"2022-03-21T17:48:52Z","published":"2022-03-21T17:48:52Z","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models","summary":"  We explore a simple ensemble strategy, self-consistency, that significantly\nimproves the reasoning accuracy of large language models. The idea is to sample\na diverse set of outputs from a language model and return the most consistent\nanswer in the set. Such ensembling method improves reasoning accuracy when\ncombined with chain of thought prompting. For arithmetic and commonsense\nreasoning benchmarks we find that self-consistency yields significant accuracy\nimprovements in a variety of datasets, such as GSM8K (+10%), SVAMP (+14%),\nMultiArith (+24%), CommonsenseQA (+5%) and ARC (easy +4%, challenge +5%).\n","authors":["Xuezhi Wang","Jason Wei","Dale Schuurmans","Quoc Le","Ed Chi","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.11171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11147v1","updated":"2022-03-21T17:26:29Z","published":"2022-03-21T17:26:29Z","title":"Teaching language models to support answers with verified quotes","summary":"  Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.\n","authors":["Jacob Menick","Maja Trebacz","Vladimir Mikulik","John Aslanides","Francis Song","Martin Chadwick","Mia Glaese","Susannah Young","Lucy Campbell-Gillingham","Geoffrey Irving","Nat McAleese"],"pdf_url":"https://arxiv.org/pdf/2203.11147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09590v2","updated":"2022-03-21T17:13:28Z","published":"2022-03-17T20:08:25Z","title":"Enhanced Temporal Knowledge Embeddings with Contextualized Language\n  Representations","summary":"  With the emerging research effort to integrate structured and unstructured\nknowledge, many approaches incorporate factual knowledge into pre-trained\nlanguage models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP\ntasks. However, (1) they only consider static factual knowledge, but knowledge\ngraphs (KGs) also contain temporal facts or events indicating evolutionary\nrelationships among entities at different timestamps. (2) PLMs cannot be\ndirectly applied to many KG tasks, such as temporal KG completion.\n  In this paper, we focus on \\textbf{e}nhancing temporal knowledge embeddings\nwith \\textbf{co}ntextualized \\textbf{la}nguage representations (ECOLA). We\nalign structured knowledge contained in temporal knowledge graphs with their\ntextual descriptions extracted from news articles and propose a novel\nknowledge-text prediction task to inject the abundant information from\ndescriptions into temporal knowledge embeddings. ECOLA jointly optimizes the\nknowledge-text prediction objective and the temporal knowledge embeddings,\nwhich can simultaneously take full advantage of textual and knowledge\ninformation. For training ECOLA, we introduce three temporal KG datasets with\naligned textual descriptions. Experimental results on the temporal knowledge\ngraph completion task show that ECOLA outperforms state-of-the-art temporal KG\nmodels by a large margin. The proposed datasets can serve as new temporal KG\nbenchmarks and facilitate future research on structured and unstructured\nknowledge integration.\n","authors":["Zhen Han","Ruotong Liao","Beiyan Liu","Yao Zhang","Zifeng Ding","Heinz Köppl","Hinrich Schütze","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2203.09590v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2203.11131v1","updated":"2022-03-21T17:05:54Z","published":"2022-03-21T17:05:54Z","title":"Towards Explainable Evaluation Metrics for Natural Language Generation","summary":"  Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics (such as BERTScore or MoverScore) are based on black-box\nlanguage models such as BERT or XLM-R. They often achieve strong correlations\nwith human judgments, but recent research indicates that the lower-quality\nclassical metrics remain dominant, one of the potential reasons being that\ntheir decision processes are transparent. To foster more widespread acceptance\nof the novel high-quality metrics, explainability thus becomes crucial. In this\nconcept paper, we identify key properties and propose key goals of explainable\nmachine translation evaluation metrics. We also provide a synthesizing overview\nover recent approaches for explainable machine translation metrics and discuss\nhow they relate to those goals and properties. Further, we conduct own novel\nexperiments, which (among others) find that current adversarial NLP techniques\nare unsuitable for automatically identifying limitations of high-quality\nblack-box evaluation metrics, as they are not meaning-preserving. Finally, we\nprovide a vision of future approaches to explainable evaluation metrics and\ntheir evaluation. We hope that our work can help catalyze and guide future\nresearch on explainable evaluation metrics and, mediately, also contribute to\nbetter and more transparent text generation systems.\n","authors":["Christoph Leiter","Piyawat Lertvittayakumjorn","Marina Fomicheva","Wei Zhao","Yang Gao","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2203.11131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11130v1","updated":"2022-03-21T17:05:23Z","published":"2022-03-21T17:05:23Z","title":"PACS: A Dataset for Physical Audiovisual CommonSense Reasoning","summary":"  In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, they should be able to reason about the\nphysical world by understanding the physical properties and affordances of\navailable objects, how they can be manipulated, and how they interact with\nother physical objects. This research field of physical commonsense reasoning\nis fundamentally a multi-sensory task since physical properties are manifested\nthrough multiple modalities, two of them being vision and acoustics. Our paper\ntakes a step towards real-world physical commonsense reasoning by contributing\nPACS: the first audiovisual benchmark annotated for physical commonsense\nattributes. PACS contains a total of 13,400 question-answer pairs, involving\n1,377 unique physical commonsense questions and 1,526 videos. Our dataset\nprovides new opportunities to advance the research field of physical reasoning\nby bringing audio as a core component of this multimodal problem. Using PACS,\nwe evaluate multiple state-of-the-art models on this new challenging task.\nWhile some models show promising results (70% accuracy), they all fall short of\nhuman performance (95% accuracy). We conclude the paper by demonstrating the\nimportance of multimodal reasoning and providing possible avenues for future\nresearch.\n","authors":["Samuel Yu","Peter Wu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2203.11130v1.pdf","comment":"38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2203.11092v1","updated":"2022-03-21T16:17:38Z","published":"2022-03-21T16:17:38Z","title":"Automated Clinical Coding: What, Why, and Where We Are?","summary":"  Clinical coding is the task of transforming medical information in a\npatient's health records into structured codes so that they can be used for\nstatistical analysis. This is a cognitive and time-consuming task that follows\na standard process in order to achieve a high level of consistency. Clinical\ncoding could potentially be supported by an automated system to improve the\nefficiency and accuracy of the process. We introduce the idea of automated\nclinical coding and summarise its challenges from the perspective of Artificial\nIntelligence (AI) and Natural Language Processing (NLP), based on the\nliterature, our project experience over the past two and half years (late 2019\n- early 2022), and discussions with clinical coding experts in Scotland and the\nUK. Our research reveals the gaps between the current deep learning-based\napproach applied to clinical coding and the need for explainability and\nconsistency in real-world practice. Knowledge-based methods that represent and\nreason the standard, explainable process of a task may need to be incorporated\ninto deep learning-based methods for clinical coding. Automated clinical coding\nis a promising task for AI, despite the technical and organisational\nchallenges. Coders are needed to be involved in the development process. There\nis much to achieve to develop and deploy an AI-based automated system to\nsupport coding in the next five years and beyond.\n","authors":["Hang Dong","Matúš Falis","William Whiteley","Beatrice Alex","Shaoxiong Ji","Jiaoyan Chen","Honghan Wu"],"pdf_url":"https://arxiv.org/pdf/2203.11092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.07368v3","updated":"2022-03-21T15:49:51Z","published":"2021-09-15T15:22:10Z","title":"Learning When to Translate for Streaming Speech","summary":"  How to find proper moments to generate partial sentence translation given a\nstreaming speech input? Existing approaches waiting-and-translating for a fixed\nduration often break the acoustic units in speech, since the boundaries between\nacoustic units in speech are not even. In this paper, we propose MoSST, a\nsimple yet effective method for translating streaming speech content. Given a\nusually long speech sequence, we develop an efficient monotonic segmentation\nmodule inside an encoder-decoder model to accumulate acoustic information\nincrementally and detect proper speech unit boundaries for the input in speech\ntranslation task. Experiments on multiple translation directions of the MuST-C\ndataset show that MoSST outperforms existing methods and achieves the best\ntrade-off between translation quality (BLEU) and latency. Our code is available\nat https://github.com/dqqcasia/mosst.\n","authors":["Qianqian Dong","Yaoming Zhu","Mingxuan Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2109.07368v3.pdf","comment":"Accept to ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2203.11054v1","updated":"2022-03-21T15:26:35Z","published":"2022-03-21T15:26:35Z","title":"Targeted Extraction of Temporal Facts from Textual Resources for\n  Improved Temporal Question Answering over Knowledge Bases","summary":"  Knowledge Base Question Answering (KBQA) systems have the goal of answering\ncomplex natural language questions by reasoning over relevant facts retrieved\nfrom Knowledge Bases (KB). One of the major challenges faced by these systems\nis their inability to retrieve all relevant facts due to factors such as\nincomplete KB and entity/relation linking errors. In this paper, we address\nthis particular challenge for systems handling a specific category of questions\ncalled temporal questions, where answer derivation involve reasoning over facts\nasserting point/intervals of time for various events. We propose a novel\napproach where a targeted temporal fact extraction technique is used to assist\nKBQA whenever it fails to retrieve temporal facts from the KB. We use\n$\\lambda$-expressions of the questions to logically represent the component\nfacts and the reasoning steps needed to derive the answer. This allows us to\nspot those facts that failed to get retrieved from the KB and generate textual\nqueries to extract them from the textual resources in an open-domain question\nanswering fashion. We evaluated our approach on a benchmark temporal question\nanswering dataset considering Wikidata and Wikipedia respectively as the KB and\ntextual resource. Experimental results show a significant $\\sim$30\\% relative\nimprovement in answer accuracy, demonstrating the effectiveness of our\napproach.\n","authors":["Nithish Kannen","Udit Sharma","Sumit Neelam","Dinesh Khandelwal","Shajith Ikbal","Hima Karanam","L Venkata Subramaniam"],"pdf_url":"https://arxiv.org/pdf/2203.11054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11048v1","updated":"2022-03-21T15:14:10Z","published":"2022-03-21T15:14:10Z","title":"How Do We Answer Complex Questions: Discourse Structure of Long-form\n  Answers","summary":"  Long-form answers, consisting of multiple sentences, can provide nuanced and\ncomprehensive answers to a broader set of questions. To better understand this\ncomplex and understudied task, we study the functional structure of long-form\nanswers collected from three datasets, ELI5, WebGPT and Natural Questions. Our\nmain goal is to understand how humans organize information to craft complex\nanswers. We develop an ontology of six sentence-level functional roles for\nlong-form answers, and annotate 3.9k sentences in 640 answer paragraphs.\nDifferent answer collection methods manifest in different discourse structures.\nWe further analyze model-generated answers -- finding that annotators agree\nless with each other when annotating model-generated answers compared to\nannotating human-written answers. Our annotated data enables training a strong\nclassifier that can be used for automatic analysis. We hope our work can\ninspire future research on discourse-level modeling and evaluation of long-form\nQA systems.\n","authors":["Fangyuan Xu","Junyi Jessy Li","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2203.11048v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2202.05262v2","updated":"2022-03-21T15:13:09Z","published":"2022-02-10T18:59:54Z","title":"Locating and Editing Factual Knowledge in GPT","summary":"  We investigate the mechanisms underlying factual knowledge recall in\nautoregressive transformer language models. First, we develop a causal\nintervention for identifying neuron activations capable of altering a model's\nfactual predictions. Within large GPT-style models, this reveals two distinct\nsets of neurons that we hypothesize correspond to knowing an abstract fact and\nsaying a concrete word, respectively. This insight inspires the development of\nROME, a novel method for editing facts stored in model weights. For evaluation,\nwe assemble CounterFact, a dataset of over twenty thousand counterfactuals and\ntools to facilitate sensitive measurements of knowledge editing. Using\nCounterFact, we confirm the distinction between saying and knowing neurons, and\nwe find that ROME achieves state-of-the-art performance in knowledge editing\ncompared to other methods. An interactive demo notebook, full code\nimplementation, and the dataset are available at https://rome.baulab.info/.\n","authors":["Kevin Meng","David Bau","Alex Andonian","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2202.05262v2.pdf","comment":"21 pages, 22 figures. Code and data at https://rome.baulab.info/"},{"id":"http://arxiv.org/abs/2203.07860v2","updated":"2022-03-21T14:47:58Z","published":"2022-03-15T13:11:07Z","title":"Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models\n  Robust with Little Cost","summary":"  State-of-the-art NLP systems represent inputs with word embeddings, but these\nare brittle when faced with Out-of-Vocabulary (OOV) words. To address this\nissue, we follow the principle of mimick-like models to generate vectors for\nunseen words, by learning the behavior of pre-trained embeddings using only the\nsurface form of words. We present a simple contrastive learning framework,\nLOVE, which extends the word representation of an existing pre-trained language\nmodel (such as BERT), and makes it robust to OOV with few additional\nparameters. Extensive evaluations demonstrate that our lightweight model\nachieves similar or even better performances than prior competitors, both on\noriginal datasets and on corrupted variants. Moreover, it can be used in a\nplug-and-play fashion with FastText and BERT, where it significantly improves\ntheir robustness.\n","authors":["Lihu Chen","Gaël Varoquaux","Fabian M. Suchanek"],"pdf_url":"https://arxiv.org/pdf/2203.07860v2.pdf","comment":"Long paper accepted by ACL main conference. 17 pages"},{"id":"http://arxiv.org/abs/2202.07959v2","updated":"2022-03-21T14:28:26Z","published":"2022-02-16T10:10:00Z","title":"EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq\n  Generation","summary":"  We propose EdgeFormer -- a parameter-efficient Transformer of the\nencoder-decoder architecture for on-device seq2seq generation, which is\ncustomized under strict computation and memory constraints. EdgeFormer proposes\ntwo novel principles for cost-effective parameterization and further enhance\nthe model with efficient layer adaptation. We conduct extensive experiments on\ntwo practical on-device seq2seq tasks: Machine Translation and Grammatical\nError Correction, and show that EdgeFormer can effectively outperform previous\nparameter-efficient Transformer baselines and achieve very competitive results\nwith knowledge distillation under both the computation and memory constraints.\nMoreover, we release the pretrained EdgeFormer -- the first publicly available\npretrained model that can be easily fine-tuned for English seq2seq tasks with\nstrong results, largely facilitating on-device seq2seq generation in practice.\n","authors":["Tao Ge","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2202.07959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.04645v4","updated":"2022-03-21T14:24:12Z","published":"2021-09-10T03:23:06Z","title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented\n  Dialog Systems","summary":"  As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n","authors":["Fei Mi","Yitong Li","Yasheng Wang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2109.04645v4.pdf","comment":"Accepted at AAAI2022"},{"id":"http://arxiv.org/abs/2203.11008v1","updated":"2022-03-21T14:23:10Z","published":"2022-03-21T14:23:10Z","title":"Transformer-based HTR for Historical Documents","summary":"  We apply the TrOCR framework to real-world, historical manuscripts and show\nthat TrOCR per se is a strong model, ideal for transfer learning. TrOCR has\nbeen trained on English only, but it can adapt to other languages that use the\nLatin alphabet fairly easily and with little training material. We compare\nTrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such\nsystems. This finding is essential since Transkribus performs best when it has\naccess to baseline information, which is not needed at all to fine-tune TrOCR.\n","authors":["Phillip Benjamin Ströbel","Simon Clematide","Martin Volk","Tobias Hodel"],"pdf_url":"https://arxiv.org/pdf/2203.11008v1.pdf","comment":"This is an abstract submitted and accepted at ComHum 2022 in\n  Lausanne. We will be elaborating on these initial findings in the paper that\n  we will submit after the conference"},{"id":"http://arxiv.org/abs/2203.10995v1","updated":"2022-03-21T14:10:15Z","published":"2022-03-21T14:10:15Z","title":"Word Order Does Matter (And Shuffled Language Models Know It)","summary":"  Recent studies have shown that language models pretrained and/or fine-tuned\non randomly permuted sentences exhibit competitive performance on GLUE, putting\ninto question the importance of word order information. Somewhat\ncounter-intuitively, some of these studies also report that position embeddings\nappear to be crucial for models' good performance with shuffled text. We probe\nthese language models for word order information and investigate what position\nembeddings learned from shuffled text encode, showing that these models retain\ninformation pertaining to the original, naturalistic word order. We show this\nis in part due to a subtlety in how shuffling is implemented in previous work\n-- before rather than after subword segmentation. Surprisingly, we find even\nLanguage models trained on text shuffled after subword segmentation retain some\nsemblance of information about word order because of the statistical\ndependencies between sentence length and unigram probabilities. Finally, we\nshow that beyond GLUE, a variety of language understanding tasks do require\nword order information, often to an extent that cannot be learned through\nfine-tuning.\n","authors":["Vinit Ravishankar","Mostafa Abdou","Artur Kulmizev","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2203.10995v1.pdf","comment":"To appear at ACL 2022; 9 pages"},{"id":"http://arxiv.org/abs/2012.09692v5","updated":"2022-03-21T14:04:53Z","published":"2020-12-17T16:00:08Z","title":"Five Psycholinguistic Characteristics for Better Interaction with Users","summary":"  When two people pay attention to each other and are interested in what the\nother has to say or write, they almost instantly adapt their writing/speaking\nstyle to match the other. For a successful interaction with a user, chatbots\nand dialogue systems should be able to do the same. We propose a framework\nconsisting of five psycholinguistic textual characteristics for better\nhuman-computer interaction. We describe the annotation processes used for\ncollecting the data, and benchmark five binary classification tasks,\nexperimenting with different training sizes and model architectures. The best\narchitectures noticeably outperform several baselines and achieve\nmacro-averaged F$_1$-scores between 72\\% and 96\\% depending on the language and\nthe task. The proposed framework proved to be fairly easy to model for various\nlanguages even with small amount of manually annotated data if right\narchitectures are used.\n","authors":["Sanja Štajner","Seren Yenikent","Marc Franco-Salvador"],"pdf_url":"https://arxiv.org/pdf/2012.09692v5.pdf","comment":"26 pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.06667v3","updated":"2022-03-21T13:55:24Z","published":"2022-03-13T14:42:53Z","title":"Towards Visual-Prompt Temporal Answering Grounding in Medical\n  Instructional Video","summary":"  The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show that the proposed VPTSL\noutperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a\nlarge margin, which demonstrates the effectiveness of visual prompt and the\ntext span predictor.\n","authors":["Bin Li","Yixuan Weng","Bin Sun","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2203.06667v3.pdf","comment":"8 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2203.02679v2","updated":"2022-03-21T13:50:34Z","published":"2022-03-05T08:40:05Z","title":"Just Rank: Rethinking Evaluation with Word and Sentence Similarities","summary":"  Word and sentence embeddings are useful feature representations in natural\nlanguage processing. However, intrinsic evaluation for embeddings lags far\nbehind, and there has been no significant update since the past decade. Word\nand sentence similarity tasks have become the de facto evaluation method. It\nleads models to overfit to such evaluations, negatively impacting embedding\nmodels' development. This paper first points out the problems using semantic\nsimilarity as the gold standard for word and sentence embedding evaluations.\nFurther, we propose a new intrinsic evaluation method called EvalRank, which\nshows a much stronger correlation with downstream tasks. Extensive experiments\nare conducted based on 60+ models and popular datasets to certify our\njudgments. Finally, the practical evaluation toolkit is released for future\nbenchmarking purposes.\n","authors":["Bin Wang","C. -C. Jay Kuo","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2203.02679v2.pdf","comment":"Accepted as Main Conference for ACL 2022. Code:\n  https://github.com/BinWang28/EvalRank-Embedding-Evaluation"},{"id":"http://arxiv.org/abs/2203.10945v1","updated":"2022-03-21T13:11:41Z","published":"2022-03-21T13:11:41Z","title":"AraBART: a Pretrained Arabic Sequence-to-Sequence Model for Abstractive\n  Summarization","summary":"  Like most natural language understanding and generation tasks,\nstate-of-the-art models for summarization are transformer-based\nsequence-to-sequence architectures that are pretrained on large corpora. While\nmost existing models focused on English, Arabic remained understudied. In this\npaper we propose AraBART, the first Arabic model in which the encoder and the\ndecoder are pretrained end-to-end, based on BART. We show that AraBART achieves\nthe best performance on multiple abstractive summarization datasets,\noutperforming strong baselines including a pretrained Arabic BERT-based model\nand multilingual mBART and mT5 models.\n","authors":["Moussa Kamal Eddine","Nadi Tomeh","Nizar Habash","Joseph Le Roux","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2203.10945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10940v1","updated":"2022-03-21T13:09:59Z","published":"2022-03-21T13:09:59Z","title":"Quality Controlled Paraphrase Generation","summary":"  Paraphrase generation has been widely used in various downstream tasks. Most\ntasks benefit mainly from high quality paraphrases, namely those that are\nsemantically similar to, yet linguistically diverse from, the original\nsentence. Generating high-quality paraphrases is challenging as it becomes\nincreasingly hard to preserve meaning as linguistic diversity increases. Recent\nworks achieve nice results by controlling specific aspects of the paraphrase,\nsuch as its syntactic tree. However, they do not allow to directly control the\nquality of the generated paraphrase, and suffer from low flexibility and\nscalability. Here we propose $QCPG$, a quality-guided controlled paraphrase\ngeneration model, that allows directly controlling the quality dimensions.\nFurthermore, we suggest a method that given a sentence, identifies points in\nthe quality control space that are expected to yield optimal generated\nparaphrases. We show that our method is able to generate paraphrases which\nmaintain the original meaning while achieving higher diversity than the\nuncontrolled baseline. The models, the code, and the data can be found in\nhttps://github.com/IBM/quality-controlled-paraphrase-generation.\n","authors":["Elron Bandel","Ranit Aharonov","Michal Shmueli-Scheuer","Ilya Shnayderman","Noam Slonim","Liat Ein-Dor"],"pdf_url":"https://arxiv.org/pdf/2203.10940v1.pdf","comment":"Accepted as a long paper at ACL 2022"},{"id":"http://arxiv.org/abs/2101.10382v2","updated":"2022-03-21T13:03:42Z","published":"2021-01-25T20:08:32Z","title":"Curriculum Learning: A Survey","summary":"  Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n","authors":["Petru Soviany","Radu Tudor Ionescu","Paolo Rota","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2101.10382v2.pdf","comment":"Accepted at the International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2203.10929v1","updated":"2022-03-21T12:49:44Z","published":"2022-03-21T12:49:44Z","title":"General and Domain Adaptive Chinese Spelling Check with Error Consistent\n  Pretraining","summary":"  The lack of label data is one of the significant bottlenecks for Chinese\nSpelling Check (CSC). Existing researches use the method of automatic\ngeneration by exploiting unlabeled data to expand the supervised corpus.\nHowever, there is a big gap between the real input scenario and automatic\ngenerated corpus. Thus, we develop a competitive general speller ECSpell which\nadopts the Error Consistent masking strategy to create data for pretraining.\nThis error consistency masking strategy is used to specify the error types of\nautomatically generated sentences which is consistent with real scene. The\nexperimental result indicates our model outperforms previous state-of-the-art\nmodels on the general benchmark. Moreover, spellers often work within a\nparticular domain in real life. Due to lots of uncommon domain terms,\nexperiments on our built domain specific datasets show that general models\nperform terribly. Inspired by the common practice of input methods, we propose\nto add an alterable user dictionary to handle the zero-shot domain adaption\nproblem. Specifically, we attach a User Dictionary guided inference module (UD)\nto a general token classification based speller. Our experiments demonstrate\nthat ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other\nbaselines largely, even approaching the performance on the general benchmark.\n","authors":["Qi Lv","Ziqiang Cao","Lei Geng","Chunhui Ai","Xu Yan","Guohong Fu"],"pdf_url":"https://arxiv.org/pdf/2203.10929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.10545v2","updated":"2022-03-21T12:06:20Z","published":"2021-11-20T08:41:54Z","title":"RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented\n  Structural Neural Encoders","summary":"  Considering a collection of RDF triples, the RDF-to-text generation task aims\nto generate a text description. Most previous methods solve this task using a\nsequence-to-sequence model or using a graph-based model to encode RDF triples\nand to generate a text sequence. Nevertheless, these approaches fail to clearly\nmodel the local and global structural information between and within RDF\ntriples. Moreover, the previous methods also face the non-negligible problem of\nlow faithfulness of the generated text, which seriously affects the overall\nperformance of these models. To solve these problems, we propose a model\ncombining two new graph-augmented structural neural encoders to jointly learn\nboth local and global structural information in the input RDF triples. To\nfurther improve text faithfulness, we innovatively introduce a reinforcement\nlearning (RL) reward based on information extraction (IE). We first extract\ntriples from the generated text using a pretrained IE model and regard the\ncorrect number of the extracted triples as the additional RL reward.\nExperimental results on two benchmark datasets demonstrate that our proposed\nmodel outperforms the state-of-the-art baselines, and the additional\nreinforcement learning reward does help to improve the faithfulness of the\ngenerated text.\n","authors":["Hanning Gao","Lingfei Wu","Hongyun Zhang","Zhihua Wei","Po Hu","Fangli Xu","Bo Long"],"pdf_url":"https://arxiv.org/pdf/2111.10545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08394v3","updated":"2022-03-21T12:05:10Z","published":"2022-03-16T04:50:27Z","title":"Bridging the Data Gap between Training and Inference for Unsupervised\n  Neural Machine Translation","summary":"  Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.\n","authors":["Zhiwei He","Xing Wang","Rui Wang","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2203.08394v3.pdf","comment":"13 pages, ACL 2022"},{"id":"http://arxiv.org/abs/2203.10909v1","updated":"2022-03-21T12:02:06Z","published":"2022-03-21T12:02:06Z","title":"x-enVENT: A Corpus of Event Descriptions with Experiencer-specific\n  Emotion and Appraisal Annotations","summary":"  Emotion classification is often formulated as the task to categorize texts\ninto a predefined set of emotion classes. So far, this task has been the\nrecognition of the emotion of writers and readers, as well as that of entities\nmentioned in the text. We argue that a classification setup for emotion\nanalysis should be performed in an integrated manner, including the different\nsemantic roles that participate in an emotion episode. Based on appraisal\ntheories in psychology, which treat emotions as reactions to events, we compile\nan English corpus of written event descriptions. The descriptions depict\nemotion-eliciting circumstances, and they contain mentions of people who\nresponded emotionally. We annotate all experiencers, including the original\nauthor, with the emotions they likely felt. In addition, we link them to the\nevent they found salient (which can be different for different experiencers in\na text) by annotating event properties, or appraisals (e.g., the perceived\nevent undesirability, the uncertainty of its outcome). Our analysis reveals\npatterns in the co-occurrence of people's emotions in interaction. Hence, this\nrichly-annotated resource provides useful data to study emotions and event\nevaluations from the perspective of different roles, and it enables the\ndevelopment of experiencer-specific emotion and appraisal classification\nsystems.\n","authors":["Enrica Troiano","Laura Oberländer","Maximilian Wegge","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2203.10909v1.pdf","comment":"submitted to LREC 2022"},{"id":"http://arxiv.org/abs/2203.06462v2","updated":"2022-03-21T11:51:21Z","published":"2022-03-12T15:34:54Z","title":"Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in\n  Practice","summary":"  Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our code so that\nothers can inspect their models.\n","authors":["Andreas Grivas","Nikolay Bogoychev","Adam Lopez"],"pdf_url":"https://arxiv.org/pdf/2203.06462v2.pdf","comment":"Preprint of conference paper accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2203.10900v1","updated":"2022-03-21T11:48:40Z","published":"2022-03-21T11:48:40Z","title":"Document-Level Relation Extraction with Adaptive Focal Loss and\n  Knowledge Distillation","summary":"  Document-level Relation Extraction (DocRE) is a more challenging task\ncompared to its sentence-level counterpart. It aims to extract relations from\nmultiple sentences at once. In this paper, we propose a semi-supervised\nframework for DocRE with three novel components. Firstly, we use an axial\nattention module for learning the interdependency among entity-pairs, which\nimproves the performance on two-hop relations. Secondly, we propose an adaptive\nfocal loss to tackle the class imbalance problem of DocRE. Lastly, we use\nknowledge distillation to overcome the differences between human annotated data\nand distantly supervised data. We conducted experiments on two DocRE datasets.\nOur model consistently outperforms strong baselines and its performance exceeds\nthe previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.\nOur code and data will be released at https://github.com/tonytan48/KD-DocRE.\n","authors":["Qingyu Tan","Ruidan He","Lidong Bing","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2203.10900v1.pdf","comment":"To appear in the Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2203.10885v1","updated":"2022-03-21T11:10:46Z","published":"2022-03-21T11:10:46Z","title":"Zoom Out and Observe: News Environment Perception for Fake News\n  Detection","summary":"  Fake news detection is crucial for preventing the dissemination of\nmisinformation on social media. To differentiate fake news from real ones,\nexisting methods observe the language patterns of the news post and \"zoom in\"\nto verify its content with knowledge sources or check its readers' replies.\nHowever, these methods neglect the information in the external news environment\nwhere a fake news post is created and disseminated. The news environment\nrepresents recent mainstream media opinion and public attention, which is an\nimportant inspiration of fake news fabrication because fake news is often\ndesigned to ride the wave of popular events and catch public attention with\nunexpected novel content for greater exposure and spread. To capture the\nenvironmental signals of news posts, we \"zoom out\" to observe the news\nenvironment and propose the News Environment Perception Framework (NEP). For\neach post, we construct its macro and micro news environment from recent\nmainstream news. Then we design a popularity-oriented and a novelty-oriented\nmodule to perceive useful signals and further assist final prediction.\nExperiments on our newly built datasets show that the NEP can efficiently\nimprove the performance of basic fake news detectors.\n","authors":["Qiang Sheng","Juan Cao","Xueyao Zhang","Rundong Li","Danding Wang","Yongchun Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.10885v1.pdf","comment":"ACL 2022 Main Conference (Long Paper)"},{"id":"http://arxiv.org/abs/2203.03463v2","updated":"2022-03-21T10:59:40Z","published":"2022-03-07T15:28:36Z","title":"Hierarchical Sketch Induction for Paraphrase Generation","summary":"  We propose a generative model of paraphrase generation, that encourages\nsyntactic diversity by conditioning on an explicit syntactic sketch. We\nintroduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE),\na method for learning decompositions of dense encodings as a sequence of\ndiscrete latent variables that make iterative refinements of increasing\ngranularity. This hierarchy of codes is learned through end-to-end training,\nand represents fine-to-coarse grained information about the input. We use\nHRQ-VAE to encode the syntactic form of an input sentence as a path through the\nhierarchy, allowing us to more easily predict syntactic sketches at test time.\nExtensive experiments, including a human evaluation, confirm that HRQ-VAE\nlearns a hierarchical representation of the input space, and generates\nparaphrases of higher quality than previous systems.\n","authors":["Tom Hosking","Hao Tang","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2203.03463v2.pdf","comment":"Accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2203.10854v1","updated":"2022-03-21T10:20:30Z","published":"2022-03-21T10:20:30Z","title":"Paraphrasing Techniques for Maritime QA system","summary":"  There has been an increasing interest in incorporating Artificial\nIntelligence (AI) into Defence and military systems to complement and augment\nhuman intelligence and capabilities. However, much work still needs to be done\ntoward achieving an effective human-machine partnership. This work is aimed at\nenhancing human-machine communications by developing a capability for\nautomatically translating human natural language into a machine-understandable\nlanguage (e.g., SQL queries). Techniques toward achieving this goal typically\ninvolve building a semantic parser trained on a very large amount of\nhigh-quality manually-annotated data. However, in many real-world Defence\nscenarios, it is not feasible to obtain such a large amount of training data.\nTo the best of our knowledge, there are few works trying to explore the\npossibility of training a semantic parser with limited manually-paraphrased\ndata, in other words, zero-shot. In this paper, we investigate how to exploit\nparaphrasing methods for the automated generation of large-scale training\ndatasets (in the form of paraphrased utterances and their corresponding logical\nforms in SQL format) and present our experimental results using real-world data\nin the maritime domain.\n","authors":["Fatemeh Shiri","Terry Yue Zhuo","Zhuang Li","Van Nguyen","Shirui Pan","Weiqing Wang","Reza Haffari","Yuan-Fang Li"],"pdf_url":"https://arxiv.org/pdf/2203.10854v1.pdf","comment":"8 pages. The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2203.10845v1","updated":"2022-03-21T10:07:17Z","published":"2022-03-21T10:07:17Z","title":"Neural Token Segmentation for High Token-Internal Complexity","summary":"  Tokenizing raw texts into word units is an essential pre-processing step for\ncritical tasks in the NLP pipeline such as tagging, parsing, named entity\nrecognition, and more. For most languages, this tokenization step\nstraightforward. However, for languages with high token-internal complexity,\nfurther token-to-word segmentation is required. Previous canonical segmentation\nstudies were based on character-level frameworks, with no contextualised\nrepresentation involved. Contextualized vectors a la BERT show remarkable\nresults in many applications, but were not shown to improve performance on\nlinguistic segmentation per se. Here we propose a novel neural segmentation\nmodel which combines the best of both worlds, contextualised token\nrepresentation and char-level decoding, which is particularly effective for\nlanguages with high token-internal complexity and extreme morphological\nambiguity. Our model shows substantial improvements in segmentation accuracy on\nHebrew and Arabic compared to the state-of-the-art, and leads to further\nimprovements on downstream tasks such as Part-of-Speech Tagging, Dependency\nParsing and Named-Entity Recognition, over existing pipelines. When comparing\nour segmentation-first pipeline with joint segmentation and labeling in the\nsame settings, we show that, contrary to pre-neural studies, the pipeline\nperformance is superior.\n","authors":["Idan Brusilovsky","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2203.10845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10839v1","updated":"2022-03-21T09:59:54Z","published":"2022-03-21T09:59:54Z","title":"TCM-SD: A Large Dataset for Syndrome Differentiation in Traditional\n  Chinese Medicine","summary":"  Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy\nthat has spread and been applied worldwide. The unique TCM diagnosis and\ntreatment system requires a comprehensive analysis of a patient's symptoms\nhidden in the clinical record written in free text. Prior studies have shown\nthat this system can be informationized and intelligentized with the aid of\nartificial intelligence (AI) technology, such as natural language processing\n(NLP). However, existing datasets are not of sufficient quality nor quantity to\nsupport the further development of data-driven AI technology in TCM. Therefore,\nin this paper, we focus on the core task of the TCM diagnosis and treatment\nsystem -- syndrome differentiation (SD) -- and we introduce the first public\nlarge-scale dataset for SD, called TCM-SD. Our dataset contains 54,152\nreal-world clinical records covering 148 syndromes. Furthermore, we collect a\nlarge-scale unlabelled textual corpus in the field of TCM and propose a\ndomain-specific pre-trained language model, called ZY-BERT. We conducted\nexperiments using deep neural networks to establish a strong performance\nbaseline, reveal various challenges in SD, and prove the potential of\ndomain-specific pre-trained language model. Our study and analysis reveal\nopportunities for incorporating computer science and linguistics knowledge to\nexplore the empirical validity of TCM theories.\n","authors":["Mucheng Ren","Heyan Huang","Yuxiang Zhou","Yuan Bu","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2203.10839v1.pdf","comment":"18 pages, preprint,Work-In-Progress"},{"id":"http://arxiv.org/abs/2110.07681v2","updated":"2022-03-21T08:30:12Z","published":"2021-10-14T19:40:37Z","title":"Large Scale Substitution-based Word Sense Induction","summary":"  We present a word-sense induction method based on pre-trained masked language\nmodels (MLMs), which can cheaply scale to large vocabularies and large corpora.\nThe result is a corpus which is sense-tagged according to a corpus-derived\nsense inventory and where each sense is associated with indicative words.\nEvaluation on English Wikipedia that was sense-tagged using our method shows\nthat both the induced senses, and the per-instance sense assignment, are of\nhigh quality even compared to WSD methods, such as Babelfy. Furthermore, by\ntraining a static word embeddings algorithm on the sense-tagged corpus, we\nobtain high-quality static senseful embeddings. These outperform existing\nsenseful embeddings methods on the WiC dataset and on a new outlier detection\ndataset we developed. The data driven nature of the algorithm allows to induce\ncorpora-specific senses, which may not appear in standard sense inventories, as\nwe demonstrate using a case study on the scientific domain.\n","authors":["Matan Eyal","Shoval Sadde","Hillel Taub-Tabib","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2110.07681v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.10796v1","updated":"2022-03-21T08:23:03Z","published":"2022-03-21T08:23:03Z","title":"Effective Token Graph Modeling using a Novel Labeling Strategy for\n  Structured Sentiment Analysis","summary":"  The state-of-the-art model for structured sentiment analysis casts the task\nas a dependency parsing problem, which has some limitations: (1) The label\nproportions for span prediction and span relation prediction are imbalanced.\n(2) The span lengths of sentiment tuple components may be very large in this\ntask, which will further exacerbate the imbalance problem. (3) Two nodes in a\ndependency graph cannot have multiple arcs, therefore some overlapped sentiment\ntuples cannot be recognized. In this work, we propose nichetargeting solutions\nfor these issues. First, we introduce a novel labeling strategy, which contains\ntwo sets of token pair labels, namely essential label set and whole label set.\nThe essential label set consists of the basic labels for this task, which are\nrelatively balanced and applied in the prediction layer. The whole label set\nincludes rich labels to help our model capture various token relations, which\nare applied in the hidden layer to softly influence our model. Moreover, we\nalso propose an effective model to well collaborate with our labeling strategy,\nwhich is equipped with the graph attention networks to iteratively refine token\nrepresentations, and the adaptive multi-label classifier to dynamically predict\nmultiple relations between token pairs. We perform extensive experiments on 5\nbenchmark datasets in four languages. Experimental results show that our model\noutperforms previous SOTA models by a large margin.\n","authors":["Wenxuan Shi","Fei Li","Jingye Li","Hao Fei","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2203.10796v1.pdf","comment":"to appear at the ACL 2022 Main conference"},{"id":"http://arxiv.org/abs/2110.06852v2","updated":"2022-03-21T08:04:25Z","published":"2021-10-13T16:43:44Z","title":"Morphosyntactic Tagging with Pre-trained Language Models for Arabic and\n  its Dialects","summary":"  We present state-of-the-art results on morphosyntactic tagging across\ndifferent varieties of Arabic using fine-tuned pre-trained transformer language\nmodels. Our models consistently outperform existing systems in Modern Standard\nArabic and all the Arabic dialects we study, achieving 2.6% absolute\nimprovement over the previous state-of-the-art in Modern Standard Arabic, 2.8%\nin Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training\nsetups for fine-tuning pre-trained transformer language models, including\ntraining data size, the use of external linguistic resources, and the use of\nannotated data from other dialects in a low-resource scenario. Our results show\nthat strategic fine-tuning using datasets from other high-resource dialects is\nbeneficial for a low-resource dialect. Additionally, we show that high-quality\nmorphological analyzers as external linguistic resources are beneficial\nespecially in low-resource settings.\n","authors":["Go Inoue","Salam Khalifa","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2110.06852v2.pdf","comment":"Accepted to Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2112.01753v2","updated":"2022-03-21T07:26:18Z","published":"2021-12-03T07:19:42Z","title":"Probing Linguistic Information For Logical Inference In Pre-trained\n  Language Models","summary":"  Progress in pre-trained language models has led to a surge of impressive\nresults on downstream tasks for natural language understanding. Recent work on\nprobing pre-trained language models uncovered a wide range of linguistic\nproperties encoded in their contextualized representations. However, it is\nunclear whether they encode semantic knowledge that is crucial to symbolic\ninference methods. We propose a methodology for probing linguistic information\nfor logical inference in pre-trained language model representations. Our\nprobing datasets cover a list of linguistic phenomena required by major\nsymbolic inference systems. We find that (i) pre-trained language models do\nencode several types of linguistic information for inference, but there are\nalso some types of information that are weakly encoded, (ii) language models\ncan effectively learn missing linguistic information through fine-tuning.\nOverall, our findings provide insights into which aspects of linguistic\ninformation for logical inference do language models and their pre-training\nprocedures capture. Moreover, we have demonstrated language models' potential\nas semantic and background knowledge bases for supporting symbolic inference\nmethods.\n","authors":["Zeming Chen","Qiyue Gao"],"pdf_url":"https://arxiv.org/pdf/2112.01753v2.pdf","comment":"AAAI 2022 camera ready version"},{"id":"http://arxiv.org/abs/2202.13587v2","updated":"2022-03-21T07:11:17Z","published":"2022-02-28T07:36:30Z","title":"Rethinking and Refining the Distinct Metric","summary":"  Distinct-$n$ score\\cite{Li2016} is a widely used automatic metric for\nevaluating diversity in language generation tasks. However, we observed that\nthe original approach for calculating distinct scores has evident biases that\ntend to assign higher penalties to longer sequences. We refine the calculation\nof distinct scores by scaling the number of distinct tokens based on their\nexpectations. We provide both empirical and theoretical evidence to show that\nour method effectively removes the biases existing in the original distinct\nscore. Our experiments show that our proposed metric,\n\\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human\njudgment in evaluating response diversity. To foster future research, we\nprovide an example implementation at\n\\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.\n","authors":["Siyang Liu","Sahand Sabour","Yinhe Zheng","Pei Ke","Xiaoyan Zhu","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2202.13587v2.pdf","comment":"4 pages, to be published at ACL2022"},{"id":"http://arxiv.org/abs/2203.10759v1","updated":"2022-03-21T07:10:19Z","published":"2022-03-21T07:10:19Z","title":"A Slot Is Not Built in One Utterance: Spoken Language Dialogs with\n  Sub-Slots","summary":"  A slot value might be provided segment by segment over multiple-turn\ninteractions in a dialog, especially for some important information such as\nphone numbers and names. It is a common phenomenon in daily life, but little\nattention has been paid to it in previous work. To fill the gap, this paper\ndefines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds\na Chinese dialog dataset SSD for boosting research on SSTOD. The dataset\nincludes a total of 40K dialogs and 500K utterances from four different\ndomains: Chinese names, phone numbers, ID numbers and license plate numbers.\nThe data is well annotated with sub-slot values, slot values, dialog states and\nactions. We find some new linguistic phenomena and interactive manners in SSTOD\nwhich raise critical challenges of building dialog agents for the task. We test\nthree state-of-the-art dialog models on SSTOD and find they cannot handle the\ntask well on any of the four domains. We also investigate an improved model by\ninvolving slot knowledge in a plug-in manner. More work should be done to meet\nthe new challenges raised from SSTOD which widely exists in real-life\napplications. The dataset and code are publicly available via\nhttps://github.com/shunjiu/SSTOD.\n","authors":["Sai Zhang","Yuwei Hu","Yuchuan Wu","Jiaman Wu","Yongbin Li","Jian Sun","Caixia Yuan","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10759v1.pdf","comment":"Accepted by ACL 2022 Findings"},{"id":"http://arxiv.org/abs/2203.10753v1","updated":"2022-03-21T06:52:38Z","published":"2022-03-21T06:52:38Z","title":"Match the Script, Adapt if Multilingual: Analyzing the Effect of\n  Multilingual Pretraining on Cross-lingual Transferability","summary":"  Pretrained multilingual models enable zero-shot learning even for unseen\nlanguages, and that performance can be further improved via adaptation prior to\nfinetuning. However, it is unclear how the number of pretraining languages\ninfluences a model's zero-shot learning for languages unseen during\npretraining. To fill this gap, we ask the following research questions: (1) How\ndoes the number of pretraining languages influence zero-shot performance on\nunseen target languages? (2) Does the answer to that question change with model\nadaptation? (3) Do the findings for our first question change if the languages\nused for pretraining are all related? Our experiments on pretraining with\nrelated languages indicate that choosing a diverse set of languages is crucial.\nWithout model adaptation, surprisingly, increasing the number of pretraining\nlanguages yields better results up to adding related languages, after which\nperformance plateaus. In contrast, with model adaptation via continued\npretraining, pretraining on a larger number of languages often gives further\nimprovement, suggesting that model adaptation is crucial to exploit additional\npretraining languages.\n","authors":["Yoshinari Fujinuma","Jordan Boyd-Graber","Katharina Kann"],"pdf_url":"https://arxiv.org/pdf/2203.10753v1.pdf","comment":"ACL 2022 camera ready"},{"id":"http://arxiv.org/abs/2203.10752v1","updated":"2022-03-21T06:50:21Z","published":"2022-03-21T06:50:21Z","title":"XTREME-S: Evaluating Cross-lingual Speech Representations","summary":"  We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual\nspeech representations in many languages. XTREME-S covers four task families:\nspeech recognition, classification, speech-to-text translation and retrieval.\nCovering 102 languages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech representation\nevaluation, as well as catalyze research in \"universal\" speech representation\nlearning. This paper describes the new benchmark and establishes the first\nspeech-only and speech-text baselines using XLS-R and mSLAM on all downstream\ntasks. We motivate the design choices and detail how to use the benchmark.\nDatasets and fine-tuning scripts are made easily accessible.\n","authors":["Alexis Conneau","Ankur Bapna","Yu Zhang","Min Ma","Patrick von Platen","Anton Lozhkov","Colin Cherry","Ye Jia","Clara Rivera","Mihir Kale","Daan Van Esch","Vera Axelrod","Simran Khanuja","Jonathan H. Clark","Orhan Firat","Sebastian Ruder","Jason Riesa","Melvin Johnson"],"pdf_url":"https://arxiv.org/pdf/2203.10752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10750v1","updated":"2022-03-21T06:42:44Z","published":"2022-03-21T06:42:44Z","title":"WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses","summary":"  In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. Both quantitative and qualitative evaluation results demonstrate\nthe effectiveness of WeSinger in terms of accuracy and naturalness, and\nWeSinger achieves state-of-the-art performance on the public corpus Opencpop.\nSome synthesized singing samples are available\nonline\\footnote{https://zzw922cn.github.io/wesinger}\n","authors":["Zewang Zhang","Yibin Zheng","Xinhui Li","Li Lu"],"pdf_url":"https://arxiv.org/pdf/2203.10750v1.pdf","comment":"Submitted to InterSpeech2022"},{"id":"http://arxiv.org/abs/2203.10744v1","updated":"2022-03-21T05:33:59Z","published":"2022-03-21T05:33:59Z","title":"Programming Language Agnostic Mining of Code and Language Pairs with\n  Sequence Labeling Based Question Answering","summary":"  Mining aligned natural language (NL) and programming language (PL) pairs is a\ncritical task to NL-PL understanding. Existing methods applied specialized\nhand-crafted features or separately-trained models for each PL. However, they\nusually suffered from low transferability across multiple PLs, especially for\nniche PLs with less annotated data. Fortunately, a Stack Overflow answer post\nis essentially a sequence of text and code blocks and its global textual\ncontext can provide PL-agnostic supplementary information. In this paper, we\npropose a Sequence Labeling based Question Answering (SLQA) method to mine\nNL-PL pairs in a PL-agnostic manner. In particular, we propose to apply the BIO\ntagging scheme instead of the conventional binary scheme to mine the code\nsolutions which are often composed of multiple blocks of a post. Experiments on\ncurrent single-PL single-block benchmarks and a manually-labeled cross-PL\nmulti-block benchmark prove the effectiveness and transferability of SLQA. We\nfurther present a parallel NL-PL corpus named Lang2Code automatically mined\nwith SLQA, which contains about 1.4M pairs on 6 PLs. Under statistical analysis\nand downstream evaluation, we demonstrate that Lang2Code is a large-scale\nhigh-quality data resource for further NL-PL research.\n","authors":["Changran Hu","Akshara Reddi Methukupalli","Yutong Zhou","Chen Wu","Yubo Chen"],"pdf_url":"https://arxiv.org/pdf/2203.10744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10743v1","updated":"2022-03-21T05:32:35Z","published":"2022-03-21T05:32:35Z","title":"Academic Resource Text Level Multi-label Classification based on\n  Attention","summary":"  Hierarchical multi-label academic text classification (HMTC) is to assign\nacademic texts into a hierarchically structured labeling system. We propose an\nattention-based hierarchical multi-label classification algorithm of academic\ntexts (AHMCA) by integrating features such as text, keywords, and hierarchical\nstructure, the academic documents are classified into the most relevant\ncategories. We utilize word2vec and BiLSTM to obtain embedding and latent\nvector representations of text, keywords, and hierarchies. We use hierarchical\nattention mechanism to capture the associations between keywords, label\nhierarchies, and text word vectors to generate hierarchical-specific document\nembedding vectors to replace the original text embeddings in HMCN-F. The\nexperimental results on the academic text dataset demonstrate the effectiveness\nof the AHMCA algorithm.\n","authors":["Yue Wang","Yawen Li","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2203.10743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10741v1","updated":"2022-03-21T05:27:35Z","published":"2022-03-21T05:27:35Z","title":"HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long\n  Document Summarization","summary":"  Document structure is critical for efficient information consumption.\nHowever, it is challenging to encode it efficiently into the modern Transformer\narchitecture. In this work, we present HIBRIDS, which injects Hierarchical\nBiases foR Incorporating Document Structure into the calculation of attention\nscores. We further present a new task, hierarchical question-summary\ngeneration, for summarizing salient content in the source document into a\nhierarchy of questions and summaries, where each follow-up question inquires\nabout the content of its parent question-summary pair. We also annotate a new\ndataset with 6,153 question-summary hierarchies labeled on long government\nreports. Experiment results show that our model produces better\nquestion-summary hierarchies than comparisons on both hierarchy quality and\ncontent coverage, a finding also echoed by human judges. Additionally, our\nmodel improves the generation of long-form summaries from lengthy government\nreports and Wikipedia articles, as measured by ROUGE scores.\n","authors":["Shuyang Cao","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10741v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2109.05238v3","updated":"2022-03-21T05:23:11Z","published":"2021-09-11T09:43:15Z","title":"Universal Simultaneous Machine Translation with Mixture-of-Experts\n  Wait-k Policy","summary":"  Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.\n","authors":["Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2109.05238v3.pdf","comment":"Accepted at EMNLP 2021 (main conference). 12 pages, 7 figures, 4\n  tables"},{"id":"http://arxiv.org/abs/2112.06386v2","updated":"2022-03-21T04:51:12Z","published":"2021-12-13T02:36:04Z","title":"Sparse Structure Learning via Graph Neural Networks for Inductive\n  Document Classification","summary":"  Recently, graph neural networks (GNNs) have been widely used for document\nclassification. However, most existing methods are based on static word\nco-occurrence graphs without sentence-level information, which poses three\nchallenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual\ndependency. To address these challenges, we propose a novel GNN-based sparse\nstructure learning model for inductive document classification. Specifically, a\ndocument-level graph is initially generated by a disjoint union of\nsentence-level word co-occurrence graphs. Our model collects a set of trainable\nedges connecting disjoint words between sentences and employs structure\nlearning to sparsely select edges with dynamic contextual dependencies. Graphs\nwith sparse structures can jointly exploit local and global contextual\ninformation in documents through GNNs. For inductive learning, the refined\ndocument graph is further fed into a general readout function for graph-level\nclassification and optimization in an end-to-end manner. Extensive experiments\non several real-world datasets demonstrate that the proposed model outperforms\nmost state-of-the-art results, and reveal the necessity to learn sparse\nstructures for each document.\n","authors":["Yinhua Piao","Sangseon Lee","Dohoon Lee","Sun Kim"],"pdf_url":"https://arxiv.org/pdf/2112.06386v2.pdf","comment":"Accepted by AAAI 2022"},{"id":"http://arxiv.org/abs/2110.07198v2","updated":"2022-03-21T04:24:28Z","published":"2021-10-14T07:44:14Z","title":"Rethinking Self-Supervision Objectives for Generalizable Coherence\n  Modeling","summary":"  Given the claims of improved text generation quality across various\npre-trained neural models, we consider the coherence evaluation of machine\ngenerated text to be one of the principal applications of coherence models that\nneeds to be investigated. Prior work in neural coherence modeling has primarily\nfocused on devising new architectures for solving the permuted document task.\nWe instead use a basic model architecture and show significant improvements\nover state of the art within the same training regime. We then design a harder\nself-supervision objective by increasing the ratio of negative samples within a\ncontrastive learning setup, and enhance the model further through automatic\nhard negative mining coupled with a large global negative queue encoded by a\nmomentum encoder. We show empirically that increasing the density of negative\nsamples improves the basic model, and using a global negative queue further\nimproves and stabilizes the model while training with hard negative samples. We\nevaluate the coherence model on task-independent test sets that resemble\nreal-world applications and show significant improvements in coherence\nevaluations of downstream tasks.\n","authors":["Prathyusha Jwalapuram","Shafiq Joty","Xiang Lin"],"pdf_url":"https://arxiv.org/pdf/2110.07198v2.pdf","comment":"Accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2105.05727v4","updated":"2022-03-21T03:48:40Z","published":"2021-05-12T15:20:01Z","title":"BertGCN: Transductive Text Classification by Combining GCN and BERT","summary":"  In this work, we propose BertGCN, a model that combines large scale\npretraining and transductive learning for text classification. BertGCN\nconstructs a heterogeneous graph over the dataset and represents documents as\nnodes using BERT representations. By jointly training the BERT and GCN modules\nwithin BertGCN, the proposed model is able to leverage the advantages of both\nworlds: large-scale pretraining which takes the advantage of the massive amount\nof raw data and transductive learning which jointly learns representations for\nboth training data and unlabeled test data by propagating label influence\nthrough graph convolution. Experiments show that BertGCN achieves SOTA\nperformances on a wide range of text classification datasets. Code is available\nat https://github.com/ZeroRin/BertGCN.\n","authors":["Yuxiao Lin","Yuxian Meng","Xiaofei Sun","Qinghong Han","Kun Kuang","Jiwei Li","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2105.05727v4.pdf","comment":"Accepted by Findings of ACL 2021"},{"id":"http://arxiv.org/abs/2203.06486v2","updated":"2022-03-21T03:45:59Z","published":"2022-03-12T17:01:38Z","title":"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization","summary":"  Charts are commonly used for exploring data and communicating insights.\nGenerating natural language summaries from charts can be very helpful for\npeople in inferring key insights that would otherwise require a lot of\ncognitive and perceptual efforts. We present Chart-to-text, a large-scale\nbenchmark with two datasets and a total of 44,096 charts covering a wide range\nof topics and chart types. We explain the dataset construction process and\nanalyze the datasets. We also introduce a number of state-of-the-art neural\nmodels as baselines that utilize image captioning and data-to-text generation\ntechniques to tackle two problem variations: one assumes the underlying data\ntable of the chart is available while the other needs to extract data from\nchart images. Our analysis with automatic and human evaluation shows that while\nour best models usually generate fluent summaries and yield reasonable BLEU\nscores, they also suffer from hallucinations and factual errors as well as\ndifficulties in correctly explaining complex patterns and trends in charts.\n","authors":["Shankar Kanthara","Rixie Tiffany Ko Leong","Xiang Lin","Ahmed Masry","Megh Thakkar","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2203.06486v2.pdf","comment":"Accepted by ACL 2022 Main Conference"},{"id":"http://arxiv.org/abs/2203.10717v1","updated":"2022-03-21T03:28:37Z","published":"2022-03-21T03:28:37Z","title":"An Intellectual Property Entity Recognition Method Based on Transformer\n  and Technological Word Information","summary":"  Patent texts contain a large amount of entity information. Through named\nentity recognition, intellectual property entity information containing key\ninformation can be extracted from it, helping researchers to understand the\npatent content faster. Therefore, it is difficult for existing named entity\nextraction methods to make full use of the semantic information at the word\nlevel brought about by professional vocabulary changes. This paper proposes a\nmethod for extracting intellectual property entities based on Transformer and\ntechnical word information , and provides accurate word vector representation\nin combination with the BERT language method. In the process of word vector\ngeneration, the technical word information extracted by IDCNN is added to\nimprove the understanding of intellectual property entities Representation\nability. Finally, the Transformer encoder that introduces relative position\nencoding is used to learn the deep semantic information of the text from the\nsequence of word vectors, and realize entity label prediction. Experimental\nresults on public datasets and annotated patent datasets show that the method\nimproves the accuracy of entity recognition.\n","authors":["Yuhui Wang","Junping Du","Yingxia Shao"],"pdf_url":"https://arxiv.org/pdf/2203.10717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10714v1","updated":"2022-03-21T03:21:32Z","published":"2022-03-21T03:21:32Z","title":"A Prompting-based Approach for Adversarial Example Generation and\n  Robustness Enhancement","summary":"  Recent years have seen the wide application of NLP models in crucial areas\nsuch as finance, medical treatment, and news media, raising concerns of the\nmodel robustness and vulnerabilities. In this paper, we propose a novel\nprompt-based adversarial attack to compromise NLP models and robustness\nenhancement technique. We first construct malicious prompts for each instance\nand generate adversarial examples via mask-and-filling under the effect of a\nmalicious purpose. Our attack technique targets the inherent vulnerabilities of\nNLP models, allowing us to generate samples even without interacting with the\nvictim NLP model, as long as it is based on pre-trained language models (PLMs).\nFurthermore, we design a prompt-based adversarial training method to improve\nthe robustness of PLMs. As our training method does not actually generate\nadversarial samples, it can be applied to large-scale training sets\nefficiently. The experimental results show that our attack method can achieve a\nhigh attack success rate with more diverse, fluent and natural adversarial\nexamples. In addition, our robustness enhancement method can significantly\nimprove the robustness of models to resist adversarial attacks. Our work\nindicates that prompting paradigm has great potential in probing some\nfundamental flaws of PLMs and fine-tuning them for downstream tasks.\n","authors":["Yuting Yang","Pei Huang","Juan Cao","Jintao Li","Yun Lin","Jin Song Dong","Feifei Ma","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10705v1","updated":"2022-03-21T02:11:35Z","published":"2022-03-21T02:11:35Z","title":"Compression of Generative Pre-trained Language Models via Quantization","summary":"  The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.\n","authors":["Chaofan Tao","Lu Hou","Wei Zhang","Lifeng Shang","Xin Jiang","Qun Liu","Ping Luo","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2203.10705v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2106.07922v2","updated":"2022-03-21T01:31:00Z","published":"2021-06-15T07:18:30Z","title":"An Automated Quality Evaluation Framework of Psychotherapy Conversations\n  with Local Quality Estimates","summary":"  Text-based computational approaches for assessing the quality of\npsychotherapy are being developed to support quality assurance and clinical\ntraining. However, due to the long durations of typical conversation based\ntherapy sessions, and due to limited annotated modeling resources,\ncomputational methods largely rely on frequency-based lexical features or\ndialogue acts to assess the overall session level characteristics. In this\nwork, we propose a hierarchical framework to automatically evaluate the quality\nof transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the\nrichly dynamic nature of the spoken dialog within a talk therapy session, to\nevaluate the overall session level quality, we propose to consider modeling it\nas a function of local variations across the interaction. To implement that\nempirically, we divide each psychotherapy session into conversation segments\nand initialize the segment-level qualities with the session-level scores.\nFirst, we produce segment embeddings by fine-tuning a BERT-based model, and\npredict segment-level (local) quality scores. These embeddings are used as the\nlower-level input to a Bidirectional LSTM-based neural network to predict the\nsession-level (global) quality estimates. In particular, we model the global\nquality as a linear function of the local quality scores, which allows us to\nupdate the segment-level quality estimates based on the session-level quality\nprediction. These newly estimated segment-level scores benefit the BERT\nfine-tuning process, which in turn results in better segment embeddings. We\nevaluate the proposed framework on automatically derived transcriptions from\nreal-world CBT clinical recordings to predict session-level behavior codes. The\nresults indicate that our approach leads to improved evaluation accuracy for\nmost codes when used for both regression and classification tasks.\n","authors":["Zhuohao Chen","Nikolaos Flemotomos","Karan Singla","Torrey A. Creed","David C. Atkins","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2106.07922v2.pdf","comment":"Accepted by Computer Speech & Language"},{"id":"http://arxiv.org/abs/2203.10693v1","updated":"2022-03-21T01:21:12Z","published":"2022-03-21T01:21:12Z","title":"Leveraging Expert Guided Adversarial Augmentation For Improving\n  Generalization in Named Entity Recognition","summary":"  Named Entity Recognition (NER) systems often demonstrate great performance on\nin-distribution data, but perform poorly on examples drawn from a shifted\ndistribution. One way to evaluate the generalization ability of NER models is\nto use adversarial examples, on which the specific variations associated with\nnamed entities are rarely considered. To this end, we propose leveraging\nexpert-guided heuristics to change the entity tokens and their surrounding\ncontexts thereby altering their entity types as adversarial attacks. Using\nexpert-guided heuristics, we augmented the CoNLL 2003 test set and manually\nannotated it to construct a high-quality challenging set. We found that\nstate-of-the-art NER systems trained on CoNLL 2003 training data drop\nperformance dramatically on our challenging set. By training on adversarial\naugmented training examples and using mixup for regularization, we were able to\nsignificantly improve the performance on the challenging set as well as improve\nout-of-domain generalization which we evaluated by using OntoNotes data. We\nhave publicly released our dataset and code at\nhttps://github.com/GT-SALT/Guided-Adversarial-Augmentation.\n","authors":["Aaron Reich","Jiaao Chen","Aastha Agrawal","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2203.10693v1.pdf","comment":"ACL 2022 (Findings)"},{"id":"http://arxiv.org/abs/2203.10692v1","updated":"2022-03-21T01:16:44Z","published":"2022-03-21T01:16:44Z","title":"Better Language Model with Hypernym Class Prediction","summary":"  Class-based language models (LMs) have been long devised to address context\nsparsity in $n$-gram LMs. In this study, we revisit this approach in the\ncontext of neural LMs. We hypothesize that class-based prediction leads to an\nimplicit context aggregation for similar words and thus can improve\ngeneralization for rare words. We map words that have a common WordNet hypernym\nto the same class and train large neural LMs by gradually annealing from\npredicting the class to token prediction during training. Empirically, this\ncurriculum learning strategy consistently improves perplexity over various\nlarge, highly-performant state-of-the-art Transformer-based models on two\ndatasets, WikiText-103 and Arxiv. Our analysis shows that the performance\nimprovement is achieved without sacrificing performance on rare words. Finally,\nwe document other attempts that failed to yield empirical gains, and discuss\nfuture directions for the adoption of class-based LMs on a larger scale.\n","authors":["He Bai","Tong Wang","Alessandro Sordoni","Peng Shi"],"pdf_url":"https://arxiv.org/pdf/2203.10692v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2104.14445v4","updated":"2022-03-21T23:23:15Z","published":"2021-04-29T16:05:31Z","title":"Trakhtenbrot's Theorem in Coq: Finite Model Theory through the\n  Constructive Lens","summary":"  We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. To showcase an application of Trakhtenbrot's theorem, we continue\nour reduction chain with a many-one reduction from FSAT to separation logic.\nAll our results are mechanised in the framework of a growing Coq library of\nsynthetic undecidability proofs.\n","authors":["Dominik Kirst","Dominique Larchey-Wendling"],"pdf_url":"https://arxiv.org/pdf/2104.14445v4.pdf","comment":"29 pages, extended version of the IJCAR 2020 paper. arXiv admin note:\n  substantial text overlap with arXiv:2004.07390"},{"id":"http://arxiv.org/abs/2109.10274v2","updated":"2022-03-21T23:20:18Z","published":"2021-09-21T15:54:31Z","title":"The Trade-offs of Domain Adaptation for Neural Language Models","summary":"  This work connects language model adaptation with concepts of machine\nlearning theory. We consider a training setup with a large out-of-domain set\nand a small in-domain set. We derive how the benefit of training a model on\neither set depends on the size of the sets and the distance between their\nunderlying distributions. We analyze how out-of-domain pre-training before\nin-domain fine-tuning achieves better generalization than either solution\nindependently. Finally, we present how adaptation techniques based on data\nselection, such as importance sampling, intelligent data selection and\ninfluence functions, can be presented in a common framework which highlights\ntheir similarity and also their subtle differences.\n","authors":["David Grangier","Dan Iter"],"pdf_url":"https://arxiv.org/pdf/2109.10274v2.pdf","comment":"Proceedings of the Annual Meeting of the Association for\n  Computational Linguistics (ACL), 2022"},{"id":"http://arxiv.org/abs/2203.11370v1","updated":"2022-03-21T22:13:53Z","published":"2022-03-21T22:13:53Z","title":"Language modeling via stochastic processes","summary":"  Modern language models can generate high-quality short texts. However, they\noften meander or are incoherent when generating longer texts. These issues\narise from the next-token-only language modeling objective. To address these\nissues, we introduce Time Control (TC), a language model that implicitly plans\nvia a latent stochastic process. TC does this by learning a representation\nwhich maps the dynamics of how text changes in a document to the dynamics of a\nstochastic process of interest. Using this representation, the language model\ncan generate text by first implicitly generating a document plan via a\nstochastic process, and then generating text that is consistent with this\nlatent plan. Compared to domain-specific methods and fine-tuning GPT2 across a\nvariety of text domains, TC improves performance on text infilling and\ndiscourse coherence. On long text generation settings, TC preserves the text\nstructure both in terms of ordering (up to +40% better) and text length\nconsistency (up to +17% better). Human evaluators also prefer TC's output 28.6%\nmore than the baselines.\n","authors":["Rose E Wang","Esin Durmus","Noah Goodman","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2203.11370v1.pdf","comment":"ICLR Oral 2022. Code:\n  https://github.com/rosewang2008/language_modeling_via_stochastic_processes"},{"id":"http://arxiv.org/abs/2203.11364v1","updated":"2022-03-21T21:51:43Z","published":"2022-03-21T21:51:43Z","title":"An Information-theoretic Approach to Prompt Engineering Without Ground\n  Truth Labels","summary":"  Pre-trained language models derive substantial linguistic and factual\nknowledge from the massive corpora on which they are trained, and prompt\nengineering seeks to align these models to specific tasks. Unfortunately,\nexisting prompt engineering methods require significant amounts of labeled\ndata, access to model parameters, or both. We introduce a new method for\nselecting prompt templates \\textit{without labeled examples} and\n\\textit{without direct access to the model}. Specifically, over a set of\ncandidate templates, we choose the template that maximizes the mutual\ninformation between the input and the corresponding model output. Across 8\ndatasets representing 7 distinct NLP tasks, we show that when a template has\nhigh mutual information, it also has high accuracy on the task. On the largest\nmodel, selecting prompts with our method gets 90\\% of the way from the average\nprompt accuracy to the best prompt accuracy and requires no ground truth\nlabels.\n","authors":["Taylor Sorensen","Joshua Robinson","Christopher Michael Rytting","Alexander Glenn Shaw","Kyle Jeffrey Rogers","Alexia Pauline Delorey","Mahmoud Khalil","Nancy Fulda","David Wingate"],"pdf_url":"https://arxiv.org/pdf/2203.11364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.08812v2","updated":"2022-03-21T20:59:47Z","published":"2021-12-16T11:57:56Z","title":"Ditch the Gold Standard: Re-evaluating Conversational Question Answering","summary":"  Conversational question answering aims to provide natural-language answers to\nusers in information-seeking conversations. Existing conversational QA\nbenchmarks compare models with pre-collected human-human conversations, using\nground-truth answers provided in conversational history. It remains unclear\nwhether we can rely on this static evaluation for model development and whether\ncurrent systems can well generalize to real-world human-machine conversations.\nIn this work, we conduct the first large-scale human evaluation of\nstate-of-the-art conversational QA systems, where human evaluators converse\nwith models and judge the correctness of their answers. We find that the\ndistribution of human machine conversations differs drastically from that of\nhuman-human conversations, and there is a disagreement between human and\ngold-history evaluation in terms of model ranking. We further investigate how\nto improve automatic evaluations, and propose a question rewriting mechanism\nbased on predicted history, which better correlates with human judgments.\nFinally, we analyze the impact of various modeling strategies and discuss\nfuture directions towards building better conversational question answering\nsystems.\n","authors":["Huihan Li","Tianyu Gao","Manan Goenka","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2112.08812v2.pdf","comment":"Accepted to ACL 2022; The dataset and code are available at\n  https://github.com/princeton-nlp/EvalConvQA/"},{"id":"http://arxiv.org/abs/2203.11331v1","updated":"2022-03-21T20:44:30Z","published":"2022-03-21T20:44:30Z","title":"On The Robustness of Offensive Language Classifiers","summary":"  Social media platforms are deploying machine learning based offensive\nlanguage classification systems to combat hateful, racist, and other forms of\noffensive speech at scale. However, despite their real-world deployment, we do\nnot yet comprehensively understand the extent to which offensive language\nclassifiers are robust against adversarial attacks. Prior work in this space is\nlimited to studying robustness of offensive language classifiers against\nprimitive attacks such as misspellings and extraneous spaces. To address this\ngap, we systematically analyze the robustness of state-of-the-art offensive\nlanguage classifiers against more crafty adversarial attacks that leverage\ngreedy- and attention-based word selection and context-aware embeddings for\nword replacement. Our results on multiple datasets show that these crafty\nadversarial attacks can degrade the accuracy of offensive language classifiers\nby more than 50% while also being able to preserve the readability and meaning\nof the modified text.\n","authors":["Jonathan Rusert","Zubair Shafiq","Padmini Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2203.11331v1.pdf","comment":"9 pages, 2 figures, Accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2203.11325v1","updated":"2022-03-21T20:28:06Z","published":"2022-03-21T20:28:06Z","title":"Enhancing Speech Recognition Decoding via Layer Aggregation","summary":"  Recently proposed speech recognition systems are designed to predict using\nrepresentations generated by their top layers, employing greedy decoding which\nisolates each timestep from the rest of the sequence. Aiming for improved\nperformance, a beam search algorithm is frequently utilized and a language\nmodel is incorporated to assist with ranking the top candidates. In this work,\nwe experiment with several speech recognition models and find that logits\npredicted using the top layers may hamper beam search from achieving optimal\nresults. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield\nhighly confident predictions, and hypothesize that the predictions are based on\nlocal information and may not take full advantage of the information encoded in\nintermediate layers. To this end, we perform a layer analysis to reveal and\nvisualize how predictions evolve throughout the inference flow. We then propose\na prediction method that aggregates the top M layers, potentially leveraging\nuseful information encoded in intermediate layers and relaxing model\nconfidence. We showcase the effectiveness of our approach via beam search\ndecoding, conducting our experiments on Librispeech test and dev sets and\nachieving WER, and CER reduction of up to 10% and 22%, respectively.\n","authors":["Tomer Wullach","Shlomo E. Chazan"],"pdf_url":"https://arxiv.org/pdf/2203.11325v1.pdf","comment":"Submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2106.16163v2","updated":"2022-03-21T20:18:01Z","published":"2021-06-30T15:56:44Z","title":"The MultiBERTs: BERT Reproductions for Robustness Analysis","summary":"  Experiments with pre-trained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact tested in the\nexperiment (i.e., the particular instance of the model), it is not always clear\nwhether they hold for the more general procedure which includes the\narchitecture, training data, initialization scheme, and loss function. Recent\nwork has shown that repeating the pre-training process can lead to\nsubstantially different performance, suggesting that an alternate strategy is\nneeded to make principled statements about procedures. To enable researchers to\ndraw more robust conclusions, we introduce the MultiBERTs, a set of 25\nBERT-Base checkpoints, trained with similar hyper-parameters as the original\nBERT model but differing in random weight initialization and shuffling of\ntraining data. We also define the Multi-Bootstrap, a non-parametric bootstrap\nmethod for statistical inference designed for settings where there are multiple\npre-trained models and limited test data. To illustrate our approach, we\npresent a case study of gender bias in coreference resolution, in which the\nMulti-Bootstrap lets us measure effects that may not be detected with a single\ncheckpoint. We release our models and statistical library along with an\nadditional set of 140 intermediate checkpoints captured during pre-training to\nfacilitate research on learning dynamics.\n","authors":["Thibault Sellam","Steve Yadlowsky","Jason Wei","Naomi Saphra","Alexander D'Amour","Tal Linzen","Jasmijn Bastings","Iulia Turc","Jacob Eisenstein","Dipanjan Das","Ian Tenney","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2106.16163v2.pdf","comment":"Accepted at ICLR'22. Checkpoints and example analyses:\n  http://goo.gle/multiberts"},{"id":"http://arxiv.org/abs/2203.11317v1","updated":"2022-03-21T20:04:23Z","published":"2022-03-21T20:04:23Z","title":"The Change that Matters in Discourse Parsing: Estimating the Impact of\n  Domain Shift on Parser Error","summary":"  Discourse analysis allows us to attain inferences of a text document that\nextend beyond the sentence-level. The current performance of discourse models\nis very low on texts outside of the training distribution's coverage,\ndiminishing the practical utility of existing models. There is need for a\nmeasure that can inform us to what extent our model generalizes from the\ntraining to the test sample when these samples may be drawn from distinct\ndistributions. While this can be estimated via distribution shift, we argue\nthat this does not directly correlate with change in the observed error of a\nclassifier (i.e. error-gap). Thus, we propose to use a statistic from the\ntheoretical domain adaptation literature which can be directly tied to\nerror-gap. We study the bias of this statistic as an estimator of error-gap\nboth theoretically and through a large-scale empirical study of over 2400\nexperiments on 6 discourse datasets from domains including, but not limited to:\nnews, biomedical texts, TED talks, Reddit posts, and fiction. Our results not\nonly motivate our proposal and help us to understand its limitations, but also\nprovide insight on the properties of discourse models and datasets which\nimprove performance in domain adaptation. For instance, we find that non-news\ndatasets are slightly easier to transfer to than news datasets when the\ntraining and test sets are very different. Our code and an associated Python\npackage are available to allow practitioners to make more informed model and\ndataset choices.\n","authors":["Katherine Atwell","Anthony Sicilia","Seong Jae Hwang","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2203.11317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.05604v2","updated":"2022-03-21T20:00:14Z","published":"2021-07-12T17:40:43Z","title":"Direct speech-to-speech translation with discrete units","summary":"  We present a direct speech-to-speech translation (S2ST) model that translates\nspeech from one language to speech in another language without relying on\nintermediate text generation. We tackle the problem by first applying a\nself-supervised discrete speech encoder on the target speech and then training\na sequence-to-sequence speech-to-unit translation (S2UT) model to predict the\ndiscrete representations of the target speech. When target text transcripts are\navailable, we design a joint speech and text training framework that enables\nthe model to generate dual modality output (speech and text) simultaneously in\nthe same inference pass. Experiments on the Fisher Spanish-English dataset show\nthat the proposed framework yields improvement of 6.7 BLEU compared with a\nbaseline direct S2ST model that predicts spectrogram features. When trained\nwithout any text transcripts, our model performance is comparable to models\nthat predict spectrograms and are trained with text supervision, showing the\npotential of our system for translation between unwritten languages. Audio\nsamples are available at\nhttps://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .\n","authors":["Ann Lee","Peng-Jen Chen","Changhan Wang","Jiatao Gu","Sravya Popuri","Xutai Ma","Adam Polyak","Yossi Adi","Qing He","Yun Tang","Juan Pino","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2107.05604v2.pdf","comment":"Accepted to ACL 2022 (long paper)"},{"id":"http://arxiv.org/abs/2110.10780v3","updated":"2022-03-21T19:54:10Z","published":"2021-10-20T21:09:41Z","title":"An Open Natural Language Processing Development Framework for EHR-based\n  Clinical Research: A case demonstration using the National COVID Cohort\n  Collaborative (N3C)","summary":"  While we pay attention to the latest advances in clinical natural language\nprocessing (NLP), we can notice some resistance in the clinical and\ntranslational research community to adopt NLP models due to limited\ntransparency, interpretability, and usability. In this study, we proposed an\nopen natural language processing development framework. We evaluated it through\nthe implementation of NLP algorithms for the National COVID Cohort\nCollaborative (N3C). Based on the interests in information extraction from\nCOVID-19 related clinical notes, our work includes 1) an open data annotation\nprocess using COVID-19 signs and symptoms as the use case, 2) a\ncommunity-driven ruleset composing platform, and 3) a synthetic text data\ngeneration workflow to generate texts for information extraction tasks without\ninvolving human subjects. The corpora were derived from texts from three\ndifferent institutions (Mayo Clinic, University of Kentucky, University of\nMinnesota). The gold standard annotations were tested with a single\ninstitution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,\nand 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,\nrespectively. The study as a consortium effort of the N3C NLP subgroup\ndemonstrates the feasibility of creating a federated NLP algorithm development\nand benchmarking platform to enhance multi-institution clinical NLP study and\nadoption. Although we use COVID-19 as a use case in this effort, our framework\nis general enough to be applied to other domains of interest in clinical NLP.\n","authors":["Sijia Liu","Andrew Wen","Liwei Wang","Huan He","Sunyang Fu","Robert Miller","Andrew Williams","Daniel Harris","Ramakanth Kavuluru","Mei Liu","Noor Abu-el-rub","Dalton Schutte","Rui Zhang","Masoud Rouhizadeh","John D. Osborne","Yongqun He","Umit Topaloglu","Stephanie S Hong","Joel H Saltz","Thomas Schaffter","Emily Pfaff","Christopher G. Chute","Tim Duong","Melissa A. Haendel","Rafael Fuentes","Peter Szolovits","Hua Xu","Hongfang Liu","National COVID Cohort Collaborative","Natural Language Processing"," Subgroup","National COVID Cohort Collaborative"],"pdf_url":"https://arxiv.org/pdf/2110.10780v3.pdf","comment":"update on contents"},{"id":"http://arxiv.org/abs/2202.12163v3","updated":"2022-03-21T19:25:04Z","published":"2022-02-24T16:01:07Z","title":"Attentive Temporal Pooling for Conformer-based Streaming Language\n  Identification in Long-form Speech","summary":"  In this paper, we introduce a novel language identification system based on\nconformer layers. We propose an attentive temporal pooling mechanism to allow\nthe model to carry information in long-form audio via a recurrent form, such\nthat the inference can be performed in a streaming fashion. Additionally, a\nsimple domain adaptation mechanism is introduced to allow adapting an existing\nlanguage identification model to a new domain where the prior language\ndistribution is different. We perform a comparative study of different model\ntopologies under different constraints of model size, and find that\nconformer-base models outperform LSTM and transformer based models. Our\nexperiments also show that attentive temporal pooling and domain adaptation\nsignificantly improve the model accuracy.\n","authors":["Quan Wang","Yang Yu","Jason Pelecanos","Yiling Huang","Ignacio Lopez Moreno"],"pdf_url":"https://arxiv.org/pdf/2202.12163v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11258v1","updated":"2022-03-21T18:36:18Z","published":"2022-03-21T18:36:18Z","title":"Efficient Classification of Long Documents Using Transformers","summary":"  Several methods have been proposed for classifying long textual documents\nusing Transformers. However, there is a lack of consensus on a benchmark to\nenable a fair comparison among different approaches. In this paper, we provide\na comprehensive evaluation of the relative efficacy measured against various\nbaselines and diverse datasets -- both in terms of accuracy as well as time and\nspace overheads. Our datasets cover binary, multi-class, and multi-label\nclassification tasks and represent various ways information is organized in a\nlong text (e.g. information that is critical to making the classification\ndecision is at the beginning or towards the end of the document). Our results\nshow that more complex models often fail to outperform simple baselines and\nyield inconsistent performance across datasets. These findings emphasize the\nneed for future studies to consider comprehensive baselines and datasets that\nbetter represent the task of long document classification to develop robust\nmodels.\n","authors":["Hyunji Hayley Park","Yogarshi Vyas","Kashif Shah"],"pdf_url":"https://arxiv.org/pdf/2203.11258v1.pdf","comment":"Accepted to ACL 2022; 8 pages"},{"id":"http://arxiv.org/abs/2203.11239v1","updated":"2022-03-21T18:04:25Z","published":"2022-03-21T18:04:25Z","title":"DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and\n  Quantization","summary":"  Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve\nstate-of-the-art performance on many generative NLP tasks. However, such models\npose a great challenge in resource-constrained scenarios owing to their large\nmemory requirements and high latency. To alleviate this issue, we propose to\njointly distill and quantize the model, where knowledge is transferred from the\nfull-precision teacher model to the quantized and distilled low-precision\nstudent model. Empirical analyses show that, despite the challenging nature of\ngenerative tasks, we were able to achieve a 16.5x model footprint compression\nratio with little performance drop relative to the full-precision counterparts\non multiple summarization and QA datasets. We further pushed the limit of\ncompression ratio to 27.7x and presented the performance-efficiency trade-off\nfor generative tasks using pre-trained models. To the best of our knowledge,\nthis is the first work aiming to effectively distill and quantize\nsequence-to-sequence pre-trained models for language generation tasks.\n","authors":["Zheng Li","Zijian Wang","Ming Tan","Ramesh Nallapati","Parminder Bhatia","Andrew Arnold","Bing Xiang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2203.11239v1.pdf","comment":"ACL 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2203.11194v1","updated":"2022-03-21T17:59:50Z","published":"2022-03-21T17:59:50Z","title":"Generating Fast and Slow: Scene Decomposition via Reconstruction","summary":"  We consider the problem of segmenting scenes into constituent entities, i.e.\nunderlying objects and their parts. Current supervised visual detectors though\nimpressive within their training distribution, often fail to segment\nout-of-distribution scenes into their constituent entities. Recent slot-centric\ngenerative models break such dependence on supervision, by attempting to\nsegment scenes into entities unsupervised, by reconstructing pixels. However,\nthey have been restricted thus far to toy scenes as they suffer from a\nreconstruction-segmentation trade-off: as the entity bottleneck gets wider,\nreconstruction improves but then the segmentation collapses. We propose\nGFS-Nets (Generating Fast and Slow Networks) that alleviate this issue with two\ningredients: i) curriculum training in the form of primitives, often missing\nfrom current generative models and, ii) test-time adaptation per scene through\ngradient descent on the reconstruction objective, what we call slow inference,\nmissing from current feed-forward detectors. We show the proposed curriculum\nsuffices to break the reconstruction-segmentation trade-off, and slow inference\ngreatly improves segmentation in out-of-distribution scenes. We evaluate\nGFS-Nets in 3D and 2D scene segmentation benchmarks of PartNet, CLEVR, Room\nDiverse++, and show large ( 50%) performance improvements against SOTA\nsupervised feed-forward detectors and unsupervised object discovery methods\n","authors":["Mihir Prabhudesai","Anirudh Goyal","Deepak Pathak","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2203.11194v1.pdf","comment":"Project website at https://mihirp1998.github.io/project_pages/gfsnets"},{"id":"http://arxiv.org/abs/2203.11192v1","updated":"2022-03-21T17:59:40Z","published":"2022-03-21T17:59:40Z","title":"Transforming Model Prediction for Tracking","summary":"  Optimization based tracking methods have been widely successful by\nintegrating a target model prediction module, providing effective global\nreasoning by minimizing an objective function. While this inductive bias\nintegrates valuable domain knowledge, it limits the expressivity of the\ntracking network. In this work, we therefore propose a tracker architecture\nemploying a Transformer-based model prediction module. Transformers capture\nglobal relations with little inductive bias, allowing it to learn the\nprediction of more powerful target models. We further extend the model\npredictor to estimate a second set of weights that are applied for accurate\nbounding box regression. The resulting tracker relies on training and on test\nframe information in order to predict all weights transductively. We train the\nproposed tracker end-to-end and validate its performance by conducting\ncomprehensive experiments on multiple tracking datasets. Our tracker sets a new\nstate of the art on three benchmarks, achieving an AUC of 68.5% on the\nchallenging LaSOT dataset.\n","authors":["Christoph Mayer","Martin Danelljan","Goutam Bhat","Matthieu Paul","Danda Pani Paudel","Fisher Yu","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.11192v1.pdf","comment":"Accepted at CVPR 2022. The code and trained models are available at\n  https://github.com/visionml/pytracking"},{"id":"http://arxiv.org/abs/2203.11191v1","updated":"2022-03-21T17:59:19Z","published":"2022-03-21T17:59:19Z","title":"Robust Visual Tracking by Segmentation","summary":"  Estimating the target extent poses a fundamental challenge in visual object\ntracking. Typically, trackers are box-centric and fully rely on a bounding box\nto define the target in the scene. In practice, objects often have complex\nshapes and are not aligned with the image axis. In these cases, bounding boxes\ndo not provide an accurate description of the target and often contain a\nmajority of background pixels. We propose a segmentation-centric tracking\npipeline that not only produces a highly accurate segmentation mask, but also\nworks internally with segmentation masks instead of bounding boxes. Thus, our\ntracker is able to better learn a target representation that clearly\ndifferentiates the target in the scene from background content. In order to\nachieve the necessary robustness for the challenging tracking scenario, we\npropose a separate instance localization component that is used to condition\nthe segmentation decoder when producing the output mask. We infer a bounding\nbox from the segmentation mask and validate our tracker on challenging tracking\ndatasets and achieve the new state of the art on LaSOT with a success AUC score\nof 69.7%. Since fully evaluating the predicted masks on tracking datasets is\nnot possible due to the missing mask annotations, we further validate our\nsegmentation quality on two popular video object segmentation datasets.\n","authors":["Matthieu Paul","Martin Danelljan","Christoph Mayer","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.11191v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.11183v1","updated":"2022-03-21T17:57:34Z","published":"2022-03-21T17:57:34Z","title":"Masked Discrimination for Self-Supervised Learning on Point Clouds","summary":"  Masked autoencoding has achieved great success for self-supervised learning\nin the image and language domains. However, mask based pretraining has yet to\nshow benefits for point cloud understanding, likely due to standard backbones\nlike PointNet being unable to properly handle the training versus testing\ndistribution mismatch introduced by masking during training. In this paper, we\nbridge this gap by proposing a discriminative mask pretraining Transformer\nframework, MaskPoint}, for point clouds. Our key idea is to represent the point\ncloud as discrete occupancy values (1 if part of the point cloud; 0 if not),\nand perform simple binary classification between masked object points and\nsampled noise points as the proxy task. In this way, our approach is robust to\nthe point sampling variance in point clouds, and facilitates learning rich\nrepresentations. We evaluate our pretrained models across several downstream\ntasks, including 3D shape classification, segmentation, and real-word object\ndetection, and demonstrate state-of-the-art results while achieving a\nsignificant pretraining speedup (e.g., 4.1x on ScanNet) compared to the prior\nstate-of-the-art Transformer baseline. Code will be publicly available at\nhttps://github.com/haotian-liu/MaskPoint.\n","authors":["Haotian Liu","Mu Cai","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2203.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11174v1","updated":"2022-03-21T17:54:30Z","published":"2022-03-21T17:54:30Z","title":"DiffPoseNet: Direct Differentiable Camera Pose Estimation","summary":"  Current deep neural network approaches for camera pose estimation rely on\nscene structure for 3D motion estimation, but this decreases the robustness and\nthereby makes cross-dataset generalization difficult. In contrast, classical\napproaches to structure from motion estimate 3D motion utilizing optical flow\nand then compute depth. Their accuracy, however, depends strongly on the\nquality of the optical flow. To avoid this issue, direct methods have been\nproposed, which separate 3D motion from depth estimation but compute 3D motion\nusing only image gradients in the form of normal flow. In this paper, we\nintroduce a network NFlowNet, for normal flow estimation which is used to\nenforce robust and direct constraints. In particular, normal flow is used to\nestimate relative camera pose based on the cheirality (depth positivity)\nconstraint. We achieve this by formulating the optimization problem as a\ndifferentiable cheirality layer, which allows for end-to-end learning of camera\npose. We perform extensive qualitative and quantitative evaluation of the\nproposed DiffPoseNet's sensitivity to noise and its generalization across\ndatasets. We compare our approach to existing state-of-the-art methods on\nKITTI, TartanAir, and TUM-RGBD datasets.\n","authors":["Chethan M. Parameshwara","Gokul Hari","Cornelia Fermüller","Nitin J. Sanket","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2203.11174v1.pdf","comment":"10 pages, 5 figures, Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11173v1","updated":"2022-03-21T17:53:22Z","published":"2022-03-21T17:53:22Z","title":"Interpreting Class Conditional GANs with Channel Awareness","summary":"  Understanding the mechanism of generative adversarial networks (GANs) helps\nus better use GANs for downstream applications. Existing efforts mainly target\ninterpreting unconditional models, leaving it less explored how a conditional\nGAN learns to render images regarding various categories. This work fills in\nthis gap by investigating how a class conditional generator unifies the\nsynthesis of multiple classes. For this purpose, we dive into the widely used\nclass-conditional batch normalization (CCBN), and observe that each feature\nchannel is activated at varying degrees given different categorical embeddings.\nTo describe such a phenomenon, we propose channel awareness, which\nquantitatively characterizes how a single channel contributes to the final\nsynthesis. Extensive evaluations and analyses on the BigGAN model pre-trained\non ImageNet reveal that only a subset of channels is primarily responsible for\nthe generation of a particular category, similar categories (e.g., cat and dog)\nusually get related to some same channels, and some channels turn out to share\ninformation across all classes. For good measure, our algorithm enables several\nnovel applications with conditional GANs. Concretely, we achieve (1) versatile\nimage editing via simply altering a single channel and manage to (2)\nharmoniously hybridize two different classes. We further verify that the\nproposed channel awareness shows promising potential in (3) segmenting the\nsynthesized image and (4) evaluating the category-wise synthesis performance.\n","authors":["Yingqing He","Zhiyi Zhang","Jiapeng Zhu","Yujun Shen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2203.11173v1.pdf","comment":"Project page: https://yingqinghe.github.io/interclassgan/"},{"id":"http://arxiv.org/abs/2107.05680v2","updated":"2022-03-21T17:38:44Z","published":"2021-07-12T18:33:49Z","title":"Hidden Convexity of Wasserstein GANs: Interpretable Generative Models\n  with Closed-Form Solutions","summary":"  Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.\n","authors":["Arda Sahiner","Tolga Ergen","Batu Ozturkler","Burak Bartan","John Pauly","Morteza Mardani","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2107.05680v2.pdf","comment":"Published as paper in ICLR 2022. First two authors contributed\n  equally to this work; 34 pages, 11 figures"},{"id":"http://arxiv.org/abs/2203.11160v1","updated":"2022-03-21T17:35:46Z","published":"2022-03-21T17:35:46Z","title":"Drive&Segment: Unsupervised Semantic Segmentation of Urban Scenes via\n  Cross-modal Distillation","summary":"  This work investigates learning pixel-wise semantic image segmentation in\nurban scenes without any manual annotation, just from the raw non-curated data\ncollected by cars which, equipped with cameras and LiDAR sensors, drive around\na city. Our contributions are threefold. First, we propose a novel method for\ncross-modal unsupervised learning of semantic image segmentation by leveraging\nsynchronized LiDAR and image data. The key ingredient of our method is the use\nof an object proposal module that analyzes the LiDAR point cloud to obtain\nproposals for spatially consistent objects. Second, we show that these 3D\nobject proposals can be aligned with the input images and reliably clustered\ninto semantically meaningful pseudo-classes. Finally, we develop a cross-modal\ndistillation approach that leverages image data partially annotated with the\nresulting pseudo-classes to train a transformer-based model for image semantic\nsegmentation. We show the generalization capabilities of our method by testing\non four different testing datasets (Cityscapes, Dark Zurich, Nighttime Driving\nand ACDC) without any finetuning, and demonstrate significant improvements\ncompared to the current state of the art on this problem. See project webpage\nhttps://vobecant.github.io/DriveAndSegment/ for the code and more.\n","authors":["Antonin Vobecky","David Hurych","Oriane Siméoni","Spyros Gidaris","Andrei Bursuc","Patrick Pérez","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2203.11160v1.pdf","comment":"See project webpage https://vobecant.github.io/DriveAndSegment/ for\n  the code and more"},{"id":"http://arxiv.org/abs/2203.11156v1","updated":"2022-03-21T17:34:18Z","published":"2022-03-21T17:34:18Z","title":"Operator Sketching for Deep Unrolling Networks","summary":"  In this work we propose a new paradigm for designing efficient deep unrolling\nnetworks using operator sketching. The deep unrolling networks are currently\nthe state-of-the-art solutions for imaging inverse problems. However, for\nhigh-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI\nimaging, the deep unrolling schemes typically become inefficient both in terms\nof memory and computation, due to the need of computing multiple times the\nhigh-dimensional forward and adjoint operators. Recently researchers have found\nthat such limitations can be partially addressed by stochastic unrolling with\nsubsets of operators, inspired by the success of stochastic first-order\noptimization. In this work, we propose a further acceleration upon stochastic\nunrolling, using sketching techniques to approximate products in the\nhigh-dimensional image space. The operator sketching can be jointly applied\nwith stochastic unrolling for the best acceleration and compression\nperformance. Our numerical experiments on X-ray CT image reconstruction\ndemonstrate the remarkable effectiveness of our sketched unrolling schemes.\n","authors":["Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2203.11156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.10175v2","updated":"2022-03-21T17:32:08Z","published":"2021-12-19T15:50:48Z","title":"On Efficient Transformer-Based Image Pre-training for Low-Level Vision","summary":"  Pre-training has marked numerous state of the arts in high-level computer\nvision, while few attempts have ever been made to investigate how pre-training\nacts in image processing systems. In this paper, we tailor transformer-based\npre-training regimes that boost various low-level tasks. To comprehensively\ndiagnose the influence of pre-training, we design a whole set of principled\nevaluation tools that uncover its effects on internal representations. The\nobservations demonstrate that pre-training plays strikingly different roles in\nlow-level tasks. For example, pre-training introduces more local information to\nhigher layers in super-resolution (SR), yielding significant performance gains,\nwhile pre-training hardly affects internal feature representations in\ndenoising, resulting in limited gains. Further, we explore different methods of\npre-training, revealing that multi-related-task pre-training is more effective\nand data-efficient than other alternatives. Finally, we extend our study to\nvarying data scales and model sizes, as well as comparisons between\ntransformers and CNNs-based architectures. Based on the study, we successfully\ndevelop state-of-the-art models for multiple low-level tasks. Code is released\nat https://github.com/fenglinglwb/EDT.\n","authors":["Wenbo Li","Xin Lu","Shengju Qian","Jiangbo Lu","Xiangyu Zhang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2112.10175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11146v1","updated":"2022-03-21T17:25:09Z","published":"2022-03-21T17:25:09Z","title":"Multispectral Satellite Data Classification using Soft Computing\n  Approach","summary":"  A satellite image is a remotely sensed image data, where each pixel\nrepresents a specific location on earth. The pixel value recorded is the\nreflection radiation from the earth's surface at that location. Multispectral\nimages are those that capture image data at specific frequencies across the\nelectromagnetic spectrum as compared to Panchromatic images which are sensitive\nto all wavelength of visible light. Because of the high resolution and high\ndimensions of these images, they create difficulties for clustering techniques\nto efficiently detect clusters of different sizes, shapes and densities as a\ntrade off for fast processing time. In this paper we propose a grid-density\nbased clustering technique for identification of objects. We also introduce an\napproach to classify a satellite image data using a rule induction based\nmachine learning algorithm. The object identification and classification\nmethods have been validated using several synthetic and benchmark datasets.\n","authors":["Purbarag Pathak Choudhury","Ujjal Kr Dutta","Dhruba Kr Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2203.11146v1.pdf","comment":"Proc. of International Conference on Advances in Communication,\n  Network, and Computing (CNC), 2014"},{"id":"http://arxiv.org/abs/2201.12693v2","updated":"2022-03-21T17:20:02Z","published":"2022-01-30T01:02:18Z","title":"Extracting Built Environment Features for Planning Research with\n  Computer Vision: A Review and Discussion of State-of-the-Art Approaches","summary":"  This is an extended abstract for a presentation at The 17th International\nConference on CUPUM - Computational Urban Planning and Urban Management in June\n2021. This study presents an interdisciplinary synthesis of the\nstate-of-the-art approaches in computer vision technologies to extract built\nenvironment features that could improve the robustness of empirical research in\nplanning. We discussed the findings from the review of studies in both planning\nand computer science.\n","authors":["Meiqing Li","Hao Sheng"],"pdf_url":"https://arxiv.org/pdf/2201.12693v2.pdf","comment":"CUPUM 2021 (The 17th International Conference on Computational Urban\n  Planning and Urban Management)"},{"id":"http://arxiv.org/abs/2203.11139v1","updated":"2022-03-21T17:14:02Z","published":"2022-03-21T17:14:02Z","title":"Not All Points Are Equal: Learning Highly Efficient Point-based\n  Detectors for 3D LiDAR Point Clouds","summary":"  We study the problem of efficient object detection of 3D LiDAR point clouds.\nTo reduce the memory and computational cost, existing point-based pipelines\nusually adopt task-agnostic random sampling or farthest point sampling to\nprogressively downsample input point clouds, despite the fact that not all\npoints are equally important to the task of object detection. In particular,\nthe foreground points are inherently more important than background points for\nobject detectors. Motivated by this, we propose a highly-efficient single-stage\npoint-based 3D detector in this paper, termed IA-SSD. The key of our approach\nis to exploit two learnable, task-oriented, instance-aware downsampling\nstrategies to hierarchically select the foreground points belonging to objects\nof interest. Additionally, we also introduce a contextual centroid perception\nmodule to further estimate precise instance centers. Finally, we build our\nIA-SSD following the encoder-only architecture for efficiency. Extensive\nexperiments conducted on several large-scale detection benchmarks demonstrate\nthe competitive performance of our IA-SSD. Thanks to the low memory footprint\nand a high degree of parallelism, it achieves a superior speed of 80+\nframes-per-second on the KITTI dataset with a single RTX2080Ti GPU. The code is\navailable at \\url{https://github.com/yifanzhang713/IA-SSD}.\n","authors":["Yifan Zhang","Qingyong Hu","Guoquan Xu","Yanxin Ma","Jianwei Wan","Yulan Guo"],"pdf_url":"https://arxiv.org/pdf/2203.11139v1.pdf","comment":"CVPR2022, code avaliable at: https://github.com/yifanzhang713/IA-SSD"},{"id":"http://arxiv.org/abs/2203.11132v1","updated":"2022-03-21T17:06:22Z","published":"2022-03-21T17:06:22Z","title":"Review of Disentanglement Approaches for Medical Applications -- Towards\n  Solving the Gordian Knot of Generative Models in Healthcare","summary":"  Deep neural networks are commonly used for medical purposes such as image\ngeneration, segmentation, or classification. Besides this, they are often\ncriticized as black boxes as their decision process is often not human\ninterpretable. Encouraging the latent representation of a generative model to\nbe disentangled offers new perspectives of control and interpretability.\nUnderstanding the data generation process could help to create artificial\nmedical data sets without violating patient privacy, synthesizing different\ndata modalities, or discovering data generating characteristics. These\ncharacteristics might unravel novel relationships that can be related to\ngenetic traits or patient outcomes. In this paper, we give a comprehensive\noverview of popular generative models, like Generative Adversarial Networks\n(GANs), Variational Autoencoders (VAEs), and Flow-based Models. Furthermore, we\nsummarize the different notions of disentanglement, review approaches to\ndisentangle latent space representations and metrics to evaluate the degree of\ndisentanglement. After introducing the theoretical frameworks, we give an\noverview of recent medical applications and discuss the impact and importance\nof disentanglement approaches for medical applications.\n","authors":["Jana Fragemann","Lynton Ardizzone","Jan Egger","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2203.11132v1.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2203.11130v1","updated":"2022-03-21T17:05:23Z","published":"2022-03-21T17:05:23Z","title":"PACS: A Dataset for Physical Audiovisual CommonSense Reasoning","summary":"  In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, they should be able to reason about the\nphysical world by understanding the physical properties and affordances of\navailable objects, how they can be manipulated, and how they interact with\nother physical objects. This research field of physical commonsense reasoning\nis fundamentally a multi-sensory task since physical properties are manifested\nthrough multiple modalities, two of them being vision and acoustics. Our paper\ntakes a step towards real-world physical commonsense reasoning by contributing\nPACS: the first audiovisual benchmark annotated for physical commonsense\nattributes. PACS contains a total of 13,400 question-answer pairs, involving\n1,377 unique physical commonsense questions and 1,526 videos. Our dataset\nprovides new opportunities to advance the research field of physical reasoning\nby bringing audio as a core component of this multimodal problem. Using PACS,\nwe evaluate multiple state-of-the-art models on this new challenging task.\nWhile some models show promising results (70% accuracy), they all fall short of\nhuman performance (95% accuracy). We conclude the paper by demonstrating the\nimportance of multimodal reasoning and providing possible avenues for future\nresearch.\n","authors":["Samuel Yu","Peter Wu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2203.11130v1.pdf","comment":"38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2108.06227v4","updated":"2022-03-21T16:43:52Z","published":"2021-08-13T13:17:58Z","title":"SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for\n  Semi-Supervised Medical Image Segmentation","summary":"  Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis, enhancement, and registration.\n","authors":["Chenyu You","Yuan Zhou","Ruihan Zhao","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2108.06227v4.pdf","comment":"IEEE Transactions on Medical Imaging (IEEE-TMI) 2022"},{"id":"http://arxiv.org/abs/2203.11113v1","updated":"2022-03-21T16:41:35Z","published":"2022-03-21T16:41:35Z","title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces","summary":"  Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n","authors":["Jia-Xing Zhong","Kaichen Zhou","Qingyong Hu","Bing Wang","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2203.11113v1.pdf","comment":"To appear at CVPR 2022 (Source Code:\n  https://github.com/jx-zhong-for-academic-purpose/Kinet )"},{"id":"http://arxiv.org/abs/2203.11111v1","updated":"2022-03-21T16:40:55Z","published":"2022-03-21T16:40:55Z","title":"Facial Expression Analysis Using Decomposed Multiscale Spatiotemporal\n  Networks","summary":"  Video-based analysis of facial expressions has been increasingly applied to\ninfer health states of individuals, such as depression and pain. Among the\nexisting approaches, deep learning models composed of structures for multiscale\nspatiotemporal processing have shown strong potential for encoding facial\ndynamics. However, such models have high computational complexity, making for a\ndifficult deployment of these solutions. To address this issue, we introduce a\nnew technique to decompose the extraction of multiscale spatiotemporal\nfeatures. Particularly, a building block structure called Decomposed Multiscale\nSpatiotemporal Network (DMSN) is presented along with three variants: DMSN-A,\nDMSN-B, and DMSN-C blocks. The DMSN-A block generates multiscale\nrepresentations by analyzing spatiotemporal features at multiple temporal\nranges, while the DMSN-B block analyzes spatiotemporal features at multiple\nranges, and the DMSN-C block analyzes spatiotemporal features at multiple\nspatial sizes. Using these variants, we design our DMSN architecture which has\nthe ability to explore a variety of multiscale spatiotemporal features,\nfavoring the adaptation to different facial behaviors. Our extensive\nexperiments on challenging datasets show that the DMSN-C block is effective for\ndepression detection, whereas the DMSN-A block is efficient for pain\nestimation. Results also indicate that our DMSN architecture provides a\ncost-effective solution for expressions that range from fewer facial variations\nover time, as in depression detection, to greater variations, as in pain\nestimation.\n","authors":["Wheidima Carneiro de Melo","Eric Granger","Miguel Bordallo Lopez"],"pdf_url":"https://arxiv.org/pdf/2203.11111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11105v1","updated":"2022-03-21T16:32:12Z","published":"2022-03-21T16:32:12Z","title":"High-fidelity GAN Inversion with Padding Space","summary":"  Inverting a Generative Adversarial Network (GAN) facilitates a wide range of\nimage editing tasks using pre-trained generators. Existing methods typically\nemploy the latent space of GANs as the inversion space yet observe the\ninsufficient recovery of spatial details. In this work, we propose to involve\nthe padding space of the generator to complement the latent space with spatial\ninformation. Concretely, we replace the constant padding (e.g., usually zeros)\nused in convolution layers with some instance-aware coefficients. In this way,\nthe inductive bias assumed in the pre-trained model can be appropriately\nadapted to fit each individual image. Through learning a carefully designed\nencoder, we manage to improve the inversion quality both qualitatively and\nquantitatively, outperforming existing alternatives. We then demonstrate that\nsuch a space extension barely affects the native GAN manifold, hence we can\nstill reuse the prior knowledge learned by GANs for various downstream\napplications. Beyond the editing tasks explored in prior arts, our approach\nallows a more flexible image manipulation, such as the separate control of face\ncontour and facial details, and enables a novel editing manner where users can\ncustomize their own manipulations highly efficiently.\n","authors":["Qingyan Bai","Yinghao Xu","Jiapeng Zhu","Weihao Xia","Yujiu Yang","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2203.11105v1.pdf","comment":"Project page: https://ezioby.github.io/padinv/; Code:\n  https://github.com/EzioBy/padinv"},{"id":"http://arxiv.org/abs/2203.09507v2","updated":"2022-03-21T16:25:54Z","published":"2022-03-17T17:56:34Z","title":"Towards Data-Efficient Detection Transformers","summary":"  Detection Transformers have achieved competitive performance on the\nsample-rich COCO dataset. However, we show most of them suffer from significant\nperformance drops on small-size datasets, like Cityscapes. In other words, the\ndetection transformers are generally data-hungry. To tackle this problem, we\nempirically analyze the factors that affect data efficiency, through a\nstep-by-step transition from a data-efficient RCNN variant to the\nrepresentative DETR. The empirical results suggest that sparse feature sampling\nfrom local image areas holds the key. Based on this observation, we alleviate\nthe data-hungry issue of existing detection transformers by simply alternating\nhow key and value sequences are constructed in the cross-attention layer, with\nminimum modifications to the original models. Besides, we introduce a simple\nyet effective label augmentation method to provide richer supervision and\nimprove data efficiency. Experiments show that our method can be readily\napplied to different detection transformers and improve their performance on\nboth small-size and sample-rich datasets. Code will be made publicly available\nat \\url{https://github.com/encounter1997/DE-DETRs}.\n","authors":["Wen Wang","Jing Zhang","Yang Cao","Yongliang Shen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2203.09507v2.pdf","comment":"Code is available at https://github.com/encounter1997/DE-DETRs and\n  https://github.com/encounter1997/DE-CondDETR"},{"id":"http://arxiv.org/abs/2203.11096v1","updated":"2022-03-21T16:23:02Z","published":"2022-03-21T16:23:02Z","title":"CLIP meets GamePhysics: Towards bug identification in gameplay videos\n  using zero-shot transfer learning","summary":"  Gameplay videos contain rich information about how players interact with the\ngame and how the game responds. Sharing gameplay videos on social media\nplatforms, such as Reddit, has become a common practice for many players.\nOften, players will share gameplay videos that showcase video game bugs. Such\ngameplay videos are software artifacts that can be utilized for game testing,\nas they provide insight for bug analysis. Although large repositories of\ngameplay videos exist, parsing and mining them in an effective and structured\nfashion has still remained a big challenge. In this paper, we propose a search\nmethod that accepts any English text query as input to retrieve relevant videos\nfrom large repositories of gameplay videos. Our approach does not rely on any\nexternal information (such as video metadata); it works solely based on the\ncontent of the video. By leveraging the zero-shot transfer capabilities of the\nContrastive Language-Image Pre-Training (CLIP) model, our approach does not\nrequire any data labeling or training. To evaluate our approach, we present the\n$\\texttt{GamePhysics}$ dataset consisting of 26,954 videos from 1,873 games,\nthat were collected from the GamePhysics section on the Reddit website. Our\napproach shows promising results in our extensive analysis of simple queries,\ncompound queries, and bug queries, indicating that our approach is useful for\nobject and event detection in gameplay videos. An example application of our\napproach is as a gameplay video search engine to aid in reproducing video game\nbugs. Please visit the following link for the code and the data:\n$\\href{https://asgaardlab.github.io/CLIPxGamePhysics/}{\\text{asgaardlab.github.io/CLIPxGamePhysics/}}$\n","authors":["Mohammad Reza Taesiri","Finlay Macklon","Cor-Paul Bezemer"],"pdf_url":"https://arxiv.org/pdf/2203.11096v1.pdf","comment":"Accepted by MSR 2022 conference"},{"id":"http://arxiv.org/abs/2203.11089v1","updated":"2022-03-21T16:12:53Z","published":"2022-03-21T16:12:53Z","title":"PersFormer: 3D Lane Detection via Perspective Transformer and the\n  OpenLane Benchmark","summary":"  Methods for 3D lane detection have been recently proposed to address the\nissue of inaccurate lane layouts in many autonomous driving scenarios\n(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to\ntheir simple designs of the spatial transformation between front view and\nbird's eye view (BEV) and the lack of a realistic dataset. Towards these\nissues, we present PersFormer: an end-to-end monocular 3D lane detector with a\nnovel Transformer-based spatial feature transformation module. Our model\ngenerates BEV features by attending to related front-view local regions with\ncamera parameters as a reference. PersFormer adopts a unified 2D/3D anchor\ndesign and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing\nthe feature consistency and sharing the benefits of multi-task learning.\nMoreover, we release one of the first large-scale real-world 3D lane datasets,\nwhich is called OpenLane, with high-quality annotation and scenario diversity.\nOpenLane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane\ncategories, along with scene tags and the closed-in-path object annotations to\nencourage the development of lane detection and more industrial-related\nautonomous driving methods. We show that PersFormer significantly outperforms\ncompetitive baselines in the 3D lane detection task on our new OpenLane dataset\nas well as Apollo 3D Lane Synthetic dataset, and is also on par with\nstate-of-the-art algorithms in the 2D task on OpenLane. The project page is\navailable at https://github.com/OpenPerceptionX/OpenLane.\n","authors":["Li Chen","Chonghao Sima","Yang Li","Zehan Zheng","Jiajie Xu","Xiangwei Geng","Hongyang Li","Conghui He","Jianping Shi","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2203.11089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11082v1","updated":"2022-03-21T16:04:21Z","published":"2022-03-21T16:04:21Z","title":"MixFormer: End-to-End Tracking with Iterative Mixed Attention","summary":"  Tracking often uses a multi-stage pipeline of feature extraction, target\ninformation integration, and bounding box estimation. To simplify this pipeline\nand unify the process of feature extraction and target information integration,\nwe present a compact tracking framework, termed as {\\em MixFormer}, built upon\ntransformers. Our core design is to utilize the flexibility of attention\noperations, and propose a Mixed Attention Module (MAM) for simultaneous feature\nextraction and target information integration. This synchronous modeling scheme\nallows to extract target-specific discriminative features and perform extensive\ncommunication between target and search area. Based on MAM, we build our\nMixFormer tracking framework simply by stacking multiple MAMs with progressive\npatch embedding and placing a localization head on top. In addition, to handle\nmultiple target templates during online tracking, we devise an asymmetric\nattention scheme in MAM to reduce computational cost, and propose an effective\nscore prediction module to select high-quality templates. Our MixFormer sets a\nnew state-of-the-art performance on five tracking benchmarks, including LaSOT,\nTrackingNet, VOT2020, GOT-10k, and UAV123. In particular, our MixFormer-L\nachieves NP score of 79.9 on LaSOT, 88.9 on TrackingNet and EAO of 0.555 on\nVOT2020. We also perform in-depth ablation studies to demonstrate the\neffectiveness of simultaneous feature extraction and information integration.\nCode and trained models are publicly available at\n\\href{https://github.com/MCG-NJU/MixFormer}{https://github.com/MCG-NJU/MixFormer}.\n","authors":["Yutao Cui","Jiang Cheng","Limin Wang","Gangshan Wu"],"pdf_url":"https://arxiv.org/pdf/2203.11082v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.11081v1","updated":"2022-03-21T16:03:07Z","published":"2022-03-21T16:03:07Z","title":"Image Classification on Accelerated Neural Networks","summary":"  For image classification problems, various neural network models are commonly\nused due to their success in yielding high accuracies. Convolutional Neural\nNetwork (CNN) is one of the most frequently used deep learning methods for\nimage classification applications. It may produce extraordinarily accurate\nresults with regard to its complexity. However, the more complex the model is\nthe longer it takes to train. In this paper, an acceleration design that uses\nthe power of FPGA is given for a basic CNN model which consists of one\nconvolutional layer and one fully connected layer for the training phase of the\nfully connected layer. Nonetheless, inference phase is also accelerated\nautomatically due to the fact that training phase includes inference. In this\ndesign, the convolutional layer is calculated by the host computer and the\nfully connected layer is calculated by an FPGA board. It should be noted that\nthe training of convolutional layer is not taken into account in this design\nand is left for future research. The results are quite encouraging as this FPGA\ndesign tops the performance of some of the state-of-the-art deep learning\nplatforms such as Tensorflow on the host computer approximately 2 times in both\ntraining and inference.\n","authors":["Ilkay Sikdokur","Inci Baytas","Arda Yurdakul"],"pdf_url":"https://arxiv.org/pdf/2203.11081v1.pdf","comment":"Presented at Basarim 2020 conference"},{"id":"http://arxiv.org/abs/2203.11078v1","updated":"2022-03-21T15:57:02Z","published":"2022-03-21T15:57:02Z","title":"MTBF-33: A multi-temporal building footprint dataset for 33 counties in\n  the United States (1900-2015)","summary":"  Despite abundant data on the spatial distribution of contemporary human\nsettlements, historical data on the long-term evolution of human settlements at\nfine spatial and temporal granularity is scarce, limiting our quantitative\nunderstanding of long-term changes of built-up areas. This is because commonly\nused mapping methods (e.g., image classification) and suitable data sources\n(i.e., aerial imagery, multi-spectral remote sensing data, LiDAR) have only\nbeen available in recent decades. However, there are alternative data sources\nsuch as cadastral records that are digitally available, containing relevant\ninformation such as building age information, allowing for an approximate,\ndigital reconstruction of past building distributions. We conducted a\nnon-exhaustive search of open and publicly available data resources from\nadministrative institutions in the United States and gathered, integrated, and\nharmonized cadastral parcel data, tax assessment data, and building footprint\ndata for 33 counties, wherever building footprint geometries and building\nconstruction year information was available. The result of this effort is a\nunique dataset which we call the Multi-Temporal Building Footprint Dataset for\n33 U.S. Counties (MTBF-33). MTBF-33 contains over 6.2 million building\nfootprints including their construction year, and can be used to derive\nretrospective depictions of built-up areas from 1900 to 2015, at fine spatial\nand temporal grain and can be used for data validation purposes, or to train\nstatistical learning approaches aiming to extract historical information on\nhuman settlements from remote sensing data, historical maps, or similar data\nsources. MTBF-33 is available at http://doi.org/10.17632/w33vbvjtdy.\n","authors":["Johannes H. Uhl","Stefan Leyk"],"pdf_url":"https://arxiv.org/pdf/2203.11078v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2203.11075v1","updated":"2022-03-21T15:55:23Z","published":"2022-03-21T15:55:23Z","title":"Dense Siamese Network","summary":"  This paper presents Dense Siamese Network (DenseSiam), a simple unsupervised\nlearning framework for dense prediction tasks. It learns visual representations\nby maximizing the similarity between two views of one image with two types of\nconsistency, i.e., pixel consistency and region consistency. Concretely,\nDenseSiam first maximizes the pixel level spatial consistency according to the\nexact location correspondence in the overlapped area. It also extracts a batch\nof region embeddings that correspond to some sub-regions in the overlapped area\nto be contrasted for region consistency. In contrast to previous methods that\nrequire negative pixel pairs, momentum encoders, or heuristic masks, DenseSiam\nbenefits from the simple Siamese network and optimizes the consistency of\ndifferent granularities. It also proves that the simple location correspondence\nand interacted region embeddings are effective enough to learn the similarity.\nWe apply DenseSiam on ImageNet and obtain competitive improvements on various\ndownstream tasks. We also show that only with some extra task-specific losses,\nthe simple framework can directly conduct dense prediction tasks. On an\nexisting unsupervised semantic segmentation benchmark, it surpasses\nstate-of-the-art segmentation methods by 2.1 mIoU with 28% training costs.\n","authors":["Wenwei Zhang","Jiangmiao Pang","Kai Chen","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2203.11075v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2203.11068v1","updated":"2022-03-21T15:45:35Z","published":"2022-03-21T15:45:35Z","title":"Learning Enriched Illuminants for Cross and Single Sensor Color\n  Constancy","summary":"  Color constancy aims to restore the constant colors of a scene under\ndifferent illuminants. However, due to the existence of camera spectral\nsensitivity, the network trained on a certain sensor, cannot work well on\nothers. Also, since the training datasets are collected in certain\nenvironments, the diversity of illuminants is limited for complex real world\nprediction. In this paper, we tackle these problems via two aspects. First, we\npropose cross-sensor self-supervised training to train the network. In detail,\nwe consider both the general sRGB images and the white-balanced RAW images from\ncurrent available datasets as the white-balanced agents. Then, we train the\nnetwork by randomly sampling the artificial illuminants in a sensor-independent\nmanner for scene relighting and supervision. Second, we analyze a previous\ncascaded framework and present a more compact and accurate model by sharing the\nbackbone parameters with learning attention specifically. Experiments show that\nour cross-sensor model and single-sensor model outperform other\nstate-of-the-art methods by a large margin on cross and single sensor\nevaluations, respectively, with only 16% parameters of the previous best model.\n","authors":["Xiaodong Cun","Zhendong Wang","Chi-Man Pun","Jianzhuang Liu","Wengang Zhou","Xu Jia","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2203.11068v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2203.06890v2","updated":"2022-03-21T15:34:03Z","published":"2022-03-14T07:11:20Z","title":"Attention based Memory video portrait matting","summary":"  We proposed a novel trimap free video matting method based on the attention\nmechanism. By the nature of the problem, most existing approaches use either\nmultiple computational expansive modules or complex algorithms to exploit\ntemporal information fully. We designed a temporal aggregation module to\ncompute the temporal coherence between the current frame and its two previous\nframes.\n","authors":["Shufeng Song"],"pdf_url":"https://arxiv.org/pdf/2203.06890v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2201.05585v2","updated":"2022-03-21T14:59:01Z","published":"2022-01-14T18:13:09Z","title":"Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip\n  Connections and Hybrid Learning","summary":"  In this paper we address the challenging problem of domain adaptation in\nLiDAR semantic segmentation. We consider the setting where we have a\nfully-labeled data set from source domain and a target domain with a few\nlabeled and many unlabeled examples. We propose a domain adaption framework\nthat mitigates the issue of domain shift and produces appealing performance on\nthe target domain. To this end, we develop a GAN-based image-to-image\ntranslation engine that has generators with alternating connections, and couple\nit with a state-of-the-art LiDAR semantic segmentation network. Our framework\nis hybrid in nature in the sense that our model learning is composed of\nself-supervision, semi-supervision and unsupervised learning. Extensive\nexperiments on benchmark LiDAR semantic segmentation data sets demonstrate that\nour method achieves superior performance in comparison to strong baselines and\nprior arts.\n","authors":["Eduardo R. Corral-Soto","Mrigank Rochan","Yannis Y. He","Shubhra Aich","Yang Liu","Liu Bingbing"],"pdf_url":"https://arxiv.org/pdf/2201.05585v2.pdf","comment":"1) Introduced Fig 1, 2) Simplified Fig. 2 diagram, 3) Fixed typos in\n  losses, 4) Introduced Fig. 3, 5) Updated evaluation results, included\n  evaluation on SemanticPOSS, 6) Introduced Table 3 - effects on covariance\n  matrix and mean, 7) Updated Fig. 5, 8) Added more references. Improved\n  writing in general, especially the motivation and description of each element\n  and contribution from the method"},{"id":"http://arxiv.org/abs/2105.01447v2","updated":"2022-03-21T14:56:12Z","published":"2021-05-04T12:14:30Z","title":"Moving Towards Centers: Re-ranking with Attention and Memory for\n  Re-identification","summary":"  Re-ranking utilizes contextual information to optimize the initial ranking\nlist of person or vehicle re-identification (re-ID), which boosts the retrieval\nperformance at post-processing steps. This paper proposes a re-ranking network\nto predict the correlations between the probe and top-ranked neighbor samples.\nSpecifically, all the feature embeddings of query and gallery images are\nexpanded and enhanced by a linear combination of their neighbors, with the\ncorrelation prediction serving as discriminative combination weights. The\ncombination process is equivalent to moving independent embeddings toward the\nidentity centers, improving cluster compactness. For correlation prediction, we\nfirst aggregate the contextual information for probe's k-nearest neighbors via\nthe Transformer encoder. Then, we distill and refine the probe-related features\ninto the Contextual Memory cell via attention mechanism. Like humans that\nretrieve images by not only considering probe images but also memorizing the\nretrieved ones, the Contextual Memory produces multi-view descriptions for each\ninstance. Finally, the neighbors are reconstructed with features fetched from\nthe Contextual Memory, and a binary classifier predicts their correlations with\nthe probe. Experiments on six widely-used person and vehicle re-ID benchmarks\ndemonstrate the effectiveness of the proposed method. Especially, our method\nsurpasses the state-of-the-art re-ranking approaches on large-scale datasets by\na significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP\nimprovements on VERI-Wild, MSMT17, and VehicleID datasets.\n","authors":["Yunhao Zhou","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2105.01447v2.pdf","comment":"13 pages. Accepted for Publication at IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2109.09416v4","updated":"2022-03-21T14:42:26Z","published":"2021-09-20T10:31:50Z","title":"ElasticFace: Elastic Margin Loss for Deep Face Recognition","summary":"  Learning discriminative face features plays a major role in building\nhigh-performing face recognition models. The recent state-of-the-art face\nrecognition solutions proposed to incorporate a fixed penalty margin on\ncommonly used classification loss function, softmax loss, in the normalized\nhypersphere to increase the discriminative power of face recognition models, by\nminimizing the intra-class variation and maximizing the inter-class variation.\nMarginal penalty softmax losses, such as ArcFace and CosFace, assume that the\ngeodesic distance between and within the different identities can be equally\nlearned using a fixed penalty margin. However, such a learning objective is not\nrealistic for real data with inconsistent inter-and intra-class variation,\nwhich might limit the discriminative and generalizability of the face\nrecognition model. In this paper, we relax the fixed penalty margin constrain\nby proposing elastic penalty margin loss (ElasticFace) that allows flexibility\nin the push for class separability. The main idea is to utilize random margin\nvalues drawn from a normal distribution in each training iteration. This aims\nat giving the decision boundary chances to extract and retract to allow space\nfor flexible class separability learning. We demonstrate the superiority of our\nElasticFace loss over ArcFace and CosFace losses, using the same geometric\ntransformation, on a large set of mainstream benchmarks. From a wider\nperspective, our ElasticFace has advanced the state-of-the-art face recognition\nperformance on seven out of nine mainstream benchmarks.\n","authors":["Fadi Boutros","Naser Damer","Florian Kirchbuchner","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2109.09416v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.04132v2","updated":"2022-03-21T14:40:26Z","published":"2022-03-08T14:58:41Z","title":"Motron: Multimodal Probabilistic Human Motion Forecasting","summary":"  Autonomous systems and humans are increasingly sharing the same space. Robots\nwork side by side or even hand in hand with humans to balance each other's\nlimitations. Such cooperative interactions are ever more sophisticated. Thus,\nthe ability to reason not just about a human's center of gravity position, but\nalso its granular motion is an important prerequisite for human-robot\ninteraction. Though, many algorithms ignore the multimodal nature of humans or\nneglect uncertainty in their motion forecasts. We present Motron, a multimodal,\nprobabilistic, graph-structured model, that captures human's multimodality\nusing probabilistic methods while being able to output deterministic\nmaximum-likelihood motions and corresponding confidence values for each mode.\nOur model aims to be tightly integrated with the robotic\nplanning-control-interaction loop; outputting physically feasible human motions\nand being computationally efficient. We demonstrate the performance of our\nmodel on several challenging real-world motion forecasting datasets,\noutperforming a wide array of generative/variational methods while providing\nstate-of-the-art single-output motions if required. Both using significantly\nless computational power than state-of-the art algorithms.\n","authors":["Tim Salzmann","Marco Pavone","Markus Ryll"],"pdf_url":"https://arxiv.org/pdf/2203.04132v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11018v1","updated":"2022-03-21T14:36:07Z","published":"2022-03-21T14:36:07Z","title":"Stereo Neural Vernier Caliper","summary":"  We propose a new object-centric framework for learning-based stereo 3D object\ndetection. Previous studies build scene-centric representations that do not\nconsider the significant variation among outdoor instances and thus lack the\nflexibility and functionalities that an instance-level model can offer. We\nbuild such an instance-level model by formulating and tackling a local update\nproblem, i.e., how to predict a refined update given an initial 3D cuboid\nguess. We demonstrate how solving this problem can complement scene-centric\napproaches in (i) building a coarse-to-fine multi-resolution system, (ii)\nperforming model-agnostic object location refinement, and (iii) conducting\nstereo 3D tracking-by-detection. Extensive experiments demonstrate the\neffectiveness of our approach, which achieves state-of-the-art performance on\nthe KITTI benchmark. Code and pre-trained models are available at\nhttps://github.com/Nicholasli1995/SNVC.\n","authors":["Shichao Li","Zechun Liu","Zhiqiang Shen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2203.11018v1.pdf","comment":"AAAI 2022 preprint version"},{"id":"http://arxiv.org/abs/2111.06349v2","updated":"2022-03-21T14:34:39Z","published":"2021-11-11T17:59:42Z","title":"Unsupervised Part Discovery from Contrastive Reconstruction","summary":"  The goal of self-supervised visual representation learning is to learn\nstrong, transferable image representations, with the majority of research\nfocusing on object or scene level. On the other hand, representation learning\nat part level has received significantly less attention. In this paper, we\npropose an unsupervised approach to object part discovery and segmentation and\nmake three contributions. First, we construct a proxy task through a set of\nobjectives that encourages the model to learn a meaningful decomposition of the\nimage into its parts. Secondly, prior work argues for reconstructing or\nclustering pre-computed features as a proxy to parts; we show empirically that\nthis alone is unlikely to find meaningful parts; mainly because of their low\nresolution and the tendency of classification networks to spatially smear out\ninformation. We suggest that image reconstruction at the level of pixels can\nalleviate this problem, acting as a complementary cue. Lastly, we show that the\nstandard evaluation based on keypoint regression does not correlate well with\nsegmentation quality and thus introduce different metrics, NMI and ARI, that\nbetter characterize the decomposition of objects into parts. Our method yields\nsemantic parts which are consistent across fine-grained but visually distinct\ncategories, outperforming the state of the art on three benchmark datasets.\nCode is available at the project page:\nhttps://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.\n","authors":["Subhabrata Choudhury","Iro Laina","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2111.06349v2.pdf","comment":"NeurIPS 2021. Project page:\n  https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/"},{"id":"http://arxiv.org/abs/2203.11009v1","updated":"2022-03-21T14:23:18Z","published":"2022-03-21T14:23:18Z","title":"Online Skeleton-based Action Recognition with Continual Spatio-Temporal\n  Graph Convolutional Networks","summary":"  Graph-based reasoning over skeleton data has emerged as a promising approach\nfor human action recognition. However, the application of prior graph-based\nmethods, which predominantly employ whole temporal sequences as their input, to\nthe setting of online inference entails considerable computational redundancy.\nIn this paper, we tackle this issue by reformulating the Spatio-Temporal Graph\nConvolutional Neural Network as a Continual Inference Network, which can\nperform step-by-step predictions in time without repeat frame processing. To\nevaluate our method, we create a continual version of ST-GCN, CoST-GCN,\nalongside two derived methods with different self-attention mechanisms, CoAGCN\nand CoS-TR. We investigate weight transfer strategies and architectural\nmodifications for inference acceleration, and perform experiments on the NTU\nRGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar\npredictive accuracy, we observe up to 109x reduction in time complexity,\non-hardware accelerations of 26x, and reductions in maximum allocated memory of\n52% during online inference.\n","authors":["Lukas Hedegaard","Negar Heidari","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2203.11009v1.pdf","comment":"11 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2203.11008v1","updated":"2022-03-21T14:23:10Z","published":"2022-03-21T14:23:10Z","title":"Transformer-based HTR for Historical Documents","summary":"  We apply the TrOCR framework to real-world, historical manuscripts and show\nthat TrOCR per se is a strong model, ideal for transfer learning. TrOCR has\nbeen trained on English only, but it can adapt to other languages that use the\nLatin alphabet fairly easily and with little training material. We compare\nTrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such\nsystems. This finding is essential since Transkribus performs best when it has\naccess to baseline information, which is not needed at all to fine-tune TrOCR.\n","authors":["Phillip Benjamin Ströbel","Simon Clematide","Martin Volk","Tobias Hodel"],"pdf_url":"https://arxiv.org/pdf/2203.11008v1.pdf","comment":"This is an abstract submitted and accepted at ComHum 2022 in\n  Lausanne. We will be elaborating on these initial findings in the paper that\n  we will submit after the conference"},{"id":"http://arxiv.org/abs/2203.11007v1","updated":"2022-03-21T14:23:00Z","published":"2022-03-21T14:23:00Z","title":"Computational ergonomics for task delegation in Human-Robot\n  Collaboration: spatiotemporal adaptation of the robot to the human through\n  contactless gesture recognition","summary":"  The high prevalence of work-related musculoskeletal disorders (WMSDs) could\nbe addressed by optimizing Human-Robot Collaboration (HRC) frameworks for\nmanufacturing applications. In this context, this paper proposes two hypotheses\nfor ergonomically effective task delegation and HRC. The first hypothesis\nstates that it is possible to quantify ergonomically professional tasks using\nmotion data from a reduced set of sensors. Then, the most dangerous tasks can\nbe delegated to a collaborative robot. The second hypothesis is that by\nincluding gesture recognition and spatial adaptation, the ergonomics of an HRC\nscenario can be improved by avoiding needless motions that could expose\noperators to ergonomic risks and by lowering the physical effort required of\noperators. An HRC scenario for a television manufacturing process is optimized\nto test both hypotheses. For the ergonomic evaluation, motion primitives with\nknown ergonomic risks were modeled for their detection in professional tasks\nand to estimate a risk score based on the European Assembly Worksheet (EAWS). A\nDeep Learning gesture recognition module trained with egocentric television\nassembly data was used to complement the collaboration between the human\noperator and the robot. Additionally, a skeleton-tracking algorithm provided\nthe robot with information about the operator's pose, allowing it to spatially\nadapt its motion to the operator's anthropometrics. Three experiments were\nconducted to determine the effect of gesture recognition and spatial adaptation\non the operator's range of motion. The rate of spatial adaptation was used as a\nkey performance indicator (KPI), and a new KPI for measuring the reduction in\nthe operator's motion is presented in this paper.\n","authors":["Brenda Elizabeth Olivas-Padilla","Dimitris Papanagiotou","Gavriela Senteri","Sotiris Manitsaris","Alina Glushkova"],"pdf_url":"https://arxiv.org/pdf/2203.11007v1.pdf","comment":"Under review in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2203.11006v1","updated":"2022-03-21T14:22:05Z","published":"2022-03-21T14:22:05Z","title":"Underwater Light Field Retention : Neural Rendering for Underwater\n  Imaging","summary":"  Underwater Image Rendering aims to generate a true-to-life underwater image\nfrom a given clean one, which could be applied to various practical\napplications such as underwater image enhancement, camera filter, and virtual\ngaming. We explore two less-touched but challenging problems in underwater\nimage rendering, namely, i) how to render diverse underwater scenes by a single\nneural network? ii) how to adaptively learn the underwater light fields from\nnatural exemplars, \\textit{i,e.}, realistic underwater images? To this end, we\npropose a neural rendering method for underwater imaging, dubbed UWNR\n(Underwater Neural Rendering). Specifically, UWNR is a data-driven neural\nnetwork that implicitly learns the natural degenerated model from authentic\nunderwater images, avoiding introducing erroneous biases by hand-craft imaging\nmodels.\n  Compared with existing underwater image generation methods, UWNR utilizes the\nnatural light field to simulate the main characteristics of the underwater\nscene. Thus, it is able to synthesize a wide variety of underwater images from\none clean image with various realistic underwater images.\n  Extensive experiments demonstrate that our approach achieves better visual\neffects and quantitative metrics over previous methods. Moreover, we adopt UWNR\nto build an open Large Neural Rendering Underwater Dataset containing various\ntypes of water quality, dubbed LNRUD.\n","authors":["Tian Ye","Sixiang Chen","Yun Liu","Erkang Chen","Yi Ye","Yuche Li"],"pdf_url":"https://arxiv.org/pdf/2203.11006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11000v1","updated":"2022-03-21T14:14:26Z","published":"2022-03-21T14:14:26Z","title":"Self-Supervised Road Layout Parsing with Graph Auto-Encoding","summary":"  Aiming for higher-level scene understanding, this work presents a neural\nnetwork approach that takes a road-layout map in bird's eye view as input, and\npredicts a human-interpretable graph that represents the road's topological\nlayout. Our approach elevates the understanding of road layouts from pixel\nlevel to the level of graphs. To achieve this goal, an image-graph-image\nauto-encoder is utilized. The network is designed to learn to regress the graph\nrepresentation at its auto-encoder bottleneck. This learning is self-supervised\nby an image reconstruction loss, without needing any external manual\nannotations. We create a synthetic dataset containing common road layout\npatterns and use it for training of the auto-encoder in addition to the\nreal-world Argoverse dataset. By using this additional synthetic dataset, which\nconceptually captures human knowledge of road layouts and makes this available\nto the network for training, we are able to stabilize and further improve the\nperformance of topological road layout understanding on the real-world\nArgoverse dataset. The evaluation shows that our approach exhibits comparable\nperformance to a strong fully-supervised baseline.\n","authors":["Chenyang Lu","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2203.11000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03106v3","updated":"2022-03-21T14:13:26Z","published":"2022-03-07T02:48:16Z","title":"Differentially Private Federated Learning with Local Regularization and\n  Sparsification","summary":"  User-level differential privacy (DP) provides certifiable privacy guarantees\nto the information that is specific to any user's data in federated learning.\nExisting methods that ensure user-level DP come at the cost of severe accuracy\ndecrease. In this paper, we study the cause of model performance degradation in\nfederated learning under user-level DP guarantee. We find the key to solving\nthis issue is to naturally restrict the norm of local updates before executing\noperations that guarantee DP. To this end, we propose two techniques, Bounded\nLocal Update Regularization and Local Update Sparsification, to increase model\nquality without sacrificing privacy. We provide theoretical analysis on the\nconvergence of our framework and give rigorous privacy guarantees. Extensive\nexperiments show that our framework significantly improves the privacy-utility\ntrade-off over the state-of-the-arts for federated learning with user-level DP\nguarantee.\n","authors":["Anda Cheng","Peisong Wang","Xi Sheryl Zhang","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2203.03106v3.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.02557v2","updated":"2022-03-21T14:02:08Z","published":"2022-03-04T20:27:16Z","title":"UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired\n  image-to-image translation","summary":"  Image-to-image translation has broad applications in art, design, and\nscientific simulations. The original CycleGAN model emphasizes one-to-one\nmapping via a cycle-consistent loss, while more recent works promote\none-to-many mapping to boost the diversity of the translated images. With\nscientific simulation and one-to-one needs in mind, this work examines if\nequipping CycleGAN with a vision transformer (ViT) and employing advanced\ngenerative adversarial network (GAN) training techniques can achieve better\nperformance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is\ncompared with previous best-performing models on open benchmark image-to-image\ntranslation datasets, Selfie2Anime and CelebA. UVCGAN performs better and\nretains a strong correlation between the original and translated images. An\naccompanying ablation study shows that the gradient penalty and BERT-like\npre-training also contribute to the improvement.~To promote reproducibility and\nopen science, the source code, hyperparameter configurations, and pre-trained\nmodel will be made available at: https://github.com/LS4GAN/uvcgan.\n","authors":["Dmitrii Torbunov","Yi Huang","Haiwang Yu","Jin Huang","Shinjae Yoo","Meifeng Lin","Brett Viren","Yihui Ren"],"pdf_url":"https://arxiv.org/pdf/2203.02557v2.pdf","comment":"5 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2203.06667v3","updated":"2022-03-21T13:55:24Z","published":"2022-03-13T14:42:53Z","title":"Towards Visual-Prompt Temporal Answering Grounding in Medical\n  Instructional Video","summary":"  The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show that the proposed VPTSL\noutperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a\nlarge margin, which demonstrates the effectiveness of visual prompt and the\ntext span predictor.\n","authors":["Bin Li","Yixuan Weng","Bin Sun","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2203.06667v3.pdf","comment":"8 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2203.10981v1","updated":"2022-03-21T13:40:10Z","published":"2022-03-21T13:40:10Z","title":"MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer","summary":"  Monocular 3D object detection is an important yet challenging task in\nautonomous driving. Some existing methods leverage depth information from an\noff-the-shelf depth estimator to assist 3D detection, but suffer from the\nadditional computational burden and achieve limited performance caused by\ninaccurate depth priors. To alleviate this, we propose MonoDTR, a novel\nend-to-end depth-aware transformer network for monocular 3D object detection.\nIt mainly consists of two components: (1) the Depth-Aware Feature Enhancement\n(DFE) module that implicitly learns depth-aware features with auxiliary\nsupervision without requiring extra computation, and (2) the Depth-Aware\nTransformer (DTR) module that globally integrates context- and depth-aware\nfeatures. Moreover, different from conventional pixel-wise positional\nencodings, we introduce a novel depth positional encoding (DPE) to inject depth\npositional hints into transformers. Our proposed depth-aware modules can be\neasily plugged into existing image-only monocular 3D object detectors to\nimprove the performance. Extensive experiments on the KITTI dataset demonstrate\nthat our approach outperforms previous state-of-the-art monocular-based methods\nand achieves real-time detection. Code is available at\nhttps://github.com/kuanchihhuang/MonoDTR\n","authors":["Kuan-Chih Huang","Tsung-Han Wu","Hung-Ting Su","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2203.10981v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10977v1","updated":"2022-03-21T13:37:23Z","published":"2022-03-21T13:37:23Z","title":"Improving anatomical plausibility in medical image segmentation via\n  hybrid graph neural networks: applications to chest x-ray analysis","summary":"  Anatomical segmentation is a fundamental task in medical image computing,\ngenerally tackled with fully convolutional neural networks which produce dense\nsegmentation masks. These models are often trained with loss functions such as\ncross-entropy or Dice, which assume pixels to be independent of each other,\nthus ignoring topological errors and anatomical inconsistencies. We address\nthis limitation by moving from pixel-level to graph representations, which\nallow to naturally incorporate anatomical constraints by construction. To this\nend, we introduce HybridGNet, an encoder-decoder neural architecture that\nleverages standard convolutions for image feature encoding and graph\nconvolutional neural networks (GCNNs) to decode plausible representations of\nanatomical structures. We also propose a novel image-to-graph skip connection\nlayer which allows localized features to flow from standard convolutional\nblocks to GCNN blocks, and show that it improves segmentation accuracy. The\nproposed architecture is extensively evaluated in a variety of domain shift and\nimage occlusion scenarios, and audited considering different types of\ndemographic domain shift. Our comprehensive experimental setup compares\nHybridGNet with other landmark and pixel-based models for anatomical\nsegmentation in chest x-ray images, and shows that it produces anatomically\nplausible results in challenging scenarios where other models tend to fail.\n","authors":["Nicolás Gaggion","Lucas Mansilla","Candelaria Mosquera","Diego H. Milone","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2203.10977v1.pdf","comment":"Source code at https://github.com/ngaggion/HybridGNet"},{"id":"http://arxiv.org/abs/2203.10974v1","updated":"2022-03-21T13:35:16Z","published":"2022-03-21T13:35:16Z","title":"Towards Self-Supervised Gaze Estimation","summary":"  Recent joint embedding-based self-supervised methods have surpassed standard\nsupervised approaches on various image recognition tasks such as image\nclassification. These self-supervised methods aim at maximizing agreement\nbetween features extracted from two differently transformed views of the same\nimage, which results in learning an invariant representation with respect to\nappearance and geometric image transformations. However, the effectiveness of\nthese approaches remains unclear in the context of gaze estimation, a\nstructured regression task that requires equivariance under geometric\ntransformations (e.g., rotations, horizontal flip). In this work, we propose\nSwAT, an equivariant version of the online clustering-based self-supervised\napproach SwAV, to learn more informative representations for gaze estimation.\nWe identify the most effective image transformations for self-supervised\npretraining and demonstrate that SwAT, with ResNet-50 and supported with\nuncurated unlabeled face images, outperforms state-of-the-art gaze estimation\nmethods and supervised baselines in various experiments. In particular, we\nachieve up to 57% and 25% improvements in cross-dataset and within-dataset\nevaluation tasks on existing benchmarks (ETH-XGaze, Gaze360, and MPIIFaceGaze).\n","authors":["Arya Farkhondeh","Cristina Palmero","Simone Scardapane","Sergio Escalera"],"pdf_url":"https://arxiv.org/pdf/2203.10974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.10382v2","updated":"2022-03-21T13:03:42Z","published":"2021-01-25T20:08:32Z","title":"Curriculum Learning: A Survey","summary":"  Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n","authors":["Petru Soviany","Radu Tudor Ionescu","Paolo Rota","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2101.10382v2.pdf","comment":"Accepted at the International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2111.07910v2","updated":"2022-03-21T12:56:00Z","published":"2021-11-15T16:59:48Z","title":"Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image\n  Reconstruction","summary":"  Hyperspectral image (HSI) reconstruction aims to recover the 3D\nspatial-spectral signal from a 2D measurement in the coded aperture snapshot\nspectral imaging (CASSI) system. The HSI representations are highly similar and\ncorrelated across the spectral dimension. Modeling the inter-spectra\ninteractions is beneficial for HSI reconstruction. However, existing CNN-based\nmethods show limitations in capturing spectral-wise similarity and long-range\ndependencies. Besides, the HSI information is modulated by a coded aperture\n(physical mask) in CASSI. Nonetheless, current algorithms have not fully\nexplored the guidance effect of the mask for HSI restoration. In this paper, we\npropose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI\nreconstruction. Specifically, we present a Spectral-wise Multi-head\nSelf-Attention (S-MSA) that treats each spectral feature as a token and\ncalculates self-attention along the spectral dimension. In addition, we\ncustomize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to\nspatial regions with high-fidelity spectral representations. Extensive\nexperiments show that our MST significantly outperforms state-of-the-art (SOTA)\nmethods on simulation and real HSI datasets while requiring dramatically\ncheaper computational and memory costs. Code and pre-trained models are\navailable at https://github.com/caiyuanhao1998/MST/\n","authors":["Yuanhao Cai","Jing Lin","Xiaowan Hu","Haoqian Wang","Xin Yuan","Yulun Zhang","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2111.07910v2.pdf","comment":"CVPR 2022; The first Transformer-based method for snapshot\n  compressive imaging"},{"id":"http://arxiv.org/abs/2203.10926v1","updated":"2022-03-21T12:44:17Z","published":"2022-03-21T12:44:17Z","title":"3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge\n  Modality Attention","summary":"  Online 3D multi-object tracking (MOT) has witnessed significant research\ninterest in recent years, largely driven by demand from the autonomous systems\ncommunity. However, 3D offline MOT is relatively less explored. Labeling 3D\ntrajectory scene data at a large scale while not relying on high-cost human\nexperts is still an open research question. In this work, we propose Batch3DMOT\nthat follows the tracking-by-detection paradigm and represents real-world\nscenes as directed, acyclic, and category-disjoint tracking graphs that are\nattributed using various modalities such as camera, LiDAR, and radar. We\npresent a multi-modal graph neural network that uses a cross-edge attention\nmechanism mitigating modality intermittence, which translates into sparsity in\nthe graph domain. Additionally, we present attention-weighted convolutions over\nframe-wise k-NN neighborhoods as suitable means to allow information exchange\nacross disconnected graph components. We evaluate our approach using various\nsensor modalities and model configurations on the challenging nuScenes and\nKITTI datasets. Extensive experiments demonstrate that our proposed approach\nyields an overall improvement of 2.8% in the AMOTA score on nuScenes thereby\nsetting a new benchmark for 3D tracking methods and successfully enhances false\npositive filtering.\n","authors":["Martin Buchner","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2203.10926v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.10925v1","updated":"2022-03-21T12:43:42Z","published":"2022-03-21T12:43:42Z","title":"Learning Occlusion-Aware Coarse-to-Fine Depth Map for Self-supervised\n  Monocular Depth Estimation","summary":"  Self-supervised monocular depth estimation, aiming to learn scene depths from\nsingle images in a self-supervised manner, has received much attention\nrecently. In spite of recent efforts in this field, how to learn accurate scene\ndepths and alleviate the negative influence of occlusions for self-supervised\ndepth estimation, still remains an open problem. Addressing this problem, we\nfirstly empirically analyze the effects of both the continuous and discrete\ndepth constraints which are widely used in the training process of many\nexisting works. Then inspired by the above empirical analysis, we propose a\nnovel network to learn an Occlusion-aware Coarse-to-Fine Depth map for\nself-supervised monocular depth estimation, called OCFD-Net. Given an arbitrary\ntraining set of stereo image pairs, the proposed OCFD-Net does not only employ\na discrete depth constraint for learning a coarse-level depth map, but also\nemploy a continuous depth constraint for learning a scene depth residual,\nresulting in a fine-level depth map. In addition, an occlusion-aware module is\ndesigned under the proposed OCFD-Net, which is able to improve the capability\nof the learnt fine-level depth map for handling occlusions. Extensive\nexperimental results on the public KITTI and Make3D datasets demonstrate that\nthe proposed method outperforms 20 existing state-of-the-art methods in most\ncases.\n","authors":["Zhengming Zhou","Qiulei Dong"],"pdf_url":"https://arxiv.org/pdf/2203.10925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10912v1","updated":"2022-03-21T12:06:27Z","published":"2022-03-21T12:06:27Z","title":"Depth Completion using Geometry-Aware Embedding","summary":"  Exploiting internal spatial geometric constraints of sparse LiDARs is\nbeneficial to depth completion, however, has been not explored well. This paper\nproposes an efficient method to learn geometry-aware embedding, which encodes\nthe local and global geometric structure information from 3D points, e.g.,\nscene layout, object's sizes and shapes, to guide dense depth estimation.\nSpecifically, we utilize the dynamic graph representation to model generalized\ngeometric relationship from irregular point clouds in a flexible and efficient\nmanner. Further, we joint this embedding and corresponded RGB appearance\ninformation to infer missing depths of the scene with well structure-preserved\ndetails. The key to our method is to integrate implicit 3D geometric\nrepresentation into a 2D learning architecture, which leads to a better\ntrade-off between the performance and efficiency. Extensive experiments\ndemonstrate that the proposed method outperforms previous works and could\nreconstruct fine depths with crisp boundaries in regions that are over-smoothed\nby them. The ablation study gives more insights into our method that could\nachieve significant gains with a simple design, while having better\ngeneralization capability and stability. The code is available at\nhttps://github.com/Wenchao-Du/GAENet.\n","authors":["Wenchao Du","Hu Chen","Hongyu Yang","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10912v1.pdf","comment":"Acceptted by ICRA22"},{"id":"http://arxiv.org/abs/2203.02533v2","updated":"2022-03-21T11:58:20Z","published":"2022-03-04T19:19:41Z","title":"BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive\n  Pseudo Labeling and Informative Active Annotation","summary":"  In this paper, we propose a novel semi-supervised learning (SSL) framework\nnamed BoostMIS that combines adaptive pseudo labeling and informative active\nannotation to unleash the potential of medical image SSL models: (1) BoostMIS\ncan adaptively leverage the cluster assumption and consistency regularization\nof the unlabeled data according to the current learning status. This strategy\ncan adaptively generate one-hot \"hard\" labels converted from task model\npredictions for better task model training. (2) For the unselected unlabeled\nimages with low confidence, we introduce an Active learning (AL) algorithm to\nfind the informative samples as the annotation candidates by exploiting virtual\nadversarial perturbation and model's density-aware entropy. These informative\ncandidates are subsequently fed into the next training cycle for better SSL\nlabel propagation. Notably, the adaptive pseudo-labeling and informative active\nannotation form a learning closed-loop that are mutually collaborative to boost\nmedical image SSL. To verify the effectiveness of the proposed method, we\ncollected a metastatic epidural spinal cord compression (MESCC) dataset that\naims to optimize MESCC diagnosis and classification for improved specialist\nreferral and treatment. We conducted an extensive experimental study of\nBoostMIS on MESCC and another public dataset COVIDx. The experimental results\nverify our framework's effectiveness and generalisability for different medical\nimage datasets with a significant improvement over various state-of-the-art\nmethods.\n","authors":["Wenqiao Zhang","Lei Zhu","James Hallinan","Andrew Makmur","Shengyu Zhang","Qingpeng Cai","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2203.02533v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2102.00264v2","updated":"2022-03-21T11:53:29Z","published":"2021-01-30T16:35:25Z","title":"Atlas Generative Models and Geodesic Interpolation","summary":"  Generative neural networks have a well recognized ability to estimate\nunderlying manifold structure of high dimensional data. However, if a single\nlatent space is used, it is not possible to faithfully represent a manifold\nwith topology different from Euclidean space. In this work we define the\ngeneral class of Atlas Generative Models (AGMs), models with hybrid\ndiscrete-continuous latent space that estimate an atlas on the underlying data\nmanifold together with a partition of unity on the data space. We identify\nexisting examples of models from various popular generative paradigms that fit\ninto this class. Due to the atlas interpretation, ideas from non-linear latent\nspace analysis and statistics, e.g. geodesic interpolation, which has\npreviously only been investigated for models with simply connected latent\nspaces, may be extended to the entire class of AGMs in a natural way. We\nexemplify this by generalizing an algorithm for graph based geodesic\ninterpolation to the setting of AGMs, and verify its performance\nexperimentally.\n","authors":["Jakob Stolberg-Larsen","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2102.00264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10897v1","updated":"2022-03-21T11:44:17Z","published":"2022-03-21T11:44:17Z","title":"Unified Multivariate Gaussian Mixture for Efficient Neural Image\n  Compression","summary":"  Modeling latent variables with priors and hyperpriors is an essential problem\nin variational image compression. Formally, trade-off between rate and\ndistortion is handled well if priors and hyperpriors precisely describe latent\nvariables. Current practices only adopt univariate priors and process each\nvariable individually. However, we find inter-correlations and\nintra-correlations exist when observing latent variables in a vectorized\nperspective. These findings reveal visual redundancies to improve\nrate-distortion performance and parallel processing ability to speed up\ncompression. This encourages us to propose a novel vectorized prior.\nSpecifically, a multivariate Gaussian mixture is proposed with means and\ncovariances to be estimated. Then, a novel probabilistic vector quantization is\nutilized to effectively approximate means, and remaining covariances are\nfurther induced to a unified mixture and solved by cascaded estimation without\ncontext models involved. Furthermore, codebooks involved in quantization are\nextended to multi-codebooks for complexity reduction, which formulates an\nefficient compression procedure. Extensive experiments on benchmark datasets\nagainst state-of-the-art indicate our model has better rate-distortion\nperformance and an impressive $3.18\\times$ compression speed up, giving us the\nability to perform real-time, high-quality variational image compression in\npractice. Our source code is publicly available at\n\\url{https://github.com/xiaosu-zhu/McQuic}.\n","authors":["Xiaosu Zhu","Jingkuan Song","Lianli Gao","Feng Zheng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2203.10897v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2201.04019v2","updated":"2022-03-21T11:38:15Z","published":"2022-01-11T16:09:25Z","title":"Pyramid Fusion Transformer for Semantic Segmentation","summary":"  The recently proposed MaskFormer gives a refreshed perspective on the task of\nsemantic segmentation: it shifts from the popular pixel-level classification\nparadigm to a mask-level classification method. In essence, it generates paired\nprobabilities and masks corresponding to category segments and combines them\nduring inference for the segmentation maps. In our study, we find that per-mask\nclassification decoder on top of a single-scale feature is not effective enough\nto extract reliable probability or mask. To mine for rich semantic information\nacross the feature pyramid, we propose a transformer-based Pyramid Fusion\nTransformer (PFT) for per-mask approach semantic segmentation with multi-scale\nfeatures. The proposed transformer decoder performs cross-attention between the\nlearnable queries and each spatial feature from the feature pyramid in parallel\nand uses cross-scale inter-query attention to exchange complimentary\ninformation. We achieve competitive performance on three widely used semantic\nsegmentation datasets. In particular, on ADE20K validation set, our result with\nSwin-B backbone surpasses that of MaskFormer's with a much larger Swin-L\nbackbone in both single-scale and multi-scale inference, achieving 54.1 mIoU\nand 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale\n56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on\nthe dataset. Extensive experiments on three widely used semantic segmentation\ndatasets verify the effectiveness of our proposed method.\n","authors":["Zipeng Qin","Jianbo Liu","Xiaolin Zhang","Maoqing Tian","Aojun Zhou","Shuai Yi","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2201.04019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10887v1","updated":"2022-03-21T11:21:41Z","published":"2022-03-21T11:21:41Z","title":"Revisiting Domain Generalized Stereo Matching Networks from a Feature\n  Consistency Perspective","summary":"  Despite recent stereo matching networks achieving impressive performance\ngiven sufficient training data, they suffer from domain shifts and generalize\npoorly to unseen domains. We argue that maintaining feature consistency between\nmatching pixels is a vital factor for promoting the generalization capability\nof stereo matching networks, which has not been adequately considered. Here we\naddress this issue by proposing a simple pixel-wise contrastive learning across\nthe viewpoints. The stereo contrastive feature loss function explicitly\nconstrains the consistency between learned features of matching pixel pairs\nwhich are observations of the same 3D points. A stereo selective whitening loss\nis further introduced to better preserve the stereo feature consistency across\ndomains, which decorrelates stereo features from stereo viewpoint-specific\nstyle information. Counter-intuitively, the generalization of feature\nconsistency between two viewpoints in the same scene translates to the\ngeneralization of stereo matching performance to unseen domains. Our method is\ngeneric in nature as it can be easily embedded into existing stereo networks\nand does not require access to the samples in the target domain. When trained\non synthetic data and generalized to four real-world testing sets, our method\nachieves superior performance over several state-of-the-art networks.\n","authors":["Jiawei Zhang","Xiang Wang","Xiao Bai","Chen Wang","Lei Huang","Yimin Chen","Lin Gu","Jun Zhou","Tatsuya Harada","Edwin R. Hancock"],"pdf_url":"https://arxiv.org/pdf/2203.10887v1.pdf","comment":"Accepted to CVPR2022"},{"id":"http://arxiv.org/abs/2203.10886v1","updated":"2022-03-21T11:19:50Z","published":"2022-03-21T11:19:50Z","title":"ELIC: Efficient Learned Image Compression with Unevenly Grouped\n  Space-Channel Contextual Adaptive Coding","summary":"  Recently, learned image compression techniques have achieved remarkable\nperformance, even surpassing the best manually designed lossy image coders.\nThey are promising to be large-scale adopted. For the sake of practicality, a\nthorough investigation of the architecture design of learned image compression,\nregarding both compression performance and running speed, is essential. In this\npaper, we first propose uneven channel-conditional adaptive coding, motivated\nby the observation of energy compaction in learned image compression. Combining\nthe proposed uneven grouping model with existing context models, we obtain a\nspatial-channel contextual adaptive model to improve the coding performance\nwithout damage to running speed. Then we study the structure of the main\ntransform and propose an efficient model, ELIC, to achieve state-of-the-art\nspeed and compression ability. With superior performance, the proposed model\nalso supports extremely fast preview decoding and progressive decoding, which\nmakes the coming application of learning-based image compression more\npromising.\n","authors":["Dailan He","Ziming Yang","Weikun Peng","Rui Ma","Hongwei Qin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10886v1.pdf","comment":"accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10882v1","updated":"2022-03-21T11:08:06Z","published":"2022-03-21T11:08:06Z","title":"Efficient Remote Photoplethysmography with Temporal Derivative Modules\n  and Time-Shift Invariant Loss","summary":"  We present a lightweight neural model for remote heart rate estimation\nfocused on the efficient spatio-temporal learning of facial\nphotoplethysmography (PPG) based on i) modelling of PPG dynamics by\ncombinations of multiple convolutional derivatives, and ii) increased\nflexibility of the model to learn possible offsets between the video facial PPG\nand the ground truth. PPG dynamics are modelled by a Temporal Derivative Module\n(TDM) constructed by the incremental aggregation of multiple convolutional\nderivatives, emulating a Taylor series expansion up to the desired order.\nRobustness to ground truth offsets is handled by the introduction of TALOS\n(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based\nmodels. We verify the effectiveness of our model by reporting accuracy and\nefficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to\nexisting models, our approach shows competitive heart rate estimation accuracy\nwith a much lower number of parameters and lower computational cost.\n","authors":["Joaquim Comas","Adria Ruiz","Federico Sukno"],"pdf_url":"https://arxiv.org/pdf/2203.10882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10856v1","updated":"2022-03-21T10:26:38Z","published":"2022-03-21T10:26:38Z","title":"RGB-Depth Fusion GAN for Indoor Depth Completion","summary":"  The raw depth image captured by the indoor depth sensor usually has an\nextensive range of missing depth values due to inherent limitations such as the\ninability to perceive transparent objects and limited distance range. The\nincomplete depth map burdens many downstream vision tasks, and a rising number\nof depth completion methods have been proposed to alleviate this issue. While\nmost existing methods can generate accurate dense depth maps from sparse and\nuniformly sampled depth maps, they are not suitable for complementing the large\ncontiguous regions of missing depth values, which is common and critical. In\nthis paper, we design a novel two-branch end-to-end fusion network, which takes\na pair of RGB and incomplete depth images as input to predict a dense and\ncompleted depth map. The first branch employs an encoder-decoder structure to\nregress the local dense depth values from the raw depth map, with the help of\nlocal guidance information extracted from the RGB image. In the other branch,\nwe propose an RGB-depth fusion GAN to transfer the RGB image to the\nfine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN\nto propagate the features across the two branches, and we append a confidence\nfusion head to fuse the two outputs of the branches for the final depth map.\nExtensive experiments on NYU-Depth V2 and SUN RGB-D demonstrate that our\nproposed method clearly improves the depth completion performance, especially\nin a more realistic setting of indoor environments with the help of the pseudo\ndepth map.\n","authors":["Haowen Wang","Mingyuan Wang","Zhengping Che","Zhiyuan Xu","Xiuquan Qiao","Mengshi Qi","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2203.10856v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10853v1","updated":"2022-03-21T10:20:21Z","published":"2022-03-21T10:20:21Z","title":"Boost Test-Time Performance with Closed-Loop Inference","summary":"  Conventional deep models predict a test sample with a single forward\npropagation, which, however, may not be sufficient for predicting\nhard-classified samples. On the contrary, we human beings may need to carefully\ncheck the sample many times before making a final decision. During the recheck\nprocess, one may refine/adjust the prediction by referring to related samples.\nMotivated by this, we propose to predict those hard-classified test samples in\na looped manner to boost the model performance. However, this idea may pose a\ncritical challenge: how to construct looped inference, so that the original\nerroneous predictions on these hard test samples can be corrected with little\nadditional effort. To address this, we propose a general Closed-Loop Inference\n(CLI) method. Specifically, we first devise a filtering criterion to identify\nthose hard-classified test samples that need additional inference loops. For\neach hard sample, we construct an additional auxiliary learning task based on\nits original top-$K$ predictions to calibrate the model, and then use the\ncalibrated model to obtain the final prediction. Promising results on ImageNet\n(in-distribution test samples) and ImageNet-C (out-of-distribution test\nsamples) demonstrate the effectiveness of CLI in improving the performance of\nany pre-trained model.\n","authors":["Shuaicheng Niu","Jiaxiang Wu","Yifan Zhang","Guanghui Xu","Haokun Li","Junzhou Huang","Yaowei Wang","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2203.10853v1.pdf","comment":"10 pages, 10 figures, conference"},{"id":"http://arxiv.org/abs/2203.10852v1","updated":"2022-03-21T10:20:04Z","published":"2022-03-21T10:20:04Z","title":"Multi-modal learning for predicting the genotype of glioma","summary":"  The isocitrate dehydrogenase (IDH) gene mutation is an essential biomarker\nfor the diagnosis and prognosis of glioma. It is promising to better predict\nglioma genotype by integrating focal tumor image and geometric features with\nbrain network features derived from MRI. Convolutions neural networks show\nreasonable performance in predicting IDH mutation, which, however, cannot learn\nfrom non-Euclidean data, e.g., geometric and network data. In this study, we\npropose a multi-modal learning framework using three separate encoders to\nextract features of focal tumor image, tumor geometrics and global brain\nnetworks. To mitigate the limited availability of diffusion MRI, we develop a\nself-supervised approach to generate brain networks from anatomical\nmulti-sequence MRI. Moreover, to extract tumor-related features from the brain\nnetwork, we design a hierarchical attention module for the brain network\nencoder. Further, we design a bi-level multi-modal contrastive loss to align\nthe multi-modal features and tackle the domain gap at the focal tumor and\nglobal brain. Finally, we propose a weighted population graph to integrate the\nmulti-modal features for genotype prediction. Experimental results on the\ntesting set show that the proposed model outperforms the baseline deep learning\nmodels. The ablation experiments validate the performance of different\ncomponents of the framework. The visualized interpretation corresponds to\nclinical knowledge with further validation. In conclusion, the proposed\nlearning framework provides a novel approach for predicting the genotype of\nglioma.\n","authors":["Yiran Wei","Xi Chen","Lei Zhu","Lipei Zhang","Carola-Bibiane Schönlieb","Stephen J. Price","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2203.10852v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2111.13539v2","updated":"2022-03-21T10:14:15Z","published":"2021-11-26T15:15:37Z","title":"GeoNeRF: Generalizing NeRF with Geometry Priors","summary":"  We present GeoNeRF, a generalizable photorealistic novel view synthesis\nmethod based on neural radiance fields. Our approach consists of two main\nstages: a geometry reasoner and a renderer. To render a novel view, the\ngeometry reasoner first constructs cascaded cost volumes for each nearby source\nview. Then, using a Transformer-based attention mechanism and the cascaded cost\nvolumes, the renderer infers geometry and appearance, and renders detailed\nimages via classical volume rendering techniques. This architecture, in\nparticular, allows sophisticated occlusion reasoning, gathering information\nfrom consistent source views. Moreover, our method can easily be fine-tuned on\na single scene, and renders competitive results with per-scene optimized neural\nrendering methods with a fraction of computational cost. Experiments show that\nGeoNeRF outperforms state-of-the-art generalizable neural rendering models on\nvarious synthetic and real datasets. Lastly, with a slight modification to the\ngeometry reasoner, we also propose an alternative model that adapts to RGBD\nimages. This model directly exploits the depth information often available\nthanks to depth sensors. The implementation code is available at\nhttps://www.idiap.ch/paper/geonerf.\n","authors":["Mohammad Mahdi Johari","Yann Lepoittevin","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2111.13539v2.pdf","comment":"CVPR2022"},{"id":"http://arxiv.org/abs/2203.10833v1","updated":"2022-03-21T09:48:23Z","published":"2022-03-21T09:48:23Z","title":"Hyperbolic Vision Transformers: Combining Improvements in Metric\n  Learning","summary":"  Metric learning aims to learn a highly discriminative model encouraging the\nembeddings of similar classes to be close in the chosen metrics and pushed\napart for dissimilar ones. The common recipe is to use an encoder to extract\nembeddings and a distance-based loss function to match the representations --\nusually, the Euclidean distance is utilized. An emerging interest in learning\nhyperbolic data embeddings suggests that hyperbolic geometry can be beneficial\nfor natural data. Following this line of work, we propose a new\nhyperbolic-based model for metric learning. At the core of our method is a\nvision transformer with output embeddings mapped to hyperbolic space. These\nembeddings are directly optimized using modified pairwise cross-entropy loss.\nWe evaluate the proposed model with six different formulations on four datasets\nachieving the new state-of-the-art performance. The source code is available at\nhttps://github.com/htdt/hyp_metric.\n","authors":["Aleksandr Ermolov","Leyla Mirvakhabova","Valentin Khrulkov","Nicu Sebe","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2203.10833v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10821v1","updated":"2022-03-21T09:15:58Z","published":"2022-03-21T09:15:58Z","title":"Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance\n  Fields","summary":"  Image translation and manipulation have gain increasing attention along with\nthe rapid development of deep generative models. Although existing approaches\nhave brought impressive results, they mainly operated in 2D space. In light of\nrecent advances in NeRF-based 3D-aware generative models, we introduce a new\ntask, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene\nmodelled by NeRF, conditioned on one single-view semantic mask as input. To\nkick-off this novel task, we propose the Sem2NeRF framework. In particular,\nSem2NeRF addresses the highly challenging task by encoding the semantic mask\ninto the latent code that controls the 3D scene representation of a pretrained\ndecoder. To further improve the accuracy of the mapping, we integrate a new\nregion-aware learning strategy into the design of both the encoder and the\ndecoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that\nit outperforms several strong baselines on two benchmark datasets.\n","authors":["Yuedong Chen","Qianyi Wu","Chuanxia Zheng","Tat-Jen Cham","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2203.10821v1.pdf","comment":"Project page: https://donydchen.github.io/sem2nerf"},{"id":"http://arxiv.org/abs/2203.10812v1","updated":"2022-03-21T09:06:26Z","published":"2022-03-21T09:06:26Z","title":"ARM: Any-Time Super-Resolution Method","summary":"  This paper proposes an Any-time super-Resolution Method (ARM) to tackle the\nover-parameterized single image super-resolution (SISR) models. Our ARM is\nmotivated by three observations: (1) The performance of different image patches\nvaries with SISR networks of different sizes. (2) There is a tradeoff between\ncomputation overhead and performance of the reconstructed image. (3) Given an\ninput image, its edge information can be an effective option to estimate its\nPSNR. Subsequently, we train an ARM supernet containing SISR subnets of\ndifferent sizes to deal with image patches of various complexity. To that\neffect, we construct an Edge-to-PSNR lookup table that maps the edge score of\nan image patch to the PSNR performance for each subnet, together with a set of\ncomputation costs for the subnets. In the inference, the image patches are\nindividually distributed to different subnets for a better\ncomputation-performance tradeoff. Moreover, each SISR subnet shares weights of\nthe ARM supernet, thus no extra parameters are introduced. The setting of\nmultiple subnets can well adapt the computational cost of SISR model to the\ndynamically available hardware resources, allowing the SISR task to be in\nservice at any time. Extensive experiments on resolution datasets of different\nsizes with popular SISR networks as backbones verify the effectiveness and the\nversatility of our ARM. The source code is available at\n\\url{https://github.com/chenbong/ARM-Net}.\n","authors":["Bohong Chen","Mingbao Lin","Kekai Sheng","Mengdan Zhang","Peixian Chen","Ke Li","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2203.10812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01882v3","updated":"2022-03-21T09:02:21Z","published":"2022-03-03T17:49:40Z","title":"DenseUNets with feedback non-local attention for the segmentation of\n  specular microscopy images of the corneal endothelium with guttae","summary":"  To estimate the corneal endothelial parameters from specular microscopy\nimages depicting cornea guttata (Fuchs dystrophy), we propose a new deep\nlearning methodology that includes a novel attention mechanism named feedback\nnon-local attention (fNLA). Our approach first infers the cell edges, then\nselects the cells that are well detected, and finally applies a postprocessing\nmethod to correct mistakes and provide the binary segmentation from which the\ncorneal parameters are estimated (cell density [ECD], coefficient of variation\n[CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired\nwith a Topcon SP-1P microscope, 500 of which contained guttae. Manual\nsegmentation was performed in all images. We compared the results of different\nnetworks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with\nfNLA provided the best performance, with a mean absolute error of 23.16\n[cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6\ntimes smaller than the error obtained by Topcon's built-in software. Our\napproach handled the cells affected by guttae remarkably well, detecting cell\nedges occluded by small guttae while discarding areas covered by large guttae.\nOverall, the proposed method obtained accurate estimations in extremely\nchallenging specular images.\n","authors":["Juan P. Vigueras-Guillén","Jeroen van Rooij","Bart T. H. van Dooren","Hans G. Lemij","Esma Islamaj","Lucas J. van Vliet","Koenraad A. Vermeer"],"pdf_url":"https://arxiv.org/pdf/2203.01882v3.pdf","comment":"9 pages, 7 figures, 2 tables. Code:\n  https://github.com/jpviguerasguillen/feedback-non-local-attention-fNLA"},{"id":"http://arxiv.org/abs/2203.10808v1","updated":"2022-03-21T09:01:37Z","published":"2022-03-21T09:01:37Z","title":"AnoViT: Unsupervised Anomaly Detection and Localization with Vision\n  Transformer-based Encoder-Decoder","summary":"  Image anomaly detection problems aim to determine whether an image is\nabnormal, and to detect anomalous areas. These methods are actively used in\nvarious fields such as manufacturing, medical care, and intelligent\ninformation. Encoder-decoder structures have been widely used in the field of\nanomaly detection because they can easily learn normal patterns in an\nunsupervised learning environment and calculate a score to identify\nabnormalities through a reconstruction error indicating the difference between\ninput and reconstructed images. Therefore, current image anomaly detection\nmethods have commonly used convolutional encoder-decoders to extract normal\ninformation through the local features of images. However, they are limited in\nthat only local features of the image can be utilized when constructing a\nnormal representation owing to the characteristics of convolution operations\nusing a filter of fixed size. Therefore, we propose a vision transformer-based\nencoder-decoder model, named AnoViT, designed to reflect normal information by\nadditionally learning the global relationship between image patches, which is\ncapable of both image anomaly detection and localization. The proposed approach\nconstructs a feature map that maintains the existing location information of\nindividual patches by using the embeddings of all patches passed through\nmultiple self-attention layers. The proposed AnoViT model performed better than\nthe convolution-based model on three benchmark datasets. In MVTecAD, which is a\nrepresentative benchmark dataset for anomaly localization, it showed improved\nresults on 10 out of 15 classes compared with the baseline. Furthermore, the\nproposed method showed good performance regardless of the class and type of the\nanomalous area when localization results were evaluated qualitatively.\n","authors":["Yunseung Lee","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2203.10808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10807v1","updated":"2022-03-21T08:56:55Z","published":"2022-03-21T08:56:55Z","title":"ViM: Out-Of-Distribution with Virtual-logit Matching","summary":"  Most of the existing Out-Of-Distribution (OOD) detection algorithms depend on\nsingle input source: the feature, the logit, or the softmax probability.\nHowever, the immense diversity of the OOD examples makes such methods fragile.\nThere are OOD samples that are easy to identify in the feature space while hard\nto distinguish in the logit space and vice versa. Motivated by this\nobservation, we propose a novel OOD scoring method named Virtual-logit Matching\n(ViM), which combines the class-agnostic score from feature space and the\nIn-Distribution (ID) class-dependent logits. Specifically, an additional logit\nrepresenting the virtual OOD class is generated from the residual of the\nfeature against the principal space, and then matched with the original logits\nby a constant scaling. The probability of this virtual logit after softmax is\nthe indicator of OOD-ness. To facilitate the evaluation of large-scale OOD\ndetection in academia, we create a new OOD dataset for ImageNet-1K, which is\nhuman-annotated and is 8.8x the size of existing datasets. We conducted\nextensive experiments, including CNNs and vision transformers, to demonstrate\nthe effectiveness of the proposed ViM score. In particular, using the BiT-S\nmodel, our method gets an average AUROC 90.91% on four difficult OOD\nbenchmarks, which is 4% ahead of the best baseline. Code and dataset are\navailable at https://github.com/haoqiwang/vim.\n","authors":["Haoqi Wang","Zhizhong Li","Litong Feng","Wayne Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10807v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10804v1","updated":"2022-03-21T08:52:57Z","published":"2022-03-21T08:52:57Z","title":"Longitudinal Self-Supervision for COVID-19 Pathology Quantification","summary":"  Quantifying COVID-19 infection over time is an important task to manage the\nhospitalization of patients during a global pandemic. Recently, deep\nlearning-based approaches have been proposed to help radiologists automatically\nquantify COVID-19 pathologies on longitudinal CT scans. However, the learning\nprocess of deep learning methods demands extensive training data to learn the\ncomplex characteristics of infected regions over longitudinal scans. It is\nchallenging to collect a large-scale dataset, especially for longitudinal\ntraining. In this study, we want to address this problem by proposing a new\nself-supervised learning method to effectively train longitudinal networks for\nthe quantification of COVID-19 infections. For this purpose, longitudinal\nself-supervision schemes are explored on clinical longitudinal COVID-19 CT\nscans. Experimental results show that the proposed method is effective, helping\nthe model better exploit the semantics of longitudinal data and improve two\nCOVID-19 quantification tasks.\n","authors":["Tobias Czempiel","Coco Rogers","Matthias Keicher","Magdalini Paschali","Rickmer Braren","Egon Burian","Marcus Makowski","Nassir Navab","Thomas Wendler","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2203.10804v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2112.01518v2","updated":"2022-03-21T08:27:06Z","published":"2021-12-02T18:59:32Z","title":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting","summary":"  Recent progress has shown that large-scale pre-training using contrastive\nimage-text pairs can be a promising alternative for high-quality visual\nrepresentation learning from natural language supervision. Benefiting from a\nbroader source of supervision, this new paradigm exhibits impressive\ntransferability to downstream classification tasks and datasets. However, the\nproblem of transferring the knowledge learned from image-text pairs to more\ncomplex dense prediction tasks has barely been visited. In this work, we\npresent a new framework for dense prediction by implicitly and explicitly\nleveraging the pre-trained knowledge from CLIP. Specifically, we convert the\noriginal image-text matching problem in CLIP to a pixel-text matching problem\nand use the pixel-text score maps to guide the learning of dense prediction\nmodels. By further using the contextual information from the image to prompt\nthe language model, we are able to facilitate our model to better exploit the\npre-trained knowledge. Our method is model-agnostic, which can be applied to\narbitrary dense prediction systems and various pre-trained visual backbones\nincluding both CLIP models and ImageNet pre-trained models. Extensive\nexperiments demonstrate the superior performance of our methods on semantic\nsegmentation, object detection, and instance segmentation tasks. Code is\navailable at https://github.com/raoyongming/DenseCLIP\n","authors":["Yongming Rao","Wenliang Zhao","Guangyi Chen","Yansong Tang","Zheng Zhu","Guan Huang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2112.01518v2.pdf","comment":"Accepted to CVPR2022. Project page:\n  https://denseclip.ivg-research.xyz"},{"id":"http://arxiv.org/abs/2109.13748v2","updated":"2022-03-21T08:17:26Z","published":"2021-09-28T14:07:24Z","title":"Improving Autoencoder Training Performance for Hyperspectral Unmixing\n  with Network Reinitialisation","summary":"  Neural networks, in particular autoencoders, are one of the most promising\nsolutions for unmixing hyperspectral data, i.e. reconstructing the spectra of\nobserved substances (endmembers) and their relative mixing fractions\n(abundances), which is needed for effective hyperspectral analysis and\nclassification. However, as we show in this paper, the training of autoencoders\nfor unmixing is highly dependent on weights initialisation; some sets of\nweights lead to degenerate or low-performance solutions, introducing negative\nbias in the expected performance. In this work, we experimentally investigate\nautoencoders stability as well as network reinitialisation methods based on\ncoefficients of neurons' dead activations. We demonstrate that the proposed\ntechniques have a positive effect on autoencoder training in terms of\nreconstruction, abundances and endmembers errors.\n","authors":["Kamil Książek","Przemysław Głomb","Michał Romaszewski","Michał Cholewa","Bartosz Grabowski","Krisztián Búza"],"pdf_url":"https://arxiv.org/pdf/2109.13748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10790v1","updated":"2022-03-21T08:08:15Z","published":"2022-03-21T08:08:15Z","title":"ScalableViT: Rethinking the Context-oriented Generalization of Vision\n  Transformer","summary":"  The vanilla self-attention mechanism inherently relies on pre-defined and\nsteadfast computational dimensions. Such inflexibility restricts it from\npossessing context-oriented generalization that can bring more contextual cues\nand global representations. To mitigate this issue, we propose a Scalable\nSelf-Attention (SSA) mechanism that leverages two scaling factors to release\ndimensions of query, key, and value matrix while unbinding them with the input.\nThis scalability fetches context-oriented generalization and enhances object\nsensitivity, which pushes the whole network into a more effective trade-off\nstate between accuracy and cost. Furthermore, we propose an Interactive\nWindow-based Self-Attention (IWSA), which establishes interaction between\nnon-overlapping regions by re-merging independent value tokens and aggregating\nspatial information from adjacent windows. By stacking the SSA and IWSA\nalternately, the Scalable Vision Transformer (ScalableViT) achieves\nstate-of-the-art performance in general-purpose vision tasks. For example,\nScalableViT-S outperforms Twins-SVT-S by 1.4% and Swin-T by 1.8% on ImageNet-1K\nclassification.\n","authors":["Rui Yang","Hailong Ma","Jie Wu","Yansong Tang","Xuefeng Xiao","Min Zheng","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2203.10790v1.pdf","comment":"The code will be released"},{"id":"http://arxiv.org/abs/2203.10789v1","updated":"2022-03-21T08:07:46Z","published":"2022-03-21T08:07:46Z","title":"Domain Generalization by Mutual-Information Regularization with\n  Pre-trained Models","summary":"  Domain generalization (DG) aims to learn a generalized model to an unseen\ntarget domain using only limited source domains. Previous attempts to DG fail\nto learn domain-invariant representations only from the source domains due to\nthe significant domain shifts between training and test domains. Instead, we\nre-formulate the DG objective using mutual information with the oracle model, a\nmodel generalized to any possible domain. We derive a tractable variational\nlower bound via approximating the oracle model by a pre-trained model, called\nMutual Information Regularization with Oracle (MIRO). Our extensive experiments\nshow that MIRO significantly improves the out-of-distribution performance.\nFurthermore, our scaling experiments show that the larger the scale of the\npre-trained model, the greater the performance improvement of MIRO. Source code\nis available at https://github.com/kakaobrain/miro.\n","authors":["Junbum Cha","Kyungjae Lee","Sungrae Park","Sanghyuk Chun"],"pdf_url":"https://arxiv.org/pdf/2203.10789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10031v2","updated":"2022-03-21T08:03:20Z","published":"2021-10-19T14:59:48Z","title":"Online Continual Learning on Class Incremental Blurry Task Configuration\n  with Anytime Inference","summary":"  Despite rapid advances in continual learning, a large body of research is\ndevoted to improving performance in the existing setups. While a handful of\nwork do propose new continual learning setups, they still lack practicality in\ncertain aspects. For better practicality, we first propose a novel continual\nlearning setup that is online, task-free, class-incremental, of blurry task\nboundaries and subject to inference queries at any moment. We additionally\npropose a new metric to better measure the performance of the continual\nlearning methods subject to inference queries at any moment. To address the\nchallenging setup and evaluation protocol, we propose an effective method that\nemploys a new memory management scheme and novel learning techniques. Our\nempirical validation demonstrates that the proposed method outperforms prior\narts by large margins. Code and data splits are available at\nhttps://github.com/naver-ai/i-Blurry.\n","authors":["Hyunseo Koh","Dahyun Kim","Jung-Woo Ha","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2110.10031v2.pdf","comment":"to appear in ICLR2022"},{"id":"http://arxiv.org/abs/2203.10786v1","updated":"2022-03-21T08:01:25Z","published":"2022-03-21T08:01:25Z","title":"Classifications of Skull Fractures using CT Scan Images via CNN with\n  Lazy Learning Approach","summary":"  Classification of skull fracture is a challenging task for both radiologists\nand researchers. Skull fractures result in broken pieces of bone, which can cut\ninto the brain and cause bleeding and other injury types. So it is vital to\ndetect and classify the fracture very early. In real world, often fractures\noccur at multiple sites. This makes it harder to detect the fracture type where\nmany fracture types might summarize a skull fracture. Unfortunately, manual\ndetection of skull fracture and the classification process is time-consuming,\nthreatening a patient's life. Because of the emergence of deep learning, this\nprocess could be automated. Convolutional Neural Networks (CNNs) are the most\nwidely used deep learning models for image categorization because they deliver\nhigh accuracy and outstanding outcomes compared to other models. We propose a\nnew model called SkullNetV1 comprising a novel CNN by taking advantage of CNN\nfor feature extraction and lazy learning approach which acts as a classifier\nfor classification of skull fractures from brain CT images to classify five\nfracture types. Our suggested model achieved a subset accuracy of 88%, an F1\nscore of 93%, the Area Under the Curve (AUC) of 0.89 to 0.98, a Hamming score\nof 92% and a Hamming loss of 0.04 for this seven-class multi-labeled\nclassification.\n","authors":["Md Moniruzzaman Emon","Tareque Rahman Ornob","Moqsadur Rahman"],"pdf_url":"https://arxiv.org/pdf/2203.10786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10785v1","updated":"2022-03-21T08:00:16Z","published":"2022-03-21T08:00:16Z","title":"GroupTransNet: Group Transformer Network for RGB-D Salient Object\n  Detection","summary":"  Salient object detection on RGB-D images is an active topic in computer\nvision. Although the existing methods have achieved appreciable performance,\nthere are still some challenges. The locality of convolutional neural network\nrequires that the model has a sufficiently deep global receptive field, which\nalways leads to the loss of local details. To address the challenge, we propose\na novel Group Transformer Network (GroupTransNet) for RGB-D salient object\ndetection. This method is good at learning the long-range dependencies of cross\nlayer features to promote more perfect feature expression. At the beginning,\nthe features of the slightly higher classes of the middle three levels and the\nlatter three levels are soft grouped to absorb the advantages of the high-level\nfeatures. The input features are repeatedly purified and enhanced by the\nattention mechanism to purify the cross modal features of color modal and depth\nmodal. The features of the intermediate process are first fused by the features\nof different layers, and then processed by several transformers in multiple\ngroups, which not only makes the size of the features of each scale unified and\ninterrelated, but also achieves the effect of sharing the weight of the\nfeatures within the group. The output features in different groups complete the\nclustering staggered by two owing to the level difference, and combine with the\nlow-level features. Extensive experiments demonstrate that GroupTransNet\noutperforms the comparison models and achieves the new state-of-the-art\nperformance.\n","authors":["Xian Fang","Jinshao Zhu","Xiuli Shao","Hongpeng Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10779v1","updated":"2022-03-21T07:50:24Z","published":"2022-03-21T07:50:24Z","title":"Adaptive and Cascaded Compressive Sensing","summary":"  Scene-dependent adaptive compressive sensing (CS) has been a long pursuing\ngoal which has huge potential in significantly improving the performance of CS.\nHowever, without accessing to the ground truth image, how to design the\nscene-dependent adaptive strategy is still an open-problem and the improvement\nin sampling efficiency is still quite limited. In this paper, a restricted\nisometry property (RIP) condition based error clamping is proposed, which could\ndirectly predict the reconstruction error, i.e. the difference between the\ncurrently-stage reconstructed image and the ground truth image, and adaptively\nallocate samples to different regions at the successive sampling stage.\nFurthermore, we propose a cascaded feature fusion reconstruction network that\ncould efficiently utilize the information derived from different adaptive\nsampling stages. The effectiveness of the proposed adaptive and cascaded CS\nmethod is demonstrated with extensive quantitative and qualitative results,\ncompared with the state-of-the-art CS algorithms.\n","authors":["Chenxi Qiu","Tao Yue","Xuemei Hu"],"pdf_url":"https://arxiv.org/pdf/2203.10779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10778v1","updated":"2022-03-21T07:49:14Z","published":"2022-03-21T07:49:14Z","title":"Delving into the Estimation Shift of Batch Normalization in a Network","summary":"  Batch normalization (BN) is a milestone technique in deep learning. It\nnormalizes the activation using mini-batch statistics during training but the\nestimated population statistics during inference. This paper focuses on\ninvestigating the estimation of population statistics. We define the estimation\nshift magnitude of BN to quantitatively measure the difference between its\nestimated population statistics and expected ones. Our primary observation is\nthat the estimation shift can be accumulated due to the stack of BN in a\nnetwork, which has detriment effects for the test performance. We further find\na batch-free normalization (BFN) can block such an accumulation of estimation\nshift. These observations motivate our design of XBNBlock that replace one BN\nwith BFN in the bottleneck block of residual-style networks. Experiments on the\nImageNet and COCO benchmarks show that XBNBlock consistently improves the\nperformance of different architectures, including ResNet and ResNeXt, by a\nsignificant margin and seems to be more robust to distribution shift.\n","authors":["Lei Huang","Yi Zhou","Tian Wang","Jie Luo","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2203.10778v1.pdf","comment":"Accepted to CVPR 2022. The Code is available at:\n  https://github.com/huangleiBuaa/XBNBlock"},{"id":"http://arxiv.org/abs/2203.10776v1","updated":"2022-03-21T07:38:59Z","published":"2022-03-21T07:38:59Z","title":"K-space and Image Domain Collaborative Energy based Model for Parallel\n  MRI Reconstruction","summary":"  Decreasing magnetic resonance (MR) image acquisition times can potentially\nmake MR examinations more accessible. Prior arts including the deep learning\nmodels have been devoted to solving the problem of long MRI imaging time.\nRecently, deep generative models have exhibited great potentials in algorithm\nrobustness and usage flexibility. Nevertheless, no existing such schemes that\ncan be learned or employed directly to the k-space measurement. Furthermore,\nhow do the deep generative models work well in hybrid domain is also worth to\nbe investigated. In this work, by taking advantage of the deep en-ergy-based\nmodels, we propose a k-space and image domain collaborative generative model to\ncomprehensively estimate the MR data from under-sampled measurement.\nExperimental comparisons with the state-of-the-arts demonstrated that the\nproposed hybrid method has less error in reconstruction and is more stable\nunder different acceleration factors.\n","authors":["Zongjiang Tu","Chen Jiang","Yu Guan","Shanshan Wang","Jijun Liu","Qiegen Liu","Dong Liang"],"pdf_url":"https://arxiv.org/pdf/2203.10776v1.pdf","comment":"10 pages,9 figures"},{"id":"http://arxiv.org/abs/2203.10773v1","updated":"2022-03-21T07:33:49Z","published":"2022-03-21T07:33:49Z","title":"Slice Imputation: Intermediate Slice Interpolation for Anisotropic 3D\n  Medical Image Segmentation","summary":"  We introduce a novel frame-interpolation-based method for slice imputation to\nimprove segmentation accuracy for anisotropic 3D medical images, in which the\nnumber of slices and their corresponding segmentation labels can be increased\nbetween two consecutive slices in anisotropic 3D medical volumes. Unlike\nprevious inter-slice imputation methods, which only focus on the smoothness in\nthe axial direction, this study aims to improve the smoothness of the\ninterpolated 3D medical volumes in all three directions: axial, sagittal, and\ncoronal. The proposed multitask inter-slice imputation method, in particular,\nincorporates a smoothness loss function to evaluate the smoothness of the\ninterpolated 3D medical volumes in the through-plane direction (sagittal and\ncoronal). It not only improves the resolution of the interpolated 3D medical\nvolumes in the through-plane direction but also transforms them into isotropic\nrepresentations, which leads to better segmentation performances. Experiments\non whole tumor segmentation in the brain, liver tumor segmentation, and\nprostate segmentation indicate that our method outperforms the competing slice\nimputation methods on both computed tomography and magnetic resonance images\nvolumes in most cases.\n","authors":["Zhaotao Wu","Jia Wei","Jiabing Wang","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2203.10773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10768v1","updated":"2022-03-21T07:20:37Z","published":"2022-03-21T07:20:37Z","title":"Upsampling Autoencoder for Self-Supervised Point Cloud Learning","summary":"  In computer-aided design (CAD) community, the point cloud data is pervasively\napplied in reverse engineering, where the point cloud analysis plays an\nimportant role. While a large number of supervised learning methods have been\nproposed to handle the unordered point clouds and demonstrated their remarkable\nsuccess, their performance and applicability are limited to the costly data\nannotation. In this work, we propose a novel self-supervised pretraining model\nfor point cloud learning without human annotations, which relies solely on\nupsampling operation to perform feature learning of point cloud in an effective\nmanner. The key premise of our approach is that upsampling operation encourages\nthe network to capture both high-level semantic information and low-level\ngeometric information of the point cloud, thus the downstream tasks such as\nclassification and segmentation will benefit from the pre-trained model.\nSpecifically, our method first conducts the random subsampling from the input\npoint cloud at a low proportion e.g., 12.5%. Then, we feed them into an\nencoder-decoder architecture, where an encoder is devised to operate only on\nthe subsampled points, along with a upsampling decoder is adopted to\nreconstruct the original point cloud based on the learned features. Finally, we\ndesign a novel joint loss function which enforces the upsampled points to be\nsimilar with the original point cloud and uniformly distributed on the\nunderlying shape surface. By adopting the pre-trained encoder weights as\ninitialisation of models for downstream tasks, we find that our UAE outperforms\nprevious state-of-the-art methods in shape classification, part segmentation\nand point cloud upsampling tasks. Code will be made publicly available upon\nacceptance.\n","authors":["Cheng Zhang","Jian Shi","Xuan Deng","Zizhao Wu"],"pdf_url":"https://arxiv.org/pdf/2203.10768v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2203.10761v1","updated":"2022-03-21T07:12:18Z","published":"2022-03-21T07:12:18Z","title":"Decoupled Mixup for Data-efficient Learning","summary":"  Mixup is an efficient data augmentation approach that improves the\ngeneralization of neural networks by smoothing the decision boundary with mixed\ndata. Recently, dynamic mixup methods improve previous static policies (e.g.,\nlinear interpolation) by maximizing discriminative regions or maintaining the\nsalient objects in mixed samples. We notice that The mixed samples from dynamic\npolicies are more separable than the static ones while preventing models from\noverfitting. Inspired by this finding, we first argue that there exists an\nover-smoothing issue in the mixup objective, which focuses on regression the\nmixing ratio instead of identifying discriminative features. We are therefore\nprompted to propose a decoupled mixup (DM) loss that can adaptively mine\ndiscriminative features without losing smoothness. DM enables static mixup\nmethods to achieve comparable performance with dynamic methods while avoiding\nheavy computational overhead. This also leads to an interesting objective\ndesign problem for mixup training that we need to focus not only on smoothing\nthe decision boundaries but also on identifying discriminative features.\nExtensive experiments on supervised and semi-supervised learning benchmarks\nacross seven classification datasets validate the effectiveness of DM by\nequipping with various mixup methods.\n","authors":["Zicheng Liu","Siyuan Li","Ge Wang","Cheng Tan","Lirong Wu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2203.10761v1.pdf","comment":"The first preprint version, 21 pages. The source code is available at\n  https://github.com/Westlake-AI/openmixup"},{"id":"http://arxiv.org/abs/2112.12496v3","updated":"2022-03-21T06:55:43Z","published":"2021-12-23T12:42:38Z","title":"FedFR: Joint Optimization Federated Framework for Generic and\n  Personalized Face Recognition","summary":"  Current state-of-the-art deep learning based face recognition (FR) models\nrequire a large number of face identities for central training. However, due to\nthe growing privacy awareness, it is prohibited to access the face images on\nuser devices to continually improve face recognition models. Federated Learning\n(FL) is a technique to address the privacy issue, which can collaboratively\noptimize the model without sharing the data between clients. In this work, we\npropose a FL based framework called FedFR to improve the generic face\nrepresentation in a privacy-aware manner. Besides, the framework jointly\noptimizes personalized models for the corresponding clients via the proposed\nDecoupled Feature Customization module. The client-specific personalized model\ncan serve the need of optimized face recognition experience for registered\nidentities at the local device. To the best of our knowledge, we are the first\nto explore the personalized face recognition in FL setup. The proposed\nframework is validated to be superior to previous approaches on several generic\nand personalized face recognition benchmarks with diverse FL scenarios. The\nsource codes and our proposed personalized FR benchmark under FL setup are\navailable at https://github.com/jackie840129/FedFR.\n","authors":["Chih-Ting Liu","Chien-Yi Wang","Shao-Yi Chien","Shang-Hong Lai"],"pdf_url":"https://arxiv.org/pdf/2112.12496v3.pdf","comment":"This paper was accepted by AAAI 2022 Conference on Artificial\n  Intelligence and selected as an oral paper"},{"id":"http://arxiv.org/abs/2203.05137v3","updated":"2022-03-21T06:26:25Z","published":"2022-03-10T03:30:12Z","title":"Cross-modal Map Learning for Vision and Language Navigation","summary":"  We consider the problem of Vision-and-Language Navigation (VLN). The majority\nof current methods for VLN are trained end-to-end using either unstructured\nmemory such as LSTM, or using cross-modal attention over the egocentric\nobservations of the agent. In contrast to other works, our key insight is that\nthe association between language and vision is stronger when it occurs in\nexplicit spatial representations. In this work, we propose a cross-modal map\nlearning model for vision-and-language navigation that first learns to predict\nthe top-down semantics on an egocentric map for both observed and unobserved\nregions, and then predicts a path towards the goal as a set of waypoints. In\nboth cases, the prediction is informed by the language through cross-modal\nattention mechanisms. We experimentally test the basic hypothesis that\nlanguage-driven navigation can be solved given a map, and then show competitive\nresults on the full VLN-CE benchmark.\n","authors":["Georgios Georgakis","Karl Schmeckpeper","Karan Wanchoo","Soham Dan","Eleni Miltsakaki","Dan Roth","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2203.05137v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09440v2","updated":"2022-03-21T06:18:32Z","published":"2022-03-17T17:00:55Z","title":"TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes","summary":"  Many basic indoor activities such as eating or writing are always conducted\nupon different tabletops (e.g., coffee tables, writing desks). It is\nindispensable to understanding tabletop scenes in 3D indoor scene parsing\napplications. Unfortunately, it is hard to meet this demand by directly\ndeploying data-driven algorithms, since 3D tabletop scenes are rarely available\nin current datasets. To remedy this defect, we introduce TO-Scene, a\nlarge-scale dataset focusing on tabletop scenes, which contains 20,740 scenes\nwith three variants. To acquire the data, we design an efficient and scalable\nframework, where a crowdsourcing UI is developed to transfer CAD objects onto\ntables from ScanNet, then the output tabletop scenes are simulated into real\nscans and annotated automatically.\n  Further, a tabletop-aware learning strategy is proposed for better perceiving\nthe small-sized tabletop instances. Notably, we also provide a real scanned\ntest set TO-Real to verify the practical value of TO-Scene. Experiments show\nthat the algorithms trained on TO-Scene indeed work on the realistic test data,\nand our proposed tabletop-aware learning strategy greatly improves the\nstate-of-the-art results on both 3D semantic segmentation and object detection\ntasks. TO-Scene and TO-Real, plus Web UI, will all be publicly available.\n","authors":["Mutian Xu","Pei Chen","Haolin Liu","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2203.09440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10747v1","updated":"2022-03-21T05:56:12Z","published":"2022-03-21T05:56:12Z","title":"EAutoDet: Efficient Architecture Search for Object Detection","summary":"  Training CNN for detection is time-consuming due to the large dataset and\ncomplex network modules, making it hard to search architectures on detection\ndatasets directly, which usually requires vast search costs (usually tens and\neven hundreds of GPU-days). In contrast, this paper introduces an efficient\nframework, named EAutoDet, that can discover practical backbone and FPN\narchitectures for object detection in 1.4 GPU-days. Specifically, we construct\na supernet for both backbone and FPN modules and adopt the differentiable\nmethod. To reduce the GPU memory requirement and computational cost, we propose\na kernel reusing technique by sharing the weights of candidate operations on\none edge and consolidating them into one convolution. A dynamic channel\nrefinement strategy is also introduced to search channel numbers. Extensive\nexperiments show significant efficacy and efficiency of our method. In\nparticular, the discovered architectures surpass state-of-the-art object\ndetection NAS methods and achieve 40.1 mAP with 120 FPS and 49.2 mAP with 41.3\nFPS on COCO test-dev set. We also transfer the discovered architectures to\nrotation detection task, which achieve 77.05 mAP$_{\\text{50}}$ on DOTA-v1.0\ntest set with 21.1M parameters.\n","authors":["Xiaoxing Wang","Jiale Lin","Junchi Yan","Juanping Zhao","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2203.10747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10739v1","updated":"2022-03-21T05:16:23Z","published":"2022-03-21T05:16:23Z","title":"Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation","summary":"  Sparsely annotated semantic segmentation (SASS) aims to train a segmentation\nnetwork with coarse-grained (i.e., point-, scribble-, and block-wise)\nsupervisions, where only a small proportion of pixels are labeled in each\nimage. In this paper, we propose a novel tree energy loss for SASS by providing\nsemantic guidance for unlabeled pixels. The tree energy loss represents images\nas minimum spanning trees to model both low-level and high-level pair-wise\naffinities. By sequentially applying these affinities to the network\nprediction, soft pseudo labels for unlabeled pixels are generated in a\ncoarse-to-fine manner, achieving dynamic online self-training. The tree energy\nloss is effective and easy to be incorporated into existing frameworks by\ncombining it with a traditional segmentation loss. Compared with previous SASS\nmethods, our method requires no multistage training strategies, alternating\noptimization procedures, additional supervised data, or time-consuming\npost-processing while outperforming them in all SASS settings. Code is\navailable at https://github.com/megviiresearch/TEL.\n","authors":["Zhiyuan Liang","Tiancai Wang","Xiangyu Zhang","Jian Sun","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2203.10739v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.10730v1","updated":"2022-03-21T04:16:25Z","published":"2022-03-21T04:16:25Z","title":"Semantic Segmentation with Active Semi-Supervised Learning","summary":"  Using deep learning, we now have the ability to create exceptionally good\nsemantic segmentation systems; however, collecting the prerequisite pixel-wise\nannotations for training images remains expensive and time-consuming.\nTherefore, it would be ideal to minimize the number of human annotations needed\nwhen creating a new dataset. Here, we address this problem by proposing a novel\nalgorithm that combines active learning and semi-supervised learning. Active\nlearning is an approach for identifying the best unlabeled samples to annotate.\nWhile there has been work on active learning for segmentation, most methods\nrequire annotating all pixel objects in each image, rather than only the most\ninformative regions. We argue that this is inefficient. Instead, our active\nlearning approach aims to minimize the number of annotations per-image. Our\nmethod is enriched with semi-supervised learning, where we use pseudo labels\ngenerated with a teacher-student framework to identify image regions that help\ndisambiguate confused classes. We also integrate mechanisms that enable better\nperformance on imbalanced label distributions, which have not been studied\npreviously for active learning in semantic segmentation. In experiments on the\nCamVid and CityScapes datasets, our method obtains over 95% of the network's\nperformance on the full-training set using less than 19% of the training data,\nwhereas the previous state of the art required 40% of the training data.\n","authors":["Aneesh Rangnekar","Christopher Kanan","Matthew Hoffman"],"pdf_url":"https://arxiv.org/pdf/2203.10730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10729v1","updated":"2022-03-21T04:14:06Z","published":"2022-03-21T04:14:06Z","title":"DSRRTracker: Dynamic Search Region Refinement for Attention-based\n  Siamese Multi-Object Tracking","summary":"  Many multi-object tracking (MOT) methods follow the framework of \"tracking by\ndetection\", which associates the target objects-of-interest based on the\ndetection results. However, due to the separate models for detection and\nassociation, the tracking results are not optimal.Moreover, the speed is\nlimited by some cumbersome association methods to achieve high tracking\nperformance. In this work, we propose an end-to-end MOT method, with a Gaussian\nfilter-inspired dynamic search region refinement module to dynamically filter\nand refine the search region by considering both the template information from\nthe past frames and the detection results from the current frame with little\ncomputational burden, and a lightweight attention-based tracking head to\nachieve the effective fine-grained instance association. Extensive experiments\nand ablation study on MOT17 and MOT20 datasets demonstrate that our method can\nachieve the state-of-the-art performance with reasonable speed.\n","authors":["JiaXu Wan","Hong Zhang","Jin Zhang","Yuan Ding","Yifan Yang","Yan Li","Xuliang Li"],"pdf_url":"https://arxiv.org/pdf/2203.10729v1.pdf","comment":"25 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2203.10726v1","updated":"2022-03-21T04:02:54Z","published":"2022-03-21T04:02:54Z","title":"TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation\n  with Transformers","summary":"  Combining information from multi-view images is crucial to improve the\nperformance and robustness of automated methods for disease diagnosis. However,\ndue to the non-alignment characteristics of multi-view images, building\ncorrelation and data fusion across views largely remain an open problem. In\nthis study, we present TransFusion, a Transformer-based architecture to merge\ndivergent multi-view imaging information using convolutional layers and\npowerful attention mechanisms. In particular, the Divergent Fusion Attention\n(DiFA) module is proposed for rich cross-view context modeling and semantic\ndependency mining, addressing the critical issue of capturing long-range\ncorrelations between unaligned data from different image views. We further\npropose the Multi-Scale Attention (MSA) to collect global correspondence of\nmulti-scale feature representations. We evaluate TransFusion on the\nMulti-Disease, Multi-View \\& Multi-Center Right Ventricular Segmentation in\nCardiac MRI (M\\&Ms-2) challenge cohort. TransFusion demonstrates leading\nperformance against the state-of-the-art methods and opens up new perspectives\nfor multi-view imaging integration towards robust medical image segmentation.\n","authors":["Di Liu","Yunhe Gao","Qilong Zhangli","Zhennan Yan","Mu Zhou","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2203.10726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10723v1","updated":"2022-03-21T03:54:53Z","published":"2022-03-21T03:54:53Z","title":"An Intermediate-level Attack Framework on The Basis of Linear Regression","summary":"  This paper substantially extends our work published at ECCV, in which an\nintermediate-level attack was proposed to improve the transferability of some\nbaseline adversarial examples. We advocate to establish a direct linear mapping\nfrom the intermediate-level discrepancies (between adversarial features and\nbenign features) to classification prediction loss of the adversarial example.\nIn this paper, we delve deep into the core components of such a framework by\nperforming comprehensive studies and extensive experiments. We show that 1) a\nvariety of linear regression models can all be considered in order to establish\nthe mapping, 2) the magnitude of the finally obtained intermediate-level\ndiscrepancy is linearly correlated with adversarial transferability, 3) further\nboost of the performance can be achieved by performing multiple runs of the\nbaseline attack with random initialization. By leveraging these findings, we\nachieve new state-of-the-arts on transfer-based $\\ell_\\infty$ and $\\ell_2$\nattacks.\n","authors":["Yiwen Guo","Qizhang Li","Wangmeng Zuo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2203.10723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06907v2","updated":"2022-03-21T03:53:11Z","published":"2022-03-14T08:01:14Z","title":"Hierarchical Memory Learning for Fine-Grained Scene Graph Generation","summary":"  As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the\ndataset due to the crowd-sourced labeling, and the long-tail problem is also\npronounced. Given this tricky situation, many existing SGG methods treat the\npredicates equally and learn the model under the supervision of\nmixed-granularity predicates in one stage, leading to relatively coarse\npredictions. In order to alleviate the negative impact of the suboptimum\nmixed-granularity annotation and long-tail effect problems, this paper proposes\na novel Hierarchical Memory Learning (HML) framework to learn the model from\nsimple to complex, which is similar to the human beings' hierarchical memory\nlearning process. After the autonomous partition of coarse and fine predicates,\nthe model is first trained on the coarse predicates and then learns the fine\npredicates. In order to realize this hierarchical learning pattern, this paper,\nfor the first time, formulates the HML framework using the new Concept\nReconstruction (CR) and Model Reconstruction (MR) constraints. It is worth\nnoticing that the HML framework can be taken as one general optimization\nstrategy to improve various SGG models, and significant improvement can be\nachieved on the SGG benchmark (i.e., Visual Genome).\n","authors":["Youming Deng","Yansheng Li","Yongjun Zhang","Xiang Xiang","Jian Wang","Jingdong Chen","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2203.06907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10719v1","updated":"2022-03-21T03:35:32Z","published":"2022-03-21T03:35:32Z","title":"LocATe: End-to-end Localization of Actions in 3D with Transformers","summary":"  Understanding a person's behavior from their 3D motion is a fundamental\nproblem in computer vision with many applications. An important component of\nthis problem is 3D Temporal Action Localization (3D-TAL), which involves\nrecognizing what actions a person is performing, and when. State-of-the-art\n3D-TAL methods employ a two-stage approach in which the action span detection\ntask and the action recognition task are implemented as a cascade. This\napproach, however, limits the possibility of error-correction. In contrast, we\npropose LocATe, an end-to-end approach that jointly localizes and recognizes\nactions in a 3D sequence. Further, unlike existing autoregressive models that\nfocus on modeling the local context in a sequence, LocATe's transformer model\nis capable of capturing long-term correlations between actions in a sequence.\nUnlike transformer-based object-detection and classification models which\nconsider image or patch features as input, the input in 3D-TAL is a long\nsequence of highly correlated frames. To handle the high-dimensional input, we\nimplement an effective input representation, and overcome the diffuse attention\nacross long time horizons by introducing sparse attention in the model. LocATe\noutperforms previous approaches on the existing PKU-MMD 3D-TAL benchmark\n(mAP=93.2%). Finally, we argue that benchmark datasets are most useful where\nthere is clear room for performance improvement. To that end, we introduce a\nnew, challenging, and more realistic benchmark dataset, BABEL-TAL-20 (BT20),\nwhere the performance of state-of-the-art methods is significantly worse. The\ndataset and code for the method will be available for research purposes.\n","authors":["Jiankai Sun","Bolei Zhou","Michael J. Black","Arjun Chandrasekaran"],"pdf_url":"https://arxiv.org/pdf/2203.10719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.03530v4","updated":"2022-03-21T03:27:47Z","published":"2021-12-07T06:59:06Z","title":"A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud\n  Completion","summary":"  3D point cloud is an important 3D representation for capturing real world 3D\nobjects. However, real-scanned 3D point clouds are often incomplete, and it is\nimportant to recover complete point clouds for downstream applications. Most\nexisting point cloud completion methods use Chamfer Distance (CD) loss for\ntraining. The CD loss estimates correspondences between two point clouds by\nsearching nearest neighbors, which does not capture the overall point density\ndistribution on the generated shape, and therefore likely leads to non-uniform\npoint cloud generation. To tackle this problem, we propose a novel Point\nDiffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of\na Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The\nCGNet uses a conditional generative model called the denoising diffusion\nprobabilistic model (DDPM) to generate a coarse completion conditioned on the\npartial observation. DDPM establishes a one-to-one pointwise mapping between\nthe generated point cloud and the uniform ground truth, and then optimizes the\nmean squared error loss to realize uniform generation. The RFNet refines the\ncoarse output of the CGNet and further improves quality of the completed point\ncloud. Furthermore, we develop a novel dual-path architecture for both\nnetworks. The architecture can (1) effectively and efficiently extract\nmulti-level features from partially observed point clouds to guide completion,\nand (2) accurately manipulate spatial locations of 3D points to obtain smooth\nsurfaces and sharp details. Extensive experimental results on various benchmark\ndatasets show that our PDR paradigm outperforms previous state-of-the-art\nmethods for point cloud completion. Remarkably, with the help of the RFNet, we\ncan accelerate the iterative generation process of the DDPM by up to 50 times\nwithout much performance drop.\n","authors":["Zhaoyang Lyu","Zhifeng Kong","Xudong Xu","Liang Pan","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2112.03530v4.pdf","comment":"Accepted to ICLR 2022. Code is released at\n  https://github.com/ZhaoyangLyu/Point_Diffusion_Refinement"},{"id":"http://arxiv.org/abs/2203.10090v1","updated":"2022-03-21T03:23:09Z","published":"2022-03-21T03:23:09Z","title":"FaceMap: Towards Unsupervised Face Clustering via Map Equation","summary":"  Face clustering is an essential task in computer vision due to the explosion\nof related applications such as augmented reality or photo album management.\nThe main challenge of this task lies in the imperfectness of similarities among\nimage feature representations. Given an existing feature extraction model, it\nis still an unresolved problem that how can the inherent characteristics of\nsimilarities of unlabelled images be leveraged to improve the clustering\nperformance. Motivated by answering the question, we develop an effective\nunsupervised method, named as FaceMap, by formulating face clustering as a\nprocess of non-overlapping community detection, and minimizing the entropy of\ninformation flows on a network of images. The entropy is denoted by the map\nequation and its minimum represents the least description of paths among images\nin expectation. Inspired by observations on the ranked transition probabilities\nin the affinity graph constructed from facial images, we develop an outlier\ndetection strategy to adaptively adjust transition probabilities among images.\nExperiments with ablation studies demonstrate that FaceMap significantly\noutperforms existing methods and achieves new state-of-the-arts on three\npopular large-scale datasets for face clustering, e.g., an absolute improvement\nof more than $10\\%$ and $4\\%$ comparing with prior unsupervised and supervised\nmethods respectively in terms of average of Pairwise F-score. Our code is\npublicly available on github.\n","authors":["Xiaotian Yu","Yifan Yang","Aibo Wang","Ling Xing","Hanling Yi","Guangming Lu","Xiaoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10712v1","updated":"2022-03-21T03:15:18Z","published":"2022-03-21T03:15:18Z","title":"What Makes RAFT Better Than PWC-Net?","summary":"  How important are training details and datasets to recent optical flow models\nlike RAFT? And do they generalize? To explore these questions, rather than\ndevelop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and\nRAFT, with a common set of modern training techniques and datasets, and observe\nsignificant performance gains, demonstrating the importance and generality of\nthese training details. Our newly trained PWC-Net and IRR-PWC models show\nsurprisingly large improvements, up to 30% versus original published results on\nSintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on\nKITTI 2015 while being 3x faster during inference. Our newly trained RAFT\nachieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all\npublished optical flow methods at the time of writing. Our results demonstrate\nthe benefits of separating the contributions of models, training techniques and\ndatasets when analyzing performance gains of optical flow methods. Our source\ncode will be publicly available.\n","authors":["Deqing Sun","Charles Herrmann","Fitsum Reda","Michael Rubinstein","David Fleet","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2203.10712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.04771v4","updated":"2022-03-21T03:02:37Z","published":"2022-03-09T14:42:26Z","title":"Multiscale Convolutional Transformer with Center Mask Pretraining for\n  Hyperspectral Image Classification","summary":"  Hyperspectral images (HSI) not only have a broad macroscopic field of view\nbut also contain rich spectral information, and the types of surface objects\ncan be identified through spectral information, which is one of the main\napplications in hyperspectral image related research.In recent years, more and\nmore deep learning methods have been proposed, among which convolutional neural\nnetworks (CNN) are the most influential. However, CNN-based methods are\ndifficult to capture long-range dependencies, and also require a large amount\nof labeled data for model training.Besides, most of the self-supervised\ntraining methods in the field of HSI classification are based on the\nreconstruction of input samples, and it is difficult to achieve effective use\nof unlabeled samples. To address the shortcomings of CNN networks, we propose a\nnoval multi-scale convolutional embedding module for HSI to realize effective\nextraction of spatial-spectral information, which can be better combined with\nTransformer network.In order to make more efficient use of unlabeled data, we\npropose a new self-supervised pretask. Similar to Mask autoencoder, but our\npre-training method only masks the corresponding token of the central pixel in\nthe encoder, and inputs the remaining token into the decoder to reconstruct the\nspectral information of the central pixel.Such a pretask can better model the\nrelationship between the central feature and the domain feature, and obtain\nmore stable training results.\n","authors":["Sen Jia","Yifan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.04771v4.pdf","comment":"8 pages, 26 figures, conference paper"},{"id":"http://arxiv.org/abs/2203.10707v1","updated":"2022-03-21T02:30:36Z","published":"2022-03-21T02:30:36Z","title":"Monocular Vision-based Prediction of Cut-in Maneuvers with LSTM Networks","summary":"  Advanced driver assistance and automated driving systems should be capable of\npredicting and avoiding dangerous situations. This study proposes a method to\npredict potentially dangerous cut-in maneuvers happening in the ego lane. We\nfollow a computer vision-based approach that only employs a single in-vehicle\nRGB camera, and we classify the target vehicle's maneuver based on the recent\nvideo frames. Our algorithm consists of a CNN-based vehicle detection and\ntracking step and an LSTM-based maneuver classification step. It is more\ncomputationally efficient than other vision-based methods since it exploits a\nsmall number of features for the classification step rather than feeding CNNs\nwith RGB frames. We evaluated our approach on a publicly available driving\ndataset and a lane change detection dataset. We obtained 0.9585 accuracy with\nside-aware two-class (cut-in vs. lane-pass) classification models. Experiment\nresults also reveal that our approach outperforms state-of-the-art approaches\nwhen used for lane change detection.\n","authors":["Yagiz Nalcakan","Yalin Bastanlar"],"pdf_url":"https://arxiv.org/pdf/2203.10707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10705v1","updated":"2022-03-21T02:11:35Z","published":"2022-03-21T02:11:35Z","title":"Compression of Generative Pre-trained Language Models via Quantization","summary":"  The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.\n","authors":["Chaofan Tao","Lu Hou","Wei Zhang","Lifeng Shang","Xin Jiang","Qun Liu","Ping Luo","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2203.10705v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.10699v1","updated":"2022-03-21T01:39:41Z","published":"2022-03-21T01:39:41Z","title":"HP-Capsule: Unsupervised Face Part Discovery by Hierarchical Parsing\n  Capsule Network","summary":"  Capsule networks are designed to present the objects by a set of parts and\ntheir relationships, which provide an insight into the procedure of visual\nperception. Although recent works have shown the success of capsule networks on\nsimple objects like digits, the human faces with homologous structures, which\nare suitable for capsules to describe, have not been explored. In this paper,\nwe propose a Hierarchical Parsing Capsule Network (HP-Capsule) for unsupervised\nface subpart-part discovery. When browsing large-scale face images without\nlabels, the network first encodes the frequently observed patterns with a set\nof explainable subpart capsules. Then, the subpart capsules are assembled into\npart-level capsules through a Transformer-based Parsing Module (TPM) to learn\nthe compositional relations between them. During training, as the face\nhierarchy is progressively built and refined, the part capsules adaptively\nencode the face parts with semantic consistency. HP-Capsule extends the\napplication of capsule networks from digits to human faces and takes a step\nforward to show how the neural networks understand homologous objects without\nhuman intervention. Besides, HP-Capsule gives unsupervised face segmentation\nresults by the covered regions of part capsules, enabling qualitative and\nquantitative evaluation. Experiments on BP4D and Multi-PIE datasets show the\neffectiveness of our method.\n","authors":["Chang Yu","Xiangyu Zhu","Xiaomei Zhang","Zidu Wang","Zhaoxiang Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2203.10699v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.10694v1","updated":"2022-03-21T01:24:53Z","published":"2022-03-21T01:24:53Z","title":"Fourier Disentangled Space-Time Attention for Aerial Video Recognition","summary":"  We present an algorithm, Fourier Activity Recognition (FAR), for UAV video\nactivity recognition. Our formulation uses a novel Fourier object\ndisentanglement method to innately separate out the human agent (which is\ntypically small) from the background. Our disentanglement technique operates in\nthe frequency domain to characterize the extent of temporal change of spatial\npixels, and exploits convolution-multiplication properties of Fourier transform\nto map this representation to the corresponding object-background entangled\nfeatures obtained from the network. To encapsulate contextual information and\nlong-range space-time dependencies, we present a novel Fourier Attention\nalgorithm, which emulates the benefits of self-attention by modeling the\nweighted outer product in the frequency domain. Our Fourier attention\nformulation uses much fewer computations than self-attention. We have evaluated\nour approach on multiple UAV datasets including UAV Human RGB, UAV Human Night,\nDrone Action, and NEC Drone. We demonstrate a relative improvement of 8.02% -\n38.69% in top-1 accuracy and up to 3 times faster over prior works.\n","authors":["Divya Kothandaraman","Tianrui Guan","Xijun Wang","Sean Hu","Ming Lin","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2203.10694v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2203.11163v1","updated":"2022-03-21T17:41:54Z","published":"2022-03-21T17:41:54Z","title":"Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math\n  Information Retrieval","summary":"  With the recent success of dense retrieval methods based on bi-encoders, a\nnumber of studies have applied this approach to various interesting downstream\nretrieval tasks with good efficiency and in-domain effectiveness. Recently, we\nhave also seen the presence of dense retrieval models in Math Information\nRetrieval (MIR) tasks, but the most effective systems remain \"classic\"\nretrieval methods that consider rich structure features. In this work, we try\nto combine the best of both worlds: a well-defined structure search method for\neffective formula search and bi-encoder dense retrieval models to capture\ncontextual similarities in mathematical documents. Specifically, we have\nevaluated two representative bi-encoder models (ColBERT and DPR) for\ntoken-level and passage-level dense retrieval on recent MIR tasks. To our best\nknowledge, this is the first time a DPR model has been evaluated in the MIR\ndomain. Our result shows that bi-encoder models are complementary to existing\nstructure search methods, and we are able to advance the state of the art on a\nrecent MIR dataset. We have made our model checkpoints and source code publicly\navailable for the reproduction of our results.\n","authors":["Wei Zhong","Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2203.11163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.00197v2","updated":"2022-03-21T08:29:35Z","published":"2020-04-01T02:09:20Z","title":"Task-adaptive Asymmetric Deep Cross-modal Hashing","summary":"  Supervised cross-modal hashing aims to embed the semantic correlations of\nheterogeneous modality data into the binary hash codes with discriminative\nsemantic labels. Because of its advantages on retrieval and storage efficiency,\nit is widely used for solving efficient cross-modal retrieval. However,\nexisting researches equally handle the different tasks of cross-modal\nretrieval, and simply learn the same couple of hash functions in a symmetric\nway for them. Under such circumstance, the uniqueness of different cross-modal\nretrieval tasks are ignored and sub-optimal performance may be brought.\nMotivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal\nHashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash\nfunctions for two sub-retrieval tasks via simultaneous modality representation\nand asymmetric hash learning. Unlike previous cross-modal hashing approaches,\nour learning framework jointly optimizes semantic preserving that transforms\ndeep features of multimedia data into binary hash codes, and the semantic\nregression which directly regresses query modality representation to explicit\nlabel. With our model, the binary codes can effectively preserve semantic\ncorrelations across different modalities, meanwhile, adaptively capture the\nquery semantics. The superiority of TA-ADCMH is proved on two standard datasets\nfrom many aspects.\n","authors":["Fengling Li","Tong Wang","Lei Zhu","Zheng Zhang","Xinhua Wang"],"pdf_url":"https://arxiv.org/pdf/2004.00197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10791v1","updated":"2022-03-21T08:12:54Z","published":"2022-03-21T08:12:54Z","title":"IoT Data Discovery: Routing Table and Summarization Techniques","summary":"  In this paper, we consider the IoT data discovery problem in very large and\ngrowing scale networks. Through analysis, examples, and experimental studies,\nwe show the importance of peer-to-peer, unstructured routing for IoT data\ndiscovery and point out the space efficiency issue that has been overlooked in\nkeyword-based routing algorithms in unstructured networks. Specifically, as the\nfirst in the field, this paper investigates routing table designs and various\ncompression techniques to support effective and space-efficient IoT data\ndiscovery routing. Novel summarization algorithms, including alphabetical,\nhash, and meaning-based summarization and their corresponding coding schemes,\nare proposed. We also consider routing table design to support summarization\nwithout degrading lookup efficiency for discovery query routing. The issue of\npotentially misleading routing due to summarization is also investigated.\nSubsequently, we analyze the strategy of when to summarize to balance the\ntradeoff between the routing table compression rate and the chance of causing\nmisleading routing. For the experimental study, we have collected 100K IoT data\nstreams from various IoT databases as the input dataset. Experimental results\nshow that our summarization solution can reduce the routing table size by 20 to\n30 folds with a 2-5% increase in latency compared with similar peer-to-peer\ndiscovery routing algorithms without summarization. Also, our approach\noutperforms DHT-based approaches by 2 to 6 folds in terms of latency and\ntraffic.\n","authors":["Hieu Tran","Son Nguyen","I-Ling Yen","Farokh Bastani"],"pdf_url":"https://arxiv.org/pdf/2203.10791v1.pdf","comment":"17 pages, 21 figures, 1 table, 3 algorithms. arXiv admin note:\n  substantial text overlap with arXiv:2107.09558"},{"id":"http://arxiv.org/abs/2203.10718v1","updated":"2022-03-21T03:30:05Z","published":"2022-03-21T03:30:05Z","title":"Prediction Algorithm for Heat Demand of Science and Technology Topics\n  Based on Time Convolution Network","summary":"  Thanks to the rapid development of deep learning, big data analysis\ntechnology is not only widely used in the field of natural language processing,\nbut also more mature in the field of numerical prediction. It is of great\nsignificance for the subject heat prediction and analysis of science and\ntechnology demand data. How to apply theme features to accurately predict the\ntheme heat of science and technology demand is the core to solve this problem.\nIn this paper, a prediction method of subject heat of science and technology\ndemand based on time convolution network (TCN) is proposed to obtain the\nsubject feature representation of science and technology demand. Time series\nprediction is carried out based on TCN network and self attention mechanism,\nwhich increases the accuracy of subject heat prediction of science and\ntechnology demand data Experiments show that the prediction accuracy of this\nalgorithm is better than other time series prediction methods on the real\nscience and technology demand datasets.\n","authors":["Cui Haiyan","Li Yawen","Xu Xin"],"pdf_url":"https://arxiv.org/pdf/2203.10718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11383v1","updated":"2022-03-21T23:03:04Z","published":"2022-03-21T23:03:04Z","title":"DIANES: A DEI Audit Toolkit for News Sources","summary":"  Professional news media organizations have always touted the importance that\nthey give to multiple perspectives. However, in practice the traditional\napproach to all-sides has favored people in the dominant culture. Hence it has\ncome under ethical critique under the new norms of diversity, equity, and\ninclusion (DEI). When DEI is applied to journalism, it goes beyond conventional\nnotions of impartiality and bias and instead democratizes the journalistic\npractice of sourcing -- who is quoted or interviewed, who is not, how often,\nfrom which demographic group, gender, and so forth. There is currently no\nreal-time or on-demand tool in the hands of reporters to analyze the persons\nthey quote. In this paper, we present DIANES, a DEI Audit Toolkit for News\nSources. It consists of a natural language processing pipeline on the backend\nto extract quotes, speakers, titles, and organizations from news articles in\nreal time. On the frontend, DIANES offers the WordPress plugins, a Web monitor,\nand a DEI annotation API service, to help news media monitor their own quoting\npatterns and push themselves towards DEI norms.\n","authors":["Xiaoxiao Shang","Zhiyuan Peng","Qiming Yuan","Sabiq Khan","Lauren Xie","Yi Fang","Subramaniam Vincent"],"pdf_url":"https://arxiv.org/pdf/2203.11383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10780v3","updated":"2022-03-21T19:54:10Z","published":"2021-10-20T21:09:41Z","title":"An Open Natural Language Processing Development Framework for EHR-based\n  Clinical Research: A case demonstration using the National COVID Cohort\n  Collaborative (N3C)","summary":"  While we pay attention to the latest advances in clinical natural language\nprocessing (NLP), we can notice some resistance in the clinical and\ntranslational research community to adopt NLP models due to limited\ntransparency, interpretability, and usability. In this study, we proposed an\nopen natural language processing development framework. We evaluated it through\nthe implementation of NLP algorithms for the National COVID Cohort\nCollaborative (N3C). Based on the interests in information extraction from\nCOVID-19 related clinical notes, our work includes 1) an open data annotation\nprocess using COVID-19 signs and symptoms as the use case, 2) a\ncommunity-driven ruleset composing platform, and 3) a synthetic text data\ngeneration workflow to generate texts for information extraction tasks without\ninvolving human subjects. The corpora were derived from texts from three\ndifferent institutions (Mayo Clinic, University of Kentucky, University of\nMinnesota). The gold standard annotations were tested with a single\ninstitution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,\nand 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,\nrespectively. The study as a consortium effort of the N3C NLP subgroup\ndemonstrates the feasibility of creating a federated NLP algorithm development\nand benchmarking platform to enhance multi-institution clinical NLP study and\nadoption. Although we use COVID-19 as a use case in this effort, our framework\nis general enough to be applied to other domains of interest in clinical NLP.\n","authors":["Sijia Liu","Andrew Wen","Liwei Wang","Huan He","Sunyang Fu","Robert Miller","Andrew Williams","Daniel Harris","Ramakanth Kavuluru","Mei Liu","Noor Abu-el-rub","Dalton Schutte","Rui Zhang","Masoud Rouhizadeh","John D. Osborne","Yongqun He","Umit Topaloglu","Stephanie S Hong","Joel H Saltz","Thomas Schaffter","Emily Pfaff","Christopher G. Chute","Tim Duong","Melissa A. Haendel","Rafael Fuentes","Peter Szolovits","Hua Xu","Hongfang Liu","National COVID Cohort Collaborative","Natural Language Processing"," Subgroup","National COVID Cohort Collaborative"],"pdf_url":"https://arxiv.org/pdf/2110.10780v3.pdf","comment":"update on contents"},{"id":"http://arxiv.org/abs/2203.12593v1","updated":"2022-03-21T04:55:21Z","published":"2022-03-21T04:55:21Z","title":"Semantic Similarity Computing for Scientific Academic Conferences fused\n  with domain features","summary":"  Aiming at the problem that the current general-purpose semantic text\nsimilarity calculation methods are difficult to use the semantic information of\nscientific academic conference data, a semantic similarity calculation\nalgorithm for scientific academic conferences by fusion with domain features is\nproposed. First, the domain feature information of the conference is obtained\nthrough entity recognition and keyword extraction, and it is input into the\nBERT network as a feature and the conference information. The structure of the\nSiamese network is used to solve the anisotropy problem of BERT. The output of\nthe network is pooled and normalized, and finally the cosine similarity is used\nto calculate the similarity between the two sessions. Experimental results show\nthat the SBFD algorithm has achieved good results on different data sets, and\nthe Spearman correlation coefficient has a certain improvement compared with\nthe comparison algorithm.\n","authors":["Runyu Yu","Yawen Li","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2203.12593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12591v1","updated":"2022-03-21T04:26:51Z","published":"2022-03-21T04:26:51Z","title":"Web Page Content Extraction Based on Multi-feature Fusion","summary":"  With the rapid development of Internet technology, people have more and more\naccess to a variety of web page resources. At the same time, the current rapid\ndevelopment of deep learning technology is often inseparable from the huge\namount of Web data resources. On the other hand, NLP is also an important part\nof data processing technology, such as web page data extraction. At present,\nthe extraction technology of web page text mainly uses a single heuristic\nfunction or strategy, and most of them need to determine the threshold\nmanually. With the rapid growth of the number and types of web resources, there\nare still problems to be solved when using a single strategy to extract the\ntext information of different pages. This paper proposes a web page text\nextraction algorithm based on multi-feature fusion. According to the text\ninformation characteristics of web resources, DOM nodes are used as the\nextraction unit to design multiple statistical features, and high-order\nfeatures are designed according to heuristic strategies. This method\nestablishes a small neural network, takes multiple features of DOM nodes as\ninput, predicts whether the nodes contain text information, makes full use of\ndifferent statistical information and extraction strategies, and adapts to more\ntypes of pages. Experimental results show that this method has a good ability\nof web page text extraction and avoids the problem of manually determining the\nthreshold.\n","authors":["Bowen Yu","Junping Du","Yingxia Shao"],"pdf_url":"https://arxiv.org/pdf/2203.12591v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2203.11194v1","updated":"2022-03-21T17:59:50Z","published":"2022-03-21T17:59:50Z","title":"Generating Fast and Slow: Scene Decomposition via Reconstruction","summary":"  We consider the problem of segmenting scenes into constituent entities, i.e.\nunderlying objects and their parts. Current supervised visual detectors though\nimpressive within their training distribution, often fail to segment\nout-of-distribution scenes into their constituent entities. Recent slot-centric\ngenerative models break such dependence on supervision, by attempting to\nsegment scenes into entities unsupervised, by reconstructing pixels. However,\nthey have been restricted thus far to toy scenes as they suffer from a\nreconstruction-segmentation trade-off: as the entity bottleneck gets wider,\nreconstruction improves but then the segmentation collapses. We propose\nGFS-Nets (Generating Fast and Slow Networks) that alleviate this issue with two\ningredients: i) curriculum training in the form of primitives, often missing\nfrom current generative models and, ii) test-time adaptation per scene through\ngradient descent on the reconstruction objective, what we call slow inference,\nmissing from current feed-forward detectors. We show the proposed curriculum\nsuffices to break the reconstruction-segmentation trade-off, and slow inference\ngreatly improves segmentation in out-of-distribution scenes. We evaluate\nGFS-Nets in 3D and 2D scene segmentation benchmarks of PartNet, CLEVR, Room\nDiverse++, and show large ( 50%) performance improvements against SOTA\nsupervised feed-forward detectors and unsupervised object discovery methods\n","authors":["Mihir Prabhudesai","Anirudh Goyal","Deepak Pathak","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2203.11194v1.pdf","comment":"Project website at https://mihirp1998.github.io/project_pages/gfsnets"},{"id":"http://arxiv.org/abs/2110.04541v3","updated":"2022-03-21T17:57:13Z","published":"2021-10-09T11:05:16Z","title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining\n  Example Design","summary":"  Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for pretraining\nexample design indicates new training schemes for self-improving\nrepresentations.\n","authors":["Yoav Levine","Noam Wies","Daniel Jannai","Dan Navon","Yedid Hoshen","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2110.04541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11178v1","updated":"2022-03-21T17:56:12Z","published":"2022-03-21T17:56:12Z","title":"Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance","summary":"  Deep learning has innovated the field of computational imaging. One of its\nbottlenecks is unavailable or insufficient training data. This article reviews\nan emerging paradigm, imaging physics-based data synthesis (IPADS), that can\nprovide huge training data in biomedical magnetic resonance without or with few\nreal data. Following the physical law of magnetic resonance, IPADS generates\nsignals from differential equations or analytical solution models, making the\nlearning more scalable, explainable, and better protecting privacy. Key\ncomponents of IPADS learning, including signal generation models, basic deep\nlearning network structures, enhanced data generation, and learning methods are\ndiscussed. Great potentials of IPADS have been demonstrated by representative\napplications in fast imaging, ultrafast signal reconstruction and accurate\nparameter quantification. Finally, open questions and future work have been\ndiscussed.\n","authors":["Qinqin Yang","Zi Wang","Kunyuan Guo","Congbo Cai","Xiaobo Qu"],"pdf_url":"https://arxiv.org/pdf/2203.11178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11176v1","updated":"2022-03-21T17:55:21Z","published":"2022-03-21T17:55:21Z","title":"One After Another: Learning Incremental Skills for a Changing World","summary":"  Reward-free, unsupervised discovery of skills is an attractive alternative to\nthe bottleneck of hand-designing rewards in environments where task supervision\nis scarce or expensive. However, current skill pre-training methods, like many\nRL techniques, make a fundamental assumption - stationary environments during\ntraining. Traditional methods learn all their skills simultaneously, which\nmakes it difficult for them to both quickly adapt to changes in the\nenvironment, and to not forget earlier skills after such adaptation. On the\nother hand, in an evolving or expanding environment, skill learning must be\nable to adapt fast to new environment situations while not forgetting\npreviously learned skills. These two conditions make it difficult for classic\nskill discovery to do well in an evolving environment. In this work, we propose\na new framework for skill discovery, where skills are learned one after another\nin an incremental fashion. This framework allows newly learned skills to adapt\nto new environment or agent dynamics, while the fixed old skills ensure the\nagent doesn't forget a learned skill. We demonstrate experimentally that in\nboth evolving and static environments, incremental skills significantly\noutperform current state-of-the-art skill discovery methods on both skill\nquality and the ability to solve downstream tasks. Videos for learned skills\nand code are made public on https://notmahi.github.io/disk\n","authors":["Nur Muhammad Shafiullah","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2203.11176v1.pdf","comment":"To be published in The International Conference on Learning\n  Representations (ICLR) 2022"},{"id":"http://arxiv.org/abs/2107.09598v4","updated":"2022-03-21T17:47:37Z","published":"2021-07-20T16:19:39Z","title":"Learning Altruistic Behaviours in Reinforcement Learning without\n  External Rewards","summary":"  Can artificial agents learn to assist others in achieving their goals without\nknowing what those goals are? Generic reinforcement learning agents could be\ntrained to behave altruistically towards others by rewarding them for\naltruistic behaviour, i.e., rewarding them for benefiting other agents in a\ngiven situation. Such an approach assumes that other agents' goals are known so\nthat the altruistic agent can cooperate in achieving those goals. However,\nexplicit knowledge of other agents' goals is often difficult to acquire. In the\ncase of human agents, their goals and preferences may be difficult to express\nfully; they might be ambiguous or even contradictory. Thus, it is beneficial to\ndevelop agents that do not depend on external supervision and learn altruistic\nbehaviour in a task-agnostic manner. We propose to act altruistically towards\nother agents by giving them more choice and allowing them to achieve their\ngoals better. Some concrete examples include opening a door for others or\nsafeguarding them to pursue their objectives without interference. We formalize\nthis concept and propose an altruistic agent that learns to increase the\nchoices another agent has by preferring to maximize the number of states that\nthe other agent can reach in its future. We evaluate our approach in three\ndifferent multi-agent environments where another agent's success depends on\naltruistic behaviour. Finally, we show that our unsupervised agents can perform\ncomparably to agents explicitly trained to work cooperatively, in some cases\neven outperforming them.\n","authors":["Tim Franzmeyer","Mateusz Malinowski","João F. Henriques"],"pdf_url":"https://arxiv.org/pdf/2107.09598v4.pdf","comment":"ICLR 2022 Spotlight Presentation"},{"id":"http://arxiv.org/abs/2203.11167v1","updated":"2022-03-21T17:46:35Z","published":"2022-03-21T17:46:35Z","title":"Force-matching Coarse-Graining without Forces","summary":"  Coarse-grained (CG) molecular simulations have become a standard tool to\nstudy molecular processes on time-~and length-scales inaccessible to all-atom\nsimulations. Learning CG force fields from all-atom data has mainly relied on\nforce-matching and relative entropy minimization. Force-matching is\nstraightforward to implement but requires the forces on the CG particles to be\nsaved during all-atom simulation, and because these instantaneous forces depend\non all degrees of freedom, they provide a very noisy signal that makes training\nthe CG force field data inefficient. Relative entropy minimization does not\nrequire forces to be saved and is more data-efficient, but requires the CG\nmodel to be re-simulated during the iterative training procedure, which can\nmake the training procedure extremely costly or lead to failure to converge.\nHere we present \\emph{flow-matching}, a new training method for CG force fields\nthat combines the advantages of force-matching and relative entropy\nminimization by leveraging normalizing flows, a generative deep learning\nmethod. Flow-matching first trains a normalizing flow to represent the CG\nprobability density by using relative entropy minimization without suffering\nfrom the re-simulation problem because flows can directly sample from the\nequilibrium distribution they represent. Subsequently, the forces of the flow\nare used to train a CG force field by matching the coarse-grained forces\ndirectly, which is a much easier problem than traditional force-matching as it\ndoes not suffer from the noise problem. Besides not requiring forces,\nflow-matching also outperforms classical force-matching by an order of\nmagnitude in terms of data efficiency and produces CG models that can capture\nthe folding and unfolding of small proteins.\n","authors":["Jonas Köhler","Yaoyi Chen","Andreas Krämer","Cecilia Clementi","Frank Noé"],"pdf_url":"https://arxiv.org/pdf/2203.11167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14846v4","updated":"2022-03-21T17:45:34Z","published":"2021-10-28T02:03:04Z","title":"How Well Does Kohn-Sham Regularizer Work for Weakly Correlated Systems?","summary":"  Kohn-Sham regularizer (KSR) is a differentiable machine learning approach to\nfinding the exchange-correlation functional in Kohn-Sham density functional\ntheory (DFT) that works for strongly correlated systems. Here we test KSR for\nweak correlation. We propose spin-adapted KSR (sKSR) with trainable local,\nsemilocal, and nonlocal approximations found by minimizing density and total\nenergy loss. We assess the atoms-to-molecules generalizability by training on\none-dimensional (1D) H, He, Li, Be, Be$^{++}$ and testing on 1D hydrogen\nchains, LiH, BeH$_2$, and helium hydride complexes. The generalization error\nfrom our semilocal approximation is comparable to other differentiable\napproaches, but our nonlocal functional outperforms any existing machine\nlearning functionals, predicting ground-state energies of test systems with a\nmean absolute error of 2.7 milli-Hartrees.\n","authors":["Bhupalee Kalita","Ryan Pederson","Jielun Chen","Li Li","Kieron Burke"],"pdf_url":"https://arxiv.org/pdf/2110.14846v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.13029v2","updated":"2022-03-21T17:42:14Z","published":"2021-10-25T15:08:07Z","title":"Fair Enough: Searching for Sufficient Measures of Fairness","summary":"  Testing machine learning software for ethical bias has become a pressing\ncurrent concern. In response, recent research has proposed a plethora of new\nfairness metrics, for example, the dozens of fairness metrics in the IBM AIF360\ntoolkit. This raises the question: How can any fairness tool satisfy such a\ndiverse range of goals? While we cannot completely simplify the task of\nfairness testing, we can certainly reduce the problem. This paper shows that\nmany of those fairness metrics effectively measure the same thing. Based on\nexperiments using seven real-world datasets, we find that (a) 26 classification\nmetrics can be clustered into seven groups, and (b) four dataset metrics can be\nclustered into three groups. Further, each reduced set may actually predict\ndifferent things. Hence, it is no longer necessary (or even possible) to\nsatisfy all fairness metrics. In summary, to simplify the fairness testing\nproblem, we recommend the following steps: (1)~determine what type of fairness\nis desirable (and we offer a handful of such types); then (2) lookup those\ntypes in our clusters; then (3) just test for one item per cluster.\n","authors":["Suvodeep Majumder","Joymallya Chakraborty","Gina R. Bai","Kathryn T. Stolee","Tim Menzies"],"pdf_url":"https://arxiv.org/pdf/2110.13029v2.pdf","comment":"8 tables and 1 figure"},{"id":"http://arxiv.org/abs/2107.05680v2","updated":"2022-03-21T17:38:44Z","published":"2021-07-12T18:33:49Z","title":"Hidden Convexity of Wasserstein GANs: Interpretable Generative Models\n  with Closed-Form Solutions","summary":"  Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.\n","authors":["Arda Sahiner","Tolga Ergen","Batu Ozturkler","Burak Bartan","John Pauly","Morteza Mardani","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2107.05680v2.pdf","comment":"Published as paper in ICLR 2022. First two authors contributed\n  equally to this work; 34 pages, 11 figures"},{"id":"http://arxiv.org/abs/2203.11156v1","updated":"2022-03-21T17:34:18Z","published":"2022-03-21T17:34:18Z","title":"Operator Sketching for Deep Unrolling Networks","summary":"  In this work we propose a new paradigm for designing efficient deep unrolling\nnetworks using operator sketching. The deep unrolling networks are currently\nthe state-of-the-art solutions for imaging inverse problems. However, for\nhigh-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI\nimaging, the deep unrolling schemes typically become inefficient both in terms\nof memory and computation, due to the need of computing multiple times the\nhigh-dimensional forward and adjoint operators. Recently researchers have found\nthat such limitations can be partially addressed by stochastic unrolling with\nsubsets of operators, inspired by the success of stochastic first-order\noptimization. In this work, we propose a further acceleration upon stochastic\nunrolling, using sketching techniques to approximate products in the\nhigh-dimensional image space. The operator sketching can be jointly applied\nwith stochastic unrolling for the best acceleration and compression\nperformance. Our numerical experiments on X-ray CT image reconstruction\ndemonstrate the remarkable effectiveness of our sketched unrolling schemes.\n","authors":["Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2203.11156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11147v1","updated":"2022-03-21T17:26:29Z","published":"2022-03-21T17:26:29Z","title":"Teaching language models to support answers with verified quotes","summary":"  Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.\n","authors":["Jacob Menick","Maja Trebacz","Vladimir Mikulik","John Aslanides","Francis Song","Martin Chadwick","Mia Glaese","Susannah Young","Lucy Campbell-Gillingham","Geoffrey Irving","Nat McAleese"],"pdf_url":"https://arxiv.org/pdf/2203.11147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11146v1","updated":"2022-03-21T17:25:09Z","published":"2022-03-21T17:25:09Z","title":"Multispectral Satellite Data Classification using Soft Computing\n  Approach","summary":"  A satellite image is a remotely sensed image data, where each pixel\nrepresents a specific location on earth. The pixel value recorded is the\nreflection radiation from the earth's surface at that location. Multispectral\nimages are those that capture image data at specific frequencies across the\nelectromagnetic spectrum as compared to Panchromatic images which are sensitive\nto all wavelength of visible light. Because of the high resolution and high\ndimensions of these images, they create difficulties for clustering techniques\nto efficiently detect clusters of different sizes, shapes and densities as a\ntrade off for fast processing time. In this paper we propose a grid-density\nbased clustering technique for identification of objects. We also introduce an\napproach to classify a satellite image data using a rule induction based\nmachine learning algorithm. The object identification and classification\nmethods have been validated using several synthetic and benchmark datasets.\n","authors":["Purbarag Pathak Choudhury","Ujjal Kr Dutta","Dhruba Kr Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2203.11146v1.pdf","comment":"Proc. of International Conference on Advances in Communication,\n  Network, and Computing (CNC), 2014"},{"id":"http://arxiv.org/abs/2203.11141v1","updated":"2022-03-21T17:18:43Z","published":"2022-03-21T17:18:43Z","title":"Can we integrate spatial verification methods into neural-network loss\n  functions for atmospheric science?","summary":"  In the last decade, much work in atmospheric science has focused on spatial\nverification (SV) methods for gridded prediction, which overcome serious\ndisadvantages of pixelwise verification. However, neural networks (NN) in\natmospheric science are almost always trained to optimize pixelwise loss\nfunctions, even when ultimately assessed with SV methods. This establishes a\ndisconnect between model verification during vs. after training. To address\nthis issue, we develop spatially enhanced loss functions (SELF) and demonstrate\ntheir use for a real-world problem: predicting the occurrence of thunderstorms\n(henceforth, \"convection\") with NNs. In each SELF we use either a neighbourhood\nfilter, which highlights convection at scales larger than a threshold, or a\nspectral filter (employing Fourier or wavelet decomposition), which is more\nflexible and highlights convection at scales between two thresholds. We use\nthese filters to spatially enhance common verification scores, such as the\nBrier score. We train each NN with a different SELF and compare their\nperformance at many scales of convection, from discrete storm cells to tropical\ncyclones. Among our many findings are that (a) for a low (high) risk threshold,\nthe ideal SELF focuses on small (large) scales; (b) models trained with a\npixelwise loss function perform surprisingly well; (c) however, models trained\nwith a spectral filter produce better-calibrated probabilities than a pixelwise\nmodel. We provide a general guide to using SELFs, including technical\nchallenges and the final Python code, as well as demonstrating their use for\nthe convection problem. To our knowledge this is the most in-depth guide to\nSELFs in the geosciences.\n","authors":["Ryan Lagerquist","Imme Ebert-Uphoff"],"pdf_url":"https://arxiv.org/pdf/2203.11141v1.pdf","comment":"48 pages + 6 pages of supplemental material, 15 figures, 3 tables,\n  submitted to Artificial Intelligence for Earth Systems"},{"id":"http://arxiv.org/abs/2203.09590v2","updated":"2022-03-21T17:13:28Z","published":"2022-03-17T20:08:25Z","title":"Enhanced Temporal Knowledge Embeddings with Contextualized Language\n  Representations","summary":"  With the emerging research effort to integrate structured and unstructured\nknowledge, many approaches incorporate factual knowledge into pre-trained\nlanguage models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP\ntasks. However, (1) they only consider static factual knowledge, but knowledge\ngraphs (KGs) also contain temporal facts or events indicating evolutionary\nrelationships among entities at different timestamps. (2) PLMs cannot be\ndirectly applied to many KG tasks, such as temporal KG completion.\n  In this paper, we focus on \\textbf{e}nhancing temporal knowledge embeddings\nwith \\textbf{co}ntextualized \\textbf{la}nguage representations (ECOLA). We\nalign structured knowledge contained in temporal knowledge graphs with their\ntextual descriptions extracted from news articles and propose a novel\nknowledge-text prediction task to inject the abundant information from\ndescriptions into temporal knowledge embeddings. ECOLA jointly optimizes the\nknowledge-text prediction objective and the temporal knowledge embeddings,\nwhich can simultaneously take full advantage of textual and knowledge\ninformation. For training ECOLA, we introduce three temporal KG datasets with\naligned textual descriptions. Experimental results on the temporal knowledge\ngraph completion task show that ECOLA outperforms state-of-the-art temporal KG\nmodels by a large margin. The proposed datasets can serve as new temporal KG\nbenchmarks and facilitate future research on structured and unstructured\nknowledge integration.\n","authors":["Zhen Han","Ruotong Liao","Beiyan Liu","Yao Zhang","Zifeng Ding","Heinz Köppl","Hinrich Schütze","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2203.09590v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2111.04131v2","updated":"2022-03-21T17:11:48Z","published":"2021-11-07T17:15:57Z","title":"Plumber: Diagnosing and Removing Performance Bottlenecks in Machine\n  Learning Data Pipelines","summary":"  Input pipelines, which ingest and transform input data, are an essential part\nof training Machine Learning (ML) models. However, it is challenging to\nimplement efficient input pipelines, as it requires reasoning about\nparallelism, asynchrony, and variability in fine-grained profiling information.\nOur analysis of over two million ML jobs in Google datacenters reveals that a\nsignificant fraction of model training jobs could benefit from faster input\ndata pipelines. At the same time, our analysis indicates that most jobs do not\nsaturate host hardware, pointing in the direction of software-based\nbottlenecks. Motivated by these findings, we propose Plumber, a tool for\nfinding bottlenecks in ML input pipelines. Plumber uses an extensible and\ninterpretable operational analysis analytical model to automatically tune\nparallelism, prefetching, and caching under host resource constraints. Across\nfive representative ML pipelines, Plumber obtains speedups of up to 47x for\nmisconfigured pipelines. By automating caching, Plumber obtains end-to-end\nspeedups of over 50% compared to state-of-the-art tuners.\n","authors":["Michael Kuchnik","Ana Klimovic","Jiri Simsa","Virginia Smith","George Amvrosiadis"],"pdf_url":"https://arxiv.org/pdf/2111.04131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.01499v3","updated":"2022-03-21T17:08:11Z","published":"2021-03-02T06:36:31Z","title":"Demystifying Batch Normalization in ReLU Networks: Equivalent Convex\n  Optimization Models and Implicit Regularization","summary":"  Batch Normalization (BN) is a commonly used technique to accelerate and\nstabilize training of deep neural networks. Despite its empirical success, a\nfull theoretical understanding of BN is yet to be developed. In this work, we\nanalyze BN through the lens of convex optimization. We introduce an analytic\nframework based on convex duality to obtain exact convex representations of\nweight-decay regularized ReLU networks with BN, which can be trained in\npolynomial-time. Our analyses also show that optimal layer weights can be\nobtained as simple closed-form formulas in the high-dimensional and/or\noverparameterized regimes. Furthermore, we find that Gradient Descent provides\nan algorithmic bias effect on the standard non-convex BN network, and we design\nan approach to explicitly encode this implicit regularization into the convex\nobjective. Experiments with CIFAR image classification highlight the\neffectiveness of this explicit regularization for mimicking and substantially\nimproving the performance of standard BN networks.\n","authors":["Tolga Ergen","Arda Sahiner","Batu Ozturkler","John Pauly","Morteza Mardani","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2103.01499v3.pdf","comment":"Accepted to ICLR 2022. First two authors contributed equally to this\n  work; 36 pages, 13 figures"},{"id":"http://arxiv.org/abs/2203.11132v1","updated":"2022-03-21T17:06:22Z","published":"2022-03-21T17:06:22Z","title":"Review of Disentanglement Approaches for Medical Applications -- Towards\n  Solving the Gordian Knot of Generative Models in Healthcare","summary":"  Deep neural networks are commonly used for medical purposes such as image\ngeneration, segmentation, or classification. Besides this, they are often\ncriticized as black boxes as their decision process is often not human\ninterpretable. Encouraging the latent representation of a generative model to\nbe disentangled offers new perspectives of control and interpretability.\nUnderstanding the data generation process could help to create artificial\nmedical data sets without violating patient privacy, synthesizing different\ndata modalities, or discovering data generating characteristics. These\ncharacteristics might unravel novel relationships that can be related to\ngenetic traits or patient outcomes. In this paper, we give a comprehensive\noverview of popular generative models, like Generative Adversarial Networks\n(GANs), Variational Autoencoders (VAEs), and Flow-based Models. Furthermore, we\nsummarize the different notions of disentanglement, review approaches to\ndisentangle latent space representations and metrics to evaluate the degree of\ndisentanglement. After introducing the theoretical frameworks, we give an\noverview of recent medical applications and discuss the impact and importance\nof disentanglement approaches for medical applications.\n","authors":["Jana Fragemann","Lynton Ardizzone","Jan Egger","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2203.11132v1.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2203.11131v1","updated":"2022-03-21T17:05:54Z","published":"2022-03-21T17:05:54Z","title":"Towards Explainable Evaluation Metrics for Natural Language Generation","summary":"  Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics (such as BERTScore or MoverScore) are based on black-box\nlanguage models such as BERT or XLM-R. They often achieve strong correlations\nwith human judgments, but recent research indicates that the lower-quality\nclassical metrics remain dominant, one of the potential reasons being that\ntheir decision processes are transparent. To foster more widespread acceptance\nof the novel high-quality metrics, explainability thus becomes crucial. In this\nconcept paper, we identify key properties and propose key goals of explainable\nmachine translation evaluation metrics. We also provide a synthesizing overview\nover recent approaches for explainable machine translation metrics and discuss\nhow they relate to those goals and properties. Further, we conduct own novel\nexperiments, which (among others) find that current adversarial NLP techniques\nare unsuitable for automatically identifying limitations of high-quality\nblack-box evaluation metrics, as they are not meaning-preserving. Finally, we\nprovide a vision of future approaches to explainable evaluation metrics and\ntheir evaluation. We hope that our work can help catalyze and guide future\nresearch on explainable evaluation metrics and, mediately, also contribute to\nbetter and more transparent text generation systems.\n","authors":["Christoph Leiter","Piyawat Lertvittayakumjorn","Marina Fomicheva","Wei Zhao","Yang Gao","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2203.11131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11130v1","updated":"2022-03-21T17:05:23Z","published":"2022-03-21T17:05:23Z","title":"PACS: A Dataset for Physical Audiovisual CommonSense Reasoning","summary":"  In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, they should be able to reason about the\nphysical world by understanding the physical properties and affordances of\navailable objects, how they can be manipulated, and how they interact with\nother physical objects. This research field of physical commonsense reasoning\nis fundamentally a multi-sensory task since physical properties are manifested\nthrough multiple modalities, two of them being vision and acoustics. Our paper\ntakes a step towards real-world physical commonsense reasoning by contributing\nPACS: the first audiovisual benchmark annotated for physical commonsense\nattributes. PACS contains a total of 13,400 question-answer pairs, involving\n1,377 unique physical commonsense questions and 1,526 videos. Our dataset\nprovides new opportunities to advance the research field of physical reasoning\nby bringing audio as a core component of this multimodal problem. Using PACS,\nwe evaluate multiple state-of-the-art models on this new challenging task.\nWhile some models show promising results (70% accuracy), they all fall short of\nhuman performance (95% accuracy). We conclude the paper by demonstrating the\nimportance of multimodal reasoning and providing possible avenues for future\nresearch.\n","authors":["Samuel Yu","Peter Wu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2203.11130v1.pdf","comment":"38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2108.06227v4","updated":"2022-03-21T16:43:52Z","published":"2021-08-13T13:17:58Z","title":"SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for\n  Semi-Supervised Medical Image Segmentation","summary":"  Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis, enhancement, and registration.\n","authors":["Chenyu You","Yuan Zhou","Ruihan Zhao","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2108.06227v4.pdf","comment":"IEEE Transactions on Medical Imaging (IEEE-TMI) 2022"},{"id":"http://arxiv.org/abs/2203.11113v1","updated":"2022-03-21T16:41:35Z","published":"2022-03-21T16:41:35Z","title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces","summary":"  Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n","authors":["Jia-Xing Zhong","Kaichen Zhou","Qingyong Hu","Bing Wang","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2203.11113v1.pdf","comment":"To appear at CVPR 2022 (Source Code:\n  https://github.com/jx-zhong-for-academic-purpose/Kinet )"},{"id":"http://arxiv.org/abs/2203.11106v1","updated":"2022-03-21T16:32:44Z","published":"2022-03-21T16:32:44Z","title":"FGAN: Federated Generative Adversarial Networks for Anomaly Detection in\n  Network Traffic","summary":"  Over the last two decades, a lot of work has been done in improving network\nsecurity, particularly in intrusion detection systems (IDS) and anomaly\ndetection. Machine learning solutions have also been employed in IDSs to detect\nknown and plausible attacks in incoming traffic. Parameters such as packet\ncontents, sender IP and sender port, connection duration, etc. have been\npreviously used to train these machine learning models to learn to\ndifferentiate genuine traffic from malicious ones. Generative Adversarial\nNetworks (GANs) have been significantly successful in detecting such anomalies,\nmostly attributed to the adversarial training of the generator and\ndiscriminator in an attempt to bypass each other and in turn increase their own\npower and accuracy. However, in large networks having a wide variety of traffic\nat possibly different regions of the network and susceptible to a large number\nof potential attacks, training these GANs for a particular kind of anomaly may\nmake it oblivious to other anomalies and attacks. In addition, the dataset\nrequired to train these models has to be made centrally available and publicly\naccessible, posing the obvious question of privacy of the communications of the\nrespective participants of the network. The solution proposed in this work aims\nat tackling the above two issues by using GANs in a federated architecture in\nnetworks of such scale and capacity. In such a setting, different users of the\nnetwork will be able to train and customize a centrally available adversarial\nmodel according to their own frequently faced conditions. Simultaneously, the\nmember users of the network will also able to gain from the experiences of the\nother users in the network.\n","authors":["Sankha Das"],"pdf_url":"https://arxiv.org/pdf/2203.11106v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2203.11103v1","updated":"2022-03-21T16:30:34Z","published":"2022-03-21T16:30:34Z","title":"Diverse Counterfactual Explanations for Anomaly Detection in Time Series","summary":"  Data-driven methods that detect anomalies in times series data are ubiquitous\nin practice, but they are in general unable to provide helpful explanations for\nthe predictions they make. In this work we propose a model-agnostic algorithm\nthat generates counterfactual ensemble explanations for time series anomaly\ndetection models. Our method generates a set of diverse counterfactual\nexamples, i.e, multiple perturbed versions of the original time series that are\nnot considered anomalous by the detection model. Since the magnitude of the\nperturbations is limited, these counterfactuals represent an ensemble of inputs\nsimilar to the original time series that the model would deem normal. Our\nalgorithm is applicable to any differentiable anomaly detection model. We\ninvestigate the value of our method on univariate and multivariate real-world\ndatasets and two deep-learning-based anomaly detection models, under several\nexplainability criteria previously proposed in other data domains such as\nValidity, Plausibility, Closeness and Diversity. We show that our algorithm can\nproduce ensembles of counterfactual examples that satisfy these criteria and\nthanks to a novel type of visualisation, can convey a richer interpretation of\na model's internal mechanism than existing methods. Moreover, we design a\nsparse variant of our method to improve the interpretability of counterfactual\nexplanations for high-dimensional time series anomalies. In this setting, our\nexplanation is localised on only a few dimensions and can therefore be\ncommunicated more efficiently to the model's user.\n","authors":["Deborah Sulem","Michele Donini","Muhammad Bilal Zafar","Francois-Xavier Aubet","Jan Gasthaus","Tim Januschowski","Sanjiv Das","Krishnaram Kenthapadi","Cedric Archambeau"],"pdf_url":"https://arxiv.org/pdf/2203.11103v1.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2106.11950v2","updated":"2022-03-21T16:28:15Z","published":"2021-06-22T17:48:36Z","title":"Fundamental limits for rank-one matrix estimation with groupwise\n  heteroskedasticity","summary":"  Low-rank matrix recovery problems involving high-dimensional and\nheterogeneous data appear in applications throughout statistics and machine\nlearning. The contribution of this paper is to establish the fundamental limits\nof recovery for a broad class of these problems. In particular, we study the\nproblem of estimating a rank-one matrix from Gaussian observations where\ndifferent blocks of the matrix are observed under different noise levels. In\nthe setting where the number of blocks is fixed while the number of variables\ntends to infinity, we prove asymptotically exact formulas for the minimum\nmean-squared error in estimating both the matrix and underlying factors. These\nresults are based on a novel reduction from the low-rank matrix tensor product\nmodel (with homogeneous noise) to a rank-one model with heteroskedastic noise.\n  As an application of our main result, we show that recently proposed methods\nbased on applying principal component analysis (PCA) to weighted combinations\nof the data are optimal in some settings but sub-optimal in others. We also\nprovide numerical results comparing our asymptotic formulas with the\nperformance of methods based on weighted PCA, gradient descent, and approximate\nmessage passing.\n","authors":["Joshua K. Behne","Galen Reeves"],"pdf_url":"https://arxiv.org/pdf/2106.11950v2.pdf","comment":"27 pages, 4 figures. To appear in: The 25th International Conference\n  on Artificial Intelligence and Statistics (AISTATS 2022)"},{"id":"http://arxiv.org/abs/2203.11087v1","updated":"2022-03-21T16:07:46Z","published":"2022-03-21T16:07:46Z","title":"Ovid: A Machine Learning Approach for Automated Vandalism Detection in\n  OpenStreetMap","summary":"  OpenStreetMap is a unique source of openly available worldwide map data,\nincreasingly adopted in real-world applications. Vandalism detection in\nOpenStreetMap is critical and remarkably challenging due to the large scale of\nthe dataset, the sheer number of contributors, various vandalism forms, and the\nlack of annotated data to train machine learning algorithms. This paper\npresents Ovid - a novel machine learning method for vandalism detection in\nOpenStreetMap. Ovid relies on a neural network architecture that adopts a\nmulti-head attention mechanism to effectively summarize information indicating\nvandalism from OpenStreetMap changesets. To facilitate automated vandalism\ndetection, we introduce a set of original features that capture changeset,\nuser, and edit information. Our evaluation results on real-world vandalism data\ndemonstrate that the proposed Ovid method outperforms the baselines by 4.7\npercentage points in F1 score.\n","authors":["Nicolas Tempelmeier","Elena Demidova"],"pdf_url":"https://arxiv.org/pdf/2203.11087v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2201.10406"},{"id":"http://arxiv.org/abs/2203.11086v1","updated":"2022-03-21T16:07:42Z","published":"2022-03-21T16:07:42Z","title":"Overcoming Oscillations in Quantization-Aware Training","summary":"  When training neural networks with simulated quantization, we observe that\nquantized weights can, rather unexpectedly, oscillate between two grid-points.\nThe importance of this effect and its impact on quantization-aware training are\nnot well-understood or investigated in literature. In this paper, we delve\ndeeper into the phenomenon of weight oscillations and show that it can lead to\na significant accuracy degradation due to wrongly estimated batch-normalization\nstatistics during inference and increased noise during training. These effects\nare particularly pronounced in low-bit ($\\leq$ 4-bits) quantization of\nefficient networks with depth-wise separable layers, such as MobileNets and\nEfficientNets. In our analysis we investigate several previously proposed\nquantization-aware training (QAT) algorithms and show that most of these are\nunable to overcome oscillations. Finally, we propose two novel QAT algorithms\nto overcome oscillations during training: oscillation dampening and iterative\nweight freezing. We demonstrate that our algorithms achieve state-of-the-art\naccuracy for low-bit (3 & 4 bits) weight and activation quantization of\nefficient architectures, such as MobileNetV2, MobileNetV3, and EfficentNet-lite\non ImageNet.\n","authors":["Markus Nagel","Marios Fournarakis","Yelysei Bondarenko","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2203.11086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11085v1","updated":"2022-03-21T16:06:07Z","published":"2022-03-21T16:06:07Z","title":"Telling Stories from Computational Notebooks: AI-Assisted Presentation\n  Slides Creation for Presenting Data Science Work","summary":"  Creating presentation slides is a critical but time-consuming task for data\nscientists. While researchers have proposed many AI techniques to lift data\nscientists' burden on data preparation and model selection, few have targeted\nthe presentation creation task. Based on the needs identified from a formative\nstudy, this paper presents NB2Slides, an AI system that facilitates users to\ncompose presentations of their data science work. NB2Slides uses deep learning\nmethods as well as example-based prompts to generate slides from computational\nnotebooks, and take users' input (e.g., audience background) to structure the\nslides. NB2Slides also provides an interactive visualization that links the\nslides with the notebook to help users further edit the slides. A follow-up\nuser evaluation with 12 data scientists shows that participants believed\nNB2Slides can improve efficiency and reduces the complexity of creating slides.\nYet, participants questioned the future of full automation and suggested a\nhuman-AI collaboration paradigm.\n","authors":["Chengbo Zheng","Dakuo Wang","April Yi Wang","Xiaojuan Ma"],"pdf_url":"https://arxiv.org/pdf/2203.11085v1.pdf","comment":"Accepted at CHI'2022"},{"id":"http://arxiv.org/abs/2203.11076v1","updated":"2022-03-21T15:55:41Z","published":"2022-03-21T15:55:41Z","title":"Collaborative Learning for Cyberattack Detection in Blockchain Networks","summary":"  This article aims to study intrusion attacks and then develop a novel\ncyberattack detection framework for blockchain networks. Specifically, we first\ndesign and implement a blockchain network in our laboratory. This blockchain\nnetwork will serve two purposes, i.e., generate the real traffic data\n(including both normal data and attack data) for our learning models and\nimplement real-time experiments to evaluate the performance of our proposed\nintrusion detection framework. To the best of our knowledge, this is the first\ndataset that is synthesized in a laboratory for cyberattacks in a blockchain\nnetwork. We then propose a novel collaborative learning model that allows\nefficient deployment in the blockchain network to detect attacks. The main idea\nof the proposed learning model is to enable blockchain nodes to actively\ncollect data, share the knowledge learned from its data, and then exchange the\nknowledge with other blockchain nodes in the network. In this way, we can not\nonly leverage the knowledge from all the nodes in the network but also do not\nneed to gather all raw data for training at a centralized node like\nconventional centralized learning solutions. Such a framework can also avoid\nthe risk of exposing local data's privacy as well as the excessive network\noverhead/congestion. Both intensive simulations and real-time experiments\nclearly show that our proposed collaborative learning-based intrusion detection\nframework can achieve an accuracy of up to 97.7% in detecting attacks.\n","authors":["Tran Viet Khoa","Do Hai Son","Dinh Thai Hoang","Nguyen Linh Trung","Tran Thi Thuy Quynh","Diep N. Nguyen","Nguyen Viet Ha","Eryk Dutkiewicz"],"pdf_url":"https://arxiv.org/pdf/2203.11076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11070v1","updated":"2022-03-21T15:48:13Z","published":"2022-03-21T15:48:13Z","title":"From Concept Drift to Model Degradation: An Overview on\n  Performance-Aware Drift Detectors","summary":"  The dynamicity of real-world systems poses a significant challenge to\ndeployed predictive machine learning (ML) models. Changes in the system on\nwhich the ML model has been trained may lead to performance degradation during\nthe system's life cycle. Recent advances that study non-stationary environments\nhave mainly focused on identifying and addressing such changes caused by a\nphenomenon called concept drift. Different terms have been used in the\nliterature to refer to the same type of concept drift and the same term for\nvarious types. This lack of unified terminology is set out to create confusion\non distinguishing between different concept drift variants. In this paper, we\nstart by grouping concept drift types by their mathematical definitions and\nsurvey the different terms used in the literature to build a consolidated\ntaxonomy of the field. We also review and classify performance-based concept\ndrift detection methods proposed in the last decade. These methods utilize the\npredictive model's performance degradation to signal substantial changes in the\nsystems. The classification is outlined in a hierarchical diagram to provide an\norderly navigation between the methods. We present a comprehensive analysis of\nthe main attributes and strategies for tracking and evaluating the model's\nperformance in the predictive system. The paper concludes by discussing open\nresearch challenges and possible research directions.\n","authors":["Firas Bayram","Bestoun S. Ahmed","Andreas Kassler"],"pdf_url":"https://arxiv.org/pdf/2203.11070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11065v1","updated":"2022-03-21T15:41:20Z","published":"2022-03-21T15:41:20Z","title":"Optimizing Revenue Maximization and Demand Learning in Airline Revenue\n  Management","summary":"  Correctly estimating how demand respond to prices is fundamental for airlines\nwilling to optimize their pricing policy. Under some conditions, these\npolicies, while aiming at maximizing short term revenue, can present too little\nprice variation which may decrease the overall quality of future demand\nforecasting. This problem, known as earning while learning problem, is not\nexclusive to airlines, and it has been investigated by academia and industry in\nrecent years. One of the most promising methods presented in literature\ncombines the revenue maximization and the demand model quality into one single\nobjective function. This method has shown great success in simulation studies\nand real life benchmarks. Nevertheless, this work needs to be adapted to\ncertain constraints that arise in the airline revenue management (RM), such as\nthe need to control the prices of several active flights of a leg\nsimultaneously. In this paper, we adjust this method to airline RM while\nassuming unconstrained capacity. Then, we show that our new algorithm\nefficiently performs price experimentation in order to generate more revenue\nover long horizons than classical methods that seek to maximize revenue only.\n","authors":["Giovanni Gatti Pinheiro","Michael Defoin-Platel","Jean-Charles Regin"],"pdf_url":"https://arxiv.org/pdf/2203.11065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06890v2","updated":"2022-03-21T15:34:03Z","published":"2022-03-14T07:11:20Z","title":"Attention based Memory video portrait matting","summary":"  We proposed a novel trimap free video matting method based on the attention\nmechanism. By the nature of the problem, most existing approaches use either\nmultiple computational expansive modules or complex algorithms to exploit\ntemporal information fully. We designed a temporal aggregation module to\ncompute the temporal coherence between the current frame and its two previous\nframes.\n","authors":["Shufeng Song"],"pdf_url":"https://arxiv.org/pdf/2203.06890v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2110.10942v2","updated":"2022-03-21T15:25:04Z","published":"2021-10-21T07:28:11Z","title":"Generalization of Neural Combinatorial Solvers Through the Lens of\n  Adversarial Robustness","summary":"  End-to-end (geometric) deep learning has seen first successes in\napproximating the solution of combinatorial optimization problems. However,\ngenerating data in the realm of NP-hard/-complete tasks brings practical and\ntheoretical challenges, resulting in evaluation protocols that are too\noptimistic. Specifically, most datasets only capture a simpler subproblem and\nlikely suffer from spurious features. We investigate these effects by studying\nadversarial robustness - a local generalization property - to reveal hard,\nmodel-specific instances and spurious features. For this purpose, we derive\nperturbation models for SAT and TSP. Unlike in other applications, where\nperturbation models are designed around subjective notions of imperceptibility,\nour perturbation models are efficient and sound, allowing us to determine the\ntrue label of perturbed samples without a solver. Surprisingly, with such\nperturbations, a sufficiently expressive neural solver does not suffer from the\nlimitations of the accuracy-robustness trade-off common in supervised learning.\nAlthough such robust solvers exist, we show empirically that the assessed\nneural solvers do not generalize well w.r.t. small perturbations of the problem\ninstance.\n","authors":["Simon Geisler","Johanna Sommer","Jan Schuchardt","Aleksandar Bojchevski","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2110.10942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12041v3","updated":"2022-03-21T15:24:01Z","published":"2022-01-28T11:08:50Z","title":"Rapid determination of protein resonance assignments and\n  three-dimensional structures from raw NMR spectra","summary":"  Nuclear Magnetic Resonance (NMR) spectroscopy is one of the major techniques\nin structural biology with over 11,800 protein structures deposited in the\nProtein Data Bank. NMR can elucidate structures and dynamics of small and\nmedium size proteins in solution, living cells, and solids, but has been\nlimited by the tedious data analysis process. It typically requires weeks or\nmonths of manual work of a trained expert to turn NMR measurements into a\nprotein structure. Automation of this process is an open problem, formulated in\nthe field over 30 years ago. Here, we present a solution to this challenge that\nenables the completely automated analysis of protein NMR data within hours\nafter completing the measurements. Our machine learning-based method, ARTINA,\nuses as input only NMR spectra and the protein sequence, and delivers signal\npositions, resonance assignments, and structures strictly without any human\nintervention. Tested on a 100-protein benchmark comprising 1329\nmultidimensional NMR spectra, ARTINA demonstrated its ability to solve\nstructures with 1.44 {\\AA} median RMSD to the PDB reference and to identify\n91.36% correct NMR resonance assignments. ARTINA can be used by non-experts,\nreducing the effort for a protein assignment or structure determination by NMR\nessentially to the preparation of the sample and the spectra measurements.\n","authors":["Piotr Klukowski","Roland Riek","Peter Güntert"],"pdf_url":"https://arxiv.org/pdf/2201.12041v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11049v1","updated":"2022-03-21T15:14:44Z","published":"2022-03-21T15:14:44Z","title":"Differentiable Duration Modeling for End-to-End Text-to-Speech","summary":"  Parallel text-to-speech (TTS) models have recently enabled fast and\nhighly-natural speech synthesis. However, such models typically require\nexternal alignment models, which are not necessarily optimized for the decoder\nas they are not jointly trained. In this paper, we propose a differentiable\nduration method for learning monotonic alignments between input and output\nsequences. Our method is based on a soft-duration mechanism that optimizes a\nstochastic process in expectation. Using this differentiable duration method, a\ndirect text to waveform TTS model is introduced to produce raw audio as output\ninstead of performing neural vocoding. Our model learns to perform\nhigh-fidelity speech synthesis through a combination of adversarial training\nand matching the total ground-truth duration. Experimental results show that\nour model obtains competitive results while enjoying a much simpler training\npipeline. Audio samples are available online.\n","authors":["Bac Nguyen","Fabien Cardinaux","Stefan Uhlich"],"pdf_url":"https://arxiv.org/pdf/2203.11049v1.pdf","comment":"Submitted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2202.05262v2","updated":"2022-03-21T15:13:09Z","published":"2022-02-10T18:59:54Z","title":"Locating and Editing Factual Knowledge in GPT","summary":"  We investigate the mechanisms underlying factual knowledge recall in\nautoregressive transformer language models. First, we develop a causal\nintervention for identifying neuron activations capable of altering a model's\nfactual predictions. Within large GPT-style models, this reveals two distinct\nsets of neurons that we hypothesize correspond to knowing an abstract fact and\nsaying a concrete word, respectively. This insight inspires the development of\nROME, a novel method for editing facts stored in model weights. For evaluation,\nwe assemble CounterFact, a dataset of over twenty thousand counterfactuals and\ntools to facilitate sensitive measurements of knowledge editing. Using\nCounterFact, we confirm the distinction between saying and knowing neurons, and\nwe find that ROME achieves state-of-the-art performance in knowledge editing\ncompared to other methods. An interactive demo notebook, full code\nimplementation, and the dataset are available at https://rome.baulab.info/.\n","authors":["Kevin Meng","David Bau","Alex Andonian","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2202.05262v2.pdf","comment":"21 pages, 22 figures. Code and data at https://rome.baulab.info/"},{"id":"http://arxiv.org/abs/2201.13299v2","updated":"2022-03-21T15:07:43Z","published":"2022-01-28T13:41:56Z","title":"Directed Weight Neural Networks for Protein Structure Representation\n  Learning","summary":"  A protein performs biological functions by folding to a particular 3D\nstructure. To accurately model the protein structures, both the overall\ngeometric topology and local fine-grained relations between amino acids (e.g.\nside-chain torsion angles and inter-amino-acid orientations) should be\ncarefully considered. In this work, we propose the Directed Weight Neural\nNetwork for better capturing geometric relations among different amino acids.\nExtending a single weight from a scalar to a 3D directed vector, our new\nframework supports a rich set of geometric operations on both classical and\nSO(3)--representation features, on top of which we construct a perceptron unit\nfor processing amino-acid information. In addition, we introduce an equivariant\nmessage passing paradigm on proteins for plugging the directed weight\nperceptrons into existing Graph Neural Networks, showing superior versatility\nin maintaining SO(3)-equivariance at the global scale. Experiments show that\nour network has remarkably better expressiveness in representing geometric\nrelations in comparison to classical neural networks and the (globally)\nequivariant networks. It also achieves state-of-the-art performance on various\ncomputational biology applications related to protein 3D structures.\n","authors":["Jiahan Li","Shitong Luo","Congyue Deng","Chaoran Cheng","Jiaqi Guan","Leonidas Guibas","Jian Peng","Jianzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2201.13299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.07052v2","updated":"2022-03-21T15:02:52Z","published":"2022-01-18T15:25:24Z","title":"Differentially Private Reinforcement Learning with Linear Function\n  Approximation","summary":"  Motivated by the wide adoption of reinforcement learning (RL) in real-world\npersonalized services, where users' sensitive and private information needs to\nbe protected, we study regret minimization in finite-horizon Markov decision\nprocesses (MDPs) under the constraints of differential privacy (DP). Compared\nto existing private RL algorithms that work only on tabular finite-state,\nfinite-actions MDPs, we take the first step towards privacy-preserving learning\nin MDPs with large state and action spaces. Specifically, we consider MDPs with\nlinear function approximation (in particular linear mixture MDPs) under the\nnotion of joint differential privacy (JDP), where the RL agent is responsible\nfor protecting users' sensitive data. We design two private RL algorithms that\nare based on value iteration and policy optimization, respectively, and show\nthat they enjoy sub-linear regret performance while guaranteeing privacy\nprotection. Moreover, the regret bounds are independent of the number of\nstates, and scale at most logarithmically with the number of actions, making\nthe algorithms suitable for privacy protection in nowadays large-scale\npersonalized services. Our results are achieved via a general procedure for\nlearning in linear mixture MDPs under changing regularizers, which not only\ngeneralizes previous results for non-private learning, but also serves as a\nbuilding block for general private reinforcement learning.\n","authors":["Xingyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.07052v2.pdf","comment":"Accepted by Sigmetrics 2022"},{"id":"http://arxiv.org/abs/2201.05585v2","updated":"2022-03-21T14:59:01Z","published":"2022-01-14T18:13:09Z","title":"Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip\n  Connections and Hybrid Learning","summary":"  In this paper we address the challenging problem of domain adaptation in\nLiDAR semantic segmentation. We consider the setting where we have a\nfully-labeled data set from source domain and a target domain with a few\nlabeled and many unlabeled examples. We propose a domain adaption framework\nthat mitigates the issue of domain shift and produces appealing performance on\nthe target domain. To this end, we develop a GAN-based image-to-image\ntranslation engine that has generators with alternating connections, and couple\nit with a state-of-the-art LiDAR semantic segmentation network. Our framework\nis hybrid in nature in the sense that our model learning is composed of\nself-supervision, semi-supervision and unsupervised learning. Extensive\nexperiments on benchmark LiDAR semantic segmentation data sets demonstrate that\nour method achieves superior performance in comparison to strong baselines and\nprior arts.\n","authors":["Eduardo R. Corral-Soto","Mrigank Rochan","Yannis Y. He","Shubhra Aich","Yang Liu","Liu Bingbing"],"pdf_url":"https://arxiv.org/pdf/2201.05585v2.pdf","comment":"1) Introduced Fig 1, 2) Simplified Fig. 2 diagram, 3) Fixed typos in\n  losses, 4) Introduced Fig. 3, 5) Updated evaluation results, included\n  evaluation on SemanticPOSS, 6) Introduced Table 3 - effects on covariance\n  matrix and mean, 7) Updated Fig. 5, 8) Added more references. Improved\n  writing in general, especially the motivation and description of each element\n  and contribution from the method"},{"id":"http://arxiv.org/abs/2110.10593v2","updated":"2022-03-21T14:55:17Z","published":"2021-10-20T14:42:50Z","title":"Progressive Learning for Stabilizing Label Selection in Speech\n  Separation with Mapping-based Method","summary":"  Speech separation has been studied in time domain because of lower latency\nand higher performance compared to time-frequency domain. The masking-based\nmethod has been mostly used in time domain, and the other common method\n(mapping-based) has been inadequately studied. We investigate the use of the\nmapping-based method in the time domain and show that it can perform better on\na large training set than the masking-based method. We also investigate the\nfrequent label-switching problem in permutation invariant training (PIT), which\nresults in suboptimal training because the labels selected by PIT differ across\ntraining epochs. Our experiment results showed that PIT works well in a shallow\nseparation model, and the label switching occurs for a deeper model. We\ninferred that layer decoupling may be the reason for the frequent label\nswitching. Therefore, we propose a training strategy based on progressive\nlearning. This approach significantly reduced inconsistent label assignment\nwithout added computational complexity or training corpus. By combining this\ntraining strategy with the mapping-based method, we significantly improved the\nseparation performance compared to the baseline.\n","authors":["Chenyang Gao","Yue Gu","Ivan Marsic"],"pdf_url":"https://arxiv.org/pdf/2110.10593v2.pdf","comment":"Submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.11034v1","updated":"2022-03-21T14:53:57Z","published":"2022-03-21T14:53:57Z","title":"A new perspective on probabilistic image modeling","summary":"  We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new\nprobabilistic approach for image modeling capable of density estimation,\nsampling and tractable inference. DCGMM instances exhibit a CNN-like layered\nstructure, in which the principal building blocks are convolutional Gaussian\nMixture (cGMM) layers. A key innovation w.r.t. related models like sum-product\nnetworks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer\noptimizes an independent loss function and therefore has an independent\nprobabilistic interpretation. This modular approach permits intervening\ntransformation layers to harness the full spectrum of (potentially\nnon-invertible) mappings available to CNNs, e.g., max-pooling or\nhalf-convolutions. DCGMM sampling and inference are realized by a deep chain of\nhierarchical priors, where a sample generated by a given cGMM layer defines the\nparameters of sampling in the next-lower cGMM layer. For sampling through\nnon-invertible transformation layers, we introduce a new gradient-based\nsharpening technique that exploits redundancy (overlap) in, e.g.,\nhalf-convolutions. DCGMMs can be trained end-to-end by SGD from random initial\nconditions, much like CNNs. We show that DCGMMs compare favorably to several\nrecent PC and SPN models in terms of inference, classification and sampling,\nthe latter particularly for challenging datasets such as SVHN. We provide a\npublic TF2 implementation.\n","authors":["Alexander Gepperth"],"pdf_url":"https://arxiv.org/pdf/2203.11034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.04132v2","updated":"2022-03-21T14:40:26Z","published":"2022-03-08T14:58:41Z","title":"Motron: Multimodal Probabilistic Human Motion Forecasting","summary":"  Autonomous systems and humans are increasingly sharing the same space. Robots\nwork side by side or even hand in hand with humans to balance each other's\nlimitations. Such cooperative interactions are ever more sophisticated. Thus,\nthe ability to reason not just about a human's center of gravity position, but\nalso its granular motion is an important prerequisite for human-robot\ninteraction. Though, many algorithms ignore the multimodal nature of humans or\nneglect uncertainty in their motion forecasts. We present Motron, a multimodal,\nprobabilistic, graph-structured model, that captures human's multimodality\nusing probabilistic methods while being able to output deterministic\nmaximum-likelihood motions and corresponding confidence values for each mode.\nOur model aims to be tightly integrated with the robotic\nplanning-control-interaction loop; outputting physically feasible human motions\nand being computationally efficient. We demonstrate the performance of our\nmodel on several challenging real-world motion forecasting datasets,\noutperforming a wide array of generative/variational methods while providing\nstate-of-the-art single-output motions if required. Both using significantly\nless computational power than state-of-the art algorithms.\n","authors":["Tim Salzmann","Marco Pavone","Markus Ryll"],"pdf_url":"https://arxiv.org/pdf/2203.04132v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2112.04314v2","updated":"2022-03-21T14:40:09Z","published":"2021-12-08T14:42:56Z","title":"A systematic approach to random data augmentation on graph neural\n  networks","summary":"  Random data augmentations (RDAs) are state of the art regarding practical\ngraph neural networks that are provably universal. There is great diversity\nregarding terminology, methodology, benchmarks, and evaluation metrics used\namong existing RDAs. Not only does this make it increasingly difficult for\npractitioners to decide which technique to apply to a given problem, but it\nalso stands in the way of systematic improvements. We propose a new\ncomprehensive framework that captures all previous RDA techniques. On the\ntheoretical side, among other results, we formally prove that under natural\nconditions all instantiations of our framework are universal. On the practical\nside, we develop a method to systematically and automatically train RDAs. This\nin turn enables us to impartially and objectively compare all existing RDAs.\nNew RDAs naturally emerge from our approach, and our experiments demonstrate\nthat they improve the state of the art.\n","authors":["Billy Joe Franks","Markus Anders","Marius Kloft","Pascal Schweitzer"],"pdf_url":"https://arxiv.org/pdf/2112.04314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09436v2","updated":"2022-03-21T14:39:23Z","published":"2022-03-17T16:48:57Z","title":"A Stochastic Halpern Iteration with Variance Reduction for Stochastic\n  Monotone Inclusion Problems","summary":"  We study stochastic monotone inclusion problems, which widely appear in\nmachine learning applications, including robust regression and adversarial\nlearning. We propose novel variants of stochastic Halpern iteration with\nrecursive variance reduction. In the cocoercive -- and more generally\nLipschitz-monotone -- setup, our algorithm attains $\\epsilon$ norm of the\noperator with $\\mathcal{O}(\\frac{1}{\\epsilon^3})$ stochastic operator\nevaluations, which significantly improves over state of the art\n$\\mathcal{O}(\\frac{1}{\\epsilon^4})$ stochastic operator evaluations required\nfor existing monotone inclusion solvers applied to the same problem classes. We\nfurther show how to couple one of the proposed variants of stochastic Halpern\niteration with a scheduled restart scheme to solve stochastic monotone\ninclusion problems with ${\\mathcal{O}}(\\frac{\\log(1/\\epsilon)}{\\epsilon^2})$\nstochastic operator evaluations under additional sharpness or strong\nmonotonicity assumptions. Finally, we argue via reductions between different\nproblem classes that our stochastic oracle complexity bounds are tight up to\nlogarithmic factors in terms of their $\\epsilon$-dependence.\n","authors":["Xufeng Cai","Chaobing Song","Cristóbal Guzmán","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2203.09436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.06349v2","updated":"2022-03-21T14:34:39Z","published":"2021-11-11T17:59:42Z","title":"Unsupervised Part Discovery from Contrastive Reconstruction","summary":"  The goal of self-supervised visual representation learning is to learn\nstrong, transferable image representations, with the majority of research\nfocusing on object or scene level. On the other hand, representation learning\nat part level has received significantly less attention. In this paper, we\npropose an unsupervised approach to object part discovery and segmentation and\nmake three contributions. First, we construct a proxy task through a set of\nobjectives that encourages the model to learn a meaningful decomposition of the\nimage into its parts. Secondly, prior work argues for reconstructing or\nclustering pre-computed features as a proxy to parts; we show empirically that\nthis alone is unlikely to find meaningful parts; mainly because of their low\nresolution and the tendency of classification networks to spatially smear out\ninformation. We suggest that image reconstruction at the level of pixels can\nalleviate this problem, acting as a complementary cue. Lastly, we show that the\nstandard evaluation based on keypoint regression does not correlate well with\nsegmentation quality and thus introduce different metrics, NMI and ARI, that\nbetter characterize the decomposition of objects into parts. Our method yields\nsemantic parts which are consistent across fine-grained but visually distinct\ncategories, outperforming the state of the art on three benchmark datasets.\nCode is available at the project page:\nhttps://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.\n","authors":["Subhabrata Choudhury","Iro Laina","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2111.06349v2.pdf","comment":"NeurIPS 2021. Project page:\n  https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/"},{"id":"http://arxiv.org/abs/1911.04250v4","updated":"2022-03-21T14:32:52Z","published":"2019-11-06T19:20:37Z","title":"Methods for Stabilizing Models across Large Samples of Projects (with\n  case studies on Predicting Defect and Project Health)","summary":"  Despite decades of research, SE lacks widely accepted models (that offer\nprecise quantitative stable predictions) about what factors most influence\nsoftware quality. This paper provides a promising result showing such stable\nmodels can be generated using a new transfer learning framework called\n\"STABILIZER\". Given a tree of recursively clustered projects (using project\nmeta-data), STABILIZER promotes a model upwards if it performs best in the\nlower clusters (stopping when the promoted model performs worse than the models\nseen at a lower level).\n  The number of models found by STABILIZER is minimal: one for defect\nprediction (756 projects) and less than a dozen for project health (1628\nprojects). Hence, via STABILIZER, it is possible to find a few projects which\ncan be used for transfer learning and make conclusions that hold across\nhundreds of projects at a time. Further, the models produced in this manner\noffer predictions that perform as well or better than the prior\nstate-of-the-art.\n  To the best of our knowledge, STABILIZER is order of magnitude faster than\nthe prior state-of-the-art transfer learners which seek to find conclusion\nstability, and these case studies are the largest demonstration of the\ngeneralizability of quantitative predictions of project quality yet reported in\nthe SE literature.\n  In order to support open science, all our scripts and data are online at\nhttps://github.com/Anonymous633671/STABILIZER.\n","authors":["Suvodeep Majumder","Tianpei Xia","Rahul Krishna","Tim Menzies"],"pdf_url":"https://arxiv.org/pdf/1911.04250v4.pdf","comment":"12 pages, 4 figures, 5 Tables"},{"id":"http://arxiv.org/abs/2109.04645v4","updated":"2022-03-21T14:24:12Z","published":"2021-09-10T03:23:06Z","title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented\n  Dialog Systems","summary":"  As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n","authors":["Fei Mi","Yitong Li","Yasheng Wang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2109.04645v4.pdf","comment":"Accepted at AAAI2022"},{"id":"http://arxiv.org/abs/2203.11009v1","updated":"2022-03-21T14:23:18Z","published":"2022-03-21T14:23:18Z","title":"Online Skeleton-based Action Recognition with Continual Spatio-Temporal\n  Graph Convolutional Networks","summary":"  Graph-based reasoning over skeleton data has emerged as a promising approach\nfor human action recognition. However, the application of prior graph-based\nmethods, which predominantly employ whole temporal sequences as their input, to\nthe setting of online inference entails considerable computational redundancy.\nIn this paper, we tackle this issue by reformulating the Spatio-Temporal Graph\nConvolutional Neural Network as a Continual Inference Network, which can\nperform step-by-step predictions in time without repeat frame processing. To\nevaluate our method, we create a continual version of ST-GCN, CoST-GCN,\nalongside two derived methods with different self-attention mechanisms, CoAGCN\nand CoS-TR. We investigate weight transfer strategies and architectural\nmodifications for inference acceleration, and perform experiments on the NTU\nRGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar\npredictive accuracy, we observe up to 109x reduction in time complexity,\non-hardware accelerations of 26x, and reductions in maximum allocated memory of\n52% during online inference.\n","authors":["Lukas Hedegaard","Negar Heidari","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2203.11009v1.pdf","comment":"11 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2203.03106v3","updated":"2022-03-21T14:13:26Z","published":"2022-03-07T02:48:16Z","title":"Differentially Private Federated Learning with Local Regularization and\n  Sparsification","summary":"  User-level differential privacy (DP) provides certifiable privacy guarantees\nto the information that is specific to any user's data in federated learning.\nExisting methods that ensure user-level DP come at the cost of severe accuracy\ndecrease. In this paper, we study the cause of model performance degradation in\nfederated learning under user-level DP guarantee. We find the key to solving\nthis issue is to naturally restrict the norm of local updates before executing\noperations that guarantee DP. To this end, we propose two techniques, Bounded\nLocal Update Regularization and Local Update Sparsification, to increase model\nquality without sacrificing privacy. We provide theoretical analysis on the\nconvergence of our framework and give rigorous privacy guarantees. Extensive\nexperiments show that our framework significantly improves the privacy-utility\ntrade-off over the state-of-the-arts for federated learning with user-level DP\nguarantee.\n","authors":["Anda Cheng","Peisong Wang","Xi Sheryl Zhang","Jian Cheng"],"pdf_url":"https://arxiv.org/pdf/2203.03106v3.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2111.10639v3","updated":"2022-03-21T14:01:09Z","published":"2021-11-20T17:21:16Z","title":"Implicit Acoustic Echo Cancellation for Keyword Spotting and\n  Device-Directed Speech Detection","summary":"  In many speech-enabled human-machine interaction scenarios, user speech can\noverlap with the device playback audio. In these instances, the performance of\ntasks such as keyword-spotting (KWS) and device-directed speech detection (DDD)\ncan degrade significantly. To address this problem, we propose an implicit\nacoustic echo cancellation (iAEC) framework where a neural network is trained\nto exploit the additional information from a reference microphone channel to\nlearn to ignore the interfering signal and improve detection performance. We\nstudy this framework for the tasks of KWS and DDD on, respectively, an\naugmented version of Google Speech Commands v2 and a real-world Alexa device\ndataset. Notably, we show a 56% reduction in false-reject rate for the DDD task\nduring device playback conditions. We also show comparable or superior\nperformance over a strong end-to-end neural echo cancellation + KWS baseline\nfor the KWS task with an order of magnitude less computational requirements.\n","authors":["Samuele Cornell","Thomas Balestri","Thibaud Sénéchal"],"pdf_url":"https://arxiv.org/pdf/2111.10639v3.pdf","comment":"Submitted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2203.10991v1","updated":"2022-03-21T13:59:43Z","published":"2022-03-21T13:59:43Z","title":"Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients","summary":"  In deep learning, fine-grained N:M sparsity reduces the data footprint and\nbandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by\nskipping computation of zero values. So far, it was only used to prune weights.\nWe examine how this method can be used also for activations and their gradients\n(i.e., \"neural gradients\"). To this end, we first establish tensor-level\noptimality criteria. Previous works aimed to minimize the mean-square-error\n(MSE) of each pruned block. We show that while minimization of the MSE works\nfine for pruning the activations, it catastrophically fails for the neural\ngradients. Instead, we show that optimal pruning of the neural gradients\nrequires an unbiased minimum-variance pruning mask. We design such specialized\nmasks, and find that in most cases, 1:2 sparsity is sufficient for training,\nand 2:4 sparsity is usually enough when this is not the case. Further, we\nsuggest combining several such methods together in order to speed up training\neven more. A reference implementation is supplied in\nhttps://github.com/brianchmiel/Act-and-Grad-structured-sparsity.\n","authors":["Brian Chmiel","Itay Hubara","Ron Banner","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2203.10991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10989v1","updated":"2022-03-21T13:55:53Z","published":"2022-03-21T13:55:53Z","title":"Hierarchical autoregressive neural networks for statistical systems","summary":"  It was recently proposed that neural networks could be used to approximate\nmany-dimensional probability distributions that appear e.g. in lattice field\ntheories or statistical mechanics. Subsequently they can be used as variational\napproximators to asses extensive properties of statistical systems, like free\nenergy, and also as neural samplers used in Monte Carlo simulations. The\npractical application of this approach is unfortunately limited by its\nunfavorable scaling both of the numerical cost required for training, and the\nmemory requirements with the system size. This is due to the fact that the\noriginal proposition involved a neural network of width which scaled with the\ntotal number of degrees of freedom, e.g. $L^2$ in case of a two dimensional\n$L\\times L$ lattice. In this work we propose a hierarchical association of\nphysical degrees of freedom, for instance spins, to neurons which replaces it\nwith the scaling with the linear extent $L$ of the system. We demonstrate our\napproach on the two-dimensional Ising model by simulating lattices of various\nsizes up to $128 \\times 128$ spins, with time benchmarks reaching lattices of\nsize $512 \\times 512$. We observe that our proposal improves the quality of\nneural network training, i.e. the approximated probability distribution is\ncloser to the target that could be previously achieved. As a consequence, the\nvariational free energy reaches a value closer to its theoretical expectation\nand, if applied in a Markov Chain Monte Carlo algorithm, the resulting\nautocorrelation time is smaller. Finally, the replacement of a single neural\nnetwork by a hierarchy of smaller networks considerably reduces the memory\nrequirements.\n","authors":["Piotr Białas","Piotr Korcyl","Tomasz Stebel"],"pdf_url":"https://arxiv.org/pdf/2203.10989v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.10983v1","updated":"2022-03-21T13:44:37Z","published":"2022-03-21T13:44:37Z","title":"BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks\n  with Boundary Node Sampling","summary":"  Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art\nmethod for graph-based learning tasks. However, training GCNs at scale is still\nchallenging, hindering both the exploration of more sophisticated GCN\narchitectures and their applications to real-world large graphs. While it might\nbe natural to consider graph partition and distributed training for tackling\nthis challenge, this direction has only been slightly scratched the surface in\nthe previous works due to the limitations of existing designs. In this work, we\nfirst analyze why distributed GCN training is ineffective and identify the\nunderlying cause to be the excessive number of boundary nodes of each\npartitioned subgraph, which easily explodes the memory and communication costs\nfor GCN training. Furthermore, we propose a simple yet effective method dubbed\nBNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and\nscalable distributed GCN training. Experiments and ablation studies\nconsistently validate the effectiveness of BNS-GCN, e.g., boosting the\nthroughput by up to 16.2x and reducing the memory usage by up to 58%, while\nmaintaining a full-graph accuracy. Furthermore, both theoretical and empirical\nanalysis show that BNS-GCN enjoys a better convergence than existing\nsampling-based methods. We believe that our BNS-GCN has opened up a new\nparadigm for enabling GCN training at scale. The code is available at\nhttps://github.com/RICE-EIC/BNS-GCN.\n","authors":["Cheng Wan","Youjie Li","Ang Li","Nam Sung Kim","Yingyan Lin"],"pdf_url":"https://arxiv.org/pdf/2203.10983v1.pdf","comment":"MLSys 2022"},{"id":"http://arxiv.org/abs/2203.10975v1","updated":"2022-03-21T13:35:55Z","published":"2022-03-21T13:35:55Z","title":"GCF: Generalized Causal Forest for Heterogeneous Treatment Effect\n  Estimation in Online Marketplace","summary":"  Uplift modeling is a rapidly growing approach that utilizes machine learning\nand causal inference methods to estimate the heterogeneous treatment effects.\nIt has been widely adopted and applied to online marketplaces to assist\nlarge-scale decision-making in recent years. The existing popular methods, like\nforest-based modeling, either work only for discrete treatments or make\npartially linear or parametric assumptions that may suffer from model\nmisspecification. To alleviate these problems, we extend causal forest (CF)\nwith non-parametric dose-response functions (DRFs) that can be estimated\nlocally using a kernel-based doubly robust estimator. Moreover, we propose a\ndistance-based splitting criterion in the functional space of conditional DRFs\nto capture the heterogeneity for the continuous treatments. We call the\nproposed algorithm generalized causal forest (GCF) as it generalizes the use\ncase of CF to a much broader setup. We show the effectiveness of GCF by\ncomparing it to popular uplift modeling models on both synthetic and real-world\ndatasets. We implement GCF in Spark and successfully deploy it into DiDi's\nreal-time pricing system. Online A/B testing results further validate the\nsuperiority of GCF.\n","authors":["Shu Wan","Chen Zheng","Zhonggen Sun","Mengfan Xu","Xiaoqing Yang","Hongtu Zhu","Jiecheng Guo"],"pdf_url":"https://arxiv.org/pdf/2203.10975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10973v1","updated":"2022-03-21T13:33:37Z","published":"2022-03-21T13:33:37Z","title":"A Local Convergence Theory for the Stochastic Gradient Descent Method in\n  Non-Convex Optimization With Non-isolated Local Minima","summary":"  Non-convex loss functions arise frequently in modern machine learning, and\nfor the theoretical analysis of stochastic optimization methods, the presence\nof non-isolated minima presents a unique challenge that has remained\nunder-explored. In this paper, we study the local convergence of the stochastic\ngradient descent method to non-isolated global minima. Under mild assumptions,\nwe estimate the probability for the iterations to stay near the minima by\nadopting the notion of stochastic stability. After establishing such stability,\nwe present the lower bound complexity in terms of various error criteria for a\ngiven error tolerance $\\epsilon$ and a failure probability $\\gamma$.\n","authors":["Taehee Ko","Xiantao Li"],"pdf_url":"https://arxiv.org/pdf/2203.10973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10949v1","updated":"2022-03-21T13:13:08Z","published":"2022-03-21T13:13:08Z","title":"Optimizing Trajectories for Highway Driving with Offline Reinforcement\n  Learning","summary":"  Implementing an autonomous vehicle that is able to output feasible, smooth\nand efficient trajectories is a long-standing challenge. Several approaches\nhave been considered, roughly falling under two categories: rule-based and\nlearning-based approaches. The rule-based approaches, while guaranteeing safety\nand feasibility, fall short when it comes to long-term planning and\ngeneralization. The learning-based approaches are able to account for long-term\nplanning and generalization to unseen situations, but may fail to achieve\nsmoothness, safety and the feasibility which rule-based approaches ensure.\nHence, combining the two approaches is an evident step towards yielding the\nbest compromise out of both. We propose a Reinforcement Learning-based\napproach, which learns target trajectory parameters for fully autonomous\ndriving on highways. The trained agent outputs continuous trajectory parameters\nbased on which a feasible polynomial-based trajectory is generated and\nexecuted. We compare the performance of our agent against four other highway\ndriving agents. The experiments are conducted in the Sumo simulator, taking\ninto consideration various realistic, dynamically changing highway scenarios,\nincluding surrounding vehicles with different driver behaviors. We demonstrate\nthat our offline trained agent, with randomly collected data, learns to drive\nsmoothly, achieving velocities as close as possible to the desired velocity,\nwhile outperforming the other agents. Code, training data and details available\nat: https://nrgit.informatik.uni-freiburg. de/branka.mirchevska/offline-rl-tp.\n","authors":["Branka Mirchevska","Moritz Werling","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2203.10949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.10382v2","updated":"2022-03-21T13:03:42Z","published":"2021-01-25T20:08:32Z","title":"Curriculum Learning: A Survey","summary":"  Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n","authors":["Petru Soviany","Radu Tudor Ionescu","Paolo Rota","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2101.10382v2.pdf","comment":"Accepted at the International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2006.10160v5","updated":"2022-03-21T13:00:18Z","published":"2020-06-17T21:05:42Z","title":"Matérn Gaussian processes on Riemannian manifolds","summary":"  Gaussian processes are an effective model class for learning unknown\nfunctions, particularly in settings where accurately representing predictive\nuncertainty is of key importance. Motivated by applications in the physical\nsciences, the widely-used Mat\\'ern class of Gaussian processes has recently\nbeen generalized to model functions whose domains are Riemannian manifolds, by\nre-expressing said processes as solutions of stochastic partial differential\nequations. In this work, we propose techniques for computing the kernels of\nthese processes on compact Riemannian manifolds via spectral theory of the\nLaplace-Beltrami operator in a fully constructive manner, thereby allowing them\nto be trained via standard scalable techniques such as inducing point methods.\nWe also extend the generalization from the Mat\\'ern to the widely-used squared\nexponential Gaussian process. By allowing Riemannian Mat\\'ern Gaussian\nprocesses to be trained using well-understood techniques, our work enables\ntheir use in mini-batch, online, and non-conjugate settings, and makes them\nmore accessible to machine learning practitioners.\n","authors":["Viacheslav Borovitskiy","Alexander Terenin","Peter Mostowsky","Marc Peter Deisenroth"],"pdf_url":"https://arxiv.org/pdf/2006.10160v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.08780v2","updated":"2022-03-21T12:57:30Z","published":"2022-01-21T16:53:32Z","title":"Real-Time Seizure Detection using EEG: A Comprehensive Comparison of\n  Recent Approaches under a Realistic Setting","summary":"  Electroencephalogram (EEG) is an important diagnostic test that physicians\nuse to record brain activity and detect seizures by monitoring the signals.\nThere have been several attempts to detect seizures and abnormalities in EEG\nsignals with modern deep learning models to reduce the clinical burden.\nHowever, they cannot be fairly compared against each other as they were tested\nin distinct experimental settings. Also, some of them are not trained in\nreal-time seizure detection tasks, making it hard for on-device applications.\nTherefore in this work, for the first time, we extensively compare multiple\nstate-of-the-art models and signal feature extractors in a real-time seizure\ndetection framework suitable for real-world application, using various\nevaluation metrics including a new one we propose to evaluate more practical\naspects of seizure detection models. Our code is available at\nhttps://github.com/AITRICS/EEG_real_time_seizure_detection.\n","authors":["Kwanhyung Lee","Hyewon Jeong","Seyun Kim","Donghwa Yang","Hoon-Chul Kang","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2201.08780v2.pdf","comment":"Real-Time Seizure Detection with EEG"},{"id":"http://arxiv.org/abs/2111.09489v2","updated":"2022-03-21T12:49:01Z","published":"2021-11-18T02:55:58Z","title":"Data-driven discoveries of Bäcklund transforms and soliton evolution\n  equations via deep neural network learning schemes","summary":"  We introduce a deep neural network learning scheme to learn the B\\\"acklund\ntransforms (BTs) of soliton evolution equations and an enhanced deep learning\nscheme for data-driven soliton equation discovery based on the known BTs,\nrespectively. The first scheme takes advantage of some solution (or soliton\nequation) information to study the data-driven BT of sine-Gordon equation, and\ncomplex and real Miura transforms between the defocusing (focusing) mKdV\nequation and KdV equation, as well as the data-driven mKdV equation discovery\nvia the Miura transforms. The second deep learning scheme uses the\nexplicit/implicit BTs generating the higher-order solitons to train the\ndata-driven discovery of mKdV and sine-Gordon equations, in which the\nhigh-order solution informations are more powerful for the enhanced leaning\nsoliton equations with higher accurates.\n","authors":["Zijian Zhou","Li Wang","Weifang Weng","Zhenya Yan"],"pdf_url":"https://arxiv.org/pdf/2111.09489v2.pdf","comment":"25 pages, 12 figures"},{"id":"http://arxiv.org/abs/2203.10926v1","updated":"2022-03-21T12:44:17Z","published":"2022-03-21T12:44:17Z","title":"3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge\n  Modality Attention","summary":"  Online 3D multi-object tracking (MOT) has witnessed significant research\ninterest in recent years, largely driven by demand from the autonomous systems\ncommunity. However, 3D offline MOT is relatively less explored. Labeling 3D\ntrajectory scene data at a large scale while not relying on high-cost human\nexperts is still an open research question. In this work, we propose Batch3DMOT\nthat follows the tracking-by-detection paradigm and represents real-world\nscenes as directed, acyclic, and category-disjoint tracking graphs that are\nattributed using various modalities such as camera, LiDAR, and radar. We\npresent a multi-modal graph neural network that uses a cross-edge attention\nmechanism mitigating modality intermittence, which translates into sparsity in\nthe graph domain. Additionally, we present attention-weighted convolutions over\nframe-wise k-NN neighborhoods as suitable means to allow information exchange\nacross disconnected graph components. We evaluate our approach using various\nsensor modalities and model configurations on the challenging nuScenes and\nKITTI datasets. Extensive experiments demonstrate that our proposed approach\nyields an overall improvement of 2.8% in the AMOTA score on nuScenes thereby\nsetting a new benchmark for 3D tracking methods and successfully enhances false\npositive filtering.\n","authors":["Martin Buchner","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2203.10926v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.10923v1","updated":"2022-03-21T12:36:12Z","published":"2022-03-21T12:36:12Z","title":"TinyMLOps: Operational Challenges for Widespread Edge AI Adoption","summary":"  Deploying machine learning applications on edge devices can bring clear\nbenefits such as improved reliability, latency and privacy but it also\nintroduces its own set of challenges. Most works focus on the limited\ncomputational resources of edge platforms but this is not the only bottleneck\nstanding in the way of widespread adoption. In this paper we list several other\nchallenges that a TinyML practitioner might need to consider when\noperationalizing an application on edge devices. We focus on tasks such as\nmonitoring and managing the application, common functionality for a MLOps\nplatform, and show how they are complicated by the distributed nature of edge\ndeployment. We also discuss issues that are unique to edge applications such as\nprotecting a model's intellectual property and verifying its integrity.\n","authors":["Sam Leroux","Pieter Simoens","Meelis Lootus","Kartik Kathore","Akshay Sharma"],"pdf_url":"https://arxiv.org/pdf/2203.10923v1.pdf","comment":"4th Workshop on Parallel AI and Systems for the Edge (PAISE2022)\n  paper"},{"id":"http://arxiv.org/abs/2111.12066v2","updated":"2022-03-21T12:36:01Z","published":"2021-11-23T18:27:54Z","title":"Physics Informed Neural Networks for Control Oriented Thermal Modeling\n  of Buildings","summary":"  This paper presents a data-driven modeling approach for developing\ncontrol-oriented thermal models of buildings. These models are developed with\nthe objective of reducing energy consumption costs while controlling the indoor\ntemperature of the building within required comfort limits. To combine the\ninterpretability of white/gray box physics models and the expressive power of\nneural networks, we propose a physics informed neural network approach for this\nmodeling task. Along with measured data and building parameters, we encode the\nneural networks with the underlying physics that governs the thermal behavior\nof these buildings. Thus, realizing a model that is guided by physics, aids in\nmodeling the temporal evolution of room temperature and power consumption as\nwell as the hidden state, i.e., the temperature of building thermal mass for\nsubsequent time steps. The main research contributions of this work are: (1) we\npropose two variants of physics informed neural network architectures for the\ntask of control-oriented thermal modeling of buildings, (2) we show that\ntraining these architectures is data-efficient, requiring less training data\ncompared to conventional, non-physics informed neural networks, and (3) we show\nthat these architectures achieve more accurate predictions than conventional\nneural networks for longer prediction horizons. We test the prediction\nperformance of the proposed architectures using simulated and real-word data to\ndemonstrate (2) and (3) and show that the proposed physics informed neural\nnetwork architectures can be used for this control-oriented modeling problem.\n","authors":["Gargya Gokhale","Bert Claessens","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2111.12066v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2110.13079v2","updated":"2022-03-21T12:28:46Z","published":"2021-10-25T16:17:26Z","title":"Which Model to Trust: Assessing the Influence of Models on the\n  Performance of Reinforcement Learning Algorithms for Continuous Control Tasks","summary":"  The need for algorithms able to solve Reinforcement Learning (RL) problems\nwith few trials has motivated the advent of model-based RL methods. The\nreported performance of model-based algorithms has dramatically increased\nwithin recent years. However, it is not clear how much of the recent progress\nis due to improved algorithms or due to improved models. While different\nmodeling options are available to choose from when applying a model-based\napproach, the distinguishing traits and particular strengths of different\nmodels are not clear. The main contribution of this work lies precisely in\nassessing the model influence on the performance of RL algorithms. A set of\ncommonly adopted models is established for the purpose of model comparison.\nThese include Neural Networks (NNs), ensembles of NNs, two different\napproximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the\nAnchored Ensembling, and Gaussian Processes (GPs). The model comparison is\nevaluated on a suite of continuous control benchmarking tasks. Our results\nreveal that significant differences in model performance do exist. The Concrete\nDropout NN reports persistently superior performance. We summarize these\ndifferences for the benefit of the modeler and suggest that the model choice is\ntailored to the standards required by each specific application.\n","authors":["Giacomo Arcieri","David Wölfle","Eleni Chatzi"],"pdf_url":"https://arxiv.org/pdf/2110.13079v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14855v2","updated":"2022-03-21T12:19:57Z","published":"2021-09-30T05:24:02Z","title":"Learning Material Parameters and Hydrodynamics of Soft Robotic Fish via\n  Differentiable Simulation","summary":"  The high dimensionality of soft mechanisms and the complex physics of\nfluid-structure interactions render the sim2real gap for soft robots\nparticularly challenging. Our framework allows high fidelity prediction of\ndynamic behavior for composite bi-morph bending structures in real hardware to\nmillimeter-accuracy. We address this gap with our differentiable simulation\ntool by learning the material parameters and hydrodynamics of our robots. We\ndemonstrate an experimentally-verified, fast optimization pipeline for learning\nthe material parameters and hydrodynamics from quasi-static and dynamic data\nvia differentiable simulation. Our method identifies physically plausible\nYoung's moduli for various soft silicone elastomers and stiff acetal copolymers\nused in creation of our three different fish robot designs and is compatible\nwith varying internal geometry of the actuators, such as number of air\nchambers. For these robots we provide a differentiable and more robust estimate\nof the thrust force than analytical models and we successfully predict\ndeformation to millimeter accuracy in dynamic experiments under various\nactuation signals. Although we focus on a specific application for underwater\nsoft robots, our framework is applicable to any pneumatically actuated soft\nmechanism. This work presents a prototypical hardware and simulation problem\nsolved using our framework that can be extended straightforwardly to higher\ndimensional parameter inference, learning control policies, and computational\ndesign enabled by its differentiability.\n","authors":["John Z. Zhang","Yu Zhang","Pingchuan Ma","Elvis Nava","Tao Du","Philip Arm","Wojciech Matusik","Robert K. Katzschmann"],"pdf_url":"https://arxiv.org/pdf/2109.14855v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2203.02533v2","updated":"2022-03-21T11:58:20Z","published":"2022-03-04T19:19:41Z","title":"BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive\n  Pseudo Labeling and Informative Active Annotation","summary":"  In this paper, we propose a novel semi-supervised learning (SSL) framework\nnamed BoostMIS that combines adaptive pseudo labeling and informative active\nannotation to unleash the potential of medical image SSL models: (1) BoostMIS\ncan adaptively leverage the cluster assumption and consistency regularization\nof the unlabeled data according to the current learning status. This strategy\ncan adaptively generate one-hot \"hard\" labels converted from task model\npredictions for better task model training. (2) For the unselected unlabeled\nimages with low confidence, we introduce an Active learning (AL) algorithm to\nfind the informative samples as the annotation candidates by exploiting virtual\nadversarial perturbation and model's density-aware entropy. These informative\ncandidates are subsequently fed into the next training cycle for better SSL\nlabel propagation. Notably, the adaptive pseudo-labeling and informative active\nannotation form a learning closed-loop that are mutually collaborative to boost\nmedical image SSL. To verify the effectiveness of the proposed method, we\ncollected a metastatic epidural spinal cord compression (MESCC) dataset that\naims to optimize MESCC diagnosis and classification for improved specialist\nreferral and treatment. We conducted an extensive experimental study of\nBoostMIS on MESCC and another public dataset COVIDx. The experimental results\nverify our framework's effectiveness and generalisability for different medical\nimage datasets with a significant improvement over various state-of-the-art\nmethods.\n","authors":["Wenqiao Zhang","Lei Zhu","James Hallinan","Andrew Makmur","Shengyu Zhang","Qingpeng Cai","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2203.02533v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2110.05283v2","updated":"2022-03-21T11:58:14Z","published":"2021-10-11T13:58:01Z","title":"Phase Collapse in Neural Networks","summary":"  Deep convolutional classifiers linearly separate image classes and improve\naccuracy as depth increases. They progressively reduce the spatial dimension\nwhereas the number of channels grows with depth. Spatial variability is\ntherefore transformed into variability along channels. A fundamental challenge\nis to understand the role of non-linearities together with convolutional\nfilters in this transformation. ReLUs with biases are often interpreted as\nthresholding operators that improve discrimination through sparsity. This paper\ndemonstrates that it is a different mechanism called phase collapse which\neliminates spatial variability while linearly separating classes. We show that\ncollapsing the phases of complex wavelet coefficients is sufficient to reach\nthe classification accuracy of ResNets of similar depths. However, replacing\nthe phase collapses with thresholding operators that enforce sparsity\nconsiderably degrades the performance. We explain these numerical results by\nshowing that the iteration of phase collapses progressively improves separation\nof classes, as opposed to thresholding non-linearities.\n","authors":["Florentin Guth","John Zarka","Stéphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2110.05283v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2203.10905v1","updated":"2022-03-21T11:56:56Z","published":"2022-03-21T11:56:56Z","title":"Self-Imitation Learning from Demonstrations","summary":"  Despite the numerous breakthroughs achieved with Reinforcement Learning (RL),\nsolving environments with sparse rewards remains a challenging task that\nrequires sophisticated exploration. Learning from Demonstrations (LfD) remedies\nthis issue by guiding the agent's exploration towards states experienced by an\nexpert. Naturally, the benefits of this approach hinge on the quality of\ndemonstrations, which are rarely optimal in realistic scenarios. Modern LfD\nalgorithms require meticulous tuning of hyperparameters that control the\ninfluence of demonstrations and, as we show in the paper, struggle with\nlearning from suboptimal demonstrations. To address these issues, we extend\nSelf-Imitation Learning (SIL), a recent RL algorithm that exploits the agent's\npast good experience, to the LfD setup by initializing its replay buffer with\ndemonstrations. We denote our algorithm as SIL from Demonstrations (SILfD). We\nempirically show that SILfD can learn from demonstrations that are noisy or far\nfrom optimal and can automatically adjust the influence of demonstrations\nthroughout the training without additional hyperparameters or handcrafted\nschedules. We also find SILfD superior to the existing state-of-the-art LfD\nalgorithms in sparse environments, especially when demonstrations are highly\nsuboptimal.\n","authors":["Georgiy Pshikhachev","Dmitry Ivanov","Vladimir Egorov","Aleksei Shpilman"],"pdf_url":"https://arxiv.org/pdf/2203.10905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.00264v2","updated":"2022-03-21T11:53:29Z","published":"2021-01-30T16:35:25Z","title":"Atlas Generative Models and Geodesic Interpolation","summary":"  Generative neural networks have a well recognized ability to estimate\nunderlying manifold structure of high dimensional data. However, if a single\nlatent space is used, it is not possible to faithfully represent a manifold\nwith topology different from Euclidean space. In this work we define the\ngeneral class of Atlas Generative Models (AGMs), models with hybrid\ndiscrete-continuous latent space that estimate an atlas on the underlying data\nmanifold together with a partition of unity on the data space. We identify\nexisting examples of models from various popular generative paradigms that fit\ninto this class. Due to the atlas interpretation, ideas from non-linear latent\nspace analysis and statistics, e.g. geodesic interpolation, which has\npreviously only been investigated for models with simply connected latent\nspaces, may be extended to the entire class of AGMs in a natural way. We\nexemplify this by generalizing an algorithm for graph based geodesic\ninterpolation to the setting of AGMs, and verify its performance\nexperimentally.\n","authors":["Jakob Stolberg-Larsen","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2102.00264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06462v2","updated":"2022-03-21T11:51:21Z","published":"2022-03-12T15:34:54Z","title":"Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in\n  Practice","summary":"  Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our code so that\nothers can inspect their models.\n","authors":["Andreas Grivas","Nikolay Bogoychev","Adam Lopez"],"pdf_url":"https://arxiv.org/pdf/2203.06462v2.pdf","comment":"Preprint of conference paper accepted at ACL 2022"},{"id":"http://arxiv.org/abs/2203.08958v2","updated":"2022-03-21T11:34:14Z","published":"2022-03-16T21:40:19Z","title":"On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration\n  of Classifiers","summary":"  Every uncalibrated classifier has a corresponding true calibration map that\ncalibrates its confidence. Deviations of this idealistic map from the identity\nmap reveal miscalibration. Such calibration errors can be reduced with many\npost-hoc calibration methods which fit some family of calibration maps on a\nvalidation dataset. In contrast, evaluation of calibration with the expected\ncalibration error (ECE) on the test set does not explicitly involve fitting.\nHowever, as we demonstrate, ECE can still be viewed as if fitting a family of\nfunctions on the test data. This motivates the fit-on-the-test view on\nevaluation: first, approximate a calibration map on the test data, and second,\nquantify its distance from the identity. Exploiting this view allows us to\nunlock missed opportunities: (1) use the plethora of post-hoc calibration\nmethods for evaluating calibration; (2) tune the number of bins in ECE with\ncross-validation. Furthermore, we introduce: (3) benchmarking on pseudo-real\ndata where the true calibration map can be estimated very precisely; and (4)\nnovel calibration and evaluation methods using new calibration map families PL\nand PL3.\n","authors":["Markus Kängsepp","Kaspar Valk","Meelis Kull"],"pdf_url":"https://arxiv.org/pdf/2203.08958v2.pdf","comment":"ECML-PKDD journal track. Update 1: removed Statements and Declaration\n  section, added a line about source code to Experiments section, fixed a\n  couple of typos"},{"id":"http://arxiv.org/abs/2006.10460v3","updated":"2022-03-21T11:32:53Z","published":"2020-06-18T12:15:37Z","title":"Confident Off-Policy Evaluation and Selection through Self-Normalized\n  Importance Weighting","summary":"  We consider off-policy evaluation in the contextual bandit setting for the\npurpose of obtaining a robust off-policy selection strategy, where the\nselection strategy is evaluated based on the value of the chosen policy in a\nset of proposal (target) policies. We propose a new method to compute a lower\nbound on the value of an arbitrary target policy given some logged data in\ncontextual bandits for a desired coverage. The lower bound is built around the\nso-called Self-normalized Importance Weighting (SN) estimator. It combines the\nuse of a semi-empirical Efron-Stein tail inequality to control the\nconcentration and a new multiplicative (rather than additive) control of the\nbias. The new approach is evaluated on a number of synthetic and real datasets\nand is found to be superior to its main competitors, both in terms of tightness\nof the confidence intervals and the quality of the policies chosen.\n","authors":["Ilja Kuzborskij","Claire Vernade","András György","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2006.10460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.04983v2","updated":"2022-03-21T11:27:11Z","published":"2021-09-10T16:48:24Z","title":"A Neural Tangent Kernel Perspective of Infinite Tree Ensembles","summary":"  In practical situations, the tree ensemble is one of the most popular models\nalong with neural networks. A soft tree is a variant of a decision tree.\nInstead of using a greedy method for searching splitting rules, the soft tree\nis trained using a gradient method in which the entire splitting operation is\nformulated in a differentiable form. Although ensembles of such soft trees have\nbeen used increasingly in recent years, little theoretical work has been done\nto understand their behavior. By considering an ensemble of infinite soft\ntrees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK),\nwhich provides new insights into the behavior of the infinite ensemble of soft\ntrees. Using the TNTK, we theoretically identify several non-trivial\nproperties, such as global convergence of the training, the equivalence of the\noblivious tree structure, and the degeneracy of the TNTK induced by the\ndeepening of the trees.\n","authors":["Ryuichi Kanoh","Mahito Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2109.04983v2.pdf","comment":"Accepted to ICLR 2022"},{"id":"http://arxiv.org/abs/2203.10883v1","updated":"2022-03-21T11:09:34Z","published":"2022-03-21T11:09:34Z","title":"Efficient Algorithms for Extreme Bandits","summary":"  In this paper, we contribute to the Extreme Bandit problem, a variant of\nMulti-Armed Bandits in which the learner seeks to collect the largest possible\nreward. We first study the concentration of the maximum of i.i.d random\nvariables under mild assumptions on the tail of the rewards distributions. This\nanalysis motivates the introduction of Quantile of Maxima (QoMax). The\nproperties of QoMax are sufficient to build an Explore-Then-Commit (ETC)\nstrategy, QoMax-ETC, achieving strong asymptotic guarantees despite its\nsimplicity. We then propose and analyze a more adaptive, anytime algorithm,\nQoMax-SDA, which combines QoMax with a subsampling method recently introduced\nby Baudry et al. (2021). Both algorithms are more efficient than existing\napproaches in two aspects (1) they lead to better empirical performance (2)\nthey enjoy a significant reduction of the memory and time complexities.\n","authors":["Dorian Baudry","Yoan Russac","Emilie Kaufmann"],"pdf_url":"https://arxiv.org/pdf/2203.10883v1.pdf","comment":"Proceedings of the 25 th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2022"},{"id":"http://arxiv.org/abs/2105.15069v3","updated":"2022-03-21T10:53:22Z","published":"2021-05-31T15:55:52Z","title":"On the Consistency of Max-Margin Losses","summary":"  The foundational concept of Max-Margin in machine learning is ill-posed for\noutput spaces with more than two labels such as in structured prediction. In\nthis paper, we show that the Max-Margin loss can only be consistent to the\nclassification task under highly restrictive assumptions on the discrete loss\nmeasuring the error between outputs. These conditions are satisfied by\ndistances defined in tree graphs, for which we prove consistency, thus being\nthe first losses shown to be consistent for Max-Margin beyond the binary\nsetting. We finally address these limitations by correcting the concept of\nMax-Margin and introducing the Restricted-Max-Margin, where the maximization of\nthe loss-augmented scores is maintained, but performed over a subset of the\noriginal domain. The resulting loss is also a generalization of the binary\nsupport vector machine and it is consistent under milder conditions on the\ndiscrete loss.\n","authors":["Alex Nowak-Vila","Alessandro Rudi","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2105.15069v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.05754v3","updated":"2022-03-21T10:32:00Z","published":"2020-12-10T15:39:25Z","title":"Optimal Thompson Sampling strategies for support-aware CVaR bandits","summary":"  In this paper we study a multi-arm bandit problem in which the quality of\neach arm is measured by the Conditional Value at Risk (CVaR) at some level\nalpha of the reward distribution. While existing works in this setting mainly\nfocus on Upper Confidence Bound algorithms, we introduce a new Thompson\nSampling approach for CVaR bandits on bounded rewards that is flexible enough\nto solve a variety of problems grounded on physical resources. Building on a\nrecent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded\nrewards and M-CVTS for multinomial distributions. On the theoretical side, we\nprovide a non-trivial extension of their analysis that enables to theoretically\nbound their CVaR regret minimization performance. Strikingly, our results show\nthat these strategies are the first to provably achieve asymptotic optimality\nin CVaR bandits, matching the corresponding asymptotic lower bounds for this\nsetting. Further, we illustrate empirically the benefit of Thompson Sampling\napproaches both in a realistic environment simulating a use-case in agriculture\nand on various synthetic examples.\n","authors":["Dorian Baudry","Romain Gautron","Emilie Kaufmann","Odalric-Ambryn Maillard"],"pdf_url":"https://arxiv.org/pdf/2012.05754v3.pdf","comment":"Presented at the Thirty-eighth International Conference on Machine\n  Learning (ICML 2021). In this version we refine Lemma 2 and correct its proof\n  (does not change the main theorems)"},{"id":"http://arxiv.org/abs/2203.10858v1","updated":"2022-03-21T10:28:50Z","published":"2022-03-21T10:28:50Z","title":"Multi-class Label Noise Learning via Loss Decomposition and Centroid\n  Estimation","summary":"  In real-world scenarios, many large-scale datasets often contain inaccurate\nlabels, i.e., noisy labels, which may confuse model training and lead to\nperformance degradation. To overcome this issue, Label Noise Learning (LNL) has\nrecently attracted much attention, and various methods have been proposed to\ndesign an unbiased risk estimator to the noise-free dataset to combat such\nlabel noise. Among them, a trend of works based on Loss Decomposition and\nCentroid Estimation (LDCE) has shown very promising performance. However,\nexisting LNL methods based on LDCE are only designed for binary classification,\nand they are not directly extendable to multi-class situations. In this paper,\nwe propose a novel multi-class robust learning method for LDCE, which is termed\n\"MC-LDCE\". Specifically, we decompose the commonly adopted loss (e.g., mean\nsquared loss) function into a label-dependent part and a label-independent\npart, in which only the former is influenced by label noise. Further, by\ndefining a new form of data centroid, we transform the recovery problem of a\nlabel-dependent part to a centroid estimation problem. Finally, by critically\nexamining the mathematical expectation of clean data centroid given the\nobserved noisy set, the centroid can be estimated which helps to build an\nunbiased risk estimator for multi-class learning. The proposed MC-LDCE method\nis general and applicable to different types (i.e., linear and nonlinear) of\nclassification models. The experimental results on five public datasets\ndemonstrate the superiority of the proposed MC-LDCE against other\nrepresentative LNL methods in tackling multi-class label noise problem.\n","authors":["Yongliang Ding","Tao Zhou","Chuang Zhang","Yijing Luo","Juan Tang","Chen Gong"],"pdf_url":"https://arxiv.org/pdf/2203.10858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10856v1","updated":"2022-03-21T10:26:38Z","published":"2022-03-21T10:26:38Z","title":"RGB-Depth Fusion GAN for Indoor Depth Completion","summary":"  The raw depth image captured by the indoor depth sensor usually has an\nextensive range of missing depth values due to inherent limitations such as the\ninability to perceive transparent objects and limited distance range. The\nincomplete depth map burdens many downstream vision tasks, and a rising number\nof depth completion methods have been proposed to alleviate this issue. While\nmost existing methods can generate accurate dense depth maps from sparse and\nuniformly sampled depth maps, they are not suitable for complementing the large\ncontiguous regions of missing depth values, which is common and critical. In\nthis paper, we design a novel two-branch end-to-end fusion network, which takes\na pair of RGB and incomplete depth images as input to predict a dense and\ncompleted depth map. The first branch employs an encoder-decoder structure to\nregress the local dense depth values from the raw depth map, with the help of\nlocal guidance information extracted from the RGB image. In the other branch,\nwe propose an RGB-depth fusion GAN to transfer the RGB image to the\nfine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN\nto propagate the features across the two branches, and we append a confidence\nfusion head to fuse the two outputs of the branches for the final depth map.\nExtensive experiments on NYU-Depth V2 and SUN RGB-D demonstrate that our\nproposed method clearly improves the depth completion performance, especially\nin a more realistic setting of indoor environments with the help of the pseudo\ndepth map.\n","authors":["Haowen Wang","Mingyuan Wang","Zhengping Che","Zhiyuan Xu","Xiuquan Qiao","Mengshi Qi","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2203.10856v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10844v1","updated":"2022-03-21T10:06:16Z","published":"2022-03-21T10:06:16Z","title":"Lean Evolutionary Reinforcement Learning by Multitasking with Importance\n  Sampling","summary":"  Studies have shown evolution strategies (ES) to be a promising approach for\nreinforcement learning (RL) with deep neural networks. However, the issue of\nhigh sample complexity persists in applications of ES to deep RL. In this\npaper, we address the shortcoming of today's methods via a novel\nneuroevolutionary multitasking (NuEMT) algorithm, designed to transfer\ninformation from a set of auxiliary tasks (of short episode length) to the\ntarget (full length) RL task at hand. The artificially generated auxiliary\ntasks allow an agent to update and quickly evaluate policies on shorter time\nhorizons. The evolved skills are then transferred to guide the longer and\nharder task towards an optimal policy. We demonstrate that the NuEMT algorithm\nachieves data-lean evolutionary RL, reducing expensive agent-environment\ninteraction data requirements. Our key algorithmic contribution in this setting\nis to introduce, for the first time, a multitask information transfer mechanism\nbased on the statistical importance sampling technique. In addition, an\nadaptive resource allocation strategy is utilized to assign computational\nresources to auxiliary tasks based on their gleaned usefulness. Experiments on\na range of continuous control tasks from the OpenAI Gym confirm that our\nproposed algorithm is efficient compared to recent ES baselines.\n","authors":["Nick Zhang","Abhishek Gupta","Zefeng Chen","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2203.10844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10837v1","updated":"2022-03-21T09:57:20Z","published":"2022-03-21T09:57:20Z","title":"Multi-class versus One-class classifier in spontaneous speech analysis\n  oriented to Alzheimer Disease diagnosis","summary":"  Most of medical developments require the ability to identify samples that are\nanomalous with respect to a target group or control group, in the sense they\ncould belong to a new, previously unseen class or are not class data. In this\ncase when there are not enough data to train two-class One-class classification\nappear like an available solution. On the other hand non-linear approaches\ncould give very useful information. The aim of our project is to contribute to\nearlier diagnosis of AD and better estimates of its severity by using automatic\nanalysis performed through new biomarkers extracted from speech signal. The\nmethods selected in this case are speech biomarkers oriented to Spontaneous\nSpeech and Emotional Response Analysis. In this approach One-class classifiers\nand two-class classifiers are analyzed. The use of information about outlier\nand Fractal Dimension features improves the system performance.\n","authors":["K. López-de-Ipiña","Marcos Faundez-Zanuy","Jordi Solé-Casals","Fernando Zelarin","Pilar Calvo"],"pdf_url":"https://arxiv.org/pdf/2203.10837v1.pdf","comment":"10 pages, published in International Conference on NONLINEAR SPEECH\n  PROCESSING, NOLISP 2015 jointly organized with the 25th Italian Workshop on\n  Neural Networks, WIRN 2015, held at May 2015, Vietri sul Mare, Salerno, Italy"},{"id":"http://arxiv.org/abs/2203.10833v1","updated":"2022-03-21T09:48:23Z","published":"2022-03-21T09:48:23Z","title":"Hyperbolic Vision Transformers: Combining Improvements in Metric\n  Learning","summary":"  Metric learning aims to learn a highly discriminative model encouraging the\nembeddings of similar classes to be close in the chosen metrics and pushed\napart for dissimilar ones. The common recipe is to use an encoder to extract\nembeddings and a distance-based loss function to match the representations --\nusually, the Euclidean distance is utilized. An emerging interest in learning\nhyperbolic data embeddings suggests that hyperbolic geometry can be beneficial\nfor natural data. Following this line of work, we propose a new\nhyperbolic-based model for metric learning. At the core of our method is a\nvision transformer with output embeddings mapped to hyperbolic space. These\nembeddings are directly optimized using modified pairwise cross-entropy loss.\nWe evaluate the proposed model with six different formulations on four datasets\nachieving the new state-of-the-art performance. The source code is available at\nhttps://github.com/htdt/hyp_metric.\n","authors":["Aleksandr Ermolov","Leyla Mirvakhabova","Valentin Khrulkov","Nicu Sebe","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2203.10833v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10830v1","updated":"2022-03-21T09:46:48Z","published":"2022-03-21T09:46:48Z","title":"Perceptual Features as Markers of Parkinson's Disease: The Issue of\n  Clinical Interpretability","summary":"  Up to 90% of patients with Parkinson's disease (PD) suffer from hypokinetic\ndysathria (HD) which is also manifested in the field of phonation. Clinical\nsigns of HD like monoloudness, monopitch or hoarse voice are usually quantified\nby conventional clinical interpretable features (jitter, shimmer,\nharmonic-to-noise ratio, etc.). This paper provides large and robust insight\ninto perceptual analysis of 5 Czech vowels of 84 PD patients and proves that\ndespite the clinical inexplicability the perceptual features outperform the\nconventional ones, especially in terms of discrimination power (classification\naccuracy ACC = 92 %, sensitivity SEN = 93 %, specificity SPE = 92 %) and\npartial correlation with clinical scores like UPDRS (Unified Parkinson's\ndisease rating scale), MMSE (Mini-mental state examination) or FOG (Freezing of\ngait questionnaire), where p < 0.0001.\n","authors":["Jiri Mekyska","Zdenek Smekal","Zoltan Galaz","Zdenek Mzourek","Irena Rektorova","Marcos Faundez-Zanuy","Karmele Lopez-De-Ipina"],"pdf_url":"https://arxiv.org/pdf/2203.10830v1.pdf","comment":"8 pages, published in International Conference on NONLINEAR SPEECH\n  PROCESSING, NOLISP 2015 jointly organized with the 25th Italian Workshop on\n  Neural Networks, WIRN 2015, held at May 2015, Vietri sul Mare, Salerno, Italy"},{"id":"http://arxiv.org/abs/2107.14742v2","updated":"2022-03-21T09:31:03Z","published":"2021-07-30T16:42:45Z","title":"Connections between Numerical Algorithms for PDEs and Neural Networks","summary":"  We investigate numerous structural connections between numerical algorithms\nfor partial differential equations (PDEs) and neural architectures. Our goal is\nto transfer the rich set of mathematical foundations from the world of PDEs to\nneural networks. Besides structural insights we provide concrete examples and\nexperimental evaluations of the resulting architectures. Using the example of\ngeneralised nonlinear diffusion in 1D, we consider explicit schemes,\nacceleration strategies thereof, implicit schemes, and multigrid approaches. We\nconnect these concepts to residual networks, recurrent neural networks, and\nU-net architectures. Our findings inspire a symmetric residual network design\nwith provable stability guarantees and justify the effectiveness of skip\nconnections in neural networks from a numerical perspective. Moreover, we\npresent U-net architectures that implement multigrid techniques for learning\nefficient solutions of partial differential equation models, and motivate\nuncommon design choices such as trainable nonmonotone activation functions.\nExperimental evaluations show that the proposed architectures save half of the\ntrainable parameters and can thus outperform standard ones with the same model\ncomplexity. Our considerations serve as a basis for explaining the success of\npopular neural architectures and provide a blueprint for developing new\nmathematically well-founded neural building blocks.\n","authors":["Tobias Alt","Karl Schrader","Matthias Augustin","Pascal Peter","Joachim Weickert"],"pdf_url":"https://arxiv.org/pdf/2107.14742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07961v2","updated":"2022-03-21T09:27:31Z","published":"2022-03-15T14:43:16Z","title":"Amortised inference of fractional Brownian motion with linear\n  computational complexity","summary":"  We introduce a simulation-based, amortised Bayesian inference scheme to infer\nthe parameters of random walks. Our approach learns the posterior distribution\nof the walks' parameters with a likelihood-free method. In the first step a\ngraph neural network is trained on simulated data to learn optimized\nlow-dimensional summary statistics of the random walk. In the second step an\ninvertible neural network generates the posterior distribution of the\nparameters from the learnt summary statistics using variational inference. We\napply our method to infer the parameters of the fractional Brownian motion\nmodel from single trajectories. The computational complexity of the amortized\ninference procedure scales linearly with trajectory length, and its precision\nscales similarly to the Cram{\\'e}r-Rao bound over a wide range of lengths. The\napproach is robust to positional noise, and generalizes well to trajectories\nlonger than those seen during training. Finally, we adapt this scheme to show\nthat a finite decorrelation time in the environment can furthermore be inferred\nfrom individual trajectories.\n","authors":["Hippolyte Verdier","François Laurent","Christian Vestergaard","Jean-Baptiste Masson","Alhassan Cassé"],"pdf_url":"https://arxiv.org/pdf/2203.07961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.02390v3","updated":"2022-03-21T09:18:22Z","published":"2020-02-06T17:31:27Z","title":"Regret analysis of the Piyavskii-Shubert algorithm for global Lipschitz\n  optimization","summary":"  We consider the problem of maximizing a non-concave Lipschitz multivariate\nfunction over a compact domain by sequentially querying its (possibly\nperturbed) values. We study a natural algorithm designed originally by\nPiyavskii and Shubert in 1972, for which we prove new bounds on the number of\nevaluations of the function needed to reach or certify a given optimization\naccuracy. Our analysis uses a bandit-optimization viewpoint and solves an open\nproblem from Hansen et al.\\ (1991) by bounding the number of evaluations to\ncertify a given accuracy with a near-optimal sum of packing numbers.\n","authors":["Clément Bouttier","Tommaso Cesari","Mélanie Ducoffe","Sébastien Gerchinovitz"],"pdf_url":"https://arxiv.org/pdf/2002.02390v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10823v1","updated":"2022-03-21T09:16:56Z","published":"2022-03-21T09:16:56Z","title":"Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning","summary":"  Reinforcement learning-based path planning for multi-agent systems of varying\nsize constitutes a research topic with increasing significance as progress in\ndomains such as urban air mobility and autonomous aerial vehicles continues.\nReinforcement learning with continuous state and action spaces is used to train\na policy network that accommodates desirable path planning behaviors and can be\nused for time-critical applications. A Long Short-Term Memory module is\nproposed to encode an unspecified number of states for a varying, indefinite\nnumber of agents. The described training strategies and policy architecture\nlead to a guidance that scales to an infinite number of agents and unlimited\nphysical dimensions, although training takes place at a smaller scale. The\nguidance is implemented on a low-cost, off-the-shelf onboard computer. The\nfeasibility of the proposed approach is validated by presenting flight test\nresults of up to four drones, autonomously navigating collision-free in a\nreal-world environment.\n","authors":["Marc R. Schlichting","Stefan Notter","Walter Fichter"],"pdf_url":"https://arxiv.org/pdf/2203.10823v1.pdf","comment":"For associated source code, see\n  https://github.com/MarcSchlichting/LSTMSpatialEncoding , For associated video\n  of flight test, see https://schlichting.page.link/lstm_flight_test , 17\n  pages, 11 figures"},{"id":"http://arxiv.org/abs/2109.04684v2","updated":"2022-03-21T09:02:53Z","published":"2021-09-10T06:14:53Z","title":"Enhancing Unsupervised Anomaly Detection with Score-Guided Network","summary":"  Anomaly detection plays a crucial role in various real-world applications,\nincluding healthcare and finance systems. Owing to the limited number of\nanomaly labels in these complex systems, unsupervised anomaly detection methods\nhave attracted great attention in recent years. Two major challenges faced by\nthe existing unsupervised methods are: (i) distinguishing between normal and\nabnormal data in the transition field, where normal and abnormal data are\nhighly mixed together; (ii) defining an effective metric to maximize the gap\nbetween normal and abnormal data in a hypothesis space, which is built by a\nrepresentation learner. To that end, this work proposes a novel scoring network\nwith a score-guided regularization to learn and enlarge the anomaly score\ndisparities between normal and abnormal data. With such score-guided strategy,\nthe representation learner can gradually learn more informative representation\nduring the model training stage, especially for the samples in the transition\nfield. We next propose a score-guided autoencoder (SG-AE), incorporating the\nscoring network into an autoencoder framework for anomaly detection, as well as\nother three state-of-the-art models, to further demonstrate the effectiveness\nand transferability of the design. Extensive experiments on both synthetic and\nreal-world datasets demonstrate the state-of-the-art performance of these\nscore-guided models (SGMs).\n","authors":["Zongyuan Huang","Baohua Zhang","Guoqiang Hu","Longyuan Li","Yanyan Xu","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2109.04684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01882v3","updated":"2022-03-21T09:02:21Z","published":"2022-03-03T17:49:40Z","title":"DenseUNets with feedback non-local attention for the segmentation of\n  specular microscopy images of the corneal endothelium with guttae","summary":"  To estimate the corneal endothelial parameters from specular microscopy\nimages depicting cornea guttata (Fuchs dystrophy), we propose a new deep\nlearning methodology that includes a novel attention mechanism named feedback\nnon-local attention (fNLA). Our approach first infers the cell edges, then\nselects the cells that are well detected, and finally applies a postprocessing\nmethod to correct mistakes and provide the binary segmentation from which the\ncorneal parameters are estimated (cell density [ECD], coefficient of variation\n[CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired\nwith a Topcon SP-1P microscope, 500 of which contained guttae. Manual\nsegmentation was performed in all images. We compared the results of different\nnetworks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with\nfNLA provided the best performance, with a mean absolute error of 23.16\n[cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6\ntimes smaller than the error obtained by Topcon's built-in software. Our\napproach handled the cells affected by guttae remarkably well, detecting cell\nedges occluded by small guttae while discarding areas covered by large guttae.\nOverall, the proposed method obtained accurate estimations in extremely\nchallenging specular images.\n","authors":["Juan P. Vigueras-Guillén","Jeroen van Rooij","Bart T. H. van Dooren","Hans G. Lemij","Esma Islamaj","Lucas J. van Vliet","Koenraad A. Vermeer"],"pdf_url":"https://arxiv.org/pdf/2203.01882v3.pdf","comment":"9 pages, 7 figures, 2 tables. Code:\n  https://github.com/jpviguerasguillen/feedback-non-local-attention-fNLA"},{"id":"http://arxiv.org/abs/2203.10808v1","updated":"2022-03-21T09:01:37Z","published":"2022-03-21T09:01:37Z","title":"AnoViT: Unsupervised Anomaly Detection and Localization with Vision\n  Transformer-based Encoder-Decoder","summary":"  Image anomaly detection problems aim to determine whether an image is\nabnormal, and to detect anomalous areas. These methods are actively used in\nvarious fields such as manufacturing, medical care, and intelligent\ninformation. Encoder-decoder structures have been widely used in the field of\nanomaly detection because they can easily learn normal patterns in an\nunsupervised learning environment and calculate a score to identify\nabnormalities through a reconstruction error indicating the difference between\ninput and reconstructed images. Therefore, current image anomaly detection\nmethods have commonly used convolutional encoder-decoders to extract normal\ninformation through the local features of images. However, they are limited in\nthat only local features of the image can be utilized when constructing a\nnormal representation owing to the characteristics of convolution operations\nusing a filter of fixed size. Therefore, we propose a vision transformer-based\nencoder-decoder model, named AnoViT, designed to reflect normal information by\nadditionally learning the global relationship between image patches, which is\ncapable of both image anomaly detection and localization. The proposed approach\nconstructs a feature map that maintains the existing location information of\nindividual patches by using the embeddings of all patches passed through\nmultiple self-attention layers. The proposed AnoViT model performed better than\nthe convolution-based model on three benchmark datasets. In MVTecAD, which is a\nrepresentative benchmark dataset for anomaly localization, it showed improved\nresults on 10 out of 15 classes compared with the baseline. Furthermore, the\nproposed method showed good performance regardless of the class and type of the\nanomalous area when localization results were evaluated qualitatively.\n","authors":["Yunseung Lee","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2203.10808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10800v1","updated":"2022-03-21T08:39:44Z","published":"2022-03-21T08:39:44Z","title":"Graph Neural Networks for Wireless Communications: From Theory to\n  Practice","summary":"  Deep learning-based approaches have been developed to solve challenging\nproblems in wireless communications, leading to promising results. Early\nattempts adopted neural network architectures inherited from applications such\nas computer vision. They often require huge amounts of training samples (i.e.,\npoor generalization), and yield poor performance in large-scale networks (i.e.,\npoor scalability). To resolve these issues, graph neural networks (GNNs) have\nbeen recently adopted, as they can effectively exploit the domain knowledge,\ni.e., the graph topology in wireless communication problems. GNN-based methods\ncan achieve near-optimal performance in large-scale networks and generalize\nwell under different system settings, but the theoretical underpinnings and\ndesign guidelines remain elusive, which may hinder their practical\nimplementations. This paper endeavors to fill both the theoretical and\npractical gaps. For theoretical guarantees, we prove that GNNs achieve\nnear-optimal performance in wireless networks with much fewer training samples\nthan traditional neural architectures. Specifically, to solve an optimization\nproblem on an $n$-node graph (where the nodes may represent users, base\nstations, or antennas), GNNs' generalization error and required number of\ntraining samples are $\\mathcal{O}(n)$ and $\\mathcal{O}(n^2)$ times lower than\nthe unstructured multi-layer perceptrons. For design guidelines, we propose a\nunified framework that is applicable to general design problems in wireless\nnetworks, which includes graph modeling, neural architecture design, and\ntheory-guided performance enhancement. Extensive simulations, which cover a\nvariety of important problems and network settings, verify our theory and\neffectiveness of the proposed design framework.\n","authors":["Yifei Shen","Jun Zhang","S. H. Song","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2203.10800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.00197v2","updated":"2022-03-21T08:29:35Z","published":"2020-04-01T02:09:20Z","title":"Task-adaptive Asymmetric Deep Cross-modal Hashing","summary":"  Supervised cross-modal hashing aims to embed the semantic correlations of\nheterogeneous modality data into the binary hash codes with discriminative\nsemantic labels. Because of its advantages on retrieval and storage efficiency,\nit is widely used for solving efficient cross-modal retrieval. However,\nexisting researches equally handle the different tasks of cross-modal\nretrieval, and simply learn the same couple of hash functions in a symmetric\nway for them. Under such circumstance, the uniqueness of different cross-modal\nretrieval tasks are ignored and sub-optimal performance may be brought.\nMotivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal\nHashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash\nfunctions for two sub-retrieval tasks via simultaneous modality representation\nand asymmetric hash learning. Unlike previous cross-modal hashing approaches,\nour learning framework jointly optimizes semantic preserving that transforms\ndeep features of multimedia data into binary hash codes, and the semantic\nregression which directly regresses query modality representation to explicit\nlabel. With our model, the binary codes can effectively preserve semantic\ncorrelations across different modalities, meanwhile, adaptively capture the\nquery semantics. The superiority of TA-ADCMH is proved on two standard datasets\nfrom many aspects.\n","authors":["Fengling Li","Tong Wang","Lei Zhu","Zheng Zhang","Xinhua Wang"],"pdf_url":"https://arxiv.org/pdf/2004.00197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01518v2","updated":"2022-03-21T08:27:06Z","published":"2021-12-02T18:59:32Z","title":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting","summary":"  Recent progress has shown that large-scale pre-training using contrastive\nimage-text pairs can be a promising alternative for high-quality visual\nrepresentation learning from natural language supervision. Benefiting from a\nbroader source of supervision, this new paradigm exhibits impressive\ntransferability to downstream classification tasks and datasets. However, the\nproblem of transferring the knowledge learned from image-text pairs to more\ncomplex dense prediction tasks has barely been visited. In this work, we\npresent a new framework for dense prediction by implicitly and explicitly\nleveraging the pre-trained knowledge from CLIP. Specifically, we convert the\noriginal image-text matching problem in CLIP to a pixel-text matching problem\nand use the pixel-text score maps to guide the learning of dense prediction\nmodels. By further using the contextual information from the image to prompt\nthe language model, we are able to facilitate our model to better exploit the\npre-trained knowledge. Our method is model-agnostic, which can be applied to\narbitrary dense prediction systems and various pre-trained visual backbones\nincluding both CLIP models and ImageNet pre-trained models. Extensive\nexperiments demonstrate the superior performance of our methods on semantic\nsegmentation, object detection, and instance segmentation tasks. Code is\navailable at https://github.com/raoyongming/DenseCLIP\n","authors":["Yongming Rao","Wenliang Zhao","Guangyi Chen","Yansong Tang","Zheng Zhu","Guan Huang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2112.01518v2.pdf","comment":"Accepted to CVPR2022. Project page:\n  https://denseclip.ivg-research.xyz"},{"id":"http://arxiv.org/abs/2109.13748v2","updated":"2022-03-21T08:17:26Z","published":"2021-09-28T14:07:24Z","title":"Improving Autoencoder Training Performance for Hyperspectral Unmixing\n  with Network Reinitialisation","summary":"  Neural networks, in particular autoencoders, are one of the most promising\nsolutions for unmixing hyperspectral data, i.e. reconstructing the spectra of\nobserved substances (endmembers) and their relative mixing fractions\n(abundances), which is needed for effective hyperspectral analysis and\nclassification. However, as we show in this paper, the training of autoencoders\nfor unmixing is highly dependent on weights initialisation; some sets of\nweights lead to degenerate or low-performance solutions, introducing negative\nbias in the expected performance. In this work, we experimentally investigate\nautoencoders stability as well as network reinitialisation methods based on\ncoefficients of neurons' dead activations. We demonstrate that the proposed\ntechniques have a positive effect on autoencoder training in terms of\nreconstruction, abundances and endmembers errors.\n","authors":["Kamil Książek","Przemysław Głomb","Michał Romaszewski","Michał Cholewa","Bartosz Grabowski","Krisztián Búza"],"pdf_url":"https://arxiv.org/pdf/2109.13748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10789v1","updated":"2022-03-21T08:07:46Z","published":"2022-03-21T08:07:46Z","title":"Domain Generalization by Mutual-Information Regularization with\n  Pre-trained Models","summary":"  Domain generalization (DG) aims to learn a generalized model to an unseen\ntarget domain using only limited source domains. Previous attempts to DG fail\nto learn domain-invariant representations only from the source domains due to\nthe significant domain shifts between training and test domains. Instead, we\nre-formulate the DG objective using mutual information with the oracle model, a\nmodel generalized to any possible domain. We derive a tractable variational\nlower bound via approximating the oracle model by a pre-trained model, called\nMutual Information Regularization with Oracle (MIRO). Our extensive experiments\nshow that MIRO significantly improves the out-of-distribution performance.\nFurthermore, our scaling experiments show that the larger the scale of the\npre-trained model, the greater the performance improvement of MIRO. Source code\nis available at https://github.com/kakaobrain/miro.\n","authors":["Junbum Cha","Kyungjae Lee","Sungrae Park","Sanghyuk Chun"],"pdf_url":"https://arxiv.org/pdf/2203.10789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10031v2","updated":"2022-03-21T08:03:20Z","published":"2021-10-19T14:59:48Z","title":"Online Continual Learning on Class Incremental Blurry Task Configuration\n  with Anytime Inference","summary":"  Despite rapid advances in continual learning, a large body of research is\ndevoted to improving performance in the existing setups. While a handful of\nwork do propose new continual learning setups, they still lack practicality in\ncertain aspects. For better practicality, we first propose a novel continual\nlearning setup that is online, task-free, class-incremental, of blurry task\nboundaries and subject to inference queries at any moment. We additionally\npropose a new metric to better measure the performance of the continual\nlearning methods subject to inference queries at any moment. To address the\nchallenging setup and evaluation protocol, we propose an effective method that\nemploys a new memory management scheme and novel learning techniques. Our\nempirical validation demonstrates that the proposed method outperforms prior\narts by large margins. Code and data splits are available at\nhttps://github.com/naver-ai/i-Blurry.\n","authors":["Hyunseo Koh","Dahyun Kim","Jung-Woo Ha","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2110.10031v2.pdf","comment":"to appear in ICLR2022"},{"id":"http://arxiv.org/abs/2203.10786v1","updated":"2022-03-21T08:01:25Z","published":"2022-03-21T08:01:25Z","title":"Classifications of Skull Fractures using CT Scan Images via CNN with\n  Lazy Learning Approach","summary":"  Classification of skull fracture is a challenging task for both radiologists\nand researchers. Skull fractures result in broken pieces of bone, which can cut\ninto the brain and cause bleeding and other injury types. So it is vital to\ndetect and classify the fracture very early. In real world, often fractures\noccur at multiple sites. This makes it harder to detect the fracture type where\nmany fracture types might summarize a skull fracture. Unfortunately, manual\ndetection of skull fracture and the classification process is time-consuming,\nthreatening a patient's life. Because of the emergence of deep learning, this\nprocess could be automated. Convolutional Neural Networks (CNNs) are the most\nwidely used deep learning models for image categorization because they deliver\nhigh accuracy and outstanding outcomes compared to other models. We propose a\nnew model called SkullNetV1 comprising a novel CNN by taking advantage of CNN\nfor feature extraction and lazy learning approach which acts as a classifier\nfor classification of skull fractures from brain CT images to classify five\nfracture types. Our suggested model achieved a subset accuracy of 88%, an F1\nscore of 93%, the Area Under the Curve (AUC) of 0.89 to 0.98, a Hamming score\nof 92% and a Hamming loss of 0.04 for this seven-class multi-labeled\nclassification.\n","authors":["Md Moniruzzaman Emon","Tareque Rahman Ornob","Moqsadur Rahman"],"pdf_url":"https://arxiv.org/pdf/2203.10786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.03019v2","updated":"2022-03-21T07:49:24Z","published":"2022-01-09T14:14:28Z","title":"Robust and Resource-Efficient Data-Free Knowledge Distillation by\n  Generative Pseudo Replay","summary":"  Data-Free Knowledge Distillation (KD) allows knowledge transfer from a\ntrained neural network (teacher) to a more compact one (student) in the absence\nof original training data. Existing works use a validation set to monitor the\naccuracy of the student over real data and report the highest performance\nthroughout the entire process. However, validation data may not be available at\ndistillation time either, making it infeasible to record the student snapshot\nthat achieved the peak accuracy. Therefore, a practical data-free KD method\nshould be robust and ideally provide monotonically increasing student accuracy\nduring distillation. This is challenging because the student experiences\nknowledge degradation due to the distribution shift of the synthetic data. A\nstraightforward approach to overcome this issue is to store and rehearse the\ngenerated samples periodically, which increases the memory footprint and\ncreates privacy concerns. We propose to model the distribution of the\npreviously observed synthetic samples with a generative network. In particular,\nwe design a Variational Autoencoder (VAE) with a training objective that is\ncustomized to learn the synthetic data representations optimally. The student\nis rehearsed by the generative pseudo replay technique, with samples produced\nby the VAE. Hence knowledge degradation can be prevented without storing any\nsamples. Experiments on image classification benchmarks show that our method\noptimizes the expected value of the distilled model accuracy while eliminating\nthe large memory overhead incurred by the sample-storing methods.\n","authors":["Kuluhan Binici","Shivam Aggarwal","Nam Trung Pham","Karianto Leman","Tulika Mitra"],"pdf_url":"https://arxiv.org/pdf/2201.03019v2.pdf","comment":"Accepted by the Thirty-Sixth AAAI Conference on Artificial\n  Intelligence (AAAI-22)"},{"id":"http://arxiv.org/abs/2203.10778v1","updated":"2022-03-21T07:49:14Z","published":"2022-03-21T07:49:14Z","title":"Delving into the Estimation Shift of Batch Normalization in a Network","summary":"  Batch normalization (BN) is a milestone technique in deep learning. It\nnormalizes the activation using mini-batch statistics during training but the\nestimated population statistics during inference. This paper focuses on\ninvestigating the estimation of population statistics. We define the estimation\nshift magnitude of BN to quantitatively measure the difference between its\nestimated population statistics and expected ones. Our primary observation is\nthat the estimation shift can be accumulated due to the stack of BN in a\nnetwork, which has detriment effects for the test performance. We further find\na batch-free normalization (BFN) can block such an accumulation of estimation\nshift. These observations motivate our design of XBNBlock that replace one BN\nwith BFN in the bottleneck block of residual-style networks. Experiments on the\nImageNet and COCO benchmarks show that XBNBlock consistently improves the\nperformance of different architectures, including ResNet and ResNeXt, by a\nsignificant margin and seems to be more robust to distribution shift.\n","authors":["Lei Huang","Yi Zhou","Tian Wang","Jie Luo","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2203.10778v1.pdf","comment":"Accepted to CVPR 2022. The Code is available at:\n  https://github.com/huangleiBuaa/XBNBlock"},{"id":"http://arxiv.org/abs/2112.01753v2","updated":"2022-03-21T07:26:18Z","published":"2021-12-03T07:19:42Z","title":"Probing Linguistic Information For Logical Inference In Pre-trained\n  Language Models","summary":"  Progress in pre-trained language models has led to a surge of impressive\nresults on downstream tasks for natural language understanding. Recent work on\nprobing pre-trained language models uncovered a wide range of linguistic\nproperties encoded in their contextualized representations. However, it is\nunclear whether they encode semantic knowledge that is crucial to symbolic\ninference methods. We propose a methodology for probing linguistic information\nfor logical inference in pre-trained language model representations. Our\nprobing datasets cover a list of linguistic phenomena required by major\nsymbolic inference systems. We find that (i) pre-trained language models do\nencode several types of linguistic information for inference, but there are\nalso some types of information that are weakly encoded, (ii) language models\ncan effectively learn missing linguistic information through fine-tuning.\nOverall, our findings provide insights into which aspects of linguistic\ninformation for logical inference do language models and their pre-training\nprocedures capture. Moreover, we have demonstrated language models' potential\nas semantic and background knowledge bases for supporting symbolic inference\nmethods.\n","authors":["Zeming Chen","Qiyue Gao"],"pdf_url":"https://arxiv.org/pdf/2112.01753v2.pdf","comment":"AAAI 2022 camera ready version"},{"id":"http://arxiv.org/abs/2203.10769v1","updated":"2022-03-21T07:20:41Z","published":"2022-03-21T07:20:41Z","title":"ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced Datasets","summary":"  Nowadays, many industries have applied classification algorithms to help them\nsolve problems in their business, like finance, medicine, manufacturing\nindustry and so on. However, in real-life scenarios, positive examples only\nmake up a small part of all instances and our datasets suffer from high\nimbalance ratio which leads to poor performance of existing classification\nmodels. To solve this problem, we come up with a bagging ensemble learning\nframework based on an anomaly detection scoring system. We test out that our\nensemble learning model can dramatically improve performance of base estimators\n(e.g. Decision Tree, Multilayer perceptron, KNN) and is more efficient than\nother existing methods under a wide range of imbalance ratio, data scale and\ndata dimension.\n","authors":["Xiayu Liang","Ying Gao","Shanrong Xu"],"pdf_url":"https://arxiv.org/pdf/2203.10769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10761v1","updated":"2022-03-21T07:12:18Z","published":"2022-03-21T07:12:18Z","title":"Decoupled Mixup for Data-efficient Learning","summary":"  Mixup is an efficient data augmentation approach that improves the\ngeneralization of neural networks by smoothing the decision boundary with mixed\ndata. Recently, dynamic mixup methods improve previous static policies (e.g.,\nlinear interpolation) by maximizing discriminative regions or maintaining the\nsalient objects in mixed samples. We notice that The mixed samples from dynamic\npolicies are more separable than the static ones while preventing models from\noverfitting. Inspired by this finding, we first argue that there exists an\nover-smoothing issue in the mixup objective, which focuses on regression the\nmixing ratio instead of identifying discriminative features. We are therefore\nprompted to propose a decoupled mixup (DM) loss that can adaptively mine\ndiscriminative features without losing smoothness. DM enables static mixup\nmethods to achieve comparable performance with dynamic methods while avoiding\nheavy computational overhead. This also leads to an interesting objective\ndesign problem for mixup training that we need to focus not only on smoothing\nthe decision boundaries but also on identifying discriminative features.\nExtensive experiments on supervised and semi-supervised learning benchmarks\nacross seven classification datasets validate the effectiveness of DM by\nequipping with various mixup methods.\n","authors":["Zicheng Liu","Siyuan Li","Ge Wang","Cheng Tan","Lirong Wu","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2203.10761v1.pdf","comment":"The first preprint version, 21 pages. The source code is available at\n  https://github.com/Westlake-AI/openmixup"}],"Multimedia":[{"id":"http://arxiv.org/abs/2203.11130v1","updated":"2022-03-21T17:05:23Z","published":"2022-03-21T17:05:23Z","title":"PACS: A Dataset for Physical Audiovisual CommonSense Reasoning","summary":"  In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, they should be able to reason about the\nphysical world by understanding the physical properties and affordances of\navailable objects, how they can be manipulated, and how they interact with\nother physical objects. This research field of physical commonsense reasoning\nis fundamentally a multi-sensory task since physical properties are manifested\nthrough multiple modalities, two of them being vision and acoustics. Our paper\ntakes a step towards real-world physical commonsense reasoning by contributing\nPACS: the first audiovisual benchmark annotated for physical commonsense\nattributes. PACS contains a total of 13,400 question-answer pairs, involving\n1,377 unique physical commonsense questions and 1,526 videos. Our dataset\nprovides new opportunities to advance the research field of physical reasoning\nby bringing audio as a core component of this multimodal problem. Using PACS,\nwe evaluate multiple state-of-the-art models on this new challenging task.\nWhile some models show promising results (70% accuracy), they all fall short of\nhuman performance (95% accuracy). We conclude the paper by demonstrating the\nimportance of multimodal reasoning and providing possible avenues for future\nresearch.\n","authors":["Samuel Yu","Peter Wu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2203.11130v1.pdf","comment":"38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2203.11113v1","updated":"2022-03-21T16:41:35Z","published":"2022-03-21T16:41:35Z","title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces","summary":"  Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n","authors":["Jia-Xing Zhong","Kaichen Zhou","Qingyong Hu","Bing Wang","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2203.11113v1.pdf","comment":"To appear at CVPR 2022 (Source Code:\n  https://github.com/jx-zhong-for-academic-purpose/Kinet )"},{"id":"http://arxiv.org/abs/2111.04093v2","updated":"2022-03-21T07:01:50Z","published":"2021-11-07T13:55:39Z","title":"Theme Transformer: Symbolic Music Generation with Theme-Conditioned\n  Transformer","summary":"  Attention-based Transformer models have been increasingly employed for\nautomatic music generation. To condition the generation process of such a model\nwith a user-specified sequence, a popular approach is to take that conditioning\nsequence as a priming sequence and ask a Transformer decoder to generate a\ncontinuation. However, this prompt-based conditioning cannot guarantee that the\nconditioning sequence would develop or even simply repeat itself in the\ngenerated continuation. In this paper, we propose an alternative conditioning\napproach, called theme-based conditioning, that explicitly trains the\nTransformer to treat the conditioning sequence as a thematic material that has\nto manifest itself multiple times in its generation result. This is achieved\nwith two main technical contributions. First, we propose a deep learning-based\napproach that uses contrastive representation learning and clustering to\nautomatically retrieve thematic materials from music pieces in the training\ndata. Second, we propose a novel gated parallel attention module to be used in\na sequence-to-sequence (seq2seq) encoder/decoder architecture to more\neffectively account for a given conditioning thematic material in the\ngeneration process of the Transformer decoder. We report on objective and\nsubjective evaluations of variants of the proposed Theme Transformer and the\nconventional prompt-based baseline, showing that our best model can generate,\nto some extent, polyphonic pop piano music with repetition and plausible\nvariations of a given condition.\n","authors":["Yi-Jen Shih","Shih-Lun Wu","Frank Zalkow","Meinard Müller","Yi-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2111.04093v2.pdf","comment":"to be published at IEEE Transactions on Multimedia"}]},"2022-03-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.10675v1","updated":"2022-03-20T23:35:09Z","published":"2022-03-20T23:35:09Z","title":"Mitigating Gender Bias in Machine Translation through Adversarial\n  Learning","summary":"  Machine translation and other NLP systems often contain significant biases\nregarding sensitive attributes, such as gender or race, that worsen system\nperformance and perpetuate harmful stereotypes. Recent preliminary research\nsuggests that adversarial learning can be used as part of a model-agnostic bias\nmitigation method that requires no data modifications. However, adapting this\nstrategy for machine translation and other modern NLP domains requires (1)\nrestructuring training objectives in the context of fine-tuning pretrained\nlarge language models and (2) developing measures for gender or other protected\nvariables for tasks in which these attributes must be deduced from the data\nitself.\n  We present an adversarial learning framework that addresses these challenges\nto mitigate gender bias in seq2seq machine translation. Our framework improves\nthe disparity in translation quality for sentences with male vs. female\nentities by 86% for English-German translation and 91% for English-French\ntranslation, with minimal effect on translation quality. The results suggest\nthat adversarial learning is a promising technique for mitigating gender bias\nin machine translation.\n","authors":["Eve Fleisig","Christiane Fellbaum"],"pdf_url":"https://arxiv.org/pdf/2203.10675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10659v1","updated":"2022-03-20T21:50:35Z","published":"2022-03-20T21:50:35Z","title":"From Stance to Concern: Adaptation of Propositional Analysis to New\n  Tasks and Domains","summary":"  We present a generalized paradigm for adaptation of propositional analysis\n(predicate-argument pairs) to new tasks and domains. We leverage an analogy\nbetween stances (belief-driven sentiment) and concerns (topical issues with\nmoral dimensions/endorsements) to produce an explanatory representation. A key\ncontribution is the combination of semi-automatic resource building for\nextraction of domain-dependent concern types (with 2-4 hours of human labor per\ndomain) and an entirely automatic procedure for extraction of\ndomain-independent moral dimensions and endorsement values. Prudent (automatic)\nselection of terms from propositional structures for lexical expansion (via\nsemantic similarity) produces new moral dimension lexicons at three levels of\ngranularity beyond a strong baseline lexicon. We develop a ground truth (GT)\nbased on expert annotators and compare our concern detection output to GT, to\nyield 231% improvement in recall over baseline, with only a 10% loss in\nprecision. F1 yields 66% improvement over baseline and 97.8% of human\nperformance. Our lexically based approach yields large savings over approaches\nthat employ costly human labor and model building. We provide to the community\na newly expanded moral dimension/value lexicon, annotation guidelines, and GT.\n","authors":["Brodie Mather","Bonnie J Dorr","Adam Dalton","William de Beaumont","Owen Rambow","Sonja M. Schmer-Galunder"],"pdf_url":"https://arxiv.org/pdf/2203.10659v1.pdf","comment":"Accepted to Findings of the Association for Computational\n  Linguistics, 2022"},{"id":"http://arxiv.org/abs/2109.08256v3","updated":"2022-03-20T21:40:38Z","published":"2021-09-17T00:18:28Z","title":"Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis","summary":"  The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups.\nAlong the way, key recommendations are made for responsible AER. The objective\nof the sheet is to facilitate and encourage more thoughtfulness on why to\nautomate, how to automate, and how to judge success well before the building of\nAER systems. Additionally, the sheet acts as a useful introductory document on\nemotion recognition (complementing survey articles).\n","authors":["Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2109.08256v3.pdf","comment":"To Appear in Computational Linguistics, June 2022"},{"id":"http://arxiv.org/abs/2203.10652v1","updated":"2022-03-20T21:22:48Z","published":"2022-03-20T21:22:48Z","title":"Continual Sequence Generation with Adaptive Compositional Modules","summary":"  Continual learning is essential for real-world deployment when there is a\nneed to quickly adapt the model to new tasks without forgetting knowledge of\nold tasks. Existing work on continual sequence generation either always reuses\nexisting parameters to learn new tasks, which is vulnerable to catastrophic\nforgetting on dissimilar tasks, or blindly adds new parameters for every new\ntask, which could prevent knowledge sharing between similar tasks. To get the\nbest of both worlds, in this work, we propose continual sequence generation\nwith adaptive compositional modules to adaptively add modules in transformer\narchitectures and compose both old and new modules for new tasks. We also\nincorporate pseudo experience replay to facilitate knowledge transfer in those\nshared modules. Experiment results on various sequences of generation tasks\nshow that our framework can adaptively add modules or reuse modules based on\ntask similarity, outperforming state-of-the-art baselines in terms of both\nperformance and parameter efficiency. We make our code public at\nhttps://github.com/GT-SALT/Adaptive-Compositional-Modules.\n","authors":["Yanzhe Zhang","Xuezhi Wang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2203.10652v1.pdf","comment":"15 pages, ACL 2022"},{"id":"http://arxiv.org/abs/2203.00236v2","updated":"2022-03-20T21:13:37Z","published":"2022-03-01T05:22:57Z","title":"TRILLsson: Distilled Universal Paralinguistic Speech Representations","summary":"  Recent advances in self-supervision have dramatically improved the quality of\nspeech representations. However, deployment of state-of-the-art embedding\nmodels on devices has been restricted due to their limited public availability\nand large resource footprint. Our work addresses these issues by publicly\nreleasing a collection of paralinguistic speech models that are small and near\nstate-of-the-art performance. Our approach is based on knowledge distillation,\nand our models are distilled on public data only. We explore different\narchitectures and thoroughly evaluate our models on the Non-Semantic Speech\n(NOSS) benchmark. Our largest distilled model is less than 15% the size of the\noriginal model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7\ntasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)\nand achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the\nopen source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model\noutperforms the open source Wav2Vec 2.0 on both emotion recognition tasks\ndespite being 7% the size.\n","authors":["Joel Shor","Subhashini Venugopalan"],"pdf_url":"https://arxiv.org/pdf/2203.00236v2.pdf","comment":"Submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.05598v3","updated":"2022-03-20T20:28:50Z","published":"2022-03-10T19:25:16Z","title":"A new approach to calculating BERTScore for automatic assessment of\n  translation quality","summary":"  The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -> Russian\ndirection. Experiments were performed with a pre-trained Multilingual BERT as\nwell as with a pair of Monolingual BERT models. To align monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nMultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human judgments. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.\n","authors":["A. A. Vetrov","E. A. Gorn"],"pdf_url":"https://arxiv.org/pdf/2203.05598v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.10627v1","updated":"2022-03-20T18:54:05Z","published":"2022-03-20T18:54:05Z","title":"Enriching Unsupervised User Embedding via Medical Concepts","summary":"  Clinical notes in Electronic Health Records (EHR) present rich documented\ninformation of patients to inference phenotype for disease diagnosis and study\npatient characteristics for cohort selection. Unsupervised user embedding aims\nto encode patients into fixed-length vectors without human supervisions.\nMedical concepts extracted from the clinical notes contain rich connections\nbetween patients and their clinical categories. However, existing unsupervised\napproaches of user embeddings from clinical notes do not explicitly incorporate\nmedical concepts. In this study, we propose a concept-aware unsupervised user\nembedding that jointly leverages text documents and medical concepts from two\nclinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both\nextrinsic and intrinsic tasks, including phenotype classification, in-hospital\nmortality prediction, patient retrieval, and patient relatedness. Experiments\non the two clinical corpora show our approach exceeds unsupervised baselines,\nand incorporating medical concepts can significantly improve the baseline\nperformance.\n","authors":["Xiaolei Huang","Franck Dernoncourt","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2203.10627v1.pdf","comment":"accepted at ACM CHIL 2022"},{"id":"http://arxiv.org/abs/2203.10623v1","updated":"2022-03-20T18:41:42Z","published":"2022-03-20T18:41:42Z","title":"Calibration of Machine Reading Systems at Scale","summary":"  In typical machine learning systems, an estimate of the probability of the\nprediction is used to assess the system's confidence in the prediction. This\nconfidence measure is usually uncalibrated; i.e.\\ the system's confidence in\nthe prediction does not match the true probability of the predicted output. In\nthis paper, we present an investigation into calibrating open setting machine\nreading systems such as open-domain question answering and claim verification\nsystems. We show that calibrating such complex systems which contain discrete\nretrieval and deep reading components is challenging and current calibration\ntechniques fail to scale to these settings. We propose simple extensions to\nexisting calibration approaches that allows us to adapt them to these settings.\nOur experimental results reveal that the approach works well, and can be useful\nto selectively predict answers when question answering systems are posed with\nunanswerable or out-of-the-training distribution questions.\n","authors":["Shehzaad Dhuliawala","Leonard Adolphs","Rajarshi Das","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2203.10623v1.pdf","comment":"Accepted at ACL 2022 Findings"},{"id":"http://arxiv.org/abs/2203.10621v1","updated":"2022-03-20T18:37:03Z","published":"2022-03-20T18:37:03Z","title":"Immersive Text Game and Personality Classification","summary":"  We designed and built a game called \\textit{Immersive Text Game}, which\nallows the player to choose a story and a character, and interact with other\ncharacters in the story in an immersive manner of dialogues. The game is based\non several latest models, including text generation language model, information\nextraction model, commonsense reasoning model, and psychology evaluation model.\nIn the past, similar text games usually let players choose from limited actions\ninstead of answering on their own, and not every time what characters said are\ndetermined by the player. Through the combination of these models and elaborate\ngame mechanics and modes, the player will find some novel experiences as driven\nthrough the storyline.\n","authors":["Wanshui Li","Yifan Bai","Jiaxuan Lu","Kexin Yi"],"pdf_url":"https://arxiv.org/pdf/2203.10621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10620v1","updated":"2022-03-20T18:34:42Z","published":"2022-03-20T18:34:42Z","title":"Differentiable Reasoning over Long Stories -- Assessing Systematic\n  Generalisation in Neural Models","summary":"  Contemporary neural networks have achieved a series of developments and\nsuccesses in many aspects; however, when exposed to data outside the training\ndistribution, they may fail to predict correct answers. In this work, we were\nconcerned about this generalisation issue and thus analysed a broad set of\nmodels systematically and robustly over long stories. Related experiments were\nconducted based on the CLUTRR, which is a diagnostic benchmark suite that can\nanalyse generalisation of natural language understanding (NLU) systems by\ntraining over small story graphs and testing on larger ones. In order to handle\nthe multi-relational story graph, we consider two classes of neural models:\n\"E-GNN\", the graph-based models that can process graph-structured data and\nconsider the edge attributes simultaneously; and \"L-Graph\", the sequence-based\nmodels which can process linearized version of the graphs. We performed an\nextensive empirical evaluation, and we found that the modified recurrent neural\nnetwork yield surprisingly accurate results across every systematic\ngeneralisation tasks which outperform the modified graph neural network, while\nthe latter produced more robust models.\n","authors":["Wanshui Li","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2203.10620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10610v1","updated":"2022-03-20T17:51:49Z","published":"2022-03-20T17:51:49Z","title":"Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue\n  Systems","summary":"  Users interacting with voice assistants today need to phrase their requests\nin a very specific manner to elicit an appropriate response. This limits the\nuser experience, and is partly due to the lack of reasoning capabilities of\ndialogue platforms and the hand-crafted rules that require extensive labor. One\npossible way to improve user experience and relieve the manual efforts of\ndesigners is to build an end-to-end dialogue system that can do reasoning\nitself while perceiving user's utterances. In this work, we propose a novel\nmethod to incorporate the knowledge reasoning capability into dialogue systems\nin a more scalable and generalizable manner. Our proposed method allows a\nsingle transformer model to directly walk on a large-scale knowledge graph to\ngenerate responses. To the best of our knowledge, this is the first work to\nhave transformer models generate responses by reasoning over differentiable\nknowledge graphs. We investigate the reasoning abilities of the proposed method\non both task-oriented and domain-specific chit-chat dialogues. Empirical\nresults show that this method can effectively and efficiently incorporate a\nknowledge graph into a dialogue system with fully-interpretable reasoning\npaths.\n","authors":["Yi-Lin Tuan","Sajjad Beygi","Maryam Fazel-Zarandi","Qiaozi Gao","Alessandra Cervone","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10610v1.pdf","comment":"accepted to the Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2107.05687v2","updated":"2022-03-20T17:32:52Z","published":"2021-07-12T18:56:04Z","title":"Revisiting Uncertainty-based Query Strategies for Active Learning with\n  Transformers","summary":"  Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively nullifies, or even outweighs the desired\ncost savings. For this reason, we revisit uncertainty-based query strategies,\nwhich had been largely outperformed before, but are particularly suited in the\ncontext of fine-tuning transformers. In an extensive evaluation, we connect\ntransformers to experiments from previous research, assessing their performance\non five widely used text classification benchmarks. For active learning with\ntransformers, several other uncertainty-based approaches outperform the\nwell-known prediction entropy query strategy, thereby challenging its status as\nmost popular uncertainty baseline in active learning for text classification.\n","authors":["Christopher Schröder","Andreas Niekler","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2107.05687v2.pdf","comment":"ACL 2022 Findings"},{"id":"http://arxiv.org/abs/2110.08501v2","updated":"2022-03-20T15:59:40Z","published":"2021-10-16T07:27:12Z","title":"Think Before You Speak: Explicitly Generating Implicit Commonsense\n  Knowledge for Response Generation","summary":"  Implicit knowledge, such as common sense, is key to fluid human\nconversations. Current neural response generation (RG) models are trained to\ngenerate responses directly, omitting unstated implicit knowledge. In this\npaper, we present Think-Before-Speaking (TBS), a generative approach to first\nexternalize implicit commonsense knowledge (think) and use this knowledge to\ngenerate responses (speak). We expect that externalizing implicit knowledge\nallows more efficient learning, produces more informative responses, and\nenables more explainable models. We analyze different choices to collect\nknowledge-aligned dialogues, represent implicit knowledge, and transition\nbetween knowledge and dialogues. Empirical results show TBS models outperform\nend-to-end and knowledge-augmented RG baselines on most automatic metrics and\ngenerate more informative, specific, and commonsense-following responses, as\nevaluated by human annotators. TBS also generates knowledge that makes sense\nand is relevant to the dialogue around 85\\% of the time.\n","authors":["Pei Zhou","Karthik Gopalakrishnan","Behnam Hedayatnia","Seokhwan Kim","Jay Pujara","Xiang Ren","Yang Liu","Dilek Hakkani-Tur"],"pdf_url":"https://arxiv.org/pdf/2110.08501v2.pdf","comment":"Accepted at ACL 2022 main conference. 16 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2201.11870v2","updated":"2022-03-20T15:41:38Z","published":"2022-01-28T00:50:01Z","title":"Multiple-Source Domain Adaptation via Coordinated Domain Encoders and\n  Paired Classifiers","summary":"  We present a novel multiple-source unsupervised model for text classification\nunder domain shift. Our model exploits the update rates in document\nrepresentations to dynamically integrate domain encoders. It also employs a\nprobabilistic heuristic to infer the error rate in the target domain in order\nto pair source classifiers. Our heuristic exploits data transformation cost and\nthe classifier accuracy in the target feature space. We have used real world\nscenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We\nalso used pretrained multi-layer transformers as the document encoder in the\nexperiments to demonstrate whether the improvement achieved by domain\nadaptation models can be delivered by out-of-the-box language model\npretraining. The experiments testify that our model is the top performing\napproach in this setting.\n","authors":["Payam Karisani"],"pdf_url":"https://arxiv.org/pdf/2201.11870v2.pdf","comment":"AAAI 2022"},{"id":"http://arxiv.org/abs/2203.10581v1","updated":"2022-03-20T15:29:34Z","published":"2022-03-20T15:29:34Z","title":"Cluster & Tune: Boost Cold Start Performance in Text Classification","summary":"  In real-world scenarios, a text classification task often begins with a cold\nstart, when labeled data is scarce. In such cases, the common practice of\nfine-tuning pre-trained models, such as BERT, for a target classification task,\nis prone to produce poor performance. We suggest a method to boost the\nperformance of such models by adding an intermediate unsupervised\nclassification task, between the pre-training and fine-tuning phases. As such\nan intermediate task, we perform clustering and train the pre-trained model on\npredicting the cluster labels. We test this hypothesis on various data sets,\nand show that this additional classification phase can significantly improve\nperformance, mainly for topical classification tasks, when the number of\nlabeled instances available for fine-tuning is only a couple of dozen to a few\nhundred.\n","authors":["Eyal Shnarch","Ariel Gera","Alon Halfon","Lena Dankin","Leshem Choshen","Ranit Aharonov","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2203.10581v1.pdf","comment":"9 pages, 6 figures; To be published in ACL 2022"},{"id":"http://arxiv.org/abs/2203.10579v1","updated":"2022-03-20T15:14:39Z","published":"2022-03-20T15:14:39Z","title":"Small Batch Sizes Improve Training of Low-Resource Neural MT","summary":"  We study the role of an essential hyper-parameter that governs the training\nof Transformers for neural machine translation in a low-resource setting: the\nbatch size. Using theoretical insights and experimental evidence, we argue\nagainst the widespread belief that batch size should be set as large as allowed\nby the memory of the GPUs. We show that in a low-resource setting, a smaller\nbatch size leads to higher scores in a shorter training time, and argue that\nthis is due to better regularization of the gradients during training.\n","authors":["Àlex R. Atrio","Andrei Popescu-Belis"],"pdf_url":"https://arxiv.org/pdf/2203.10579v1.pdf","comment":"To be published in 18th International Conference on Natural Language\n  Processing (ICON 2021)"},{"id":"http://arxiv.org/abs/2110.07602v3","updated":"2022-03-20T15:13:08Z","published":"2021-10-14T17:58:47Z","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks","summary":"  Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work reveals that prompt tuning does not\nperform well for normal-sized pretrained models. We also find that existing\nmethods of prompt tuning cannot handle hard sequence labeling tasks, indicating\na lack of universality. We present a novel empirical finding that properly\noptimized prompt tuning can be universally effective across a wide range of\nmodel scales and NLU tasks. It matches the performance of finetuning while\nhaving only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an\nimplementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning}\noptimized and adapted for NLU. Given the universality and simplicity of\nP-Tuning v2, we believe it can serve as an alternative to finetuning and a\nstrong baseline for future research.Our code and data are released at\nhttps://github.com/THUDM/P-tuning-v2.\n","authors":["Xiao Liu","Kaixuan Ji","Yicheng Fu","Weng Lam Tam","Zhengxiao Du","Zhilin Yang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2110.07602v3.pdf","comment":"Proceedings of the 60th Annual Meeting of the Association of\n  Computational Linguistics, 2022"},{"id":"http://arxiv.org/abs/2109.04500v2","updated":"2022-03-20T14:38:14Z","published":"2021-09-09T18:23:45Z","title":"Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based\n  Encoder","summary":"  We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for\nsemantic parsing in task-oriented dialog. Our model consists of an encoder\nnetwork that incrementally builds the semantic parse tree by predicting the\nnon-terminal label and its positions in the linearized tree. At the generation\ntime, the model constructs the semantic parse tree by recursively inserting the\npredicted non-terminal labels at the predicted positions until termination.\nRINE achieves state-of-the-art exact match accuracy on low- and high-resource\nversions of the conversational semantic parsing benchmark TOP (Gupta et al.,\n2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and\ntransition-based parsers. We also show that our model design is applicable to\nnested named entity recognition task, where it performs on par with\nstate-of-the-art approach designed for that task. Finally, we demonstrate that\nour approach is 2-3.5 times faster than the sequence-to-sequence model at\ninference time.\n","authors":["Elman Mansimov","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2109.04500v2.pdf","comment":"Accepted to AAAI-22"},{"id":"http://arxiv.org/abs/2108.05682v2","updated":"2022-03-20T14:28:18Z","published":"2021-08-12T12:06:47Z","title":"(Un)solving Morphological Inflection: Lemma Overlap Artificially\n  Inflates Models' Performance","summary":"  In the domain of Morphology, Inflection is a fundamental and important task\nthat gained a lot of traction in recent years, mostly via SIGMORPHON's\nshared-tasks. With average accuracy above 0.9 over the scores of all languages,\nthe task is considered mostly solved using relatively generic neural seq2seq\nmodels, even with little data provided. In this work, we propose to re-evaluate\nmorphological inflection models by employing harder train-test splits that will\nchallenge the generalization capacity of the models. In particular, as opposed\nto the na{\\\"i}ve split-by-form, we propose a split-by-lemma method to challenge\nthe performance on existing benchmarks. Our experiments with the three\ntop-ranked systems on the SIGMORPHON's 2020 shared-task show that the\nlemma-split presents an average drop of 30 percentage points in macro-average\nfor the 90 languages included. The effect is most significant for low-resourced\nlanguages with a drop as high as 95 points, but even high-resourced languages\nlose about 10 points on average. Our results clearly show that generalizing\ninflection to unseen lemmas is far from being solved, presenting a simple yet\neffective means to promote more sophisticated models.\n","authors":["Omer Goldman","David Guriel","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2108.05682v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.10560v1","updated":"2022-03-20T14:26:20Z","published":"2022-03-20T14:26:20Z","title":"Who will share Fake-News on Twitter? Psycholinguistic cues in online\n  post histories discriminate Between actors in the misinformation ecosystem","summary":"  The spread of misinformation or fake-news is a global concern that undermines\nprogress on issues such as protecting democracy and public health. Past\nresearch aiming to combat its spread has largely focused on identifying its\nsemantic content and media outlets publishing such news. In contrast, we aim to\nidentify individuals who are more likely to share fake-news by studying the\nlanguage of actors in the fake-news ecosystem (such as fake-news sharers,\nfact-check sharers and random twitter users), and creating a linguistic profile\nof them. Fake-news sharers and fact-check sharers use significantly more\nhigh-arousal negative emotions in their language, but fake-news sharers express\nmore existentially-based needs than other actors. Incorporating\npsycholinguistic cues as inferred from their tweets into a model of\nsocio-demographic predictors considerably improves classification accuracy of\nfake-news sharers. The finding that fake-news sharers differ in important ways\nfrom other actors in the fake-news ecosystem (such as in their existential\nneeds), but are also similar to them in other ways (such as in their anger\nlevels), highlights the importance of studying the entire fake-news ecosystem\nto increase accuracy in identification and prediction. Our approach can help\nmitigate fake-news sharing by enabling platforms to pre-emptively screen\npotential fake-news sharers' posts.\n","authors":["Verena Schoenmueller","Simon J. Blanchard","Gita V. Johar"],"pdf_url":"https://arxiv.org/pdf/2203.10560v1.pdf","comment":"34 pages, 6 figures and 6 tables"},{"id":"http://arxiv.org/abs/2203.10557v1","updated":"2022-03-20T14:12:44Z","published":"2022-03-20T14:12:44Z","title":"A Neural-Symbolic Approach to Natural Language Understanding","summary":"  Deep neural networks, empowered by pre-trained language models, have achieved\nremarkable results in natural language understanding (NLU) tasks. However,\ntheir performances can deteriorate drastically when logical reasoning is needed\nin the process. This is because, ideally, NLU needs to depend on not only\nanalogical reasoning, which deep neural networks are good at, but also logical\nreasoning. According to the dual-process theory, analogical reasoning and\nlogical reasoning are respectively carried out by System 1 and System 2 in the\nhuman brain. Inspired by the theory, we present a novel framework for NLU\ncalled Neural-Symbolic Processor (NSP), which performs analogical reasoning\nbased on neural processing and performs logical reasoning based on both neural\nand symbolic processing. As a case study, we conduct experiments on two NLU\ntasks, question answering (QA) and natural language inference (NLI), when\nnumerical reasoning (a type of logical reasoning) is necessary. The\nexperimental results show that our method significantly outperforms\nstate-of-the-art methods in both tasks.\n","authors":["Zhixuan Liu","Zihao Wang","Yuan Lin","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2203.10557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08527v2","updated":"2022-03-20T14:12:06Z","published":"2022-03-16T10:47:29Z","title":"Morphological Reinflection with Multiple Arguments: An Extended\n  Annotation schema and a Georgian Case Study","summary":"  In recent years, a flurry of morphological datasets had emerged, most notably\nUniMorph, a multi-lingual repository of inflection tables. However, the flat\nstructure of the current morphological annotation schema makes the treatment of\nsome languages quirky, if not impossible, specifically in cases of polypersonal\nagreement, where verbs agree with multiple arguments using true affixes. In\nthis paper, we propose to address this phenomenon by expanding the UniMorph\nannotation schema to a hierarchical feature structure that naturally\naccommodates complex argument marking. We apply this extended schema to one\nsuch language, Georgian, and provide a human-verified, accurate and balanced\nmorphological dataset for Georgian verbs. The dataset has 4 times more tables\nand 6 times more verb forms compared to the existing UniMorph dataset, covering\nall possible variants of argument marking, demonstrating the adequacy of our\nproposed scheme. Experiments with a standard reinflection model show that\ngeneralization is easy when the data is split at the form level, but extremely\nhard when splitting along lemma lines. Expanding the other languages in\nUniMorph to this schema is expected to improve both the coverage, consistency\nand interpretability of this benchmark.\n","authors":["David Guriel","Omer Goldman","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2203.08527v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.10545v1","updated":"2022-03-20T13:01:25Z","published":"2022-03-20T13:01:25Z","title":"Parallel Instance Query Network for Named Entity Recognition","summary":"  Named entity recognition (NER) is a fundamental task in natural language\nprocessing. Recent works treat named entity recognition as a reading\ncomprehension task, constructing type-specific queries manually to extract\nentities. This paradigm suffers from three issues. First, type-specific queries\ncan only extract one type of entities per inference, which is inefficient.\nSecond, the extraction for different types of entities is isolated, ignoring\nthe dependencies between them. Third, query construction relies on external\nknowledge and is difficult to apply to realistic scenarios with hundreds of\nentity types. To deal with them, we propose Parallel Instance Query Network\n(PIQN), which sets up global and learnable instance queries to extract entities\nfrom a sentence in a parallel manner. Each instance query predicts one entity,\nand by feeding all instance queries simultaneously, we can query all entities\nin parallel. Instead of being constructed from external knowledge, instance\nqueries can learn their different query semantics during training. For training\nthe model, we treat label assignment as a one-to-many Linear Assignment Problem\n(LAP) and dynamically assign gold entities to instance queries with minimal\nassignment cost. Experiments on both nested and flat NER datasets demonstrate\nthat our proposed method outperforms previous state-of-the-art models.\n","authors":["Yongliang Shen","Xiaobin Wang","Zeqi Tan","Guangwei Xu","Pengjun Xie","Fei Huang","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2203.10545v1.pdf","comment":"Accepted to ACL 2022, camera ready version"},{"id":"http://arxiv.org/abs/2112.00061v3","updated":"2022-03-20T12:55:44Z","published":"2021-11-30T19:36:20Z","title":"Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context\n  Images via Online Resources","summary":"  Misinformation is now a major problem due to its potential high risks to our\ncore democratic and societal values and orders. Out-of-context misinformation\nis one of the easiest and effective ways used by adversaries to spread viral\nfalse stories. In this threat, a real image is re-purposed to support other\nnarratives by misrepresenting its context and/or elements. The internet is\nbeing used as the go-to way to verify information using different sources and\nmodalities. Our goal is an inspectable method that automates this\ntime-consuming and reasoning-intensive process by fact-checking the\nimage-caption pairing using Web evidence. To integrate evidence and cues from\nboth modalities, we introduce the concept of 'multi-modal cycle-consistency\ncheck'; starting from the image/caption, we gather textual/visual evidence,\nwhich will be compared against the other paired caption/image, respectively.\nMoreover, we propose a novel architecture, Consistency-Checking Network (CCN),\nthat mimics the layered human reasoning across the same and different\nmodalities: the caption vs. textual evidence, the image vs. visual evidence,\nand the image vs. caption. Our work offers the first step and benchmark for\nopen-domain, content-based, multi-modal fact-checking, and significantly\noutperforms previous baselines that did not leverage external evidence.\n","authors":["Sahar Abdelnabi","Rakibul Hasan","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2112.00061v3.pdf","comment":"CVPR'22"},{"id":"http://arxiv.org/abs/2203.09210v2","updated":"2022-03-20T12:11:41Z","published":"2022-03-17T10:00:33Z","title":"Universal Conditional Masked Language Pre-training for Neural Machine\n  Translation","summary":"  Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching & masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low- to extremely high-resource\nlanguages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on\naverage for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it\ncan also produce consistent performance gains, i.e., up to +5.3 BLEU. To the\nbest of our knowledge, this is the first work to pre-train a unified model for\nfine-tuning on both NMT tasks. Code, data, and pre-trained models are available\nat https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT\n","authors":["Pengfei Li","Liangyou Li","Meng Zhang","Minghao Wu","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2203.09210v2.pdf","comment":"Accepted to ACL 2022 Main conference"},{"id":"http://arxiv.org/abs/2106.04897v2","updated":"2022-03-20T09:46:53Z","published":"2021-06-09T08:33:20Z","title":"Unsupervised Automatic Speech Recognition: A Review","summary":"  Automatic Speech Recognition (ASR) systems can be trained to achieve\nremarkable performance given large amounts of manually transcribed speech, but\nlarge labeled data sets can be difficult or expensive to acquire for all\nlanguages of interest. In this paper, we review the research literature to\nidentify models and ideas that could lead to fully unsupervised ASR, including\nunsupervised segmentation of the speech signal, unsupervised mapping from\nspeech segments to text, and semi-supervised models with nominal amounts of\nlabeled examples. The objective of the study is to identify the limitations of\nwhat can be learned from speech data alone and to understand the minimum\nrequirements for speech recognition. Identifying these limitations would help\noptimize the resources and efforts in ASR development for low-resource\nlanguages.\n","authors":["Hanan Aldarmaki","Asad Ullah","Nazar Zaki"],"pdf_url":"https://arxiv.org/pdf/2106.04897v2.pdf","comment":"26 pages + 10 pages of references, 3 figures. Speech Communication\n  (2022)"},{"id":"http://arxiv.org/abs/2203.10484v1","updated":"2022-03-20T08:06:44Z","published":"2022-03-20T08:06:44Z","title":"Hierarchical Inductive Transfer for Continual Dialogue Learning","summary":"  Pre-trained models have achieved excellent performance on the dialogue task.\nHowever, for the continual increase of online chit-chat scenarios, directly\nfine-tuning these models for each of the new tasks not only explodes the\ncapacity of the dialogue system on the embedded devices but also causes\nknowledge forgetting on pre-trained models and knowledge interference among\ndiverse dialogue tasks. In this work, we propose a hierarchical inductive\ntransfer framework to learn and deploy the dialogue skills continually and\nefficiently. First, we introduce the adapter module into pre-trained models for\nlearning new dialogue tasks. As the only trainable module, it is beneficial for\nthe dialogue system on the embedded devices to acquire new dialogue skills with\nnegligible additional parameters. Then, for alleviating knowledge interference\nbetween tasks yet benefiting the regularization between them, we further design\nhierarchical inductive transfer that enables new tasks to use general knowledge\nin the base adapter without being misled by diverse knowledge in task-specific\nadapters. Empirical evaluation and analysis indicate that our framework obtains\ncomparable performance under deployment-friendly model capacity.\n","authors":["Shaoxiong Feng","Xuancheng Ren","Kan Li","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2203.10484v1.pdf","comment":"Accepted by Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2203.10483v1","updated":"2022-03-20T08:02:09Z","published":"2022-03-20T08:02:09Z","title":"Entailment Relation Aware Paraphrase Generation","summary":"  We introduce a new task of entailment relation aware paraphrase generation\nwhich aims at generating a paraphrase conforming to a given entailment relation\n(e.g. equivalent, forward entailing, or reverse entailing) with respect to a\ngiven input. We propose a reinforcement learning-based weakly-supervised\nparaphrasing system, ERAP, that can be trained using existing paraphrase and\nnatural language inference (NLI) corpora without an explicit task-specific\ncorpus. A combination of automated and human evaluations show that ERAP\ngenerates paraphrases conforming to the specified entailment relation and are\nof good quality as compared to the baselines and uncontrolled paraphrasing\nsystems. Using ERAP for augmenting training data for downstream textual\nentailment task improves performance over an uncontrolled paraphrasing system,\nand introduces fewer training artifacts, indicating the benefit of explicit\ncontrol during paraphrasing.\n","authors":["Abhilasha Sancheti","Balaji Vasan Srinivasan","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2203.10483v1.pdf","comment":"11 pages, 10 tables, 2 figures"},{"id":"http://arxiv.org/abs/2203.10482v1","updated":"2022-03-20T07:59:42Z","published":"2022-03-20T07:59:42Z","title":"DEIM: An effective deep encoding and interaction model for sentence\n  matching","summary":"  Natural language sentence matching is the task of comparing two sentences and\nidentifying the relationship between them.It has a wide range of applications\nin natural language processing tasks such as reading comprehension, question\nand answer systems. The main approach is to compute the interaction between\ntext representations and sentence pairs through an attention mechanism, which\ncan extract the semantic information between sentence pairs well. However,this\nkind of method can not gain satisfactory results when dealing with complex\nsemantic features. To solve this problem, we propose a sentence matching method\nbased on deep encoding and interaction to extract deep semantic information. In\nthe encoder layer,we refer to the information of another sentence in the\nprocess of encoding a single sentence, and later use a heuristic algorithm to\nfuse the information. In the interaction layer, we use a bidirectional\nattention mechanism and a self-attention mechanism to obtain deep semantic\ninformation.Finally, we perform a pooling operation and input it to the MLP for\nclassification. we evaluate our model on three tasks: recognizing textual\nentailment, paraphrase recognition, and answer selection. We conducted\nexperiments on the SNLI and SciTail datasets for the recognizing textual\nentailment task, the Quora dataset for the paraphrase recognition task, and the\nWikiQA dataset for the answer selection task. The experimental results show\nthat the proposed algorithm can effectively extract deep semantic features that\nverify the effectiveness of the algorithm on sentence matching tasks.\n","authors":["Kexin Jiang","Yahui Zhao","Rongyi Cui","Zhenguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.10482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00725v2","updated":"2022-03-20T04:56:40Z","published":"2022-03-01T20:17:31Z","title":"A Conformer Based Acoustic Model for Robust Automatic Speech Recognition","summary":"  This study addresses robust automatic speech recognition (ASR) by introducing\na Conformer-based acoustic model. The proposed model builds on a\nstate-of-the-art recognition system using a bi-directional long short-term\nmemory (BLSTM) model with utterance-wise dropout and iterative speaker\nadaptation, but employs a Conformer encoder instead of the BLSTM network. The\nConformer encoder uses a convolution-augmented attention mechanism for acoustic\nmodeling. The proposed system is evaluated on the monaural ASR task of the\nCHiME-4 corpus. Coupled with utterance-wise normalization and speaker\nadaptation, our model achieves $6.25\\%$ word error rate, which outperforms the\nprevious best system by $8.4\\%$ relatively. In addition, the proposed\nConformer-based model is $18.3\\%$ smaller in model size and reduces total\ntraining time by $79.6\\%$.\n","authors":["Yufeng Yang","Peidong Wang","DeLiang Wang"],"pdf_url":"https://arxiv.org/pdf/2203.00725v2.pdf","comment":"5 pages, 2 figures, submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2104.08793v5","updated":"2022-03-20T04:02:52Z","published":"2021-04-18T09:59:46Z","title":"SalKG: Learning From Knowledge Graph Explanations for Commonsense\n  Reasoning","summary":"  Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, to\nmotivate saliency-based supervision, we analyze oracle KG-augmented models\nwhich directly use saliency explanations as extra inputs for guiding their\nattention. Third, we propose SalKG, a framework for KG-augmented models to\nlearn from coarse and/or fine saliency explanations. Given saliency\nexplanations created from a task's training set, SalKG jointly trains the model\nto predict the explanations, then solve the task by attending to KG features\nhighlighted by the predicted explanations. On three commonsense QA benchmarks\n(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can\nyield considerable performance gains -- up to 2.76% absolute improvement on\nCSQA.\n","authors":["Aaron Chan","Jiashu Xu","Boyuan Long","Soumya Sanyal","Tanishq Gupta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2104.08793v5.pdf","comment":"NeurIPS 2021"},{"id":"http://arxiv.org/abs/2203.10442v1","updated":"2022-03-20T03:42:03Z","published":"2022-03-20T03:42:03Z","title":"Towards Structuring Real-World Data at Scale: Deep Learning for\n  Extracting Key Oncology Information from Clinical Text with Patient-Level\n  Supervision","summary":"  Objective: The majority of detailed patient information in real-world data\n(RWD) is only consistently available in free-text clinical documents. Manual\ncuration is expensive and time-consuming. Developing natural language\nprocessing (NLP) methods for structuring RWD is thus essential for scaling\nreal-world evidence generation.\n  Materials and Methods: Traditional rule-based systems are vulnerable to the\nprevalent linguistic variations and ambiguities in clinical text, and prior\napplications of machine-learning methods typically require sentence-level or\nreport-level labeled examples that are hard to produce at scale. We propose\nleveraging patient-level supervision from medical registries, which are often\nreadily available and capture key patient information, for general RWD\napplications. To combat the lack of sentence-level or report-level annotations,\nwe explore advanced deep-learning methods by combining domain-specific\npretraining, recurrent neural networks, and hierarchical attention.\n  Results: We conduct an extensive study on 135,107 patients from the cancer\nregistry of a large integrated delivery network (IDN) comprising healthcare\nsystems in five western US states. Our deep learning methods attain test AUROC\nof 94-99% for key tumor attributes and comparable performance on held-out data\nfrom separate health systems and states.\n  Discussion and Conclusion: Ablation results demonstrate clear superiority of\nthese advanced deep-learning methods over prior approaches. Error analysis\nshows that our NLP system sometimes even corrects errors in registrar labels.\nWe also conduct a preliminary investigation in accelerating registry curation\nand general RWD structuring via assisted curation for over 1.2 million cancer\npatients in this healthcare network.\n","authors":["Sam Preston","Mu Wei","Rajesh Rao","Robert Tinn","Naoto Usuyama","Michael Lucas","Roshanthi Weerasinghe","Soohee Lee","Brian Piening","Paul Tittel","Naveen Valluri","Tristan Naumann","Carlo Bifulco","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2203.10442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10432v1","updated":"2022-03-20T02:34:51Z","published":"2022-03-20T02:34:51Z","title":"Interpretability of Fine-grained Classification of Sadness and\n  Depression","summary":"  While sadness is a human emotion that people experience at certain times\nthroughout their lives, inflicting them with emotional disappointment and pain,\ndepression is a longer term mental illness which impairs social, occupational,\nand other vital regions of functioning making it a much more serious issue and\nneeds to be catered to at the earliest. NLP techniques can be utilized for the\ndetection and subsequent diagnosis of these emotions. Most of the open sourced\ndata on the web deal with sadness as a part of depression, as an emotion even\nthough the difference in severity of both is huge. Thus, we create our own\nnovel dataset illustrating the difference between the two. In this paper, we\naim to highlight the difference between the two and highlight how interpretable\nour models are to distinctly label sadness and depression. Due to the sensitive\nnature of such information, privacy measures need to be taken for handling and\ntraining of such data. Hence, we also explore the effect of Federated Learning\n(FL) on contextualised language models.\n","authors":["Tiasa Singha Roy","Priyam Basu","Aman Priyanshu","Rakshit Naidu"],"pdf_url":"https://arxiv.org/pdf/2203.10432v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2203.10430v1","updated":"2022-03-20T02:28:25Z","published":"2022-03-20T02:28:25Z","title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation\n  in Mandarin","summary":"  Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have benefited from this\nproblem because of pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by the strategies, we\nproposed a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments showed that learning a soft-weighting function for the candidate\nphonemes benefits performance. Besides, our g2pW does not require extra\npre-trained POS tagging models while using POS tags as auxiliary features since\nwe train the POS tagging model simultaneously with the unified encoder. The\nexperiments show that our g2pW outperforms existing methods on the public\ndataset. All codes, model weights, and a user-friendly package are publicly\navailable.\n","authors":["Yi-Chang Chen","Yu-Chuan Chang","Yen-Cheng Chang","Yi-Ren Yeh"],"pdf_url":"https://arxiv.org/pdf/2203.10430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10426v1","updated":"2022-03-20T01:49:53Z","published":"2022-03-20T01:49:53Z","title":"STEMM: Self-learning with Speech-text Manifold Mixup for Speech\n  Translation","summary":"  How to learn a better speech representation for end-to-end speech-to-text\ntranslation (ST) with limited labeled data? Existing techniques often attempt\nto transfer powerful machine translation (MT) capabilities to ST, but neglect\nthe representation discrepancy across modalities. In this paper, we propose the\nSpeech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.\nSpecifically, we mix up the representation sequences of different modalities,\nand take both unimodal speech sequences and multimodal mixed sequences as input\nto the translation model in parallel, and regularize their output predictions\nwith a self-learning framework. Experiments on MuST-C speech translation\nbenchmark and further analysis show that our method effectively alleviates the\ncross-modal representation discrepancy, and achieves significant improvements\nover a strong baseline on eight translation directions.\n","authors":["Qingkai Fang","Rong Ye","Lei Li","Yang Feng","Mingxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10426v1.pdf","comment":"ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2110.06490v2","updated":"2022-03-20T01:32:34Z","published":"2021-10-13T04:29:14Z","title":"Dict-BERT: Enhancing Language Model Pre-training with Dictionary","summary":"  Pre-trained language models (PLMs) aim to learn universal language\nrepresentations by conducting self-supervised training tasks on large-scale\ncorpora. Since PLMs capture word semantics in different contexts, the quality\nof word representations highly depends on word frequency, which usually follows\na heavy-tailed distributions in the pre-training corpus. Therefore, the\nembeddings of rare words on the tail are usually poorly optimized. In this\nwork, we focus on enhancing language model pre-training by leveraging\ndefinitions of the rare words in dictionaries (e.g., Wiktionary). To\nincorporate a rare word definition as a part of input, we fetch its definition\nfrom the dictionary and append it to the end of the input text sequence. In\naddition to training with the masked language modeling objective, we propose\ntwo novel self-supervised pre-training tasks on word and sentence-level\nalignment between input text sequence and rare word definitions to enhance\nlanguage modeling representation with dictionary. We evaluate the proposed\nDict-BERT model on the language understanding benchmark GLUE and eight\nspecialized domain benchmark datasets. Extensive experiments demonstrate that\nDict-BERT can significantly improve the understanding of rare words and boost\nmodel performance on various NLP downstream tasks.\n","authors":["Wenhao Yu","Chenguang Zhu","Yuwei Fang","Donghan Yu","Shuohang Wang","Yichong Xu","Michael Zeng","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2110.06490v2.pdf","comment":"ACL 2022 (Findings)"},{"id":"http://arxiv.org/abs/2203.10424v1","updated":"2022-03-20T01:01:05Z","published":"2022-03-20T01:01:05Z","title":"MetaOnce: A Metaverse Framework Based on Multi-scene Relations and\n  Entity-relation-event Game","summary":"  Existing metaverse systems lack rich relation types between entities and\nevents. The challenge is that there is no portable framework to introduce rich\nconcepts, relations, events into the metaverse. This paper introduces a new\nmetaverse framework, MetaOnce. This framework proposes to build multi-scene\ngraphs. This framework not only describes rich relations in a single scene but\nalso combines multiple scene graphs into a complete graph for more\ncomprehensive analysis and inference. Prior social network systems mainly\ndescribe friend relations. They ignore the effect of entity-relation-event\ngames on the metaverse system and existing rule constraints. We propose a rule\ncontroller and impose constraints on the relations that allow the framework to\nbehave in a compliant manner. We build a metaverse system to test the features\nof the framework, and experimental results show that our framework can build a\nmulti-scene metaverse with memory and rule constraints.\n","authors":["Hongyin Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.10424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.05354v4","updated":"2022-03-20T00:19:52Z","published":"2021-10-06T23:03:29Z","title":"Internal Language Model Adaptation with Text-Only Data for End-to-End\n  Speech Recognition","summary":"  Text-only adaptation of an end-to-end (E2E) model remains a challenging task\nfor automatic speech recognition (ASR). Language model (LM) fusion-based\napproaches require an additional external LM during inference, significantly\nincreasing the computation cost. To overcome this, we propose an internal LM\nadaptation (ILMA) of the E2E model using text-only data. Trained with\naudio-transcript pairs, an E2E model implicitly learns an internal LM that\ncharacterizes the token sequence probability which is approximated by the E2E\nmodel output after zeroing out the encoder contribution. During ILMA, we\nfine-tune the internal LM, i.e., the E2E components excluding the encoder, to\nminimize a cross-entropy loss. To make ILMA effective, it is essential to train\nthe E2E model with an internal LM loss besides the standard E2E loss.\nFurthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler\ndivergence between the output distributions of the adapted and unadapted\ninternal LMs. ILMA is the most effective when we update only the last linear\nlayer of the joint network. ILMA enables a fast text-only adaptation of the E2E\nmodel without increasing the run-time computational cost. Experimented with\n30K-hour trained transformer transducer models, ILMA achieves up to 34.9%\nrelative word error rate reduction from the unadapted baseline.\n","authors":["Zhong Meng","Yashesh Gaur","Naoyuki Kanda","Jinyu Li","Xie Chen","Yu Wu","Yifan Gong"],"pdf_url":"https://arxiv.org/pdf/2110.05354v4.pdf","comment":"5 pages, v4: adding shallow fusion and fast text-only adaptation\n  results as baseline, submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.10415v1","updated":"2022-03-20T00:02:10Z","published":"2022-03-20T00:02:10Z","title":"How does the pre-training objective affect what large language models\n  learn about linguistic properties?","summary":"  Several pre-training objectives, such as masked language modeling (MLM), have\nbeen proposed to pre-train language models (e.g. BERT) with the aim of learning\nbetter language representations. However, to the best of our knowledge, no\nprevious work so far has investigated how different pre-training objectives\naffect what BERT learns about linguistics properties. We hypothesize that\nlinguistically motivated objectives such as MLM should help BERT to acquire\nbetter linguistic knowledge compared to other non-linguistically motivated\nobjectives that are not intuitive or hard for humans to guess the association\nbetween the input and the label to be predicted. To this end, we pre-train BERT\nwith two linguistically motivated objectives and three non-linguistically\nmotivated ones. We then probe for linguistic characteristics encoded in the\nrepresentation of the resulting models. We find strong evidence that there are\nonly small differences in probing performance between the representations\nlearned by the two different types of objectives. These surprising results\nquestion the dominant narrative of linguistically informed pre-training.\n","authors":["Ahmed Alajrami","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2203.10415v1.pdf","comment":"Accepted at ACL 2022"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2203.10629v1","updated":"2022-03-20T19:03:18Z","published":"2022-03-20T19:03:18Z","title":"Explicit User Manipulation in Reinforcement Learning Based Recommender\n  Systems","summary":"  Recommender systems are highly prevalent in the modern world due to their\nvalue to both users and platforms and services that employ them. Generally,\nthey can improve the user experience and help to increase satisfaction, but\nthey do not come without risks. One such risk is that of their effect on users\nand their ability to play an active role in shaping user preferences. This risk\nis more significant for reinforcement learning based recommender systems. These\nare capable of learning for instance, how recommended content shown to a user\ntoday may tamper that user's preference for other content recommended in the\nfuture. Reinforcement learning based recommendation systems can thus implicitly\nlearn to influence users if that means maximizing clicks, engagement, or\nconsumption. On social news and media platforms, in particular, this type of\nbehavior is cause for alarm. Social media undoubtedly plays a role in public\nopinion and has been shown to be a contributing factor to increased political\npolarization. Recommender systems on such platforms, therefore, have great\npotential to influence users in undesirable ways. However, it may also be\npossible for this form of manipulation to be used intentionally. With\nadvancements in political opinion dynamics modeling and larger collections of\nuser data, explicit user manipulation in which the beliefs and opinions of\nusers are tailored towards a certain end emerges as a significant concern in\nreinforcement learning based recommender systems.\n","authors":["Matthew Sparr"],"pdf_url":"https://arxiv.org/pdf/2203.10629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.13322v2","updated":"2022-03-20T15:40:05Z","published":"2021-12-26T06:15:16Z","title":"Publication and collaboration anomalies in academic papers originating\n  from a paper mill: evidence from a Russia-based paper mill","summary":"  This study attempts to detect papers originating from the Russia-based paper\nmill International publisher LLC. A total of 1009 offers published during\n2019-2021 on the 123mi.ru website were analysed. The study allowed us to\nidentify at least 434 papers that are potentially linked to the paper mill\nincluding one preprint, a duplication paper and 15 republications of papers\nerroneously published in hijacked journals. Evidence of suspicious provenance\nfrom the paper mill is provided: matches in title, number of coauthorship\nslots, year of publication, country of the journal, country of a coauthorship\nslot and similarities of abstracts. These problematic papers are coauthored by\nscholars associated with at least 39 countries and submitted both to predatory\nand reputable journals. This study also demonstrates collaboration anomalies\nand the phenomenon of suspicious collaboration in questionable papers and\nexamines the predictors of the Russia-based paper mill. The value of\ncoauthorship slots offered by International Publisher LLC in 2019-2021 is\nestimated at $6.5 million. Since the study analysed a particular paper mill, it\nis likely that the number of papers with forged authorship is much higher.\n","authors":["Anna Abalkina"],"pdf_url":"https://arxiv.org/pdf/2112.13322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10576v1","updated":"2022-03-20T15:13:28Z","published":"2022-03-20T15:13:28Z","title":"Multi-view Multi-behavior Contrastive Learning in Recommendation","summary":"  Multi-behavior recommendation (MBR) aims to jointly consider multiple\nbehaviors to improve the target behavior's performance. We argue that MBR\nmodels should: (1) model the coarse-grained commonalities between different\nbehaviors of a user, (2) consider both individual sequence view and global\ngraph view in multi-behavior modeling, and (3) capture the fine-grained\ndifferences between multiple behaviors of a user. In this work, we propose a\nnovel Multi-behavior Multi-view Contrastive Learning Recommendation (MMCLR)\nframework, including three new CL tasks to solve the above challenges,\nrespectively. The multi-behavior CL aims to make different user single-behavior\nrepresentations of the same user in each view to be similar. The multi-view CL\nattempts to bridge the gap between a user's sequence-view and graph-view\nrepresentations. The behavior distinction CL focuses on modeling fine-grained\ndifferences of different behaviors. In experiments, we conduct extensive\nevaluations and ablation tests to verify the effectiveness of MMCLR and various\nCL tasks on two real-world datasets, achieving SOTA performance over existing\nbaselines. Our code will be available on\n\\url{https://github.com/wyqing20/MMCLR}\n","authors":["Yiqing Wu","Ruobing Xie","Yongchun Zhu","Xiang Ao","Xin Chen","Xu Zhang","Fuzhen Zhuang","Leyu Lin","Qing He"],"pdf_url":"https://arxiv.org/pdf/2203.10576v1.pdf","comment":"DASFAA 2022 Main Conference Long Paper"},{"id":"http://arxiv.org/abs/2109.04200v2","updated":"2022-03-20T05:22:31Z","published":"2021-09-09T12:19:49Z","title":"Double-Scale Self-Supervised Hypergraph Learning for Group\n  Recommendation","summary":"  With the prevalence of social media, there has recently been a proliferation\nof recommenders that shift their focus from individual modeling to group\nrecommendation. Since the group preference is a mixture of various\npredilections from group members, the fundamental challenge of group\nrecommendation is to model the correlations among members. Existing methods\nmostly adopt heuristic or attention-based preference aggregation strategies to\nsynthesize group preferences. However, these models mainly focus on the\npairwise connections of users and ignore the complex high-order interactions\nwithin and beyond groups. Besides, group recommendation suffers seriously from\nthe problem of data sparsity due to severely sparse group-item interactions. In\nthis paper, we propose a self-supervised hypergraph learning framework for\ngroup recommendation to achieve two goals: (1) capturing the intra- and\ninter-group interactions among users; (2) alleviating the data sparsity issue\nwith the raw data itself. Technically, for (1), a hierarchical hypergraph\nconvolutional network based on the user- and group-level hypergraphs is\ndeveloped to model the complex tuplewise correlations among users within and\nbeyond groups. For (2), we design a double-scale node dropout strategy to\ncreate self-supervision signals that can regularize user representations with\ndifferent granularities against the sparsity issue. The experimental analysis\non multiple benchmark datasets demonstrates the superiority of the proposed\nmodel and also elucidates the rationality of the hypergraph modeling and the\ndouble-scale self-supervision.\n","authors":["Junwei Zhang","Min Gao","Junliang Yu","Lei Guo","Jundong Li","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2109.04200v2.pdf","comment":"11 pages, 6 figures, CIKM 2021"},{"id":"http://arxiv.org/abs/2203.12596v1","updated":"2022-03-20T04:46:11Z","published":"2022-03-20T04:46:11Z","title":"ZOOMER: Boosting Retrieval on Web-scale Graphs by Regions of Interest","summary":"  We introduce ZOOMER, a system deployed at Taobao, the largest e-commerce\nplatform in China, for training and serving GNN-based recommendations over\nweb-scale graphs. ZOOMER is designed for tackling two challenges presented by\nthe massive user data at Taobao: low training/serving efficiency due to the\nhuge scale of the graphs, and low recommendation quality due to the information\noverload which distracts the recommendation model from specific user\nintentions. ZOOMER achieves this by introducing a key concept, Region of\nInterests (ROI) in GNNs for recommendations, i.e., a neighborhood region in the\ngraph with significant relevance to a strong user intention. ZOOMER narrows the\nfocus from the whole graph and \"zooms in\" on the more relevant ROIs, thereby\nreducing the training/serving cost and mitigating the information overload at\nthe same time. With carefully designed mechanisms, ZOOMER identifies the\ninterest expressed by each recommendation request, constructs an ROI subgraph\nby sampling with respect to the interest, and guides the GNN to reweigh\ndifferent parts of the ROI towards the interest by a multi-level attention\nmodule. Deployed as a large-scale distributed system, ZOOMER supports graphs\nwith billions of nodes for training and thousands of requests per second for\nserving. ZOOMER achieves up to 14x speedup when downsizing sampling scales with\ncomparable (even better) AUC performance than baseline methods. Besides, both\nthe offline evaluation and online A/B test demonstrate the effectiveness of\nZOOMER.\n","authors":["Yuezihan Jiang","Yu Cheng","Hanyu Zhao","Wentao Zhang","Xupeng Miao","Yu He","Liang Wang","Zhi Yang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2203.12596v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2203.06935v3","updated":"2022-03-20T09:19:29Z","published":"2022-03-14T08:57:12Z","title":"A Systematic Review on Affective Computing: Emotion Models, Databases,\n  and Recent Advances","summary":"  Affective computing plays a key role in human-computer interactions,\nentertainment, teaching, safe driving, and multimedia integration. Major\nbreakthroughs have been made recently in the areas of affective computing\n(i.e., emotion recognition and sentiment analysis). Affective computing is\nrealized based on unimodal or multimodal data, primarily consisting of physical\ninformation (e.g., textual, audio, and visual data) and physiological signals\n(e.g., EEG and ECG signals). Physical-based affect recognition caters to more\nresearchers due to multiple public databases. However, it is hard to reveal\none's inner emotion hidden purposely from facial expressions, audio tones, body\ngestures, etc. Physiological signals can generate more precise and reliable\nemotional results; yet, the difficulty in acquiring physiological signals also\nhinders their practical application. Thus, the fusion of physical information\nand physiological signals can provide useful features of emotional states and\nlead to higher accuracy. Instead of focusing on one specific field of affective\nanalysis, we systematically review recent advances in the affective computing,\nand taxonomize unimodal affect recognition as well as multimodal affective\nanalysis. Firstly, we introduce two typical emotion models followed by commonly\nused databases for affective computing. Next, we survey and taxonomize\nstate-of-the-art unimodal affect recognition and multimodal affective analysis\nin terms of their detailed architectures and performances. Finally, we discuss\nsome important aspects on affective computing and their applications and\nconclude this review with an indication of the most promising future\ndirections, such as the establishment of baseline dataset, fusion strategies\nfor multimodal affective analysis, and unsupervised learning models.\n","authors":["Yan Wang","Wei Song","Wei Tao","Antonio Liotta","Dawei Yang","Xinlei Li","Shuyong Gao","Yixuan Sun","Weifeng Ge","Wei Zhang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.06935v3.pdf","comment":"Accepted for Information Fusion"}]},"2022-03-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.08994v2","updated":"2022-03-19T19:26:03Z","published":"2022-03-17T00:07:02Z","title":"AI Autonomy: Self-Initiation, Adaptation and Continual Learning","summary":"  As more and more AI agents are used in practice, it is time to think about\nhow to make these agents fully autonomous so that they can (1) learn by\nthemselves continually in a self-motivated and self-initiated manner rather\nthan being retrained offline periodically on the initiation of human engineers\nand (2) accommodate or adapt to unexpected or novel circumstances. As the\nreal-world is an open environment that is full of unknowns or novelties,\ndetecting novelties, characterizing them, accommodating or adapting to them,\nand gathering ground-truth training data and incrementally learning the\nunknowns/novelties are critical to making the AI agent more and more\nknowledgeable and powerful over time. The key challenge is how to automate the\nprocess so that it is carried out continually on the agent's own initiative and\nthrough its own interactions with humans, other agents and the environment just\nlike human on-the-job learning. This paper proposes a framework (called SOLA)\nfor this learning paradigm to promote the research of building autonomous and\ncontinual learning enabled AI agents. To show feasibility, an implemented agent\nis also described.\n","authors":["Bing Liu","Sahisnu Mazumder","Eric Robertson","Scott Grigsby"],"pdf_url":"https://arxiv.org/pdf/2203.08994v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2110.11385"},{"id":"http://arxiv.org/abs/2203.10378v1","updated":"2022-03-19T18:52:47Z","published":"2022-03-19T18:52:47Z","title":"On Robust Prefix-Tuning for Text Classification","summary":"  Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.\n","authors":["Zonghan Yang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2203.10378v1.pdf","comment":"Accepted in ICLR 2022. We release the code at\n  https://github.com/minicheshire/Robust-Prefix-Tuning"},{"id":"http://arxiv.org/abs/2203.10369v1","updated":"2022-03-19T18:22:06Z","published":"2022-03-19T18:22:06Z","title":"The Online Behaviour of the Algerian Abusers in Social Media Networks","summary":"  Connecting to social media networks becomes a daily task for the majority of\npeople around the world, and the amount of shared information is growing\nexponentially. Thus, controlling the way in which people communicate is\nnecessary, in order to protect them from disorientation, conflicts,\naggressions, etc. In this paper, we conduct a statistical study on the\ncyber-bullying and the abusive content in social media (i.e. Facebook), where\nwe try to spot the online behaviour of the abusers in the Algerian community.\nMore specifically, we have involved 200 Facebook users from different regions\namong 600 to carry out this study. The aim of this investigation is to aid\nautomatic systems of abuse detection to take decision by incorporating the\nonline activity. Abuse detection systems require a large amount of data to\nperform better on such kind of texts (i.e. unstructured and informal texts),\nand this is due to the lack of standard orthography, where there are various\nAlgerian dialects and languages spoken.\n","authors":["Kheireddine Abainia"],"pdf_url":"https://arxiv.org/pdf/2203.10369v1.pdf","comment":"BigDML 2021"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2203.10365v1","updated":"2022-03-19T18:04:12Z","published":"2022-03-19T18:04:12Z","title":"Domain Representative Keywords Selection: A Probabilistic Approach","summary":"  We propose a probabilistic approach to select a subset of a \\textit{target\ndomain representative keywords} from a candidate set, contrasting with a\ncontext domain. Such a task is crucial for many downstream tasks in natural\nlanguage processing. To contrast the target domain and the context domain, we\nadapt the \\textit{two-component mixture model} concept to generate a\ndistribution of candidate keywords. It provides more importance to the\n\\textit{distinctive} keywords of the target domain than common keywords\ncontrasting with the context domain. To support the \\textit{representativeness}\nof the selected keywords towards the target domain, we introduce an\n\\textit{optimization algorithm} for selecting the subset from the generated\ncandidate distribution. We have shown that the optimization algorithm can be\nefficiently implemented with a near-optimal approximation guarantee. Finally,\nextensive experiments on multiple domains demonstrate the superiority of our\napproach over other baselines for the tasks of keyword summary generation and\ntrending keywords selection.\n","authors":["Pritom Saha Akash","Jie Huang","Kevin Chen-Chuan Chang","Yunyao Li","Lucian Popa","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2203.10365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10354v1","updated":"2022-03-19T16:27:30Z","published":"2022-03-19T16:27:30Z","title":"Meta-Learning for Online Update of Recommender Systems","summary":"  Online recommender systems should be always aligned with users' current\ninterest to accurately suggest items that each user would like. Since user\ninterest usually evolves over time, the update strategy should be flexible to\nquickly catch users' current interest from continuously generated new user-item\ninteractions. Existing update strategies focus either on the importance of each\nuser-item interaction or the learning rate for each recommender parameter, but\nsuch one-directional flexibility is insufficient to adapt to varying\nrelationships between interactions and parameters. In this paper, we propose\nMeLON, a meta-learning based novel online recommender update strategy that\nsupports two-directional flexibility. It is featured with an adaptive learning\nrate for each parameter-interaction pair for inducing a recommender to quickly\nlearn users' up-to-date interest. The procedure of MeLON is optimized following\na meta-learning approach: it learns how a recommender learns to generate the\noptimal learning rates for future updates. Specifically, MeLON first enriches\nthe meaning of each interaction based on previous interactions and identifies\nthe role of each parameter for the interaction; and then combines these two\npieces of information to generate an adaptive learning rate. Theoretical\nanalysis and extensive evaluation on three real-world online recommender\ndatasets validate the effectiveness of MeLON.\n","authors":["Minseok Kim","Hwanjun Song","Yooju Shin","Dongmin Park","Kijung Shin","Jae-Gil Lee"],"pdf_url":"https://arxiv.org/pdf/2203.10354v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.10258v1","updated":"2022-03-19T06:48:50Z","published":"2022-03-19T06:48:50Z","title":"Doubly Robust Collaborative Targeted Learning for Recommendation on Data\n  Missing Not at Random","summary":"  In recommender systems, the feedback data received is always missing not at\nrandom (MNAR), which poses challenges for accurate rating prediction. To\naddress this issue, many recent studies have been conducted on the doubly\nrobust (DR) method and its variants to reduce bias. However, theoretical\nanalysis shows that the DR method has a relatively large variance, while that\nof the error imputation-based (EIB) method is smaller. In this paper, we\npropose {\\bf DR-TMLE} that effectively captures the merits of both EIB and DR,\nby leveraging the targeted maximum likelihood estimation (TMLE) technique.\nDR-TMLE first obtains an initial EIB estimator and then updates the error\nimputation model along with the bias-reduced direction. Furthermore, we propose\na novel RCT-free collaborative targeted learning algorithm for DR-TMLE, called\n{\\bf DR-TMLE-TL}, which updates the propensity model adaptively to reduce the\nbias of imputed errors. Both theoretical analysis and experiments demonstrate\nthe advantages of the proposed methods compared with existing debiasing\nmethods.\n","authors":["Peng Wu","Haoxuan Li","Yan Lyu","Xiao-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.10258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10232v1","updated":"2022-03-19T03:24:53Z","published":"2022-03-19T03:24:53Z","title":"DuReader_retrieval: A Large-scale Chinese Benchmark for Passage\n  Retrieval from Web Search Engine","summary":"  In this paper, we present DuReader_retrieval, a large-scale Chinese dataset\nfor passage retrieval. DuReader_retrieval contains more than 90K queries and\nover 8M unique passages from Baidu search. To ensure the quality of our\nbenchmark and address the shortcomings in other existing datasets, we (1)\nreduce the false negatives in development and testing sets by pooling the\nresults from multiple retrievers with human annotations, (2) and remove the\nsemantically similar questions between training with development and testing\nsets. We further introduce two extra out-of-domain testing sets for\nbenchmarking the domain generalization capability. Our experiment results\ndemonstrate that DuReader_retrieval is challenging and there is still plenty of\nroom for the community to improve, e.g. the generalization across domains,\nsalient phrase and syntax mismatch between query and paragraph and robustness.\nDuReader_retrieval will be publicly available at\nhttps://github.com/baidu/DuReader/tree/master/DuReader-Retrieval\n","authors":["Yifu Qiu","Hongyu Li","Yingqi Qu","Ying Chen","Qiaoqiao She","Jing Liu","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2203.10232v1.pdf","comment":null}]},"2022-03-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.11933v1","updated":"2022-03-22T17:59:04Z","published":"2022-03-22T17:59:04Z","title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models\n  with Adversarial Learning","summary":"  Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n","authors":["Hugo Berg","Siobhan Mackenzie Hall","Yash Bhalgat","Wonsuk Yang","Hannah Rose Kirk","Aleksandar Shtedritski","Max Bain"],"pdf_url":"https://arxiv.org/pdf/2203.11933v1.pdf","comment":"24 pages, 10 figures. For code and trained token embeddings, see\n  https://github.com/oxai/debias-vision-lang"},{"id":"http://arxiv.org/abs/2110.08173v2","updated":"2022-03-22T17:31:10Z","published":"2021-10-15T16:00:11Z","title":"Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge\n  of Pre-trained Language Models","summary":"  Knowledge probing is crucial for understanding the knowledge transfer\nmechanism behind the pre-trained language models (PLMs). Despite the growing\nprogress of probing knowledge for PLMs in the general domain, specialised areas\nsuch as biomedical domain are vastly under-explored. To catalyse the research\nin this direction, we release a well-curated biomedical knowledge probing\nbenchmark, MedLAMA, which is constructed based on the Unified Medical Language\nSystem (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs\nand probing approaches on our benchmark, reaching at most 3% of acc@10. While\nhighlighting various sources of domain-specific challenges that amount to this\nunderwhelming performance, we illustrate that the underlying PLMs have a higher\npotential for probing tasks. To achieve this, we propose Contrastive-Probe, a\nnovel self-supervised contrastive probing approach, that adjusts the underlying\nPLMs without using any probing data. While Contrastive-Probe pushes the acc@10\nto 28%, the performance gap still remains notable. Our human expert evaluation\nsuggests that the probing performance of our Contrastive-Probe is still\nunder-estimated as UMLS still does not include the full spectrum of factual\nknowledge. We hope MedLAMA and Contrastive-Probe facilitate further\ndevelopments of more suited probing techniques for this domain.\n","authors":["Zaiqiao Meng","Fangyu Liu","Ehsan Shareghi","Yixuan Su","Charlotte Collins","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2110.08173v2.pdf","comment":"ACL 2022; code and data are released at\n  https://github.com/cambridgeltl/medlama"},{"id":"http://arxiv.org/abs/2110.04655v2","updated":"2022-03-22T17:28:44Z","published":"2021-10-09T22:27:19Z","title":"Disentangled Sequence to Sequence Learning for Compositional\n  Generalization","summary":"  There is mounting evidence that existing neural network models, in particular\nthe very popular sequence-to-sequence architecture, struggle to systematically\ngeneralize to unseen compositions of seen components. We demonstrate that one\nof the reasons hindering compositional generalization relates to\nrepresentations being entangled. We propose an extension to\nsequence-to-sequence models which encourages disentanglement by adaptively\nre-encoding (at each time step) the source input. Specifically, we condition\nthe source representations on the newly decoded target context which makes it\neasier for the encoder to exploit specialized information for each prediction\nrather than capturing it all in a single forward pass. Experimental results on\nsemantic parsing and machine translation empirically show that our proposal\ndelivers more disentangled representations and better generalization.\n","authors":["Hao Zheng","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2110.04655v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.11899v1","updated":"2022-03-22T17:11:18Z","published":"2022-03-22T17:11:18Z","title":"Transformer based ensemble for emotion detection","summary":"  Detecting emotions in languages is important to accomplish a complete\ninteraction between humans and machines. This paper describes our contribution\nto the WASSA 2022 shared task which handles this crucial task of emotion\ndetection. We have to identify the following emotions: sadness, surprise,\nneutral, anger, fear, disgust, joy based on a given essay text. We are using an\nensemble of ELECTRA and BERT models to tackle this problem achieving an F1\nscore of 62.76%. Our codebase (https://bit.ly/WASSA_shared_task) and our WandB\nproject (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is available.\n","authors":["Aditya Kane","Shantanu Patankar","Sahil Khose","Neeraja Kirtane"],"pdf_url":"https://arxiv.org/pdf/2203.11899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11856v1","updated":"2022-03-22T16:32:51Z","published":"2022-03-22T16:32:51Z","title":"A Computational Approach to Understand Mental Health from Reddit:\n  Knowledge-aware Multitask Learning Framework","summary":"  Analyzing gender is critical to study mental health (MH) support in CVD\n(cardiovascular disease). The existing studies on using social media for\nextracting MH symptoms consider symptom detection and tend to ignore user\ncontext, disease, or gender. The current study aims to design and evaluate a\nsystem to capture how MH symptoms associated with CVD are expressed differently\nwith the gender on social media. We observe that the reliable detection of MH\nsymptoms expressed by persons with heart disease in user posts is challenging\nbecause of the co-existence of (dis)similar MH symptoms in one post and due to\nvariation in the description of symptoms based on gender. We collect a corpus\nof $150k$ items (posts and comments) annotated using the subreddit labels and\ntransfer learning approaches. We propose GeM, a novel task-adaptive multi-task\nlearning approach to identify the MH symptoms in CVD patients based on gender.\nSpecifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to\ncapture CVD-related MH symptoms. Moreover, it enhances the reliability for\ndifferentiating the gender language in MH symptoms when compared to the\nstate-of-art language models. Our model achieves high (statistically\nsignificant) performance and predicts four labels of MH issues and two gender\nlabels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom\nidentification task and by 2.55% on the gender identification task.\n","authors":["Usha Lokala","Aseem Srivastava","Triyasha Ghosh Dastidar","Tanmoy Chakraborty","Md Shad Akthar","Maryam Panahiazar","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2203.11856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11849v1","updated":"2022-03-22T16:26:09Z","published":"2022-03-22T16:26:09Z","title":"A Girl Has A Name, And It's ... Adversarial Authorship Attribution for\n  Deobfuscation","summary":"  Recent advances in natural language processing have enabled powerful\nprivacy-invasive authorship attribution. To counter authorship attribution,\nresearchers have proposed a variety of rule-based and learning-based text\nobfuscation approaches. However, existing authorship obfuscation approaches do\nnot consider the adversarial threat model. Specifically, they are not evaluated\nagainst adversarially trained authorship attributors that are aware of\npotential obfuscation. To fill this gap, we investigate the problem of\nadversarial authorship attribution for deobfuscation. We show that\nadversarially trained authorship attributors are able to degrade the\neffectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate\nthe effectiveness of adversarial training when the attributor makes incorrect\nassumptions about whether and which obfuscator was used. While there is a a\nclear degradation in attribution accuracy, it is noteworthy that this\ndegradation is still at or above the attribution accuracy of the attributor\nthat is not adversarially trained at all. Our results underline the need for\nstronger obfuscation approaches that are resistant to deobfuscation\n","authors":["Wanyue Zhai","Jonathan Rusert","Zubair Shafiq","Padmini Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2203.11849v1.pdf","comment":"9 pages, 7 figures, 3 tables, ACL 2022"},{"id":"http://arxiv.org/abs/2203.07836v2","updated":"2022-03-22T16:16:16Z","published":"2022-03-15T12:47:00Z","title":"Graph Pre-training for AMR Parsing and Generation","summary":"  Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.\n","authors":["Xuefeng Bai","Yulong Chen","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.07836v2.pdf","comment":"ACL2022 camera-ready version"},{"id":"http://arxiv.org/abs/2203.11841v1","updated":"2022-03-22T16:09:34Z","published":"2022-03-22T16:09:34Z","title":"SU-NLP at SemEval-2022 Task 11: Complex Named Entity Recognition with\n  Entity Linking","summary":"  This paper describes the system proposed by Sabanc{\\i} University Natural\nLanguage Processing Group in the SemEval-2022 MultiCoNER task. We developed an\nunsupervised entity linking pipeline that detects potential entity mentions\nwith the help of Wikipedia and also uses the corresponding Wikipedia context to\nhelp the classifier in finding the named entity type of that mention. Our\nresults showed that our pipeline improved performance significantly, especially\nfor complex entities in low-context settings.\n","authors":["Buse Çarık","Fatih Beyhan","Reyyan Yeniterzi"],"pdf_url":"https://arxiv.org/pdf/2203.11841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.06951v2","updated":"2022-03-22T14:59:28Z","published":"2021-04-14T16:21:37Z","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine\n  Translation: A Survey","summary":"  The development of deep learning techniques has allowed Neural Machine\nTranslation (NMT) models to become extremely powerful, given sufficient\ntraining data and training time. However, systems struggle when translating\ntext from a new domain with a distinct style or vocabulary. Fine-tuning on\nin-domain data allows good domain adaptation, but requires sufficient relevant\nbilingual data. Even if this is available, simple fine-tuning can cause\noverfitting to new data and `catastrophic forgetting' of previously learned\nbehaviour.\n  We concentrate on robust approaches to domain adaptation for NMT,\nparticularly where a system may need to translate across multiple domains. We\ndivide techniques into those revolving around data selection or generation,\nmodel architecture, parameter adaptation procedure, and inference procedure. We\nfinally highlight the benefits of domain adaptation and multi-domain adaptation\ntechniques to other lines of NMT research.\n","authors":["Danielle Saunders"],"pdf_url":"https://arxiv.org/pdf/2104.06951v2.pdf","comment":"43 pages + references"},{"id":"http://arxiv.org/abs/2103.04941v3","updated":"2022-03-22T14:51:14Z","published":"2021-03-08T17:59:41Z","title":"InFillmore: Frame-Guided Language Generation with Bidirectional Context","summary":"  We propose a structured extension to bidirectional-context conditional\nlanguage generation, or \"infilling,\" inspired by Frame Semantic theory\n(Fillmore, 1976). Guidance is provided through two approaches: (1) model\nfine-tuning, conditioning directly on observed symbolic frames, and (2) a novel\nextension to disjunctive lexically constrained decoding that leverages frame\nsemantic lexical units. Automatic and human evaluations confirm that\nframe-guided generation allows for explicit manipulation of intended infill\nsemantics, with minimal loss in distinguishability from human-generated text.\nOur methods flexibly apply to a variety of use scenarios, and we provide a\ncodebase and interactive demo available from\nhttps://nlp.jhu.edu/demos/infillmore.\n","authors":["Jiefu Ou","Nathaniel Weir","Anton Belyy","Felix Yu","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2103.04941v3.pdf","comment":"Appearing in *SEM 2021"},{"id":"http://arxiv.org/abs/2202.03086v3","updated":"2022-03-22T14:41:52Z","published":"2022-02-07T11:54:07Z","title":"Machine Translation from Signed to Spoken Languages: State of the Art\n  and Challenges","summary":"  Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n","authors":["Mathieu De Coster","Dimitar Shterionov","Mieke Van Herreweghe","Joni Dambre"],"pdf_url":"https://arxiv.org/pdf/2202.03086v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11764v1","updated":"2022-03-22T14:24:56Z","published":"2022-03-22T14:24:56Z","title":"Listening to Affected Communities to Define Extreme Speech: Dataset and\n  Experiments","summary":"  Building on current work on multilingual hate speech (e.g., Ousidhoum et al.\n(2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present\nXTREMESPEECH, a new hate speech dataset containing 20,297 social media passages\nfrom Brazil, Germany, India and Kenya. The key novelty is that we directly\ninvolve the affected communities in collecting and annotating the data - as\nopposed to giving companies and governments control over defining and\ncombatting hate speech. This inclusive approach results in datasets more\nrepresentative of actually occurring online speech and is likely to facilitate\nthe removal of the social media content that marginalized communities view as\ncausing the most harm. Based on XTREMESPEECH, we establish novel tasks with\naccompanying baselines, provide evidence that cross-country training is\ngenerally not feasible due to cultural differences between countries and\nperform an interpretability analysis of BERT's predictions.\n","authors":["Antonis Maronikolakis","Axel Wisiorek","Leah Nann","Haris Jabbar","Sahana Udupa","Hinrich Schuetze"],"pdf_url":"https://arxiv.org/pdf/2203.11764v1.pdf","comment":"Accepted to ACL 2022 Findings"},{"id":"http://arxiv.org/abs/2203.08928v2","updated":"2022-03-22T13:41:23Z","published":"2022-03-16T20:30:05Z","title":"C-MORE: Pretraining to Answer Open-Domain Questions by Consulting\n  Millions of References","summary":"  We consider the problem of pretraining a two-stage open-domain question\nanswering (QA) system (retriever + reader) with strong transfer capabilities.\nThe key challenge is how to construct a large amount of high-quality\nquestion-answer-context triplets without task-specific annotations.\nSpecifically, the triplets should align well with downstream tasks by: (i)\ncovering a wide range of domains (for open-domain applications), (ii) linking a\nquestion to its semantically relevant context with supporting evidence (for\ntraining the retriever), and (iii) identifying the correct answer in the\ncontext (for training the reader). Previous pretraining approaches generally\nfall short of one or more of these requirements. In this work, we automatically\nconstruct a large-scale corpus that meets all three criteria by consulting\nmillions of references cited within Wikipedia. The well-aligned pretraining\nsignals benefit both the retriever and the reader significantly. Our pretrained\nretriever leads to 2%-10% absolute gains in top-20 accuracy. And with our\npretrained reader, the entire system improves by up to 4% in exact match.\n","authors":["Xiang Yue","Xiaoman Pan","Wenlin Yao","Dian Yu","Dong Yu","Jianshu Chen"],"pdf_url":"https://arxiv.org/pdf/2203.08928v2.pdf","comment":"ACL 2022 Main Conference"},{"id":"http://arxiv.org/abs/2203.11702v1","updated":"2022-03-22T13:12:27Z","published":"2022-03-22T13:12:27Z","title":"BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning\n  in Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) task aims to associate a piece of text\nwith a set of aspects and meanwhile infer their respective sentimental\npolarities. Up to now, the state-of-the-art approaches are built upon\nfine-tuning of various pre-trained language models. They commonly aim to learn\nthe aspect-specific representation in the corpus. Unfortunately, the aspect is\noften expressed implicitly through a set of representatives and thus renders\nimplicit mapping process unattainable unless sufficient labeled examples.\n  In this paper, we propose to jointly address aspect categorization and\naspect-based sentiment subtasks in a unified framework. Specifically, we first\nintroduce a simple but effective mechanism that collaborates the semantic and\nsyntactic information to construct auxiliary-sentences for the implicit aspect.\nThen, we encourage BERT to learn the aspect-specific representation in response\nto the automatically constructed auxiliary-sentence instead of the aspect\nitself. Finally, we empirically evaluate the performance of the proposed\nsolution by a comparative study on real benchmark datasets for both ABSA and\nTargeted-ABSA tasks. Our extensive experiments show that it consistently\nachieves state-of-the-art performance in terms of aspect categorization and\naspect-based sentiment across all datasets and the improvement margins are\nconsiderable.\n","authors":["Ahmed Murtadha","Shengfeng Pan","Bo Wen","Jianlin Su","Wenze Zhang","Yunfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11702v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2202.09950v2","updated":"2022-03-22T12:45:11Z","published":"2022-02-21T02:05:14Z","title":"CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech\n  Editing","summary":"  The text-based speech editor allows the editing of speech through intuitive\ncutting, copying, and pasting operations to speed up the process of editing\nspeech. However, the major drawback of current systems is that edited speech\noften sounds unnatural due to cut-copy-paste operation. In addition, it is not\nobvious how to synthesize records according to a new word not appearing in the\ntranscript. This paper proposes a novel end-to-end text-based speech editing\nmethod called context-aware mask prediction network (CampNet). The model can\nsimulate the text-based speech editing process by randomly masking part of\nspeech and then predicting the masked region by sensing the speech context. It\ncan solve unnatural prosody in the edited region and synthesize the speech\ncorresponding to the unseen words in the transcript. Secondly, for the possible\noperation of text-based speech editing, we design three text-based operations\nbased on CampNet: deletion, insertion, and replacement. These operations can\ncover various situations of speech editing. Thirdly, to synthesize the speech\ncorresponding to long text in insertion and replacement operations, a\nword-level autoregressive generation method is proposed. Fourthly, we propose a\nspeaker adaptation method using only one sentence for CampNet and explore the\nability of few-shot learning based on CampNet, which provides a new idea for\nspeech forgery tasks. The subjective and objective experiments on VCTK and\nLibriTTS datasets show that the speech editing results based on CampNet are\nbetter than TTS technology, manual editing, and VoCo method. We also conduct\ndetailed ablation experiments to explore the effect of the CampNet structure on\nits performance. Finally, the experiment shows that speaker adaptation with\nonly one sentence can further improve the naturalness of speech. Examples of\ngenerated speech can be found at https://hairuo55.github.io/CampNet.\n","authors":["Tao Wang","Jiangyan Yi","Ruibo Fu","Jianhua Tao","Zhengqi Wen"],"pdf_url":"https://arxiv.org/pdf/2202.09950v2.pdf","comment":"under review, 14 pages, 14 figures, demo page is available at\n  https://hairuo55.github.io/CampNet"},{"id":"http://arxiv.org/abs/2203.11670v1","updated":"2022-03-22T12:41:55Z","published":"2022-03-22T12:41:55Z","title":"Improving Meta-learning for Low-resource Text Classification and\n  Generation via Memory Imitation","summary":"  Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.\n","authors":["Yingxiu Zhao","Zhiliang Tian","Huaxiu Yao","Yinhe Zheng","Dongkyu Lee","Yiping Song","Jian Sun","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11670v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.11669v1","updated":"2022-03-22T12:41:42Z","published":"2022-03-22T12:41:42Z","title":"Are You Misinformed? A Study of Covid-Related Fake News in Bengali on\n  Facebook","summary":"  Our opinions and views of life can be shaped by how we perceive the opinions\nof others on social media like Facebook. This dependence has increased during\nCOVID-19 periods when we have fewer means to connect with others. However, fake\nnews related to COVID-19 has become a significant problem on Facebook. Bengali\nis the seventh most spoken language worldwide, yet we are aware of no previous\nresearch that studied the prevalence of COVID-19 related fake news in Bengali\non Facebook. In this paper, we develop machine learning models to detect fake\nnews in Bengali automatically. The best performing model is BERT, with an\nF1-score of 0.97. We apply BERT on all Facebook Bengali posts related to\nCOVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into\nthree categories: System (e.g., medical system), belief (e.g., religious\nrituals), and social (e.g., scientific awareness).\n","authors":["Protik Bose Pranto","Syed Zami-Ul-Haque Navid","Protik Dey","Gias Uddin","Anindya Iqbal"],"pdf_url":"https://arxiv.org/pdf/2203.11669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.05557v4","updated":"2022-03-22T12:02:01Z","published":"2021-05-12T10:18:14Z","title":"Supporting Land Reuse of Former Open Pit Mining Sites using Text\n  Classification and Active Learning","summary":"  Open pit mines left many regions worldwide inhospitable or uninhabitable. To\nput these regions back into use, entire stretches of land must be\nrenaturalized. For the sustainable subsequent use or transfer to a new primary\nuse, many contaminated sites and soil information have to be permanently\nmanaged. In most cases, this information is available in the form of expert\nreports in unstructured data collections or file folders, which in the best\ncase are digitized. Due to size and complexity of the data, it is difficult for\na single person to have an overview of this data in order to be able to make\nreliable statements. This is one of the most important obstacles to the rapid\ntransfer of these areas to after-use. An information-based approach to this\nissue supports fulfilling several Sustainable Development Goals regarding\nenvironment issues, health and climate action. We use a stack of Optical\nCharacter Recognition, Text Classification, Active Learning and Geographic\nInformation System Visualization to effectively mine and visualize this\ninformation. Subsequently, we link the extracted information to geographic\ncoordinates and visualize them using a Geographic Information System. Active\nLearning plays a vital role because our dataset provides no training data. In\ntotal, we process nine categories and actively learn their representation in\nour dataset. We evaluate the OCR, Active Learning and Text Classification\nseparately to report the performance of the system. Active Learning and text\nclassification results are twofold: Whereas our categories about restrictions\nwork sufficient ($>$.85 F1), the seven topic-oriented categories were\ncomplicated for human coders and hence the results achieved mediocre evaluation\nscores ($<$.70 F1).\n","authors":["Christopher Schröder","Kim Bürgl","Yves Annanias","Andreas Niekler","Lydia Müller","Daniel Wiegreffe","Christian Bender","Christoph Mengs","Gerik Scheuermann","Gerhard Heyer"],"pdf_url":"https://arxiv.org/pdf/2105.05557v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11639v1","updated":"2022-03-22T11:45:48Z","published":"2022-03-22T11:45:48Z","title":"Learning Relation-Specific Representations for Few-shot Knowledge Graph\n  Completion","summary":"  Recent years have witnessed increasing interest in few-shot knowledge graph\ncompletion (FKGC), which aims to infer unseen query triples for a few-shot\nrelation using a handful of reference triples of the relation. The primary\nfocus of existing FKGC methods lies in learning the relation representations\nthat can reflect the common information shared by the query and reference\ntriples. To this end, these methods learn the embeddings of entities with their\ndirect neighbors, and use the concatenation of the entity embeddings as the\nrelation representations. However, the entity embeddings learned only from\ndirect neighborhoods may have low expressiveness when the entity has sparse\nneighbors or shares a common local neighborhood with other entities. Moreover,\nthe embeddings of two entities are insufficient to represent the semantic\ninformation of their relationship, especially when they have multiple\nrelations. To address these issues, we propose a Relation-Specific Context\nLearning (RSCL) framework, which exploits graph contexts of triples to capture\nthe semantic information of relations and entities simultaneously.\nSpecifically, we first extract graph contexts for each triple, which can\nprovide long-term entity-relation dependencies. To model the graph contexts, we\nthen develop a hierarchical relation-specific learner to learn global and local\nrelation-specific representations for relations by capturing contextualized\ninformation of triples and incorporating local information of entities.\nFinally, we utilize the learned representations to predict the likelihood of\nthe query triples. Experimental results on two public datasets demonstrate that\nRSCL outperforms state-of-the-art FKGC methods.\n","authors":["Yuling Li","Kui Yu","Yuhong Zhang","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2203.11639v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.09163v2","updated":"2022-03-22T11:40:06Z","published":"2022-03-17T08:35:36Z","title":"Modeling Dual Read/Write Paths for Simultaneous Machine Translation","summary":"  Simultaneous machine translation (SiMT) outputs translation while reading\nsource sentence and hence requires a policy to decide whether to wait for the\nnext source word (READ) or generate a target word (WRITE), the actions of which\nform a read/write path. Although the read/write path is essential to SiMT\nperformance, no direct supervision is given to the path in the existing\nmethods. In this paper, we propose a method of dual-path SiMT which introduces\nduality constraints to direct the read/write path. According to duality\nconstraints, the read/write path in source-to-target and target-to-source SiMT\nmodels can be mapped to each other. As a result, the two SiMT models can be\noptimized jointly by forcing their read/write paths to satisfy the mapping.\nExperiments on En-Vi and De-En tasks show that our method can outperform strong\nbaselines under all latency.\n","authors":["Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2203.09163v2.pdf","comment":"Accept to ACL 2022 main conference. 19 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2105.03842v5","updated":"2022-03-22T11:31:08Z","published":"2021-05-09T05:35:36Z","title":"FastCorrect: Fast Error Correction with Edit Alignment for Automatic\n  Speech Recognition","summary":"  Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n","authors":["Yichong Leng","Xu Tan","Linchen Zhu","Jin Xu","Renqian Luo","Linquan Liu","Tao Qin","Xiang-Yang Li","Ed Lin","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2105.03842v5.pdf","comment":"NeurIPS 2021. Code URL:\n  https://github.com/microsoft/NeuralSpeech/tree/master/FastCorrect"},{"id":"http://arxiv.org/abs/2103.00293v4","updated":"2022-03-22T10:31:08Z","published":"2021-02-27T18:55:12Z","title":"N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking","summary":"  Augmentation of task-oriented dialogues has followed standard methods used\nfor plain-text such as back-translation, word-level manipulation, and\nparaphrasing despite its richly annotated structure. In this work, we introduce\nan augmentation framework that utilizes belief state annotations to match turns\nfrom various dialogues and form new synthetic dialogues in a bottom-up manner.\nUnlike other augmentation strategies, it operates with as few as five examples.\nOur augmentation strategy yields significant improvements when both adapting a\nDST model to a new domain, and when adapting a language model to the DST task,\non evaluations with TRADE and TOD-BERT models. Further analysis shows that our\nmodel performs better on seen values during training, and it is also more\nrobust to unseen values. We conclude that exploiting belief state annotations\nenhances dialogue augmentation and results in improved models in n-shot\ntraining scenarios.\n","authors":["Taha Aksu","Zhengyuan Liu","Min-Yen Kan","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2103.00293v4.pdf","comment":"Accepted by ACL 2022 Findings"},{"id":"http://arxiv.org/abs/2203.11591v1","updated":"2022-03-22T10:17:12Z","published":"2022-03-22T10:17:12Z","title":"HOP: History-and-Order Aware Pre-training for Vision-and-Language\n  Navigation","summary":"  Pre-training has been adopted in a few of recent works for\nVision-and-Language Navigation (VLN). However, previous pre-training methods\nfor VLN either lack the ability to predict future actions or ignore the\ntrajectory contexts, which are essential for a greedy navigation process. In\nthis work, to promote the learning of spatio-temporal visual-textual\ncorrespondence as well as the agent's capability of decision making, we propose\na novel history-and-order aware pre-training paradigm (HOP) with VLN-specific\nobjectives that exploit the past observations and support future action\nprediction. Specifically, in addition to the commonly used Masked Language\nModeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy\ntasks to model temporal order information: Trajectory Order Modeling (TOM) and\nGroup Order Modeling (GOM). Moreover, our navigation action prediction is also\nenhanced by introducing the task of Action Prediction with History (APH), which\ntakes into account the history visual perceptions. Extensive experimental\nresults on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the\neffectiveness of our proposed method compared against several state-of-the-art\nagents.\n","authors":["Yanyuan Qiao","Yuankai Qi","Yicong Hong","Zheng Yu","Peng Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2203.11591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11587v1","updated":"2022-03-22T10:13:27Z","published":"2022-03-22T10:13:27Z","title":"Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue","summary":"  Context modeling plays a significant role in building multi-turn dialogue\nsystems. In order to make full use of context information, systems can use\nIncomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue\ninto single-turn by merging current utterance and context information into a\nself-contained utterance. However, previous approaches ignore the intent\nconsistency between the original query and rewritten query. The detection of\nomitted or coreferred locations in the original query can be further improved.\nIn this paper, we introduce contrastive learning and multi-task learning to\njointly model the problem. Our method benefits from carefully designed\nself-supervised objectives, which act as auxiliary tasks to capture semantics\nat both sentence-level and token-level. The experiments show that our proposed\nmodel achieves state-of-the-art performance on several public datasets.\n","authors":["Zhihao Wang","Tangjian Duan","Zihao Wang","Minghui Yang","Zujie Wen","Yongliang Wang"],"pdf_url":"https://arxiv.org/pdf/2203.11587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10752v2","updated":"2022-03-22T10:10:19Z","published":"2022-03-21T06:50:21Z","title":"XTREME-S: Evaluating Cross-lingual Speech Representations","summary":"  We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual\nspeech representations in many languages. XTREME-S covers four task families:\nspeech recognition, classification, speech-to-text translation and retrieval.\nCovering 102 languages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech representation\nevaluation, as well as catalyze research in \"universal\" speech representation\nlearning. This paper describes the new benchmark and establishes the first\nspeech-only and speech-text baselines using XLS-R and mSLAM on all downstream\ntasks. We motivate the design choices and detail how to use the benchmark.\nDatasets and fine-tuning scripts are made easily accessible at\nhttps://hf.co/datasets/google/xtreme_s.\n","authors":["Alexis Conneau","Ankur Bapna","Yu Zhang","Min Ma","Patrick von Platen","Anton Lozhkov","Colin Cherry","Ye Jia","Clara Rivera","Mihir Kale","Daan Van Esch","Vera Axelrod","Simran Khanuja","Jonathan H. Clark","Orhan Firat","Michael Auli","Sebastian Ruder","Jason Riesa","Melvin Johnson"],"pdf_url":"https://arxiv.org/pdf/2203.10752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11562v1","updated":"2022-03-22T09:34:21Z","published":"2022-03-22T09:34:21Z","title":"A Text-to-Speech Pipeline, Evaluation Methodology, and Initial\n  Fine-Tuning Results for Child Speech Synthesis","summary":"  Speech synthesis has come a long way as current text-to-speech (TTS) models\ncan now generate natural human-sounding speech. However, most of the TTS\nresearch focuses on using adult speech data and there has been very limited\nwork done on child speech synthesis. This study developed and validated a\ntraining pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models\nusing child speech datasets. This approach adopts a multispeaker TTS retuning\nworkflow to provide a transfer-learning pipeline. A publicly available child\nspeech dataset was cleaned to provide a smaller subset of approximately 19\nhours, which formed the basis of our fine-tuning experiments. Both subjective\nand objective evaluations were performed using a pretrained MOSNet for\nobjective evaluation and a novel subjective framework for mean opinion score\n(MOS) evaluations. Subjective evaluations achieved the MOS of 3.92 for speech\nintelligibility, 3.85 for voice naturalness, and 3.96 for voice consistency.\nObjective evaluation using a pretrained MOSNet showed a strong correlation\nbetween real and synthetic child voices. The final trained model was able to\nsynthesize child-like speech from reference audio samples as short as 5\nseconds.\n","authors":["Rishabh Jain","Mariam Yiwere","Dan Bigioi","Peter Corcoran","Horia Cucu"],"pdf_url":"https://arxiv.org/pdf/2203.11562v1.pdf","comment":"Submitted to IEEE ACCESS"},{"id":"http://arxiv.org/abs/2203.11552v1","updated":"2022-03-22T09:15:53Z","published":"2022-03-22T09:15:53Z","title":"Factual Consistency of Multilingual Pretrained Language Models","summary":"  Pretrained language models can be queried for factual knowledge, with\npotential applications in knowledge base acquisition and tasks that require\ninference. However, for that, we need to know how reliable this knowledge is,\nand recent work has shown that monolingual English language models lack\nconsistency when predicting factual knowledge, that is, they fill-in-the-blank\ndifferently for paraphrases describing the same fact. In this paper, we extend\nthe analysis of consistency to a multilingual setting. We introduce a resource,\nmParaRel, and investigate (i) whether multilingual language models such as\nmBERT and XLM-R are more consistent than their monolingual counterparts; and\n(ii) if such models are equally consistent across languages. We find that mBERT\nis as inconsistent as English BERT in English paraphrases, but that both mBERT\nand XLM-R exhibit a high degree of inconsistency in English and even more so\nfor all the other 45 languages.\n","authors":["Constanza Fierro","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2203.11552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14747v2","updated":"2022-03-22T09:14:27Z","published":"2021-10-27T20:17:47Z","title":"Dynamic Review-based Recommenders","summary":"  Just as user preferences change with time, item reviews also reflect those\nsame preference changes. In a nutshell, if one is to sequentially incorporate\nreview content knowledge into recommender systems, one is naturally led to\ndynamical models of text. In the present work we leverage the known power of\nreviews to enhance rating predictions in a way that (i) respects the causality\nof review generation and (ii) includes, in a bidirectional fashion, the ability\nof ratings to inform language review models and vice-versa, language\nrepresentations that help predict ratings end-to-end. Moreover, our\nrepresentations are time-interval aware and thus yield a continuous-time\nrepresentation of the dynamics. We provide experiments on real-world datasets\nand show that our methodology is able to outperform several state-of-the-art\nmodels. Source code for all models can be found at [1].\n","authors":["Kostadin Cvejoski","Ramses J. Sanchez","Christian Bauckhage","Cesar Ojeda"],"pdf_url":"https://arxiv.org/pdf/2110.14747v2.pdf","comment":"6pages, Published at International Data Science Conference 2021\n  (iDSC21)"},{"id":"http://arxiv.org/abs/2203.11486v1","updated":"2022-03-22T06:33:01Z","published":"2022-03-22T06:33:01Z","title":"Approaches for Improving the Performance of Fake News Detection in\n  Bangla: Imbalance Handling and Model Stacking","summary":"  Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.\n","authors":["Md Muzakker Hossain","Zahin Awosaf","Md. Salman Hossan Prottoy","Abu Saleh Muhammod Alvy","Md. Kishor Morol"],"pdf_url":"https://arxiv.org/pdf/2203.11486v1.pdf","comment":"12 pages, 8 figures, To appear in the Proceedings of the\n  International Conference on 4th Industrial Revolution and Beyond (IC4IR),\n  10-11 December 2021, Dhaka, Bangladesh"},{"id":"http://arxiv.org/abs/2203.11480v1","updated":"2022-03-22T06:12:20Z","published":"2022-03-22T06:12:20Z","title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models","summary":"  Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n","authors":["Sha Yuan","Zhao Shuai","Leng Jiahong","Xue Zhao","Zhao Hanyu","Tang Jie"],"pdf_url":"https://arxiv.org/pdf/2203.11480v1.pdf","comment":"7 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2203.11476v1","updated":"2022-03-22T06:04:34Z","published":"2022-03-22T06:04:34Z","title":"Modeling speech recognition and synthesis simultaneously: Encoding and\n  decoding lexical and sublexical semantic information into speech with no\n  direct access to speech data","summary":"  Human speakers encode information into raw speech which is then decoded by\nthe listeners. This complex relationship between encoding (production) and\ndecoding (perception) is often modeled separately. Here, we test how decoding\nof lexical and sublexical semantic information can emerge automatically from\nraw speech in unsupervised generative deep convolutional networks that combine\nboth the production and perception principle. We introduce, to our knowledge,\nthe most challenging objective in unsupervised lexical learning: an\nunsupervised network that must learn to assign unique representations for\nlexical items with no direct access to training data. We train several models\n(ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic\nlexical items in the unobserved test data. Strong evidence in favor of lexical\nlearning emerges. The architecture that combines the production and perception\nprinciples is thus able to learn to decode unique information from raw acoustic\ndata in an unsupervised manner without ever accessing real training data. We\npropose a technique to explore lexical and sublexical learned representations\nin the classifier network. The results bear implications for both unsupervised\nspeech synthesis and recognition as well as for unsupervised semantic modeling\nas language models increasingly bypass text and operate from raw acoustics.\n","authors":["Gašper Beguš","Alan Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.11476v1.pdf","comment":"Submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.10326v2","updated":"2022-03-22T06:01:39Z","published":"2022-03-19T13:29:48Z","title":"Pretraining with Artificial Language: Studying Transferable Knowledge in\n  Language Models","summary":"  We investigate what kind of structural knowledge learned in neural network\nencoders is transferable to processing natural language. We design artificial\nlanguages with structural properties that mimic natural language, pretrain\nencoders on the data, and see how much performance the encoder exhibits on\ndownstream tasks in natural language. Our experimental results show that\npretraining with an artificial language with a nesting dependency structure\nprovides some knowledge transferable to natural language. A follow-up probing\nanalysis indicates that its success in the transfer is related to the amount of\nencoded contextual information and what is transferred is the knowledge of\nposition-aware context dependence of language. Our results provide insights\ninto how neural network encoders process human languages and the source of\ncross-lingual transferability of recent multilingual language models.\n","authors":["Ryokan Ri","Yoshimasa Tsuruoka"],"pdf_url":"https://arxiv.org/pdf/2203.10326v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2109.07368v4","updated":"2022-03-22T05:55:07Z","published":"2021-09-15T15:22:10Z","title":"Learning When to Translate for Streaming Speech","summary":"  How to find proper moments to generate partial sentence translation given a\nstreaming speech input? Existing approaches waiting-and-translating for a fixed\nduration often break the acoustic units in speech, since the boundaries between\nacoustic units in speech are not even. In this paper, we propose MoSST, a\nsimple yet effective method for translating streaming speech content. Given a\nusually long speech sequence, we develop an efficient monotonic segmentation\nmodule inside an encoder-decoder model to accumulate acoustic information\nincrementally and detect proper speech unit boundaries for the input in speech\ntranslation task. Experiments on multiple translation directions of the MuST-C\ndataset show that MoSST outperforms existing methods and achieves the best\ntrade-off between translation quality (BLEU) and latency. Our code is available\nat https://github.com/dqqcasia/mosst.\n","authors":["Qianqian Dong","Yaoming Zhu","Mingxuan Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2109.07368v4.pdf","comment":"Accept to ACL 2022 main conference. 15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2111.11133v9","updated":"2022-03-22T05:02:36Z","published":"2021-11-22T11:48:26Z","title":"L-Verse: Bidirectional Generation Between Image and Text","summary":"  Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n","authors":["Taehoon Kim","Gwangmo Song","Sihaeng Lee","Sangyun Kim","Yewon Seo","Soonyoung Lee","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae"],"pdf_url":"https://arxiv.org/pdf/2111.11133v9.pdf","comment":"Accepted to CVPR 2022 (Oral)"},{"id":"http://arxiv.org/abs/2104.08790v4","updated":"2022-03-22T04:42:13Z","published":"2021-04-18T09:50:11Z","title":"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News\n  Headlines","summary":"  Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.\nfeeling distrust), and behaviorally (e.g. sharing the news with their friends).\nSuch reactions are instantaneous and yet complex, as they rely on factors that\ngo beyond interpreting factual content of news. We propose Misinfo Reaction\nFrames (MRF), a pragmatic formalism for modeling how readers might react to a\nnews headline. In contrast to categorical schema, our free-text dimensions\nprovide a more nuanced way of understanding intent beyond being benign or\nmalicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced\ndataset of reactions to over 25k news headlines focusing on global crises: the\nCovid-19 pandemic, climate change, and cancer. Empirical results confirm that\nit is indeed possible for neural models to predict the prominent patterns of\nreaders' reactions to previously unseen news headlines. Additionally, our user\nstudy shows that displaying machine-generated MRF implications alongside news\nheadlines to readers can increase their trust in real news while decreasing\ntheir trust in misinformation. Our work demonstrates the feasibility and\nimportance of pragmatic inferences on news headlines to help enhance AI-guided\nmisinformation detection and mitigation.\n","authors":["Saadia Gabriel","Skyler Hallinan","Maarten Sap","Pemi Nguyen","Franziska Roesner","Eunsol Choi","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2104.08790v4.pdf","comment":"ACL 2022 camera-ready"},{"id":"http://arxiv.org/abs/2203.11443v1","updated":"2022-03-22T03:34:10Z","published":"2022-03-22T03:34:10Z","title":"Demo of the Linguistic Field Data Management and Analysis System -- LiFE","summary":"  In the proposed demo, we will present a new software - Linguistic Field Data\nManagement and Analysis System - LiFE (https://github.com/kmi-linguistics/life)\n- an open-source, web-based linguistic data management and analysis application\nthat allows for systematic storage, management, sharing and usage of linguistic\ndata collected from the field. The application allows users to store lexical\nitems, sentences, paragraphs, audio-visual content with rich glossing /\nannotation; generate interactive and print dictionaries; and also train and use\nnatural language processing tools and models for various purposes using this\ndata. Since its a web-based application, it also allows for seamless\ncollaboration among multiple persons and sharing the data, models, etc with\neach other.\n  The system uses the Python-based Flask framework and MongoDB in the backend\nand HTML, CSS and Javascript at the frontend. The interface allows creation of\nmultiple projects that could be shared with the other users. At the backend,\nthe application stores the data in RDF format so as to allow its release as\nLinked Data over the web using semantic web technologies - as of now it makes\nuse of the OntoLex-Lemon for storing the lexical data and Ligt for storing the\ninterlinear glossed text and then internally linking it to the other linked\nlexicons and databases such as DBpedia and WordNet. Furthermore it provides\nsupport for training the NLP systems using scikit-learn and HuggingFace\nTransformers libraries as well as make use of any model trained using these\nlibraries - while the user interface itself provides limited options for tuning\nthe system, an externally-trained model could be easily incorporated within the\napplication; similarly the dataset itself could be easily exported into a\nstandard machine-readable format like JSON or CSV that could be consumed by\nother programs and pipelines.\n","authors":["Siddharth Singh","Ritesh Kumar","Shyam Ratan","Sonal Sinha"],"pdf_url":"https://arxiv.org/pdf/2203.11443v1.pdf","comment":"Accepted in the 19th International Conference on Natural Language\n  Processing (ICON-2021)"},{"id":"http://arxiv.org/abs/2203.11431v1","updated":"2022-03-22T03:11:39Z","published":"2022-03-22T03:11:39Z","title":"Task-guided Disentangled Tuning for Pretrained Language Models","summary":"  Pretrained language models (PLMs) trained on large-scale unlabeled corpus are\ntypically fine-tuned on task-specific downstream datasets, which have produced\nstate-of-the-art results on various NLP tasks. However, the data discrepancy\nissue in domain and scale makes fine-tuning fail to efficiently capture\ntask-specific patterns, especially in the low data regime. To address this\nissue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which\nenhances the generalization of representations by disentangling task-relevant\nsignals from the entangled representations. For a given task, we introduce a\nlearnable confidence model to detect indicative guidance from context, and\nfurther propose a disentangled regularization to mitigate the over-reliance\nproblem. Experimental results on GLUE and CLUE benchmarks show that TDT gives\nconsistently better results than fine-tuning with different PLMs, and extensive\nanalysis demonstrates the effectiveness and robustness of our method. Code is\navailable at https://github.com/lemon0830/TDT.\n","authors":["Jiali Zeng","Yufan Jiang","Shuangzhi Wu","Yongjing Yin","Mu Li"],"pdf_url":"https://arxiv.org/pdf/2203.11431v1.pdf","comment":"Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2203.11425v1","updated":"2022-03-22T02:44:39Z","published":"2022-03-22T02:44:39Z","title":"Towards Abstractive Grounded Summarization of Podcast Transcripts","summary":"  Podcasts have recently shown a rapid rise in popularity. Summarization of\npodcast transcripts is of practical benefit to both content providers and\nconsumers. It helps consumers to quickly decide whether they will listen to the\npodcasts and reduces the cognitive load of content providers to write\nsummaries. Nevertheless, podcast summarization faces significant challenges\nincluding factual inconsistencies with respect to the inputs. The problem is\nexacerbated by speech disfluencies and recognition errors in transcripts of\nspoken language. In this paper, we explore a novel abstractive summarization\nmethod to alleviate these challenges. Specifically, our approach learns to\nproduce an abstractive summary while grounding summary segments in specific\nportions of the transcript to allow for full inspection of summary details. We\nconduct a series of analyses of the proposed approach on a large podcast\ndataset and show that the approach can achieve promising results. Grounded\nsummaries bring clear benefits in locating the summary and transcript segments\nthat contain inconsistent information, and hence significantly improve\nsummarization quality in both automatic and human evaluation metrics.\n","authors":["Kaiqiang Song","Chen Li","Xiaoyang Wang","Dong Yu","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.07476v2","updated":"2022-03-22T02:03:37Z","published":"2021-10-14T15:49:40Z","title":"Query and Extract: Refining Event Extraction as Type-oriented Binary\n  Decoding","summary":"  Event extraction is typically modeled as a multi-class classification problem\nwhere event types and argument roles are treated as atomic symbols. These\napproaches are usually limited to a set of pre-defined types. We propose a\nnovel event extraction framework that uses event types and argument roles as\nnatural language queries to extract candidate triggers and arguments from the\ninput text. With the rich semantics in the queries, our framework benefits from\nthe attention mechanisms to better capture the semantic correlation between the\nevent types or argument roles and the input text. Furthermore, the\nquery-and-extract formulation allows our approach to leverage all available\nevent annotations from various ontologies as a unified model. Experiments on\nACE and ERE demonstrate that our approach achieves state-of-the-art performance\non each dataset and significantly outperforms existing methods on zero-shot\nevent extraction.\n","authors":["Sijia Wang","Mo Yu","Shiyu Chang","Lichao Sun","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2110.07476v2.pdf","comment":"14pages, ACL'2022"},{"id":"http://arxiv.org/abs/2203.10430v2","updated":"2022-03-22T01:52:48Z","published":"2022-03-20T02:28:25Z","title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation\n  in Mandarin","summary":"  Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.\n","authors":["Yi-Chang Chen","Yu-Chuan Chang","Yen-Cheng Chang","Yi-Ren Yeh"],"pdf_url":"https://arxiv.org/pdf/2203.10430v2.pdf","comment":"submitted to Insterspeech 2022"},{"id":"http://arxiv.org/abs/2203.11413v1","updated":"2022-03-22T01:51:58Z","published":"2022-03-22T01:51:58Z","title":"Learning Confidence for Transformer-based Neural Machine Translation","summary":"  Confidence estimation aims to quantify the confidence of the model\nprediction, providing an expectation of success. A well-calibrated confidence\nestimate enables accurate failure prediction and proper risk measurement when\ngiven noisy samples and out-of-distribution data in real-world settings.\nHowever, this task remains a severe challenge for neural machine translation\n(NMT), where probabilities from softmax distribution fail to describe when the\nmodel is probably mistaken. To address this problem, we propose an unsupervised\nconfidence estimate learning jointly with the training of the NMT model. We\nexplain confidence as how many hints the NMT model needs to make a correct\nprediction, and more hints indicate low confidence. Specifically, the NMT model\nis given the option to ask for hints to improve translation accuracy at the\ncost of some slight penalty. Then, we approximate their level of confidence by\ncounting the number of hints the model uses. We demonstrate that our learned\nconfidence estimate achieves high accuracy on extensive sentence/word-level\nquality estimation tasks. Analytical results verify that our confidence\nestimate can correctly assess underlying risk in two real-world scenarios: (1)\ndiscovering noisy samples and (2) detecting out-of-domain data. We further\npropose a novel confidence-based instance-specific label smoothing approach\nbased on our learned confidence estimate, which outperforms standard label\nsmoothing.\n","authors":["Yu Lu","Jiali Zeng","Jiajun Zhang","Shuangzhi Wu","Mu Li"],"pdf_url":"https://arxiv.org/pdf/2203.11413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11401v1","updated":"2022-03-22T00:45:39Z","published":"2022-03-22T00:45:39Z","title":"Suum Cuique: Studying Bias in Taboo Detection with a Community\n  Perspective","summary":"  Prior research has discussed and illustrated the need to consider linguistic\nnorms at the community level when studying taboo (hateful/offensive/toxic etc.)\nlanguage. However, a methodology for doing so, that is firmly founded on\ncommunity language norms is still largely absent. This can lead both to biases\nin taboo text classification and limitations in our understanding of the causes\nof bias. We propose a method to study bias in taboo classification and\nannotation where a community perspective is front and center. This is\naccomplished by using special classifiers tuned for each community's language.\nIn essence, these classifiers represent community level language norms. We use\nthese to study bias and find, for example, biases are largest against African\nAmericans (7/10 datasets and all 3 classifiers examined). In contrast to\nprevious papers we also study other communities and find, for example, strong\nbiases against South Asians. In a small scale user study we illustrate our key\nidea which is that common utterances, i.e., those with high alignment scores\nwith a community (community classifier confidence scores) are unlikely to be\nregarded taboo. Annotators who are community members contradict taboo\nclassification decisions and annotations in a majority of instances. This paper\nis a significant step toward reducing false positive taboo decisions that over\ntime harm minority communities.\n","authors":["Osama Khalid","Jonathan Rusert","Padmini Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2203.11401v1.pdf","comment":"9 pages, 3 figures, Accepted to the Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2203.11400v1","updated":"2022-03-22T00:44:41Z","published":"2022-03-22T00:44:41Z","title":"VLSP 2021 Shared Task: Vietnamese Machine Reading Comprehension","summary":"  One of the emerging research trends in natural language understanding is\nmachine reading comprehension (MRC) which is the task to find answers to human\nquestions based on textual data. Existing Vietnamese datasets for MRC research\nconcentrate solely on answerable questions. However, in reality, questions can\nbe unanswerable for which the correct answer is not stated in the given textual\ndata. To address the weakness, we provide the research community with a\nbenchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question\nanswering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a\nbenchmark dataset for the shared task on Vietnamese MRC at the Eighth Workshop\non Vietnamese Language and Speech Processing (VLSP 2021). This task attracted\n77 participant teams from 34 universities and other organizations. In this\narticle, we present details of the organization of the shared task, an overview\nof the methods employed by shared-task participants, and the results. The\nhighest performances are 77.24% EM and 67.43% F1-score on the private test set.\nThe Vietnamese MRC systems proposed by the top 3 teams use XLM-RoBERTa, a\npowerful pre-trained language model using the transformer architecture. The\nUIT-ViQuAD 2.0 dataset motivates more researchers to explore Vietnamese machine\nreading comprehension, question answering, and question generation.\n","authors":["Kiet Van Nguyen","Son Quoc Tran","Luan Thanh Nguyen","Tin Van Huynh","Son T. Luu","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2203.11400v1.pdf","comment":"VLSP 2021"},{"id":"http://arxiv.org/abs/2203.11399v1","updated":"2022-03-22T00:42:27Z","published":"2022-03-22T00:42:27Z","title":"Achieving Conversational Goals with Unsupervised Post-hoc Knowledge\n  Injection","summary":"  A limitation of current neural dialog models is that they tend to suffer from\na lack of specificity and informativeness in generated responses, primarily due\nto dependence on training data that covers a limited variety of scenarios and\nconveys limited knowledge. One way to alleviate this issue is to extract\nrelevant knowledge from external sources at decoding time and incorporate it\ninto the dialog response. In this paper, we propose a post-hoc\nknowledge-injection technique where we first retrieve a diverse set of relevant\nknowledge snippets conditioned on both the dialog history and an initial\nresponse from an existing dialog model. We construct multiple candidate\nresponses, individually injecting each retrieved snippet into the initial\nresponse using a gradient-based decoding method, and then select the final\nresponse with an unsupervised ranking step. Our experiments in goal-oriented\nand knowledge-grounded dialog settings demonstrate that human annotators judge\nthe outputs from the proposed method to be more engaging and informative\ncompared to responses from prior dialog systems. We further show that\nknowledge-augmentation promotes success in achieving conversational goals in\nboth experimental settings.\n","authors":["Bodhisattwa Prasad Majumder","Harsh Jhamtani","Taylor Berg-Kirkpatrick","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2203.11399v1.pdf","comment":"Accepted at ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2203.11396v1","updated":"2022-03-22T00:11:46Z","published":"2022-03-22T00:11:46Z","title":"Towards Textual Out-of-Domain Detection without In-Domain Labels","summary":"  In many real-world settings, machine learning models need to identify user\ninputs that are out-of-domain (OOD) so as to avoid performing wrong actions.\nThis work focuses on a challenging case of OOD detection, where no labels for\nin-domain data are accessible (e.g., no intent labels for the intent\nclassification task). To this end, we first evaluate different language model\nbased approaches that predict likelihood for a sequence of tokens. Furthermore,\nwe propose a novel representation learning based method by combining\nunsupervised clustering and contrastive learning so that better data\nrepresentations for OOD detection can be learned. Through extensive\nexperiments, we demonstrate that this method can significantly outperform\nlikelihood-based methods and can be even competitive to the state-of-the-art\nsupervised approaches with label information.\n","authors":["Di Jin","Shuyang Gao","Seokhwan Kim","Yang Liu","Dilek Hakkani-Tur"],"pdf_url":"https://arxiv.org/pdf/2203.11396v1.pdf","comment":"Accepted by IEEE/ACM Transactions on Audio Speech and Language"},{"id":"http://arxiv.org/abs/2203.12067v1","updated":"2022-03-22T21:59:29Z","published":"2022-03-22T21:59:29Z","title":"Building Robust Spoken Language Understanding by Cross Attention between\n  Phoneme Sequence and ASR Hypothesis","summary":"  Building Spoken Language Understanding (SLU) robust to Automatic Speech\nRecognition (ASR) errors is an essential issue for various voice-enabled\nvirtual assistants. Considering that most ASR errors are caused by phonetic\nconfusion between similar-sounding expressions, intuitively, leveraging the\nphoneme sequence of speech can complement ASR hypothesis and enhance the\nrobustness of SLU. This paper proposes a novel model with Cross Attention for\nSLU (denoted as CASLU). The cross attention block is devised to catch the\nfine-grained interactions between phoneme and word embeddings in order to make\nthe joint representations catch the phonetic and semantic features of input\nsimultaneously and for overcoming the ASR errors in downstream natural language\nunderstanding (NLU) tasks. Extensive experiments are conducted on three\ndatasets, showing the effectiveness and competitiveness of our approach.\nAdditionally, We also validate the universality of CASLU and prove its\ncomplementarity when combining with other robust SLU techniques.\n","authors":["Zexun Wang","Yuquan Le","Yi Zhu","Yuming Zhao","Mingchao Feng","Meng Chen","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2203.12067v1.pdf","comment":"ICASSP 2022"},{"id":"http://arxiv.org/abs/2203.08307v2","updated":"2022-03-22T21:40:55Z","published":"2022-03-15T22:51:22Z","title":"Improving Word Translation via Two-Stage Contrastive Learning","summary":"  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n","authors":["Yaoyiran Li","Fangyu Liu","Nigel Collier","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2203.08307v2.pdf","comment":"ACL 2022 Main"},{"id":"http://arxiv.org/abs/2203.09742v2","updated":"2022-03-22T19:56:04Z","published":"2022-03-18T04:52:54Z","title":"GRS: Combining Generation and Revision in Unsupervised Sentence\n  Simplification","summary":"  We propose GRS: an unsupervised approach to sentence simplification that\ncombines text generation and text revision. We start with an iterative\nframework in which an input sentence is revised using explicit edit operations,\nand add paraphrasing as a new edit operation. This allows us to combine the\nadvantages of generative and revision-based approaches: paraphrasing captures\ncomplex edit operations, and the use of explicit edit operations in an\niterative manner provides controllability and interpretability. We demonstrate\nthese advantages of GRS compared to existing methods on the Newsela and ASSET\ndatasets.\n","authors":["Mohammad Dehghan","Dhruv Kumar","Lukasz Golab"],"pdf_url":"https://arxiv.org/pdf/2203.09742v2.pdf","comment":"The paper has been accepted to Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2110.02952v2","updated":"2022-03-22T19:09:39Z","published":"2021-10-06T17:58:42Z","title":"Hierarchical prosody modeling and control in non-autoregressive parallel\n  neural TTS","summary":"  Neural text-to-speech (TTS) synthesis can generate speech that is\nindistinguishable from natural speech. However, the synthetic speech often\nrepresents the average prosodic style of the database instead of having more\nversatile prosodic variation. Moreover, many models lack the ability to control\nthe output prosody, which does not allow for different styles for the same text\ninput. In this work, we train a non-autoregressive parallel neural TTS\nfront-end model hierarchically conditioned on both coarse and fine-grained\nacoustic speech features to learn a latent prosody space with intuitive and\nmeaningful dimensions. Experiments show that a non-autoregressive TTS model\nhierarchically conditioned on utterance-wise pitch, pitch range, duration,\nenergy, and spectral tilt can effectively control each prosodic dimension,\ngenerate a wide variety of speaking styles, and provide word-wise emphasis\ncontrol, while maintaining equal or better quality to the baseline model.\n","authors":["Tuomo Raitio","Jiangchuan Li","Shreyas Seshadri"],"pdf_url":"https://arxiv.org/pdf/2110.02952v2.pdf","comment":"5 pages, 5 figures, preprint accepted to ICASSP 2022. arXiv admin\n  note: text overlap with arXiv:2009.06775"},{"id":"http://arxiv.org/abs/2203.12000v1","updated":"2022-03-22T19:02:43Z","published":"2022-03-22T19:02:43Z","title":"Text Transformations in Contrastive Self-Supervised Learning: A Review","summary":"  Contrastive self-supervised learning has become a prominent technique in\nrepresentation learning. The main step in these methods is to contrast\nsemantically similar and dissimilar pairs of samples. However, in the domain of\nNatural Language, the augmentation methods used in creating similar pairs with\nregard to contrastive learning assumptions are challenging. This is because,\neven simply modifying a word in the input might change the semantic meaning of\nthe sentence, and hence, would violate the distributional hypothesis. In this\nreview paper, we formalize the contrastive learning framework in the domain of\nnatural language processing. We emphasize the considerations that need to be\naddressed in the data transformation step and review the state-of-the-art\nmethods and evaluations for contrastive representation learning in NLP.\nFinally, we describe some challenges and potential directions for learning\nbetter text representations using contrastive methods.\n","authors":["Amrita Bhattacharjee","Mansooreh Karami","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2203.12000v1.pdf","comment":"under review at IJCAI'22 Survey Track"},{"id":"http://arxiv.org/abs/2203.12667v1","updated":"2022-03-22T16:58:10Z","published":"2022-03-22T16:58:10Z","title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future\n  Directions","summary":"  A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n","authors":["Jing Gu","Eliana Stefani","Qi Wu","Jesse Thomason","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12667v1.pdf","comment":"18 pages. Accepted to ACL 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2203.11938v1","updated":"2022-03-22T17:59:57Z","published":"2022-03-22T17:59:57Z","title":"φ-SfT: Shape-from-Template with a Physics-Based Deformation Model","summary":"  Shape-from-Template (SfT) methods estimate 3D surface deformations from a\nsingle monocular RGB camera while assuming a 3D state known in advance (a\ntemplate). This is an important yet challenging problem due to the\nunder-constrained nature of the monocular setting. Existing SfT techniques\npredominantly use geometric and simplified deformation models, which often\nlimits their reconstruction abilities. In contrast to previous works, this\npaper proposes a new SfT approach explaining 2D observations through physical\nsimulations accounting for forces and material properties. Our differentiable\nphysics simulator regularises the surface evolution and optimises the material\nelastic properties such as bending coefficients, stretching stiffness and\ndensity. We use a differentiable renderer to minimise the dense reprojection\nerror between the estimated 3D states and the input images and recover the\ndeformation parameters using an adaptive gradient-based optimisation. For the\nevaluation, we record with an RGB-D camera challenging real surfaces exposed to\nphysical forces with various material properties and textures. Our approach\nsignificantly reduces the 3D reconstruction error compared to multiple\ncompeting methods. For the source code and data, see\nhttps://4dqv.mpi-inf.mpg.de/phi-SfT/.\n","authors":["Navami Kairanda","Edith Tretschk","Mohamed Elgharib","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2203.11938v1.pdf","comment":"11 pages, 8 figures and one table; Computer Vision and Pattern\n  Recognition (CVPR) 2022"},{"id":"http://arxiv.org/abs/2203.11937v1","updated":"2022-03-22T17:59:45Z","published":"2022-03-22T17:59:45Z","title":"4D-OR: Semantic Scene Graphs for OR Domain Modeling","summary":"  Surgical procedures are conducted in highly complex operating rooms (OR),\ncomprising different actors, devices, and interactions. To date, only medically\ntrained human experts are capable of understanding all the links and\ninteractions in such a demanding environment. This paper aims to bring the\ncommunity one step closer to automated, holistic and semantic understanding and\nmodeling of OR domain. Towards this goal, for the first time, we propose using\nsemantic scene graphs (SSG) to describe and summarize the surgical scene. The\nnodes of the scene graphs represent different actors and objects in the room,\nsuch as medical staff, patients, and medical equipment, whereas edges are the\nrelationships between them. To validate the possibilities of the proposed\nrepresentation, we create the first publicly available 4D surgical SSG dataset,\n4D-OR, containing ten simulated total knee replacement surgeries recorded with\nsix RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734\nframes and is richly annotated with SSGs, human and object poses, and clinical\nroles. We propose an end-to-end neural network-based SSG generation pipeline,\nwith a rate of success of 0.75 macro F1, indeed being able to infer semantic\nreasoning in the OR. We further demonstrate the representation power of our\nscene graphs by using it for the problem of clinical role prediction, where we\nachieve 0.85 macro F1. The code and dataset will be made available upon\nacceptance.\n","authors":["Ege Özsoy","Evin Pınar Örnek","Ulrich Eck","Tobias Czempiel","Federico Tombari","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2203.11937v1.pdf","comment":"11 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2203.11933v1","updated":"2022-03-22T17:59:04Z","published":"2022-03-22T17:59:04Z","title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models\n  with Adversarial Learning","summary":"  Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n","authors":["Hugo Berg","Siobhan Mackenzie Hall","Yash Bhalgat","Wonsuk Yang","Hannah Rose Kirk","Aleksandar Shtedritski","Max Bain"],"pdf_url":"https://arxiv.org/pdf/2203.11933v1.pdf","comment":"24 pages, 10 figures. For code and trained token embeddings, see\n  https://github.com/oxai/debias-vision-lang"},{"id":"http://arxiv.org/abs/2203.11934v1","updated":"2022-03-22T17:59:04Z","published":"2022-03-22T17:59:04Z","title":"Learning from All Vehicles","summary":"  In this paper, we present a system to train driving policies from experiences\ncollected not just from the ego-vehicle, but all vehicles that it observes.\nThis system uses the behaviors of other agents to create more diverse driving\nscenarios without collecting additional data. The main difficulty in learning\nfrom other vehicles is that there is no sensor information. We use a set of\nsupervisory tasks to learn an intermediate representation that is invariant to\nthe viewpoint of the controlling vehicle. This not only provides a richer\nsignal at training time but also allows more complex reasoning during\ninference. Learning how all vehicles drive helps predict their behavior at test\ntime and can avoid collisions. We evaluate this system in closed-loop driving\nsimulations. Our system outperforms all prior methods on the public CARLA\nLeaderboard by a wide margin, improving driving score by 25 and route\ncompletion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving\nchallenge. Demo videos are available at https://dotchen.github.io/LAV/.\n","authors":["Dian Chen","Philipp Krähenbühl"],"pdf_url":"https://arxiv.org/pdf/2203.11934v1.pdf","comment":"Paper accepted to CVPR 2022; Code and data available at\n  https://github.com/dotchen/LAV"},{"id":"http://arxiv.org/abs/2203.11932v1","updated":"2022-03-22T17:58:59Z","published":"2022-03-22T17:58:59Z","title":"Dataset Distillation by Matching Training Trajectories","summary":"  Dataset distillation is the task of synthesizing a small dataset such that a\nmodel trained on the synthetic set will match the test accuracy of the model\ntrained on the full dataset. In this paper, we propose a new formulation that\noptimizes our distilled data to guide networks to a similar state as those\ntrained on real data across many training steps. Given a network, we train it\nfor several iterations on our distilled data and optimize the distilled data\nwith respect to the distance between the synthetically trained parameters and\nthe parameters trained on real data. To efficiently obtain the initial and\ntarget network parameters for large-scale datasets, we pre-compute and store\ntraining trajectories of expert networks trained on the real dataset. Our\nmethod handily outperforms existing methods and also allows us to distill\nhigher-resolution visual data.\n","authors":["George Cazenavette","Tongzhou Wang","Antonio Torralba","Alexei A. Efros","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.11932v1.pdf","comment":"CVPR 2022 website:\n  https://georgecazenavette.github.io/mtt-distillation/ code:\n  https://github.com/GeorgeCazenavette/mtt-distillation"},{"id":"http://arxiv.org/abs/2203.11926v1","updated":"2022-03-22T17:54:50Z","published":"2022-03-22T17:54:50Z","title":"Focal Modulation Networks","summary":"  In this work, we propose focal modulation network (FocalNet in short), where\nself-attention (SA) is completely replaced by a focal modulation module that is\nmore effective and efficient for modeling token interactions. Focal modulation\ncomprises three components: $(i)$ hierarchical contextualization, implemented\nusing a stack of depth-wise convolutional layers, to encode visual contexts\nfrom short to long ranges at different granularity levels, $(ii)$ gated\naggregation to selectively aggregate context features for each visual token\n(query) based on its content, and $(iii)$ modulation or element-wise affine\ntransformation to fuse the aggregated features into the query vector. Extensive\nexperiments show that FocalNets outperform the state-of-the-art SA counterparts\n(e.g., Swin Transformers) with similar time and memory cost on the tasks of\nimage classification, object detection, and semantic segmentation.\nSpecifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9%\ntop-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains\n86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\\times$224\nand 384$\\times$384, respectively. FocalNets exhibit remarkable superiority when\ntransferred to downstream tasks. For object detection with Mask R-CNN, our\nFocalNet base trained with 1$\\times$ already surpasses Swin trained with\n3$\\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet,\nFocalNet base evaluated at single-scale outperforms Swin evaluated at\nmulti-scale (50.5 v.s. 49.7). These results render focal modulation a favorable\nalternative to SA for effective and efficient visual modeling in real-world\napplications. Code is available at https://github.com/microsoft/FocalNet.\n","authors":["Jianwei Yang","Chunyuan Li","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2203.11926v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2109.02748v3","updated":"2022-03-22T17:53:38Z","published":"2021-09-06T21:27:43Z","title":"Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model\n  CLIP","summary":"  In an out-of-distribution (OOD) detection problem, samples of known\nclasses(also called in-distribution classes) are used to train a special\nclassifier. In testing, the classifier can (1) classify the test samples of\nknown classes to their respective classes and also (2) detect samples that do\nnot belong to any of the known classes (i.e., they belong to some unknown or\nOOD classes). This paper studies the problem of zero-shot\nout-of-distribution(OOD) detection, which still performs the same two tasks in\ntesting but has no training except using the given known class names. This\npaper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC\nbuilds on top of the recent advances in zero-shot classification through\nmulti-modal representation learning. It first extends the pre-trained\nlanguage-vision model CLIP by training a text-based image description generator\non top of CLIP. In testing, it uses the extended model to generate candidate\nunknown class names for each test sample and computes a confidence score based\non both the known class names and candidate unknown class names for zero-shot\nOOD detection. Experimental results on 5 benchmark datasets for OOD detection\ndemonstrate that ZOC outperforms the baselines by a large margin.\n","authors":["Sepideh Esmaeilpour","Bing Liu","Eric Robertson","Lei Shu"],"pdf_url":"https://arxiv.org/pdf/2109.02748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11910v1","updated":"2022-03-22T17:27:22Z","published":"2022-03-22T17:27:22Z","title":"Improving Neural Predictivity in the Visual Cortex with Gated Recurrent\n  Connections","summary":"  Computational models of vision have traditionally been developed in a\nbottom-up fashion, by hierarchically composing a series of straightforward\noperations - i.e. convolution and pooling - with the aim of emulating simple\nand complex cells in the visual cortex, resulting in the introduction of deep\nconvolutional neural networks (CNNs). Nevertheless, data obtained with recent\nneuronal recording techniques support that the nature of the computations\ncarried out in the ventral visual stream is not completely captured by current\ndeep CNN models. To fill the gap between the ventral visual stream and deep\nmodels, several benchmarks have been designed and organized into the\nBrain-Score platform, granting a way to perform multi-layer (V1, V2, V4, IT)\nand behavioral comparisons between the two counterparts. In our work, we aim to\nshift the focus on architectures that take into account lateral recurrent\nconnections, a ubiquitous feature of the ventral visual stream, to devise\nadaptive receptive fields. Through recurrent connections, the input s\nlong-range spatial dependencies can be captured in a local multi-step fashion\nand, as introduced with Gated Recurrent CNNs (GRCNN), the unbounded expansion\nof the neuron s receptive fields can be modulated through the use of gates. In\norder to increase the robustness of our approach and the biological fidelity of\nthe activations, we employ specific data augmentation techniques in line with\nseveral of the scoring benchmarks. Enforcing some form of invariance, through\nheuristics, was found to be beneficial for better neural predictivity.\n","authors":["Simone Azeglio","Simone Poetto","Luca Savant Aira","Marco Nurisso"],"pdf_url":"https://arxiv.org/pdf/2203.11910v1.pdf","comment":"6 pages, 1 figure, BrainScore Workshop 2022"},{"id":"http://arxiv.org/abs/2203.11903v1","updated":"2022-03-22T17:15:56Z","published":"2022-03-22T17:15:56Z","title":"Enabling faster and more reliable sonographic assessment of gestational\n  age through machine learning","summary":"  Fetal ultrasounds are an essential part of prenatal care and can be used to\nestimate gestational age (GA). Accurate GA assessment is important for\nproviding appropriate prenatal care throughout pregnancy and identifying\ncomplications such as fetal growth disorders. Since derivation of GA from\nmanual fetal biometry measurements (head, abdomen, femur) are\noperator-dependent and time-consuming, there have been a number of research\nefforts focused on using artificial intelligence (AI) models to estimate GA\nusing standard biometry images, but there is still room to improve the accuracy\nand reliability of these AI systems for widescale adoption. To improve GA\nestimates, without significant change to provider workflows, we leverage AI to\ninterpret standard plane ultrasound images as well as 'fly-to' ultrasound\nvideos, which are 5-10s videos automatically recorded as part of the standard\nof care before the still image is captured. We developed and validated three AI\nmodels: an image model using standard plane images, a video model using fly-to\nvideos, and an ensemble model (combining both image and video). All three were\nstatistically superior to standard fetal biometry-based GA estimates derived by\nexpert sonographers, the ensemble model has the lowest mean absolute error\n(MAE) compared to the clinical standard fetal biometry (mean difference: -1.51\n$\\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404\nparticipants. We showed that our models outperform standard biometry by a more\nsubstantial margin on fetuses that were small for GA. Our AI models have the\npotential to empower trained operators to estimate GA with higher accuracy\nwhile reducing the amount of time required and user variability in measurement\nacquisition.\n","authors":["Chace Lee","Angelica Willis","Christina Chen","Marcin Sieniek","Akib Uddin","Jonny Wong","Rory Pilgrim","Katherine Chou","Daniel Tse","Shravya Shetty","Ryan G. Gomes"],"pdf_url":"https://arxiv.org/pdf/2203.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11900v1","updated":"2022-03-22T17:11:24Z","published":"2022-03-22T17:11:24Z","title":"Detection, Recognition, and Tracking: A Survey","summary":"  For humans, object detection, recognition, and tracking are innate. These\nprovide the ability for human to perceive their environment and objects within\ntheir environment. This ability however doesn't translate well in computers. In\nComputer Vision and Multimedia, it is becoming increasingly more important to\ndetect, recognize and track objects in images and/or videos. Many of these\napplications, such as facial recognition, surveillance, animation, are used for\ntracking features and/or people. However, these tasks prove challenging for\ncomputers to do effectively, as there is a significant amount of data to parse\nthrough. Therefore, many techniques and algorithms are needed and therefore\nresearched to try to achieve human like perception. In this literature review,\nwe focus on some novel techniques on object detection and recognition, and how\nto apply tracking algorithms to the detected features to track the objects'\nmovements.\n","authors":["Shiyao Chen","Dale Chen-Song"],"pdf_url":"https://arxiv.org/pdf/2203.11900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.05360v2","updated":"2022-03-22T17:10:15Z","published":"2020-10-11T22:14:22Z","title":"A range characterization of the single-quadrant ADRT","summary":"  This work characterizes the range of the single-quadrant approximate discrete\nRadon transform (ADRT) of square images. The characterization follows from a\nset of linear constraints on the codomain. We show that for data satisfying\nthese constraints, the exact and fast inversion formula [Rim, Appl. Math. Lett.\n102 106159, 2020] yields a square image in a stable manner. The range\ncharacterization is obtained by first showing that the ADRT is a bijection\nbetween images supported on infinite half-strips, then identifying the linear\nsubspaces that stay finitely supported under the inversion formula.\n","authors":["Weilin Li","Kui Ren","Donsub Rim"],"pdf_url":"https://arxiv.org/pdf/2010.05360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11894v1","updated":"2022-03-22T17:06:07Z","published":"2022-03-22T17:06:07Z","title":"GradViT: Gradient Inversion of Vision Transformers","summary":"  In this work we demonstrate the vulnerability of vision transformers (ViTs)\nto gradient-based inversion attacks. During this attack, the original data\nbatch is reconstructed given model weights and the corresponding gradients. We\nintroduce a method, named GradViT, that optimizes random noise into naturally\nlooking images via an iterative process. The optimization objective consists of\n(i) a loss on matching the gradients, (ii) image prior in the form of distance\nto batch-normalization statistics of a pretrained CNN model, and (iii) a total\nvariation regularization on patches to guide correct recovery locations. We\npropose a unique loss scheduling function to overcome local minima during\noptimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and\nobserve unprecedentedly high fidelity and closeness to the original (hidden)\ndata. During the analysis we find that vision transformers are significantly\nmore vulnerable than previously studied CNNs due to the presence of the\nattention mechanism. Our method demonstrates new state-of-the-art results for\ngradient inversion in both qualitative and quantitative metrics. Project page\nat https://gradvit.github.io/.\n","authors":["Ali Hatamizadeh","Hongxu Yin","Holger Roth","Wenqi Li","Jan Kautz","Daguang Xu","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2203.11894v1.pdf","comment":"CVPR 2021 Accepted Paper"},{"id":"http://arxiv.org/abs/2202.08510v2","updated":"2022-03-22T17:00:57Z","published":"2022-02-17T08:33:52Z","title":"A hybrid 2-stage vision transformer for artificial intelligence-assisted\n  5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic\n  tool for guiding gastric cancer treatment","summary":"  Gastric endoscopic screening is an effective way to decide appropriate\ngastric cancer (GC) treatment at an early stage, reducing GC-associated\nmortality rate. Although artificial intelligence (AI) has brought a great\npromise to assist pathologist to screen digitalized whole slide images,\nautomatic classification systems for guiding proper GC treatment based on\nclinical guideline are still lacking. We propose an AI system classifying 5\nclasses of GC histology, which can be perfectly matched to general GC treatment\nguidance. The AI system was designed to mimic the way pathologist understand\nslides through multi-scale self-attention mechanism using a 2-stage Vision\nTransformer network. The AI system performance was evaluated on 876 internal\nendoscopic slides and 336 external endoscopic slides from clinical cohort. We\nfurther evaluated practical usability of the AI system on observation of\nAI-assisted 6 pathologist performance. The AI system demonstrates clinical\ncapability by achieving class-average diagnostic sensitivity of above 85% for\nboth internal and external cohort analysis. Furthermore, AI-assisted\npathologists showed significantly improved diagnostic sensitivity by 10% within\n18% saved screening time compared to human pathologists (p-values of 0.006 and\n0.030, respectively). The reliable performance of the AI system in multi-center\ncohort testing and its clinical applicability demonstrate that AI-assisted\nendoscopic CG screening would help reduce the workload of limited pathologists.\nFurthermore, the AI system has a great potential for providing presumptive\npathologic opinion for deciding proper treatment for early GC patients.\n","authors":["Yujin Oh","Go Eun Bae","Kyung-Hee Kim","Min-Kyung Yeo","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2202.08510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11878v1","updated":"2022-03-22T16:56:05Z","published":"2022-03-22T16:56:05Z","title":"Under the Hood of Transformer Networks for Trajectory Forecasting","summary":"  Transformer Networks have established themselves as the de-facto\nstate-of-the-art for trajectory forecasting but there is currently no\nsystematic study on their capability to model the motion patterns of people,\nwithout interactions with other individuals nor the social context. This paper\nproposes the first in-depth study of Transformer Networks (TF) and\nBidirectional Transformers (BERT) for the forecasting of the individual motion\nof people, without bells and whistles. We conduct an exhaustive evaluation of\ninput/output representations, problem formulations and sequence modeling,\nincluding a novel analysis of their capability to predict multi-modal futures.\nOut of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are\ntop performers in predicting individual motions, definitely overcoming RNNs and\nLSTMs. Furthermore, they remain within a narrow margin wrt more complex\ntechniques, which include both social interactions and scene contexts. Source\ncode will be released for all conducted experiments.\n","authors":["Luca Franco","Leonardo Placidi","Francesco Giuliari","Irtiza Hasan","Marco Cristani","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2203.11878v1.pdf","comment":"Under review in Pattern Recognition journal"},{"id":"http://arxiv.org/abs/2203.11876v1","updated":"2022-03-22T16:54:52Z","published":"2022-03-22T16:54:52Z","title":"Open-Vocabulary DETR with Conditional Matching","summary":"  Open-vocabulary object detection, which is concerned with the problem of\ndetecting novel objects guided by natural language, has gained increasing\nattention from the community. Ideally, we would like to extend an\nopen-vocabulary detector such that it can produce bounding box predictions\nbased on user inputs in form of either natural language or exemplar image. This\noffers great flexibility and user experience for human-computer interaction. To\nthis end, we propose a novel open-vocabulary detector based on DETR -- hence\nthe name OV-DETR -- which, once trained, can detect any object given its class\nname or an exemplar image. The biggest challenge of turning DETR into an\nopen-vocabulary detector is that it is impossible to calculate the\nclassification cost matrix of novel classes without access to their labeled\nimages. To overcome this challenge, we formulate the learning objective as a\nbinary matching one between input queries (class name or exemplar image) and\nthe corresponding objects, which learns useful correspondence to generalize to\nunseen queries during testing. For training, we choose to condition the\nTransformer decoder on the input embeddings obtained from a pre-trained\nvision-language model like CLIP, in order to enable matching for both text and\nimage queries. With extensive experiments on LVIS and COCO datasets, we\ndemonstrate that our OV-DETR -- the first end-to-end Transformer-based\nopen-vocabulary detector -- achieves non-trivial improvements over current\nstate of the arts.\n","authors":["Yuhang Zang","Wei Li","Kaiyang Zhou","Chen Huang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2203.11876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11862v1","updated":"2022-03-22T16:38:52Z","published":"2022-03-22T16:38:52Z","title":"Generating natural images with direct Patch Distributions Matching","summary":"  Many traditional computer vision algorithms generate realistic images by\nrequiring that each patch in the generated image be similar to a patch in a\ntraining image and vice versa. Recently, this classical approach has been\nreplaced by adversarial training with a patch discriminator. The adversarial\napproach avoids the computational burden of finding nearest neighbors of\npatches but often requires very long training times and may fail to match the\ndistribution of patches. In this paper we leverage the recently developed\nSliced Wasserstein Distance and develop an algorithm that explicitly and\nefficiently minimizes the distance between patch distributions in two images.\nOur method is conceptually simple, requires no training and can be implemented\nin a few lines of codes. On a number of image generation tasks we show that our\nresults are often superior to single-image-GANs, require no training, and can\ngenerate high quality images in a few seconds. Our implementation is available\nat https://github.com/ariel415el/GPDM\n","authors":["Ariel Elnekave","Yair Weiss"],"pdf_url":"https://arxiv.org/pdf/2203.11862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03990v2","updated":"2022-03-22T16:30:14Z","published":"2022-03-08T10:36:55Z","title":"Audio-Visual MLP for Scoring Sport","summary":"  Figure skating scoring is a challenging task because it requires judging\nplayers' technical moves as well as coordination with the background music.\nPrior learning-based work cannot solve it well for two reasons: 1) each move in\nfigure skating changes quickly, hence simply applying traditional frame\nsampling will lose a lot of valuable information, especially in a 3-5 minutes\nlasting video, so an extremely long-range representation learning is necessary;\n2) prior methods rarely considered the critical audio-visual relationship in\ntheir models. Thus, we introduce a multimodal MLP architecture, named\nSkating-Mixer. It extends the MLP-Mixer-based framework into a multimodal\nfashion and effectively learns long-term representations through our designed\nmemory recurrent unit (MRU). Aside from the model, we also collected a\nhigh-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8\ntypes of programs with 7 different rating metrics, overtaking other datasets in\nboth quantity and diversity. Experiments show the proposed method outperforms\nSOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In\naddition, we include an analysis applying our method to recent competitions\nthat occurred in Beijing 2022 Winter Olympic Games, proving our method has\nstrong robustness.\n","authors":["Jingfei Xia","Mingchen Zhuge","Tiantian Geng","Shun Fan","Yuantai Wei","Zhenyu He","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2203.03990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.12564v2","updated":"2022-03-22T16:19:13Z","published":"2021-09-26T11:28:27Z","title":"Vision Transformer Hashing for Image Retrieval","summary":"  Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet. The code is\nreleased at \\url{https://github.com/shivram1987/VisionTransformerHashing}.\n","authors":["Shiv Ram Dubey","Satish Kumar Singh","Wei-Ta Chu"],"pdf_url":"https://arxiv.org/pdf/2109.12564v2.pdf","comment":"Accepted in IEEE International Conference on Multimedia and Expo\n  (ICME), 2022"},{"id":"http://arxiv.org/abs/2203.11836v1","updated":"2022-03-22T16:03:24Z","published":"2022-03-22T16:03:24Z","title":"A Real-time Junk Food Recognition System based on Machine Learning","summary":"  $ $As a result of bad eating habits, humanity may be destroyed. People are\nconstantly on the lookout for tasty foods, with junk foods being the most\ncommon source. As a consequence, our eating patterns are shifting, and we're\ngravitating toward junk food more than ever, which is bad for our health and\nincreases our risk of acquiring health problems. Machine learning principles\nare applied in every aspect of our lives, and one of them is object recognition\nvia image processing. However, because foods vary in nature, this procedure is\ncrucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a\nlow accuracy rate. All of these issues were defeated by the Deep Neural\nNetwork. In this work, we created a fresh dataset of 10,000 data points from 20\njunk food classifications to try to recognize junk foods. All of the data in\nthe data set was gathered using the Google search engine, which is thought to\nbe one-of-a-kind in every way. The goal was achieved using Convolution Neural\nNetwork (CNN) technology, which is well-known for image processing. We achieved\na 98.05\\% accuracy rate throughout the research, which was satisfactory. In\naddition, we conducted a test based on a real-life event, and the outcome was\nextraordinary. Our goal is to advance this research to the next level, so that\nit may be applied to a future study. Our ultimate goal is to create a system\nthat would encourage people to avoid eating junk food and to be\nhealth-conscious. \\keywords{ Machine Learning \\and junk food \\and object\ndetection \\and YOLOv3 \\and custom food dataset.}\n","authors":["Sirajum Munira Shifat","Takitazwar Parthib","Sabikunnahar Talukder Pyaasa","Nila Maitra Chaity","Niloy Kumar","Md. Kishor Morol"],"pdf_url":"https://arxiv.org/pdf/2203.11836v1.pdf","comment":"15 pages, 7 figures, accepted in ICBBDB conference"},{"id":"http://arxiv.org/abs/2203.11834v1","updated":"2022-03-22T16:01:04Z","published":"2022-03-22T16:01:04Z","title":"Improving Generalization in Federated Learning by Seeking Flat Minima","summary":"  Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2203.11834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11832v1","updated":"2022-03-22T15:59:44Z","published":"2022-03-22T15:59:44Z","title":"Cross-View Panorama Image Synthesis","summary":"  In this paper, we tackle the problem of synthesizing a ground-view panorama\nimage conditioned on a top-view aerial image, which is a challenging problem\ndue to the large gap between the two image domains with different view-points.\nInstead of learning cross-view mapping in a feedforward pass, we propose a\nnovel adversarial feedback GAN framework named PanoGAN with two key components:\nan adversarial feedback module and a dual branch discrimination strategy.\nFirst, the aerial image is fed into the generator to produce a target panorama\nimage and its associated segmentation map in favor of model training with\nlayout semantics. Second, the feature responses of the discriminator encoded by\nour adversarial feedback module are fed back to the generator to refine the\nintermediate representations, so that the generation performance is continually\nimproved through an iterative generation process. Third, to pursue\nhigh-fidelity and semantic consistency of the generated panorama image, we\npropose a pixel-segmentation alignment mechanism under the dual branch\ndiscrimiantion strategy to facilitate cooperation between the generator and the\ndiscriminator. Extensive experimental results on two challenging cross-view\nimage datasets show that PanoGAN enables high-quality panorama image generation\nwith more convincing details than state-of-the-art approaches. The source code\nand trained models are available at \\url{https://github.com/sswuai/PanoGAN}.\n","authors":["Songsong Wu","Hao Tang","Xiao-Yuan Jing","Haifeng Zhao","Jianjun Qian","Nicu Sebe","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2203.11832v1.pdf","comment":"Accepted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2111.12448v4","updated":"2022-03-22T15:52:17Z","published":"2021-11-24T11:53:33Z","title":"3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch\n  Feature Swapping for Bodies and Faces","summary":"  Learning a disentangled, interpretable, and structured latent representation\nin 3D generative models of faces and bodies is still an open problem. The\nproblem is particularly acute when control over identity features is required.\nIn this paper, we propose an intuitive yet effective self-supervised approach\nto train a 3D shape variational autoencoder (VAE) which encourages a\ndisentangled latent representation of identity features. Curating the\nmini-batch generation by swapping arbitrary features across different shapes\nallows to define a loss function leveraging known differences and similarities\nin the latent representations. Experimental results conducted on 3D meshes show\nthat state-of-the-art methods for latent disentanglement are not able to\ndisentangle identity features of faces and bodies. Our proposed method properly\ndecouples the generation of such features while maintaining good representation\nand reconstruction capabilities.\n","authors":["Simone Foti","Bongjin Koo","Danail Stoyanov","Matthew J. Clarkson"],"pdf_url":"https://arxiv.org/pdf/2111.12448v4.pdf","comment":"Accepted for publication at CVPR2022"},{"id":"http://arxiv.org/abs/2203.11824v1","updated":"2022-03-22T15:47:22Z","published":"2022-03-22T15:47:22Z","title":"Was that so hard? Estimating human classification difficulty","summary":"  When doctors are trained to diagnose a specific disease, they learn faster\nwhen presented with cases in order of increasing difficulty. This creates the\nneed for automatically estimating how difficult it is for doctors to classify a\ngiven case. In this paper, we introduce methods for estimating how hard it is\nfor a doctor to diagnose a case represented by a medical image, both when\nground truth difficulties are available for training, and when they are not.\nOur methods are based on embeddings obtained with deep metric learning.\nAdditionally, we introduce a practical method for obtaining ground truth human\ndifficulty for each image case in a dataset using self-assessed certainty. We\napply our methods to two different medical datasets, achieving high Kendall\nrank correlation coefficients, showing that we outperform existing methods by a\nlarge margin on our problem and data.\n","authors":["Morten Rieger Hannemose","Josefine Vilsbøll Sundgaard","Niels Kvorning Ternov","Rasmus R. Paulsen","Anders Nymark Christensen"],"pdf_url":"https://arxiv.org/pdf/2203.11824v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2203.11819v1","updated":"2022-03-22T15:38:36Z","published":"2022-03-22T15:38:36Z","title":"A Broad Study of Pre-training for Domain Generalization and Adaptation","summary":"  Deep models must learn robust and transferable representations in order to\nperform well on new domains. While domain transfer methods (e.g., domain\nadaptation, domain generalization) have been proposed to learn transferable\nrepresentations across domains, they are typically applied to ResNet backbones\npre-trained on ImageNet. Thus, existing works pay little attention to the\neffects of pre-training on domain transfer tasks. In this paper, we provide a\nbroad study and in-depth analysis of pre-training for domain adaptation and\ngeneralization, namely: network architectures, size, pre-training loss, and\ndatasets. We observe that simply using a state-of-the-art backbone outperforms\nexisting state-of-the-art domain adaptation baselines and set new baselines on\nOffice-Home and DomainNet improving by 10.7\\% and 5.5\\%. We hope that this work\ncan provide more insights for future domain transfer research.\n","authors":["Donghyun Kim","Kaihong Wang","Stan Sclaroff","Kate Saenko"],"pdf_url":"https://arxiv.org/pdf/2203.11819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11807v1","updated":"2022-03-22T15:16:54Z","published":"2022-03-22T15:16:54Z","title":"A New Approach to Improve Learning-based Deepfake Detection in Realistic\n  Conditions","summary":"  Deep convolutional neural networks have achieved exceptional results on\nmultiple detection and recognition tasks. However, the performance of such\ndetectors are often evaluated in public benchmarks under constrained and\nnon-realistic situations. The impact of conventional distortions and processing\noperations found in imaging workflows such as compression, noise, and\nenhancement are not sufficiently studied. Currently, only a few researches have\nbeen done to improve the detector robustness to unseen perturbations. This\npaper proposes a more effective data augmentation scheme based on real-world\nimage degradation process. This novel technique is deployed for deepfake\ndetection tasks and has been evaluated by a more realistic assessment\nframework. Extensive experiments show that the proposed data augmentation\nscheme improves generalization ability to unpredictable data distortions and\nunseen datasets.\n","authors":["Yuhang Lu","Touradj Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2203.11807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.05734v2","updated":"2022-03-22T15:09:52Z","published":"2021-10-12T04:48:10Z","title":"Learning Efficient Multi-Agent Cooperative Visual Exploration","summary":"  We tackle the problem of cooperative visual exploration where multiple agents\nneed to jointly explore unseen regions as fast as possible based on visual\nsignals. Classical planning-based methods often suffer from expensive\ncomputation overhead at each step and a limited expressiveness of complex\ncooperation strategy. By contrast, reinforcement learning (RL) has recently\nbecome a popular paradigm for tackling this challenge due to its modeling\ncapability of arbitrarily complex strategies and minimal inference overhead. In\nthis paper, we extend the state-of-the-art single-agent visual navigation\nmethod, Active Neural SLAM (ANS), to the multi-agent setting by introducing a\nnovel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages\na transformer-based architecture, Spatial-TeamFormer, which effectively\ncaptures spatial relations and intra-agent interactions via hierarchical\nspatial self-attentions. In addition, we also implement a few multi-agent\nenhancements to process local information from each agent for an aligned\nspatial representation and more precise planning. Finally, we perform policy\ndistillation to extract a meta policy to significantly improve the\ngeneralization capability of final policy. We call this overall solution,\nMulti-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms\nclassical planning-based baselines for the first time in a photo-realistic 3D\nsimulator, Habitat. Code and videos can be found at\nhttps://sites.google.com/view/maans.\n","authors":["Chao Yu","Xinyi Yang","Jiaxuan Gao","Huazhong Yang","Yu Wang","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2110.05734v2.pdf","comment":"First three authors share equal contribution"},{"id":"http://arxiv.org/abs/2105.11111v3","updated":"2022-03-22T15:08:22Z","published":"2021-05-24T06:18:23Z","title":"Oriented RepPoints for Aerial Object Detection","summary":"  In contrast to the generic object, aerial targets are often non-axis aligned\nwith arbitrary orientations having the cluttered surroundings. Unlike the\nmainstreamed approaches regressing the bounding box orientations, this paper\nproposes an effective adaptive points learning approach to aerial object\ndetection by taking advantage of the adaptive points representation, which is\nable to capture the geometric information of the arbitrary-oriented instances.\nTo this end, three oriented conversion functions are presented to facilitate\nthe classification and localization with accurate orientation. Moreover, we\npropose an effective quality assessment and sample assignment scheme for\nadaptive points learning toward choosing the representative oriented reppoints\nsamples during training, which is able to capture the non-axis aligned features\nfrom adjacent objects or background noises. A spatial constraint is introduced\nto penalize the outlier points for roust adaptive learning. Experimental\nresults on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD\nand DIOR-R, demonstrate the efficacy of our proposed approach. The source code\nis availabel at: https://github.com/LiWentomng/OrientedRepPoints.\n","authors":["Wentong Li","Yijie Chen","Kaixuan Hu","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2105.11111v3.pdf","comment":"10 pages, 4 figures, Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.11799v1","updated":"2022-03-22T15:04:37Z","published":"2022-03-22T15:04:37Z","title":"AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric\n  PD and Blind-Spot Network","summary":"  Blind-spot network (BSN) and its variants have made significant advances in\nself-supervised denoising. Nevertheless, they are still bound to synthetic\nnoisy inputs due to less practical assumptions like pixel-wise independent\nnoise. Hence, it is challenging to deal with spatially correlated real-world\nnoise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has\nbeen proposed to remove the spatial correlation of real-world noise. However,\nit is not trivial to integrate PD and BSN directly, which prevents the fully\nself-supervised denoising model on real-world images. We propose an Asymmetric\nPD (AP) to address this issue, which introduces different PD stride factors for\ntraining and inference. We systematically demonstrate that the proposed AP can\nresolve inherent trade-offs caused by specific PD stride factors and make BSN\napplicable to practical scenarios. To this end, we develop AP-BSN, a\nstate-of-the-art self-supervised denoising method for real-world sRGB images.\nWe further propose random-replacing refinement, which significantly improves\nthe performance of our AP-BSN without any additional parameters. Extensive\nstudies demonstrate that our method outperforms the other self-supervised and\neven unpaired denoising methods by a large margin, without using any additional\nknowledge, e.g., noise level, regarding the underlying unknown noise.\n","authors":["Wooseok Lee","Sanghyun Son","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2203.11799v1.pdf","comment":"Accepted to CVPR2022"},{"id":"http://arxiv.org/abs/2203.11797v1","updated":"2022-03-22T15:03:56Z","published":"2022-03-22T15:03:56Z","title":"A Novel Framework for Assessment of Learning-based Detectors in\n  Realistic Conditions with Application to Deepfake Detection","summary":"  Deep convolutional neural networks have shown remarkable results on multiple\ndetection tasks. Despite the significant progress, the performance of such\ndetectors are often assessed in public benchmarks under non-realistic\nconditions. Specifically, impact of conventional distortions and processing\noperations such as compression, noise, and enhancement are not sufficiently\nstudied. This paper proposes a rigorous framework to assess performance of\nlearning-based detectors in more realistic situations. An illustrative example\nis shown under deepfake detection context. Inspired by the assessment results,\na data augmentation strategy based on natural image degradation process is\ndesigned, which significantly improves the generalization ability of two\ndeepfake detectors.\n","authors":["Yuhang Lu","Ruizhi Luo","Touradj Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2203.11797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.04041v2","updated":"2022-03-22T14:43:14Z","published":"2022-03-08T12:21:35Z","title":"Shape-invariant 3D Adversarial Point Clouds","summary":"  Adversary and invisibility are two fundamental but conflict characters of\nadversarial perturbations. Previous adversarial attacks on 3D point cloud\nrecognition have often been criticized for their noticeable point outliers,\nsince they just involve an \"implicit constrain\" like global distance loss in\nthe time-consuming optimization to limit the generated noise. While point cloud\nis a highly structured data format, it is hard to constrain its perturbation\nwith a simple loss or metric properly. In this paper, we propose a novel\nPoint-Cloud Sensitivity Map to boost both the efficiency and imperceptibility\nof point perturbations. This map reveals the vulnerability of point cloud\nrecognition models when encountering shape-invariant adversarial noises. These\nnoises are designed along the shape surface with an \"explicit constrain\"\ninstead of extra distance loss. Specifically, we first apply a reversible\ncoordinate transformation on each point of the point cloud input, to reduce one\ndegree of point freedom and limit its movement on the tangent plane. Then we\ncalculate the best attacking direction with the gradients of the transformed\npoint cloud obtained on the white-box model. Finally we assign each point with\na non-negative score to construct the sensitivity map, which benefits both\nwhite-box adversarial invisibility and black-box query-efficiency extended in\nour work. Extensive evaluations prove that our method can achieve the superior\nperformance on various point cloud recognition models, with its satisfying\nadversarial imperceptibility and strong resistance to different point cloud\ndefense settings. Our code is available at: https://github.com/shikiw/SI-Adv.\n","authors":["Qidong Huang","Xiaoyi Dong","Dongdong Chen","Hang Zhou","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2203.04041v2.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11754v1","updated":"2022-03-22T14:10:16Z","published":"2022-03-22T14:10:16Z","title":"Exploring and Evaluating Image Restoration Potential in Dynamic Scenes","summary":"  In dynamic scenes, images often suffer from dynamic blur due to superposition\nof motions or low signal-noise ratio resulted from quick shutter speed when\navoiding motions. Recovering sharp and clean results from the captured images\nheavily depends on the ability of restoration methods and the quality of the\ninput. Although existing research on image restoration focuses on developing\nmodels for obtaining better restored results, fewer have studied to evaluate\nhow and which input image leads to superior restored quality. In this paper, to\nbetter study an image's potential value that can be explored for restoration,\nwe propose a novel concept, referring to image restoration potential (IRP).\nSpecifically, We first establish a dynamic scene imaging dataset containing\ncomposite distortions and applied image restoration processes to validate the\nrationality of the existence to IRP. Based on this dataset, we investigate\nseveral properties of IRP and propose a novel deep model to accurately predict\nIRP values. By gradually distilling and selective fusing the degradation\nfeatures, the proposed model shows its superiority in IRP prediction. Thanks to\nthe proposed model, we are then able to validate how various image restoration\nrelated applications are benefited from IRP prediction. We show the potential\nusages of IRP as a filtering principle to select valuable frames, an auxiliary\nguidance to improve restoration models, and even an indicator to optimize\ncamera settings for capturing better images under dynamic scenarios.\n","authors":["Cheng Zhang","Shaolin Su","Yu Zhu","Qingsen Yan","Jinqiu Sun","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11754v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.11732v1","updated":"2022-03-22T13:40:26Z","published":"2022-03-22T13:40:26Z","title":"ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based\n  Motion Segmentation","summary":"  Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting\napparent motion of objects with microsecond resolution, and shows great\napplication potential in monitoring and other fields. However, the output event\nstream of existing DVS inevitably contains background activity noise (BA noise)\ndue to dark current and junction leakage current, which will affect the\ntemporal correlation of objects, resulting in deteriorated motion estimation\nperformance. Particularly, the existing filter-based denoising methods cannot\nbe directly applied to suppress the noise in event stream, since there is no\nspatial correlation. To address this issue, this paper presents a novel\nprogressive framework, in which a Motion Estimation (ME) module and an Event\nDenoising (ED) module are jointly optimized in a mutually reinforced manner.\nSpecifically, based on the maximum sharpness criterion, ME module divides the\ninput event into several segments by adaptive clustering in a motion\ncompensating warp field, and captures the temporal correlation of event stream\naccording to the clustered motion parameters. Taking temporal correlation as\nguidance, ED module calculates the confidence that each event belongs to real\nactivity events, and transmits it to ME module to update energy function of\nmotion segmentation for noise suppression. The two steps are iteratively\nupdated until stable motion segmentation results are obtained. Extensive\nexperimental results on both synthetic and real datasets demonstrate the\nsuperiority of our proposed approaches against the State-Of-The-Art (SOTA)\nmethods.\n","authors":["Jinze Chen","Yang Wang","Yang Cao","Feng Wu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2203.11732v1.pdf","comment":"AAAI2022"},{"id":"http://arxiv.org/abs/2203.11725v1","updated":"2022-03-22T13:32:42Z","published":"2022-03-22T13:32:42Z","title":"Unsupervised Anomaly Detection in Medical Images with a Memory-augmented\n  Multi-level Cross-attentional Masked Autoencoder","summary":"  Unsupervised anomaly detection (UAD) aims to find anomalous images by\noptimising a detector using a training set that contains only normal images.\nUAD approaches can be based on reconstruction methods, self-supervised\napproaches, and Imagenet pre-trained models. Reconstruction methods, which\ndetect anomalies from image reconstruction errors, are advantageous because\nthey do not rely on the design of problem-specific pretext tasks needed by\nself-supervised approaches, and on the unreliable translation of models\npre-trained from non-medical datasets. However, reconstruction methods may fail\nbecause they can have low reconstruction errors even for anomalous images. In\nthis paper, we introduce a new reconstruction-based UAD approach that addresses\nthis low-reconstruction error issue for anomalous images. Our UAD approach, the\nmemory-augmented multi-level cross-attentional masked autoencoder (MemMC-MAE),\nis a transformer-based approach, consisting of a novel memory-augmented\nself-attention operator for the encoder and a new multi-level cross-attention\noperator for the decoder. MemMC-MAE masks large parts of the input image during\nits reconstruction, reducing the risk that it will produce low reconstruction\nerrors because anomalies are likely to be masked and cannot be reconstructed.\nHowever, when the anomaly is not masked, then the normal patterns stored in the\nencoder's memory combined with the decoder's multi-level cross-attention will\nconstrain the accurate reconstruction of the anomaly. We show that our method\nachieves SOTA anomaly detection and localisation on colonoscopy and Covid-19\nChest X-ray datasets.\n","authors":["Yu Tian","Guansong Pang","Yuyuan Liu","Chong Wang","Yuanhong Chen","Fengbei Liu","Rajvinder Singh","Johan W Verjans","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2203.11725v1.pdf","comment":"Technical report, 11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2203.01057v2","updated":"2022-03-22T13:31:53Z","published":"2022-03-02T12:13:08Z","title":"Colar: Effective and Efficient Online Action Detection by Consulting\n  Exemplars","summary":"  Online action detection has attracted increasing research interests in recent\nyears. Current works model historical dependencies and anticipate the future to\nperceive the action evolution within a video segment and improve the detection\naccuracy. However, the existing paradigm ignores category-level modeling and\ndoes not pay sufficient attention to efficiency. Considering a category, its\nrepresentative frames exhibit various characteristics. Thus, the category-level\nmodeling can provide complimentary guidance to the temporal dependencies\nmodeling. This paper develops an effective exemplar-consultation mechanism that\nfirst measures the similarity between a frame and exemplary frames, and then\naggregates exemplary features based on the similarity weights. This is also an\nefficient mechanism, as both similarity measurement and feature aggregation\nrequire limited computations. Based on the exemplar-consultation mechanism, the\nlong-term dependencies can be captured by regarding historical frames as\nexemplars, while the category-level modeling can be achieved by regarding\nrepresentative frames from a category as exemplars. Due to the complementarity\nfrom the category-level modeling, our method employs a lightweight architecture\nbut achieves new high performance on three benchmarks. In addition, using a\nspatio-temporal network to tackle video frames, our method makes a good\ntrade-off between effectiveness and efficiency. Code is available at\nhttps://github.com/VividLe/Online-Action-Detection.\n","authors":["Le Yang","Junwei Han","Dingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.01057v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11722v1","updated":"2022-03-22T13:31:47Z","published":"2022-03-22T13:31:47Z","title":"Convolutional Neural Network to Restore Low-Dose Digital Breast\n  Tomosynthesis Projections in a Variance Stabilization Domain","summary":"  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible\nradiation dose while maintaining sufficiently good image quality for accurate\nmedical diagnosis. In this work, we propose a convolution neural network (CNN)\nto restore low-dose (LD) DBT projections to achieve an image quality equivalent\nto a standard full-dose (FD) acquisition. The proposed network architecture\nbenefits from priors in terms of layers that were inspired by traditional\nmodel-based (MB) restoration methods, considering a model-based deep learning\napproach, where the network is trained to operate in the variance stabilization\ntransformation (VST) domain. To accurately control the network operation point,\nin terms of noise and blur of the restored image, we propose a loss function\nthat minimizes the bias and matches residual noise between the input and the\noutput. The training dataset was composed of clinical data acquired at the\nstandard FD and low-dose pairs obtained by the injection of quantum noise. The\nnetwork was tested using real DBT projections acquired with a physical\nanthropomorphic breast phantom. The proposed network achieved superior results\nin terms of the mean normalized squared error (MNSE), training time and noise\nspatial correlation compared with networks trained with traditional data-driven\nmethods. The proposed approach can be extended for other medical imaging\napplication that requires LD acquisitions.\n","authors":["Rodrigo de Barros Vimieiro","Chuang Niu","Hongming Shan","Lucas Rodrigues Borges","Ge Wang","Marcelo Andrade da Costa Vieira"],"pdf_url":"https://arxiv.org/pdf/2203.11722v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2203.11709v1","updated":"2022-03-22T13:21:49Z","published":"2022-03-22T13:21:49Z","title":"CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation","summary":"  Recent advances in self-supervised contrastive learning yield good\nimage-level representation, which favors classification tasks but usually\nneglects pixel-level detailed information, leading to unsatisfactory transfer\nperformance to dense prediction tasks such as semantic segmentation. In this\nwork, we propose a pixel-wise contrastive learning method called CP2\n(Copy-Paste Contrastive Pretraining), which facilitates both image- and\npixel-level representation learning and therefore is more suitable for\ndownstream dense prediction tasks. In detail, we copy-paste a random crop from\nan image (the foreground) onto different background images and pretrain a\nsemantic segmentation model with the objective of 1) distinguishing the\nforeground pixels from the background pixels, and 2) identifying the composed\nimages that share the same foreground.Experiments show the strong performance\nof CP2 in downstream semantic segmentation: By finetuning CP2 pretrained models\non PASCAL VOC 2012, we obtain 78.6% mIoU with a ResNet-50 and 79.5% with a\nViT-S.\n","authors":["Feng Wang","Huiyu Wang","Chen Wei","Alan Yuille","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2203.11709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10636v2","updated":"2022-03-22T13:16:10Z","published":"2022-03-20T20:13:59Z","title":"Transform your Smartphone into a DSLR Camera: Learning the ISP in the\n  Wild","summary":"  We propose a trainable Image Signal Processing (ISP) framework that produces\nDSLR quality images given RAW images captured by a smartphone. To address the\ncolor misalignments between training image pairs, we employ a color-conditional\nISP network and optimize a novel parametric color mapping between each input\nRAW and reference DSLR image. During inference, we predict the target color\nimage by designing a color prediction network with efficient Global Context\nTransformer modules. The latter effectively leverage global information to\nlearn consistent color and tone mappings. We further propose a robust masked\naligned loss to identify and discard regions with inaccurate motion estimation\nduring training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,\nconsisting of weakly paired phone RAW and DSLR sRGB images. We extensively\nevaluate our method, setting a new state-of-the-art on two datasets.\n","authors":["Ardhendu Shekhar Tripathi","Martin Danelljan","Samarth Shukla","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.10636v2.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2203.11686v1","updated":"2022-03-22T13:01:59Z","published":"2022-03-22T13:01:59Z","title":"End-to-End Learned Block-Based Image Compression with Block-Level Masked\n  Convolutions and Asymptotic Closed Loop Training","summary":"  Learned image compression research has achieved state-of-the-art compression\nperformance with auto-encoder based neural network architectures, where the\nimage is mapped via convolutional neural networks (CNN) into a latent\nrepresentation that is quantized and processed again with CNN to obtain the\nreconstructed image. CNN operate on entire input images. On the other hand,\ntraditional state-of-the-art image and video compression methods process images\nwith a block-by-block processing approach for various reasons. Very recently,\nwork on learned image compression with block based approaches have also\nappeared, which use the auto-encoder architecture on large blocks of the input\nimage and introduce additional neural networks that perform intra/spatial\nprediction and deblocking/post-processing functions. This paper explores an\nalternative learned block-based image compression approach in which neither an\nexplicit intra prediction neural network nor an explicit deblocking neural\nnetwork is used. A single auto-encoder neural network with block-level masked\nconvolutions is used and the block size is much smaller (8x8). By using\nblock-level masked convolutions, each block is processed using reconstructed\nneighboring left and upper blocks both at the encoder and decoder. Hence, the\nmutual information between adjacent blocks is exploited during compression and\neach block is reconstructed using neighboring blocks, resolving the need for\nexplicit intra prediction and deblocking neural networks. Since the explored\nsystem is a closed loop system, a special optimization procedure, the\nasymptotic closed loop design, is used with standard stochastic gradient\ndescent based training. The experimental results indicate competitive image\ncompression performance.\n","authors":["Fatih Kamisli"],"pdf_url":"https://arxiv.org/pdf/2203.11686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11684v1","updated":"2022-03-22T12:58:39Z","published":"2022-03-22T12:58:39Z","title":"Meta-attention for ViT-backed Continual Learning","summary":"  Continual learning is a longstanding research topic due to its crucial role\nin tackling continually arriving tasks. Up to now, the study of continual\nlearning in computer vision is mainly restricted to convolutional neural\nnetworks (CNNs). However, recently there is a tendency that the newly emerging\nvision transformers (ViTs) are gradually dominating the field of computer\nvision, which leaves CNN-based continual learning lagging behind as they can\nsuffer from severe performance degradation if straightforwardly applied to\nViTs. In this paper, we study ViT-backed continual learning to strive for\nhigher performance riding on recent advances of ViTs. Inspired by mask-based\ncontinual learning methods in CNNs, where a mask is learned per task to adapt\nthe pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,\nattention to self-attention, to adapt a pre-trained ViT to new tasks without\nsacrificing performance on already learned tasks. Unlike prior mask-based\nmethods like Piggyback, where all parameters are associated with corresponding\nmasks, MEAT leverages the characteristics of ViTs and only masks a portion of\nits parameters. It renders MEAT more efficient and effective with less overhead\nand higher accuracy. Extensive experiments demonstrate that MEAT exhibits\nsignificant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%\nabsolute boosts in accuracy. Our code has been released at\nhttps://github.com/zju-vipa/MEAT-TIL.\n","authors":["Mengqi Xue","Haofei Zhang","Jie Song","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2203.11684v1.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11660v1","updated":"2022-03-22T12:35:20Z","published":"2022-03-22T12:35:20Z","title":"Channel Self-Supervision for Online Knowledge Distillation","summary":"  Recently, researchers have shown an increased interest in the online\nknowledge distillation. Adopting an one-stage and end-to-end training fashion,\nonline knowledge distillation uses aggregated intermediated predictions of\nmultiple peer models for training. However, the absence of a powerful teacher\nmodel may result in the homogeneity problem between group peers, affecting the\neffectiveness of group distillation adversely. In this paper, we propose a\nnovel online knowledge distillation method, \\textbf{C}hannel\n\\textbf{S}elf-\\textbf{S}upervision for Online Knowledge Distillation (CSS),\nwhich structures diversity in terms of input, target, and network to alleviate\nthe homogenization problem. Specifically, we construct a dual-network\nmulti-branch structure and enhance inter-branch diversity through\nself-supervised learning, adopting the feature-level transformation and\naugmenting the corresponding labels. Meanwhile, the dual network structure has\na larger space of independent parameters to resist the homogenization problem\nduring distillation. Extensive quantitative experiments on CIFAR-100 illustrate\nthat our method provides greater diversity than OKDDip and we also give pretty\nperformance improvement, even over the state-of-the-art such as PCL. The\nresults on three fine-grained datasets (StanfordDogs, StanfordCars,\nCUB-200-211) also show the significant generalization capability of our\napproach.\n","authors":["Shixiao Fan","Xuan Cheng","Xiaomin Wang","Chun Yang","Pan Deng","Minghui Liu","Jiali Deng","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11654v1","updated":"2022-03-22T12:26:56Z","published":"2022-03-22T12:26:56Z","title":"Fine-Grained Scene Graph Generation with Data Transfer","summary":"  Scene graph generation (SGG) aims to extract (subject, predicate, object)\ntriplets in images. Recent works have made a steady progress on SGG, and\nprovide useful tools for high-level vision and language understanding. However,\ndue to the data distribution problems including long-tail distribution and\nsemantic ambiguity, the predictions of current SGG models tend to collapse to\nseveral frequent but uninformative predicates (e.g., \\textit{on}, \\textit{at}),\nwhich limits practical application of these models in downstream tasks. To deal\nwith the problems above, we propose a novel Internal and External Data Transfer\n(IETrans) method, which can be applied in a play-and-plug fashion and expanded\nto large SGG with 1,807 predicate classes. Our IETrans tries to relieve the\ndata distribution problem by automatically creating an enhanced dataset that\nprovides more sufficient and coherent annotations for all predicates. By\ntraining on the transferred dataset, a Neural Motif model doubles the macro\nperformance while maintaining competitive micro performance. The data and code\nfor this paper are publicly available at\n\\url{https://github.com/waxnkw/IETrans-SGG.pytorch}\n","authors":["Ao Zhang","Yuan Yao","Qianyu Chen","Wei Ji","Zhiyuan Liu","Maosong Sun","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2203.11654v1.pdf","comment":"15 pages, 10 figures, conference"},{"id":"http://arxiv.org/abs/2203.11652v1","updated":"2022-03-22T12:16:05Z","published":"2022-03-22T12:16:05Z","title":"Weakly-Supervised Salient Object Detection Using Point Supervison","summary":"  Current state-of-the-art saliency detection models rely heavily on large\ndatasets of accurate pixel-wise annotations, but manually labeling pixels is\ntime-consuming and labor-intensive. There are some weakly supervised methods\ndeveloped for alleviating the problem, such as image label, bounding box label,\nand scribble label, while point label still has not been explored in this\nfield. In this paper, we propose a novel weakly-supervised salient object\ndetection method using point supervision. To infer the saliency map, we first\ndesign an adaptive masked flood filling algorithm to generate pseudo labels.\nThen we develop a transformer-based point-supervised saliency detection model\nto produce the first round of saliency maps. However, due to the sparseness of\nthe label, the weakly supervised model tends to degenerate into a general\nforeground detection model. To address this issue, we propose a Non-Salient\nSuppression (NSS) method to optimize the erroneous saliency maps generated in\nthe first round and leverage them for the second round of training. Moreover,\nwe build a new point-supervised dataset (P-DUTS) by relabeling the DUTS\ndataset. In P-DUTS, there is only one labeled point for each salient object.\nComprehensive experiments on five largest benchmark datasets demonstrate our\nmethod outperforms the previous state-of-the-art methods trained with the\nstronger supervision and even surpass several fully supervised state-of-the-art\nmodels. The code is available at: https://github.com/shuyonggao/PSOD.\n","authors":["Shuyong Gao","Wei Zhang","Yan Wang","Qianyu Guo","Chenglong Zhang","Yangji He","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11652v1.pdf","comment":"accepted by AAAI2022"},{"id":"http://arxiv.org/abs/2203.10492v2","updated":"2022-03-22T12:06:41Z","published":"2022-03-20T08:43:10Z","title":"SimAN: Exploring Self-Supervised Representation Learning of Scene Text\n  via Similarity-Aware Normalization","summary":"  Recently self-supervised representation learning has drawn considerable\nattention from the scene text recognition community. Different from previous\nstudies using contrastive learning, we tackle the issue from an alternative\nperspective, i.e., by formulating the representation learning scheme in a\ngenerative manner. Typically, the neighboring image patches among one text line\ntend to have similar styles, including the strokes, textures, colors, etc.\nMotivated by this common sense, we augment one image patch and use its\nneighboring patch as guidance to recover itself. Specifically, we propose a\nSimilarity-Aware Normalization (SimAN) module to identify the different\npatterns and align the corresponding styles from the guiding patch. In this\nway, the network gains representation capability for distinguishing complex\npatterns such as messy strokes and cluttered backgrounds. Experiments show that\nthe proposed SimAN significantly improves the representation quality and\nachieves promising performance. Moreover, we surprisingly find that our\nself-supervised generative network has impressive potential for data synthesis,\ntext image editing, and font interpolation, which suggests that the proposed\nSimAN has a wide range of practical applications.\n","authors":["Canjie Luo","Lianwen Jin","Jingdong Chen"],"pdf_url":"https://arxiv.org/pdf/2203.10492v2.pdf","comment":"Accepted to appear in CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11647v1","updated":"2022-03-22T11:59:52Z","published":"2022-03-22T11:59:52Z","title":"Semantic State Estimation in Cloth Manipulation Tasks","summary":"  Understanding of deformable object manipulations such as textiles is a\nchallenge due to the complexity and high dimensionality of the problem.\nParticularly, the lack of a generic representation of semantic states (e.g.,\n\\textit{crumpled}, \\textit{diagonally folded}) during a continuous manipulation\nprocess introduces an obstacle to identify the manipulation type. In this\npaper, we aim to solve the problem of semantic state estimation in cloth\nmanipulation tasks. For this purpose, we introduce a new large-scale\nfully-annotated RGB image dataset showing various human demonstrations of\ndifferent complicated cloth manipulations. We provide a set of baseline deep\nnetworks and benchmark them on the problem of semantic state estimation using\nour proposed dataset. Furthermore, we investigate the scalability of our\nsemantic state estimation framework in robot monitoring tasks of long and\ncomplex cloth manipulations.\n","authors":["Georgies Tzelepis","Eren Erdal Aksoy","Júlia Borràs","Guillem Alenyà"],"pdf_url":"https://arxiv.org/pdf/2203.11647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11637v1","updated":"2022-03-22T11:45:10Z","published":"2022-03-22T11:45:10Z","title":"Look for the Change: Learning Object States and State-Modifying Actions\n  from Untrimmed Web Videos","summary":"  Human actions often induce changes of object states such as \"cutting an\napple\", \"cleaning shoes\" or \"pouring coffee\". In this paper, we seek to\ntemporally localize object states (e.g. \"empty\" and \"full\" cup) together with\nthe corresponding state-modifying actions (\"pouring coffee\") in long uncurated\nvideos with minimal supervision. The contributions of this work are threefold.\nFirst, we develop a self-supervised model for jointly learning state-modifying\nactions together with the corresponding object states from an uncurated set of\nvideos from the Internet. The model is self-supervised by the causal ordering\nsignal, i.e. initial object state $\\rightarrow$ manipulating action\n$\\rightarrow$ end state. Second, to cope with noisy uncurated training data,\nour model incorporates a noise adaptive weighting module supervised by a small\nnumber of annotated still images, that allows to efficiently filter out\nirrelevant videos during training. Third, we collect a new dataset with more\nthan 2600 hours of video and 34 thousand changes of object states, and manually\nannotate a part of this data to validate our approach. Our results demonstrate\nsubstantial improvements over prior work in both action and object\nstate-recognition in video.\n","authors":["Tomáš Souček","Jean-Baptiste Alayrac","Antoine Miech","Ivan Laptev","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2203.11637v1.pdf","comment":"To be published in Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR), 2022"},{"id":"http://arxiv.org/abs/2203.11632v1","updated":"2022-03-22T11:34:40Z","published":"2022-03-22T11:34:40Z","title":"QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human\n  Motion Animation","summary":"  This paper studies the task of conditional Human Motion Animation (cHMA).\nGiven a source image and a driving video, the model should animate the new\nframe sequence, in which the person in the source image should perform a\nsimilar motion as the pose sequence from the driving video. Despite the success\nof Generative Adversarial Network (GANs) methods in image and video synthesis,\nit is still very challenging to conduct cHMA due to the difficulty in\nefficiently utilizing the conditional guided information such as images or\nposes, and generating images of good visual quality. To this end, this paper\nproposes a novel model of learning to Quantize, Scrabble, and Craft (QS-Craft)\nfor conditional human motion animation. The key novelties come from the newly\nintroduced three key steps: quantize, scrabble and craft. Particularly, our\nQS-Craft employs transformer in its structure to utilize the attention\narchitectures. The guided information is represented as a pose coordinate\nsequence extracted from the driving videos. Extensive experiments on human\nmotion datasets validate the efficacy of our model.\n","authors":["Yuxin Hong","Xuelin Qian","Simian Luo","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2203.11632v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2203.04042v2","updated":"2022-03-22T11:27:33Z","published":"2022-03-08T12:22:31Z","title":"Abandoning the Bayer-Filter to See in the Dark","summary":"  Low-light image enhancement - a pervasive but challenging problem, plays a\ncentral role in enhancing the visibility of an image captured in a poor\nillumination environment. Due to the fact that not all photons can pass the\nBayer-Filter on the sensor of the color camera, in this work, we first present\na De-Bayer-Filter simulator based on deep neural networks to generate a\nmonochrome raw image from the colored raw image. Next, a fully convolutional\nnetwork is proposed to achieve the low-light image enhancement by fusing\ncolored raw data with synthesized monochrome raw data. Channel-wise attention\nis also introduced to the fusion process to establish a complementary\ninteraction between features from colored and monochrome raw images. To train\nthe convolutional networks, we propose a dataset with monochrome and color raw\npairs named Mono-Colored Raw paired dataset (MCR) collected by using a\nmonochrome camera without Bayer-Filter and a color camera with Bayer-Filter.\nThe proposed pipeline take advantages of the fusion of the virtual monochrome\nand the color raw images and our extensive experiments indicate that\nsignificant improvement can be achieved by leveraging raw sensor data and\ndata-driven learning.\n","authors":["Xingbo Dong","Wanyan Xu","Zhihui Miao","Lan Ma","Chao Zhang","Jiewen Yang","Zhe Jin","Andrew Beng Jin Teoh","Jiajun Shen"],"pdf_url":"https://arxiv.org/pdf/2203.04042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11624v1","updated":"2022-03-22T11:20:21Z","published":"2022-03-22T11:20:21Z","title":"High-resolution Iterative Feedback Network for Camouflaged Object\n  Detection","summary":"  Spotting camouflaged objects that are visually assimilated into the\nbackground is tricky for both object detection algorithms and humans who are\nusually confused or cheated by the perfectly intrinsic similarities between the\nforeground objects and the background surroundings. To tackle this challenge,\nwe aim to extract the high-resolution texture details to avoid the detail\ndegradation that causes blurred vision in edges and boundaries. We introduce a\nnovel HitNet to refine the low-resolution representations by high-resolution\nfeatures in an iterative feedback manner, essentially a global loop-based\nconnection among the multi-scale resolutions. In addition, an iterative\nfeedback loss is proposed to impose more constraints on each feedback\nconnection. Extensive experiments on four challenging datasets demonstrate that\nour \\ourmodel~breaks the performance bottleneck and achieves significant\nimprovements compared with 29 state-of-the-art methods. To address the data\nscarcity in camouflaged scenarios, we provide an application example by\nemploying cross-domain learning to extract the features that can reflect the\ncamouflaged object properties and embed the features into salient objects,\nthereby generating more camouflaged training samples from the diverse salient\nobject datasets The code will be available at\nhttps://github.com/HUuxiaobin/HitNet.\n","authors":["Xiaobin Hu","Deng-Ping Fan","Xuebin Qin","Hang Dai","Wenqi Ren","Ying Tai","Chengjie Wang","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2203.11624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.06398v2","updated":"2022-03-22T11:13:03Z","published":"2021-12-13T03:16:19Z","title":"Shaping Visual Representations with Attributes for Few-Shot Learning","summary":"  Few-shot recognition aims to recognize novel categories under low-data\nregimes. Some recent few-shot recognition methods introduce auxiliary semantic\nmodality, i.e., category attribute information, into representation learning,\nwhich enhances the feature discrimination and improves the recognition\nperformance. Most of these existing methods only consider the attribute\ninformation of support set while ignoring the query set, resulting in a\npotential loss of performance. In this letter, we propose a novel\nattribute-shaped learning (ASL) framework, which can jointly perform query\nattributes generation and discriminative visual representation learning for\nfew-shot recognition. Specifically, a visual-attribute generator (VAG) is\nconstructed to predict the attributes of queries. By leveraging the attributes\ninformation, an attribute-visual attention module (AVAM) is designed, which can\nadaptively utilize attributes and visual representations to learn more\ndiscriminative features. Under the guidance of attribute modality, our method\ncan learn enhanced semantic-aware representation for classification.\nExperiments demonstrate that our method can achieve competitive results on CUB\nand SUN benchmarks. Our source code is available at:\n\\url{https://github.com/chenhaoxing/ASL}.\n","authors":["Haoxing Chen","Huaxiong Li","Yaohui Li","Chunlin Chen"],"pdf_url":"https://arxiv.org/pdf/2112.06398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11611v1","updated":"2022-03-22T10:58:02Z","published":"2022-03-22T10:58:02Z","title":"Dense Residual Networks for Gaze Mapping on Indian Roads","summary":"  In the recent past, greater accessibility to powerful computational resources\nhas enabled progress in the field of Deep Learning and Computer Vision to grow\nby leaps and bounds. This in consequence has lent progress to the domain of\nAutonomous Driving and Navigation Systems. Most of the present research work\nhas been focused on driving scenarios in the European or American roads. Our\npaper draws special attention to the Indian driving context. To this effect, we\npropose a novel architecture, DR-Gaze, which is used to map the driver's gaze\nonto the road. We compare our results with previous works and state-of-the-art\nresults on the DGAZE dataset. Our code will be made publicly available upon\nacceptance of our paper.\n","authors":["Chaitanya Kapoor","Kshitij Kumar","Soumya Vishnoi","Sriram Ramanathan"],"pdf_url":"https://arxiv.org/pdf/2203.11611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.14358v2","updated":"2022-03-22T10:56:39Z","published":"2021-11-29T07:22:53Z","title":"IDR: Self-Supervised Image Denoising via Iterative Data Refinement","summary":"  The lack of large-scale noisy-clean image pairs restricts supervised\ndenoising methods' deployment in actual applications. While existing\nunsupervised methods are able to learn image denoising without ground-truth\nclean images, they either show poor performance or work under impractical\nsettings (e.g., paired noisy images). In this paper, we present a practical\nunsupervised image denoising method to achieve state-of-the-art denoising\nperformance. Our method only requires single noisy images and a noise model,\nwhich is easily accessible in practical raw image denoising. It performs two\nsteps iteratively: (1) Constructing a noisier-noisy dataset with random noise\nfrom the noise model; (2) training a model on the noisier-noisy dataset and\nusing the trained model to refine noisy images to obtain the targets used in\nthe next round. We further approximate our full iterative method with a fast\nalgorithm for more efficient training while keeping its original high\nperformance. Experiments on real-world, synthetic, and correlated noise show\nthat our proposed unsupervised denoising approach has superior performances\nover existing unsupervised methods and competitive performance with supervised\nmethods. In addition, we argue that existing denoising datasets are of low\nquality and contain only a small number of scenes. To evaluate raw image\ndenoising performance in real-world applications, we build a high-quality raw\nimage dataset SenseNoise-500 that contains 500 real-life scenes. The dataset\ncan serve as a strong benchmark for better evaluating raw image denoising. Code\nand dataset will be released at https://github.com/zhangyi-3/IDR\n","authors":["Yi Zhang","Dasong Li","Ka Lung Law","Xiaogang Wang","Hongwei Qin","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2111.14358v2.pdf","comment":"CVPR2022; code & dataset: https://github.com/zhangyi-3/IDR"},{"id":"http://arxiv.org/abs/2106.05113v3","updated":"2022-03-22T10:44:32Z","published":"2021-06-09T14:46:09Z","title":"More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain\n  Activity","summary":"  In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages & depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.\n","authors":["Guy Gaziv","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2106.05113v3.pdf","comment":"Code: https://github.com/WeizmannVision/SelfSuperReconst"},{"id":"http://arxiv.org/abs/2202.13981v2","updated":"2022-03-22T10:43:32Z","published":"2022-02-28T17:36:12Z","title":"\"If you could see me through my eyes\": Predicting Pedestrian Perception","summary":"  Pedestrians are particularly vulnerable road users in urban traffic. With the\narrival of autonomous driving, novel technologies can be developed specifically\nto protect pedestrians. We propose a machine learning toolchain to train\nartificial neural networks as models of pedestrian behavior. In a preliminary\nstudy, we use synthetic data from simulations of a specific pedestrian crossing\nscenario to train a variational autoencoder and a long short-term memory\nnetwork to predict a pedestrian's future visual perception. We can accurately\npredict a pedestrian's future perceptions within relevant time horizons. By\niteratively feeding these predicted frames into these networks, they can be\nused as simulations of pedestrians as indicated by our results. Such trained\nnetworks can later be used to predict pedestrian behaviors even from the\nperspective of the autonomous car. Another future extension will be to re-train\nthese networks with real-world video data.\n","authors":["Julian Petzold","Mostafa Wahby","Franek Stark","Ulrich Behrje","Heiko Hamann"],"pdf_url":"https://arxiv.org/pdf/2202.13981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11156v2","updated":"2022-03-22T10:25:45Z","published":"2022-03-21T17:34:18Z","title":"Operator Sketching for Deep Unrolling Networks","summary":"  In this work we propose a new paradigm for designing efficient deep unrolling\nnetworks using operator sketching. The deep unrolling networks are currently\nthe state-of-the-art solutions for imaging inverse problems. However, for\nhigh-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI\nimaging, the deep unrolling schemes typically become inefficient both in terms\nof memory and computation, due to the need of computing multiple times the\nhigh-dimensional forward and adjoint operators. Recently researchers have found\nthat such limitations can be partially addressed by stochastic unrolling with\nsubsets of operators, inspired by the success of stochastic first-order\noptimization. In this work, we propose a further acceleration upon stochastic\nunrolling, using sketching techniques to approximate products in the\nhigh-dimensional image space. The operator sketching can be jointly applied\nwith stochastic unrolling for the best acceleration and compression\nperformance. Our numerical experiments on X-ray CT image reconstruction\ndemonstrate the remarkable effectiveness of our sketched unrolling schemes.\n","authors":["Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2203.11156v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11593v1","updated":"2022-03-22T10:21:11Z","published":"2022-03-22T10:21:11Z","title":"Unified Negative Pair Generation toward Well-discriminative Feature\n  Space for Face Recognition","summary":"  The goal of face recognition (FR) can be viewed as a pair similarity\noptimization problem, maximizing a similarity set $\\mathcal{S}^p$ over positive\npairs, while minimizing similarity set $\\mathcal{S}^n$ over negative pairs.\nIdeally, it is expected that FR models form a well-discriminative feature space\n(WDFS) that satisfies $\\inf{\\mathcal{S}^p} > \\sup{\\mathcal{S}^n}$. With regard\nto WDFS, the existing deep feature learning paradigms (i.e., metric and\nclassification losses) can be expressed as a unified perspective on different\npair generation (PG) strategies. Unfortunately, in the metric loss (ML), it is\ninfeasible to generate negative pairs taking all classes into account in each\niteration because of the limited mini-batch size. In contrast, in\nclassification loss (CL), it is difficult to generate extremely hard negative\npairs owing to the convergence of the class weight vectors to their center.\nThis leads to a mismatch between the two similarity distributions of the\nsampled pairs and all negative pairs. Thus, this paper proposes a unified\nnegative pair generation (UNPG) by combining two PG strategies (i.e., MLPG and\nCLPG) from a unified perspective to alleviate the mismatch. UNPG introduces\nuseful information about negative pairs using MLPG to overcome the CLPG\ndeficiency. Moreover, it includes filtering the similarities of noisy negative\npairs to guarantee reliable convergence and improved performance. Exhaustive\nexperiments show the superiority of UNPG by achieving state-of-the-art\nperformance across recent loss functions on public benchmark datasets. Our code\nand pretrained models are publicly available.\n","authors":["Junuk Jung","Seonhoon Lee","Heung-Seon Oh","Yongjun Park","Joochan Park","Sungbin Son"],"pdf_url":"https://arxiv.org/pdf/2203.11593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13280v2","updated":"2022-03-22T10:18:28Z","published":"2021-11-26T00:35:09Z","title":"Efficient Self-Ensemble for Semantic Segmentation","summary":"  Ensemble of predictions is known to perform better than individual\npredictions taken separately. However, for tasks that require heavy\ncomputational resources, e.g. semantic segmentation, creating an ensemble of\nlearners that needs to be trained separately is hardly tractable. In this work,\nwe propose to leverage the performance boost offered by ensemble methods to\nenhance the semantic segmentation, while avoiding the traditional heavy\ntraining cost of the ensemble. Our self-ensemble approach takes advantage of\nthe multi-scale features set produced by feature pyramid network methods to\nfeed independent decoders, thus creating an ensemble within a single model.\nSimilar to the ensemble, the final prediction is the aggregation of the\nprediction made by each learner. In contrast to previous works, our model can\nbe trained end-to-end, alleviating the traditional cumbersome multi-stage\ntraining of ensembles. Our self-ensemble approach outperforms the current\nstate-of-the-art on the benchmark datasets Pascal Context and COCO-Stuff-10K\nfor semantic segmentation and is competitive on ADE20K and Cityscapes. Code is\npublicly available at github.com/WalBouss/SenFormer.\n","authors":["Walid Bousselham","Guillaume Thibault","Lucas Pagano","Archana Machireddy","Joe Gray","Young Hwan Chang","Xubo Song"],"pdf_url":"https://arxiv.org/pdf/2111.13280v2.pdf","comment":"Code available at https://github.com/WalBouss/SenFormer"},{"id":"http://arxiv.org/abs/2203.11591v1","updated":"2022-03-22T10:17:12Z","published":"2022-03-22T10:17:12Z","title":"HOP: History-and-Order Aware Pre-training for Vision-and-Language\n  Navigation","summary":"  Pre-training has been adopted in a few of recent works for\nVision-and-Language Navigation (VLN). However, previous pre-training methods\nfor VLN either lack the ability to predict future actions or ignore the\ntrajectory contexts, which are essential for a greedy navigation process. In\nthis work, to promote the learning of spatio-temporal visual-textual\ncorrespondence as well as the agent's capability of decision making, we propose\na novel history-and-order aware pre-training paradigm (HOP) with VLN-specific\nobjectives that exploit the past observations and support future action\nprediction. Specifically, in addition to the commonly used Masked Language\nModeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy\ntasks to model temporal order information: Trajectory Order Modeling (TOM) and\nGroup Order Modeling (GOM). Moreover, our navigation action prediction is also\nenhanced by introducing the task of Action Prediction with History (APH), which\ntakes into account the history visual perceptions. Extensive experimental\nresults on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the\neffectiveness of our proposed method compared against several state-of-the-art\nagents.\n","authors":["Yanyuan Qiao","Yuankai Qi","Yicong Hong","Zheng Yu","Peng Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2203.11591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11590v1","updated":"2022-03-22T10:14:08Z","published":"2022-03-22T10:14:08Z","title":"IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding\n  Alignment","summary":"  This paper investigates the problem of temporally interpolating dynamic 3D\npoint clouds with large non-rigid deformation. We formulate the problem as\nestimation of point-wise trajectories (i.e., smooth curves) and further reason\nthat temporal irregularity and under-sampling are two major challenges. To\ntackle the challenges, we propose IDEA-Net, an end-to-end deep learning\nframework, which disentangles the problem under the assistance of the\nexplicitly learned temporal consistency. Specifically, we propose a temporal\nconsistency learning module to align two consecutive point cloud frames\npoint-wisely, based on which we can employ linear interpolation to obtain\ncoarse trajectories/in-between frames. To compensate the high-order nonlinear\ncomponents of trajectories, we apply aligned feature embeddings that encode\nlocal geometry properties to regress point-wise increments, which are combined\nwith the coarse estimations. We demonstrate the effectiveness of our method on\nvarious point cloud sequences and observe large improvement over\nstate-of-the-art methods both quantitatively and visually. Our framework can\nbring benefits to 3D motion data acquisition. The source code is publicly\navailable at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.\n","authors":["Yiming Zeng","Yue Qian","Qijian Zhang","Junhui Hou","Yixuan Yuan","Ying He"],"pdf_url":"https://arxiv.org/pdf/2203.11590v1.pdf","comment":"This paper was accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11589v1","updated":"2022-03-22T10:13:48Z","published":"2022-03-22T10:13:48Z","title":"Adaptive Patch Exiting for Scalable Single Image Super-Resolution","summary":"  Since the future of computing is heterogeneous, scalability is a crucial\nproblem for single image super-resolution. Recent works try to train one\nnetwork, which can be deployed on platforms with different capacities. However,\nthey rely on the pixel-wise sparse convolution, which is not hardware-friendly\nand achieves limited practical speedup. As image can be divided into patches,\nwhich have various restoration difficulties, we present a scalable method based\non Adaptive Patch Exiting (APE) to achieve more practical speedup.\nSpecifically, we propose to train a regressor to predict the incremental\ncapacity of each layer for the patch. Once the incremental capacity is below\nthe threshold, the patch can exit at the specific layer. Our method can easily\nadjust the trade-off between performance and efficiency by changing the\nthreshold of incremental capacity. Furthermore, we propose a novel strategy to\nenable the network training of our method. We conduct extensive experiments\nacross various backbones, datasets and scaling factors to demonstrate the\nadvantages of our method. Code will be released.\n","authors":["Shizun Wang","Ming Lu","Kaixin Chen","Xiaoqi Li","Jiaming Liu","Yandong Guo"],"pdf_url":"https://arxiv.org/pdf/2203.11589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.02857v4","updated":"2022-03-22T10:04:03Z","published":"2021-12-06T08:28:05Z","title":"PTTR: Relational 3D Point Cloud Object Tracking with Transformer","summary":"  In a point cloud sequence, 3D object tracking aims to predict the location\nand orientation of an object in the current search point cloud given a template\npoint cloud. Motivated by the success of transformers, we propose Point\nTracking TRansformer (PTTR), which efficiently predicts high-quality 3D\ntracking results in a coarse-to-fine manner with the help of transformer\noperations. PTTR consists of three novel designs. 1) Instead of random\nsampling, we design Relation-Aware Sampling to preserve relevant points to\ngiven templates during subsampling. 2) Furthermore, we propose a Point Relation\nTransformer (PRT) consisting of a self-attention and a cross-attention module.\nThe global self-attention operation captures long-range dependencies to enhance\nencoded point features for the search area and the template, respectively.\nSubsequently, we generate the coarse tracking results by matching the two sets\nof point features via cross-attention. 3) Based on the coarse tracking results,\nwe employ a novel Prediction Refinement Module to obtain the final refined\nprediction. In addition, we create a large-scale point cloud single object\ntracking benchmark based on the Waymo Open Dataset. Extensive experiments show\nthat PTTR achieves superior point cloud tracking in both accuracy and\nefficiency.\n","authors":["Changqing Zhou","Zhipeng Luo","Yueru Luo","Tianrui Liu","Liang Pan","Zhongang Cai","Haiyu Zhao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2112.02857v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.15093v2","updated":"2022-03-22T09:44:20Z","published":"2021-05-31T16:22:33Z","title":"Pho(SC)-CTC -- A Hybrid Approach Towards Zero-shot Word Image\n  Recognition","summary":"  Annotating words in a historical document image archive for word image\nrecognition purpose demands time and skilled human resource (like historians,\npaleographers). In a real-life scenario, obtaining sample images for all\npossible words is also not feasible. However, Zero-shot learning methods could\naptly be used to recognize unseen/out-of-lexicon words in such historical\ndocument images. Based on previous state-of-the-art method for zero-shot word\nrecognition Pho(SC)Net, we propose a hybrid model based on the CTC framework\n(Pho(SC)-CTC) that takes advantage of the rich features learned by Pho(SC)Net\nfollowed by a connectionist temporal classification (CTC) framework to perform\nthe final classification. Encouraging results were obtained on two publicly\navailable historical document datasets and one synthetic handwritten dataset,\nwhich justifies the efficacy of Pho(SC)-CTC and Pho(SC)Net.\n","authors":["Ravi Bhatt","Anuj Rai","Narayanan C. Krishnan","Sukalpa Chanda"],"pdf_url":"https://arxiv.org/pdf/2105.15093v2.pdf","comment":"Under Review (International Journal on Document Analysis and\n  Recognition). This paper is the extension of the paper titled \"Pho(SC)Net: An\n  Approach Towards Zero-shot Word Image Recognition in Historical Documents\"\n  published in ICDAR 2021"},{"id":"http://arxiv.org/abs/2203.10833v2","updated":"2022-03-22T09:40:22Z","published":"2022-03-21T09:48:23Z","title":"Hyperbolic Vision Transformers: Combining Improvements in Metric\n  Learning","summary":"  Metric learning aims to learn a highly discriminative model encouraging the\nembeddings of similar classes to be close in the chosen metrics and pushed\napart for dissimilar ones. The common recipe is to use an encoder to extract\nembeddings and a distance-based loss function to match the representations --\nusually, the Euclidean distance is utilized. An emerging interest in learning\nhyperbolic data embeddings suggests that hyperbolic geometry can be beneficial\nfor natural data. Following this line of work, we propose a new\nhyperbolic-based model for metric learning. At the core of our method is a\nvision transformer with output embeddings mapped to hyperbolic space. These\nembeddings are directly optimized using modified pairwise cross-entropy loss.\nWe evaluate the proposed model with six different formulations on four datasets\nachieving the new state-of-the-art performance. The source code is available at\nhttps://github.com/htdt/hyp_metric.\n","authors":["Aleksandr Ermolov","Leyla Mirvakhabova","Valentin Khrulkov","Nicu Sebe","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2203.10833v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11565v1","updated":"2022-03-22T09:38:41Z","published":"2022-03-22T09:38:41Z","title":"Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose\n  CT Image Reconstruction","summary":"  The recently proposed sparsifying transform models incur low computational\ncost and have been applied to medical imaging. Meanwhile, deep models with\nnested network structure reveal great potential for learning features in\ndifferent layers. In this study, we propose a network-structured sparsifying\ntransform learning approach for X-ray computed tomography (CT), which we refer\nto as multi-layer clustering-based residual sparsifying transform (MCST)\nlearning. The proposed MCST scheme learns multiple different unitary transforms\nin each layer by dividing each layer's input into several classes. We apply the\nMCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST\nmodel into the regularizer in penalized weighted least squares (PWLS)\nreconstruction. We conducted LDCT reconstruction experiments on XCAT phantom\ndata and Mayo Clinic data and trained the MCST model with 2 (or 3) layers and\nwith 5 clusters in each layer. The learned transforms in the same layer showed\nrich features while additional information is extracted from representation\nresiduals. Our simulation results demonstrate that PWLS-MCST achieves better\nimage reconstruction quality than the conventional FBP method and PWLS with\nedge-preserving (EP) regularizer. It also outperformed recent advanced methods\nlike PWLS with a learned multi-layer residual sparsifying transform prior\n(MARS) and PWLS with a union of learned transforms (ULTRA), especially for\ndisplaying clear edges and preserving subtle details.\n","authors":["Xikai Yang","Zhishen Huang","Yong Long","Saiprasad Ravishankar"],"pdf_url":"https://arxiv.org/pdf/2203.11565v1.pdf","comment":"19 pages, 12 figures, submitted to the Medical Physics"},{"id":"http://arxiv.org/abs/2203.11564v1","updated":"2022-03-22T09:37:24Z","published":"2022-03-22T09:37:24Z","title":"Reinforcement-based frugal learning for satellite image change detection","summary":"  In this paper, we introduce a novel interactive satellite image change\ndetection algorithm based on active learning. The proposed approach is\niterative and asks the user (oracle) questions about the targeted changes and\naccording to the oracle's responses updates change detections. We consider a\nprobabilistic framework which assigns to each unlabeled sample a relevance\nmeasure modeling how critical is that sample when training change detection\nfunctions. These relevance measures are obtained by minimizing an objective\nfunction mixing diversity, representativity and uncertainty. These criteria\nwhen combined allow exploring different data modes and also refining change\ndetections. To further explore the potential of this objective function, we\nconsider a reinforcement learning approach that finds the best combination of\ndiversity, representativity and uncertainty, through active learning\niterations, leading to better generalization as corroborated through\nexperiments in interactive satellite image change detection.\n","authors":["Sebastien Deschamps","Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2203.11564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10562v2","updated":"2022-03-22T09:34:04Z","published":"2022-03-20T14:28:38Z","title":"CRISPnet: Color Rendition ISP Net","summary":"  Image signal processors (ISPs) are historically grown legacy software systems\nfor reconstructing color images from noisy raw sensor measurements. They are\nusually composited of many heuristic blocks for denoising, demosaicking, and\ncolor restoration. Color reproduction in this context is of particular\nimportance, since the raw colors are often severely distorted, and each smart\nphone manufacturer has developed their own characteristic heuristics for\nimproving the color rendition, for example of skin tones and other visually\nimportant colors.\n  In recent years there has been strong interest in replacing the historically\ngrown ISP systems with deep learned pipelines. Much progress has been made in\napproximating legacy ISPs with such learned models. However, so far the focus\nof these efforts has been on reproducing the structural features of the images,\nwith less attention paid to color rendition.\n  Here we present CRISPnet, the first learned ISP model to specifically target\ncolor rendition accuracy relative to a complex, legacy smart phone ISP. We\nachieve this by utilizing both image metadata (like a legacy ISP would), as\nwell as by learning simple global semantics based on image classification --\nsimilar to what a legacy ISP does to determine the scene type. We also\ncontribute a new ISP image dataset consisting of both high dynamic range\nmonitor data, as well as real-world data, both captured with an actual cell\nphone ISP pipeline under a variety of lighting conditions, exposure times, and\ngain settings.\n","authors":["Matheus Souza","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2203.10562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11559v1","updated":"2022-03-22T09:29:42Z","published":"2022-03-22T09:29:42Z","title":"Frugal Learning of Virtual Exemplars for Label-Efficient Satellite Image\n  Change Detection","summary":"  In this paper, we devise a novel interactive satellite image change detection\nalgorithm based on active learning. The proposed framework is iterative and\nrelies on a question and answer model which asks the oracle (user) questions\nabout the most informative display (subset of critical images), and according\nto the user's responses, updates change detections. The contribution of our\nframework resides in a novel display model which selects the most\nrepresentative and diverse virtual exemplars that adversely challenge the\nlearned change detection functions, thereby leading to highly discriminating\nfunctions in the subsequent iterations of active learning. Extensive\nexperiments, conducted on the challenging task of interactive satellite image\nchange detection, show the superiority of the proposed virtual display model\nagainst the related work.\n","authors":["Hichem Sahbi","Sebastien Deschamps"],"pdf_url":"https://arxiv.org/pdf/2203.11559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08657v2","updated":"2022-03-22T09:25:48Z","published":"2022-03-16T14:47:45Z","title":"Occlusion Fields: An Implicit Representation for Non-Line-of-Sight\n  Surface Reconstruction","summary":"  Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality\nthat aims to recover objects or scene parts outside the field of view from\nmeasurements of light that is indirectly scattered off a directly visible,\ndiffuse wall. Despite recent advances in acquisition and reconstruction\ntechniques, the well-posedness of the problem at large, and the recoverability\nof objects and their shapes in particular, remains an open question. The\ncommonly employed Fermat path criterion is rather conservative with this\nregard, as it classifies some surfaces as unrecoverable, although they\ncontribute to the signal.\n  In this paper, we use a simpler necessary criterion for an opaque surface\npatch to be recoverable. Such piece of surface must be directly visible from\nsome point on the wall, and it must occlude the space behind itself. Inspired\nby recent advances in neural implicit representations, we devise a new\nrepresentation and reconstruction technique for NLoS scenes that unifies the\ntreatment of recoverability with the reconstruction itself. Our approach, which\nwe validate on various synthetic and experimental datasets, exhibits\ninteresting properties. Unlike memory-inefficient volumetric representations,\nours allows to infer adaptively tessellated surfaces from time-of-flight\nmeasurements of moderate resolution. It can further recover features beyond the\nFermat path criterion, and it is robust to significant amounts of\nself-occlusion. We believe that this is the first time that these properties\nhave been achieved in one system that, as an additional benefit, is trainable\nand hence suited for data-driven approaches.\n","authors":["Javier Grau","Markus Plack","Patrick Haehn","Michael Weinmann","Matthias Hullin"],"pdf_url":"https://arxiv.org/pdf/2203.08657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09416v2","updated":"2022-03-22T08:57:37Z","published":"2022-03-17T16:16:03Z","title":"Bi-directional Object-context Prioritization Learning for Saliency\n  Ranking","summary":"  The saliency ranking task is recently proposed to study the visual behavior\nthat humans would typically shift their attention over different objects of a\nscene based on their degrees of saliency. Existing approaches focus on learning\neither object-object or object-scene relations. Such a strategy follows the\nidea of object-based attention in Psychology, but it tends to favor those\nobjects with strong semantics (e.g., humans), resulting in unrealistic saliency\nranking. We observe that spatial attention works concurrently with object-based\nattention in the human visual recognition system. During the recognition\nprocess, the human spatial attention mechanism would move, engage, and\ndisengage from region to region (i.e., context to context). This inspires us to\nmodel the region-level interactions, in addition to the object-level reasoning,\nfor saliency ranking. To this end, we propose a novel bi-directional method to\nunify spatial attention and object-based attention for saliency ranking. Our\nmodel includes two novel modules: (1) a selective object saliency (SOS) module\nthat models objectbased attention via inferring the semantic representation of\nthe salient object, and (2) an object-context-object relation (OCOR) module\nthat allocates saliency ranks to objects by jointly modeling the object-context\nand context-object interactions of the salient objects. Extensive experiments\nshow that our approach outperforms existing state-of-theart methods. Our code\nand pretrained model are available at https://github.com/GrassBro/OCOR.\n","authors":["Xin Tian","Ke Xu","Xin Yang","Lin Du","Baocai Yin","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2203.09416v2.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11007v2","updated":"2022-03-22T08:56:44Z","published":"2022-03-21T14:23:00Z","title":"Computational ergonomics for task delegation in Human-Robot\n  Collaboration: spatiotemporal adaptation of the robot to the human through\n  contactless gesture recognition","summary":"  The high prevalence of work-related musculoskeletal disorders (WMSDs) could\nbe addressed by optimizing Human-Robot Collaboration (HRC) frameworks for\nmanufacturing applications. In this context, this paper proposes two hypotheses\nfor ergonomically effective task delegation and HRC. The first hypothesis\nstates that it is possible to quantify ergonomically professional tasks using\nmotion data from a reduced set of sensors. Then, the most dangerous tasks can\nbe delegated to a collaborative robot. The second hypothesis is that by\nincluding gesture recognition and spatial adaptation, the ergonomics of an HRC\nscenario can be improved by avoiding needless motions that could expose\noperators to ergonomic risks and by lowering the physical effort required of\noperators. An HRC scenario for a television manufacturing process is optimized\nto test both hypotheses. For the ergonomic evaluation, motion primitives with\nknown ergonomic risks were modeled for their detection in professional tasks\nand to estimate a risk score based on the European Assembly Worksheet (EAWS). A\nDeep Learning gesture recognition module trained with egocentric television\nassembly data was used to complement the collaboration between the human\noperator and the robot. Additionally, a skeleton-tracking algorithm provided\nthe robot with information about the operator's pose, allowing it to spatially\nadapt its motion to the operator's anthropometrics. Three experiments were\nconducted to determine the effect of gesture recognition and spatial adaptation\non the operator's range of motion. The rate of spatial adaptation was used as a\nkey performance indicator (KPI), and a new KPI for measuring the reduction in\nthe operator's motion is presented in this paper.\n","authors":["Brenda Elizabeth Olivas-Padilla","Dimitris Papanagiotou","Gavriela Senteri","Sotiris Manitsaris","Alina Glushkova"],"pdf_url":"https://arxiv.org/pdf/2203.11007v2.pdf","comment":"Under review in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2203.11542v1","updated":"2022-03-22T08:50:41Z","published":"2022-03-22T08:50:41Z","title":"Mask Usage Recognition using Vision Transformer with Transfer Learning\n  and Data Augmentation","summary":"  The COVID-19 pandemic has disrupted various levels of society. The use of\nmasks is essential in preventing the spread of COVID-19 by identifying an image\nof a person using a mask. Although only 23.1% of people use masks correctly,\nArtificial Neural Networks (ANN) can help classify the use of good masks to\nhelp slow the spread of the Covid-19 virus. However, it requires a large\ndataset to train an ANN that can classify the use of masks correctly.\nMaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4\nclass labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth.\nMask classification training utilizes Vision Transformers (ViT) architecture\nwith transfer learning method using pre-trained weights on ImageNet-21k, with\nrandom augmentation. In addition, the hyper-parameters of training of 20\nepochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of\n0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation\nfunction, and a Cross-Entropy loss function are used to be applied on the\ntraining of three architectures of ViT, namely Base-16, Large-16, and Huge-14.\nFurthermore, comparisons of with and without augmentation and transfer learning\nare conducted. This study found that the best classification is transfer\nlearning and augmentation using ViT Huge-14. Using this method on\nMaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training\ndata, 0.9412 on validation data, and 0.9534 on test data. This research shows\nthat training the ViT model with data augmentation and transfer learning\nimproves classification of the mask usage, even better than convolutional-based\nResidual Network (ResNet).\n","authors":["Hensel Donato Jahja","Novanto Yudistira"," Sutrisno"],"pdf_url":"https://arxiv.org/pdf/2203.11542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08674v2","updated":"2022-03-22T08:45:00Z","published":"2022-03-16T15:01:17Z","title":"Know your sensORs -- A Modality Study For Surgical Action Classification","summary":"  The surgical operating room (OR) presents many opportunities for automation\nand optimization. Videos from various sources in the OR are becoming\nincreasingly available. The medical community seeks to leverage this wealth of\ndata to develop automated methods to advance interventional care, lower costs,\nand improve overall patient outcomes. Existing datasets from OR room cameras\nare thus far limited in size or modalities acquired, leaving it unclear which\nsensor modalities are best suited for tasks such as recognizing surgical action\nfrom videos. This study demonstrates that surgical action recognition\nperformance can vary depending on the image modalities used. We perform a\nmethodical analysis on several commonly available sensor modalities, presenting\ntwo fusion approaches that improve classification performance. The analyses are\ncarried out on a set of multi-view RGB-D video recordings of 18 laparoscopic\nprocedures.\n","authors":["Lennart Bastian","Tobias Czempiel","Christian Heiliger","Konrad Karcz","Ulrich Eck","Benjamin Busam","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2203.08674v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2111.04739v2","updated":"2022-03-22T08:42:40Z","published":"2021-11-08T14:52:13Z","title":"DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet","summary":"  Accurate retinal vessel segmentation is an important task for many\ncomputer-aided diagnosis systems. Yet, it is still a challenging problem due to\nthe complex vessel structures of an eye. Numerous vessel segmentation methods\nhave been proposed recently, however more research is needed to deal with poor\nsegmentation of thin and tiny vessels. To address this, we propose a new deep\nlearning pipeline combining the efficiency of residual dense net blocks and,\nresidual squeeze and excitation blocks. We validate experimentally our approach\non three datasets and show that our pipeline outperforms current state of the\nart techniques on the sensitivity metric relevant to assess capture of small\nvessels.\n","authors":["Ali Karaali","Rozenn Dahyot","Donal J. Sexton"],"pdf_url":"https://arxiv.org/pdf/2111.04739v2.pdf","comment":"Accepted to ICPRAI 2022 - 3rd International Conference on Pattern\n  Recognition and Artificial Intelligence"},{"id":"http://arxiv.org/abs/2201.12329v3","updated":"2022-03-22T08:28:59Z","published":"2022-01-28T18:51:09Z","title":"DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR","summary":"  We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as\nqueries in Transformer decoders and dynamically updates them layer-by-layer.\nUsing box coordinates not only helps using explicit positional priors to\nimprove the query-to-feature similarity and eliminate the slow training\nconvergence issue in DETR, but also allows us to modulate the positional\nattention map using the box width and height information. Such a design makes\nit clear that queries in DETR can be implemented as performing soft ROI pooling\nlayer-by-layer in a cascade manner. As a result, it leads to the best\nperformance on MS-COCO benchmark among the DETR-like detection models under the\nsame setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50\nepochs. We also conducted extensive experiments to confirm our analysis and\nverify the effectiveness of our methods. Code is available at\n\\url{https://github.com/SlongLiu/DAB-DETR}.\n","authors":["Shilong Liu","Feng Li","Hao Zhang","Xiao Yang","Xianbiao Qi","Hang Su","Jun Zhu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2201.12329v3.pdf","comment":"Accepted to ICLR 2022"},{"id":"http://arxiv.org/abs/2203.11537v1","updated":"2022-03-22T08:28:50Z","published":"2022-03-22T08:28:50Z","title":"Convolutional Neural Network-based Efficient Dense Point Cloud\n  Generation using Unsigned Distance Fields","summary":"  Dense point cloud generation from a sparse or incomplete point cloud is a\ncrucial and challenging problem in 3D computer vision and computer graphics. So\nfar, the existing methods are either computationally too expensive, suffer from\nlimited resolution, or both. In addition, some methods are strictly limited to\nwatertight surfaces -- another major obstacle for a number of applications. To\naddress these issues, we propose a lightweight Convolutional Neural Network\nthat learns and predicts the unsigned distance field for arbitrary 3D shapes\nfor dense point cloud generation using the recently emerged concept of implicit\nfunction learning. Experiments demonstrate that the proposed architecture\nachieves slightly better quality results than the state of the art with 87%\nless model parameters and 40% less GPU memory usage.\n","authors":["Abol Basher","Jani Boutellier"],"pdf_url":"https://arxiv.org/pdf/2203.11537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13410v3","updated":"2022-03-22T07:59:05Z","published":"2021-11-26T10:28:45Z","title":"Modeling Annotator Preference and Stochastic Annotation Error for\n  Medical Image Segmentation","summary":"  Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n","authors":["Zehui Liao","Shishuai Hu","Yutong Xie","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2111.13410v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.04886v3","updated":"2022-03-22T07:40:38Z","published":"2020-12-09T06:42:30Z","title":"DS-Net: Dynamic Spatiotemporal Network for Video Salient Object\n  Detection","summary":"  As moving objects always draw more attention of human eyes, the temporal\nmotive information is always exploited complementarily with spatial information\nto detect salient objects in videos. Although efficient tools such as optical\nflow have been proposed to extract temporal motive information, it often\nencounters difficulties when used for saliency detection due to the movement of\ncamera or the partial movement of salient objects. In this paper, we\ninvestigate the complimentary roles of spatial and temporal information and\npropose a novel dynamic spatiotemporal network (DS-Net) for more effective\nfusion of spatiotemporal information. We construct a symmetric two-bypass\nnetwork to explicitly extract spatial and temporal features. A dynamic weight\ngenerator (DWG) is designed to automatically learn the reliability of\ncorresponding saliency branch. And a top-down cross attentive aggregation (CAA)\nprocedure is designed so as to facilitate dynamic complementary aggregation of\nspatiotemporal features. Finally, the features are modified by spatial\nattention with the guidance of coarse saliency map and then go through decoder\npart for final saliency map. Experimental results on five benchmarks VOS,\nDAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method\nachieves superior performance than state-of-the-art algorithms. The source code\nis available at https://github.com/TJUMMG/DS-Net.\n","authors":["Jing Liu","Jiaxiang Wang","Weikang Wang","Yuting Su"],"pdf_url":"https://arxiv.org/pdf/2012.04886v3.pdf","comment":"The article has made some format changes"},{"id":"http://arxiv.org/abs/2203.11509v1","updated":"2022-03-22T07:37:08Z","published":"2022-03-22T07:37:08Z","title":"Unsupervised Deraining: Where Contrastive Learning Meets Self-similarity","summary":"  Image deraining is a typical low-level image restoration task, which aims at\ndecomposing the rainy image into two distinguishable layers: the clean image\nlayer and the rain layer. Most of the existing learning-based deraining methods\nare supervisedly trained on synthetic rainy-clean pairs. The domain gap between\nthe synthetic and real rains makes them less generalized to different real\nrainy scenes. Moreover, the existing methods mainly utilize the property of the\ntwo layers independently, while few of them have considered the mutually\nexclusive relationship between the two layers. In this work, we propose a novel\nnon-local contrastive learning (NLCL) method for unsupervised image deraining.\nConsequently, we not only utilize the intrinsic self-similarity property within\nsamples but also the mutually exclusive property between the two layers, so as\nto better differ the rain layer from the clean image. Specifically, the\nnon-local self-similarity image layer patches as the positives are pulled\ntogether and similar rain layer patches as the negatives are pushed away. Thus\nthe similar positive/negative samples that are close in the original space\nbenefit us to enrich more discriminative representation. Apart from the\nself-similarity sampling strategy, we analyze how to choose an appropriate\nfeature encoder in NLCL. Extensive experiments on different real rainy datasets\ndemonstrate that the proposed method obtains state-of-the-art performance in\nreal deraining.\n","authors":["Ye Yuntong","Yu Changfeng","Chang Yi","Zhu Lin","Zhao Xile","Yan Luxin","Tian Yonghong"],"pdf_url":"https://arxiv.org/pdf/2203.11509v1.pdf","comment":"10 pages, 10 figures, accept to 2022CVPR"},{"id":"http://arxiv.org/abs/2203.11506v1","updated":"2022-03-22T07:30:38Z","published":"2022-03-22T07:30:38Z","title":"Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition","summary":"  Deep neural networks perform poorly on heavily class-imbalanced datasets.\nGiven the promising performance of contrastive learning, we propose\n$\\mathbf{Re}$balanced $\\mathbf{S}$iamese $\\mathbf{Co}$ntrastive\n$\\mathbf{m}$ining ( $\\mathbf{ResCom}$) to tackle imbalanced recognition. Based\non the mathematical analysis and simulation results, we claim that supervised\ncontrastive learning suffers a dual class-imbalance problem at both the\noriginal batch and Siamese batch levels, which is more serious than long-tailed\nclassification learning. In this paper, at the original batch level, we\nintroduce a class-balanced supervised contrastive loss to assign adaptive\nweights for different classes. At the Siamese batch level, we present a\nclass-balanced queue, which maintains the same number of keys for all classes.\nFurthermore, we note that the contrastive loss gradient with respect to the\ncontrastive logits can be decoupled into the positives and negatives, and easy\npositives and easy negatives will make the contrastive gradient vanish. We\npropose supervised hard positive and negative pairs mining to pick up\ninformative pairs for contrastive computation and improve representation\nlearning. Finally, to approximately maximize the mutual information between the\ntwo views, we propose Siamese Balanced Softmax and joint it with the\ncontrastive loss for one-stage training. ResCom outperforms the previous\nmethods by large margins on multiple long-tailed recognition benchmarks. Our\ncode will be made publicly available at:\nhttps://github.com/dvlab-research/ResCom.\n","authors":["Zhisheng Zhong","Jiequan Cui","Eric Lo","Zeming Li","Jian Sun","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2203.11506v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2203.10489v2","updated":"2022-03-22T07:28:02Z","published":"2022-03-20T08:29:06Z","title":"TVConv: Efficient Translation Variant Convolution for Layout-aware\n  Visual Processing","summary":"  As convolution has empowered many smart applications, dynamic convolution\nfurther equips it with the ability to adapt to diverse inputs. However, the\nstatic and dynamic convolutions are either layout-agnostic or\ncomputation-heavy, making it inappropriate for layout-specific applications,\ne.g., face recognition and medical image segmentation. We observe that these\napplications naturally exhibit the characteristics of large intra-image\n(spatial) variance and small cross-image variance. This observation motivates\nour efficient translation variant convolution (TVConv) for layout-aware visual\nprocessing. Technically, TVConv is composed of affinity maps and a\nweight-generating block. While affinity maps depict pixel-paired relationships\ngracefully, the weight-generating block can be explicitly overparameterized for\nbetter training while maintaining efficient inference. Although conceptually\nsimple, TVConv significantly improves the efficiency of the convolution and can\nbe readily plugged into various network architectures. Extensive experiments on\nface recognition show that TVConv reduces the computational cost by up to 3.1x\nand improves the corresponding throughput by 2.3x while maintaining a high\naccuracy compared to the depthwise convolution. Moreover, for the same\ncomputation cost, we boost the mean accuracy by up to 4.21%. We also conduct\nexperiments on the optic disc/cup segmentation task and obtain better\ngeneralization performance, which helps mitigate the critical data scarcity\nissue. Code is available at https://github.com/JierunChen/TVConv.\n","authors":["Jierun Chen","Tianlang He","Weipeng Zhuo","Li Ma","Sangtae Ha","S. -H. Gary Chan"],"pdf_url":"https://arxiv.org/pdf/2203.10489v2.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11496v1","updated":"2022-03-22T07:15:13Z","published":"2022-03-22T07:15:13Z","title":"TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with\n  Transformers","summary":"  LiDAR and camera are two important sensors for 3D object detection in\nautonomous driving. Despite the increasing popularity of sensor fusion in this\nfield, the robustness against inferior image conditions, e.g., bad illumination\nand sensor misalignment, is under-explored. Existing fusion methods are easily\naffected by such conditions, mainly due to a hard association of LiDAR points\nand image pixels, established by calibration matrices. We propose TransFusion,\na robust solution to LiDAR-camera fusion with a soft-association mechanism to\nhandle inferior image conditions. Specifically, our TransFusion consists of\nconvolutional backbones and a detection head based on a transformer decoder.\nThe first layer of the decoder predicts initial bounding boxes from a LiDAR\npoint cloud using a sparse set of object queries, and its second decoder layer\nadaptively fuses the object queries with useful image features, leveraging both\nspatial and contextual relationships. The attention mechanism of the\ntransformer enables our model to adaptively determine where and what\ninformation should be taken from the image, leading to a robust and effective\nfusion strategy. We additionally design an image-guided query initialization\nstrategy to deal with objects that are difficult to detect in point clouds.\nTransFusion achieves state-of-the-art performance on large-scale datasets. We\nprovide extensive experiments to demonstrate its robustness against degenerated\nimage quality and calibration errors. We also extend the proposed method to the\n3D tracking task and achieve the 1st place in the leaderboard of nuScenes\ntracking, showing its effectiveness and generalization capability.\n","authors":["Xuyang Bai","Zeyu Hu","Xinge Zhu","Qingqiu Huang","Yilun Chen","Hongbo Fu","Chiew-Lan Tai"],"pdf_url":"https://arxiv.org/pdf/2203.11496v1.pdf","comment":"Accepted to CVPR2022; Code at\n  \\url{https://github.com/XuyangBai/TransFusion}; Based on this work, we\n  achieve the 1st place in the leaderboard of nuScenes tracking"},{"id":"http://arxiv.org/abs/2203.11493v1","updated":"2022-03-22T07:05:57Z","published":"2022-03-22T07:05:57Z","title":"FrameHopper: Selective Processing of Video Frames in Detection-driven\n  Real-Time Video Analytics","summary":"  Detection-driven real-time video analytics require continuous detection of\nobjects contained in the video frames using deep learning models like YOLOV3,\nEfficientDet. However, running these detectors on each and every frame in\nresource-constrained edge devices is computationally intensive. By taking the\ntemporal correlation between consecutive video frames into account, we note\nthat detection outputs tend to be overlapping in successive frames. Elimination\nof similar consecutive frames will lead to a negligible drop in performance\nwhile offering significant performance benefits by reducing overall computation\nand communication costs. The key technical questions are, therefore, (a) how to\nidentify which frames to be processed by the object detector, and (b) how many\nsuccessive frames can be skipped (called skip-length) once a frame is selected\nto be processed. The overall goal of the process is to keep the error due to\nskipping frames as small as possible. We introduce a novel error vs processing\nrate optimization problem with respect to the object detection task that\nbalances between the error rate and the fraction of frames filtering.\nSubsequently, we propose an off-line Reinforcement Learning (RL)-based\nalgorithm to determine these skip-lengths as a state-action policy of the RL\nagent from a recorded video and then deploy the agent online for live video\nstreams. To this end, we develop FrameHopper, an edge-cloud collaborative video\nanalytics framework, that runs a lightweight trained RL agent on the camera and\npasses filtered frames to the server where the object detection model runs for\na set of applications. We have tested our approach on a number of live videos\ncaptured from real-life scenarios and show that FrameHopper processes only a\nhandful of frames but produces detection results closer to the oracle solution\nand outperforms recent state-of-the-art solutions in most cases.\n","authors":["Md Adnan Arefeen","Sumaiya Tabassum Nimi","Md Yusuf Sarwar Uddin"],"pdf_url":"https://arxiv.org/pdf/2203.11493v1.pdf","comment":"Accepted in The 18th International Conference on Distributed\n  Computing in Sensor Systems (DCOSS 2022)"},{"id":"http://arxiv.org/abs/2203.02916v2","updated":"2022-03-22T07:01:11Z","published":"2022-03-06T09:22:20Z","title":"PanFormer: a Transformer Based Model for Pan-sharpening","summary":"  Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)\nimage from a low-resolution (LR) multi-spectral (MS) image and its\ncorresponding panchromatic (PAN) image acquired by a same satellite. Inspired\nby a new fashion in recent deep learning community, we propose a novel\nTransformer based model for pan-sharpening. We explore the potential of\nTransformer in image feature extraction and fusion. Following the successful\ndevelopment of vision transformers, we design a two-stream network with the\nself-attention to extract the modality-specific features from the PAN and MS\nmodalities and apply a cross-attention module to merge the spectral and spatial\nfeatures. The pan-sharpened image is produced from the enhanced fused features.\nExtensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our\nTransformer based model achieves impressive results and outperforms many\nexisting CNN based methods, which shows the great potential of introducing\nTransformer to the pan-sharpening task. Codes are available at\nhttps://github.com/zhysora/PanFormer.\n","authors":["Huanyu Zhou","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2203.02916v2.pdf","comment":"Accepted by ICME 2022"},{"id":"http://arxiv.org/abs/2203.11490v1","updated":"2022-03-22T06:54:29Z","published":"2022-03-22T06:54:29Z","title":"SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for\n  Lightweight Skin Lesion Classification Using Dermoscopic Images","summary":"  Skin cancer is one of the most common types of malignancy, affecting a large\npopulation and causing a heavy economic burden worldwide. Over the last few\nyears, computer-aided diagnosis has been rapidly developed and make great\nprogress in healthcare and medical practices due to the advances in artificial\nintelligence. However, most studies in skin cancer detection keep pursuing high\nprediction accuracies without considering the limitation of computing resources\non portable devices. In this case, knowledge distillation (KD) has been proven\nas an efficient tool to help improve the adaptability of lightweight models\nunder limited resources, meanwhile keeping a high-level representation\ncapability. To bridge the gap, this study specifically proposes a novel method,\ntermed SSD-KD, that unifies diverse knowledge into a generic KD framework for\nskin diseases classification. Our method models an intra-instance relational\nfeature representation and integrates it with existing KD research. A dual\nrelational knowledge distillation architecture is self-supervisedly trained\nwhile the weighted softened outputs are also exploited to enable the student\nmodel to capture richer knowledge from the teacher model. To demonstrate the\neffectiveness of our method, we conduct experiments on ISIC 2019, a large-scale\nopen-accessed benchmark of skin diseases dermoscopic images. Experiments show\nthat our distilled lightweight model can achieve an accuracy as high as 85% for\nthe classification tasks of 8 different skin diseases with minimal parameters\nand computing requirements. Ablation studies confirm the effectiveness of our\nintra- and inter-instance relational knowledge integration strategy. Compared\nwith state-of-the-art knowledge distillation techniques, the proposed method\ndemonstrates improved performances for multi-diseases classification on the\nlarge-scale dermoscopy database.\n","authors":["Yongwei Wang","Yuheng Wang","Tim K. Lee","Chunyan Miao","Z. Jane Wang"],"pdf_url":"https://arxiv.org/pdf/2203.11490v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2108.07917v5","updated":"2022-03-22T06:30:40Z","published":"2021-08-18T01:00:02Z","title":"Classification of Abnormal Hand Movement for Aiding in Autism Detection:\n  Machine Learning Study","summary":"  A formal autism diagnosis can be an inefficient and lengthy process. Families\nmay wait months or longer before receiving a diagnosis for their child despite\nevidence that earlier intervention leads to better treatment outcomes. Digital\ntechnologies which detect the presence of behaviors related to autism can scale\naccess to pediatric diagnoses. This work aims to demonstrate the feasibility of\ndeep learning technologies for detecting hand flapping from unstructured home\nvideos as a first step towards validating whether models and digital\ntechnologies can be leveraged to aid with autism diagnoses. We used the\nSelf-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand\nflapping, head banging, and spinning exhibited by children. From all the hand\nflapping videos, we extracted 100 positive and control videos of hand flapping,\neach between 2 to 5 seconds in duration. Utilizing both\nlandmark-driven-approaches and MobileNet V2's pretrained convolutional layers,\nour highest performing model achieved a testing F1 score of 84% (90% precision\nand 80% recall) when evaluating with 5-fold cross validation 100 times. This\nwork provides the first step towards developing precise deep learning methods\nfor activity detection of autism-related behaviors.\n","authors":["Anish Lakkapragada","Aaron Kline","Onur Cezmi Mutlu","Kelley Paskov","Brianna Chrisman","Nate Stockham","Peter Washington","Dennis Wall"],"pdf_url":"https://arxiv.org/pdf/2108.07917v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11483v1","updated":"2022-03-22T06:20:25Z","published":"2022-03-22T06:20:25Z","title":"Practical Stereo Matching via Cascaded Recurrent Network with Adaptive\n  Correlation","summary":"  With the advent of convolutional neural networks, stereo matching algorithms\nhave recently gained tremendous progress. However, it remains a great challenge\nto accurately extract disparities from real-world image pairs taken by\nconsumer-level devices like smartphones, due to practical complicating factors\nsuch as thin structures, non-ideal rectification, camera module inconsistencies\nand various hard-case scenes. In this paper, we propose a set of innovative\ndesigns to tackle the problem of practical stereo matching: 1) to better\nrecover fine depth details, we design a hierarchical network with recurrent\nrefinement to update disparities in a coarse-to-fine manner, as well as a\nstacked cascaded architecture for inference; 2) we propose an adaptive group\ncorrelation layer to mitigate the impact of erroneous rectification; 3) we\nintroduce a new synthetic dataset with special attention to difficult cases for\nbetter generalizing to real-world scenes. Our results not only rank 1st on both\nMiddlebury and ETH3D benchmarks, outperforming existing state-of-the-art\nmethods by a notable margin, but also exhibit high-quality details for\nreal-life photos, which clearly demonstrates the efficacy of our contributions.\n","authors":["Jiankun Li","Peisen Wang","Pengfei Xiong","Tao Cai","Ziwei Yan","Lei Yang","Jiangyu Liu","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11483v1.pdf","comment":"This work has been accepted to CVPR2022. The project link is\n  https://github.com/megvii-research/CREStereo"},{"id":"http://arxiv.org/abs/2203.11481v1","updated":"2022-03-22T06:15:43Z","published":"2022-03-22T06:15:43Z","title":"Mixed Differential Privacy in Computer Vision","summary":"  We introduce AdaMix, an adaptive differentially private algorithm for\ntraining deep neural network classifiers using both private and public image\ndata. While pre-training language models on large public datasets has enabled\nstrong differential privacy (DP) guarantees with minor loss of accuracy, a\nsimilar practice yields punishing trade-offs in vision tasks. A few-shot or\neven zero-shot learning baseline that ignores private data can outperform\nfine-tuning on a large private dataset. AdaMix incorporates few-shot training,\nor cross-modal zero-shot learning, on public data prior to private fine-tuning,\nto improve the trade-off. AdaMix reduces the error increase from the\nnon-private upper bound from the 167-311\\% of the baseline, on average across 6\ndatasets, to 68-92\\% depending on the desired privacy level selected by the\nuser. AdaMix tackles the trade-off arising in visual classification, whereby\nthe most privacy sensitive data, corresponding to isolated points in\nrepresentation space, are also critical for high classification accuracy. In\naddition, AdaMix comes with strong theoretical privacy guarantees and\nconvergence analysis.\n","authors":["Aditya Golatkar","Alessandro Achille","Yu-Xiang Wang","Aaron Roth","Michael Kearns","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2203.11481v1.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11480v1","updated":"2022-03-22T06:12:20Z","published":"2022-03-22T06:12:20Z","title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models","summary":"  Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n","authors":["Sha Yuan","Zhao Shuai","Leng Jiahong","Xue Zhao","Zhao Hanyu","Tang Jie"],"pdf_url":"https://arxiv.org/pdf/2203.11480v1.pdf","comment":"7 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2203.11474v1","updated":"2022-03-22T05:59:33Z","published":"2022-03-22T05:59:33Z","title":"Remember Intentions: Retrospective-Memory-based Trajectory Prediction","summary":"  To realize trajectory prediction, most previous methods adopt the\nparameter-based approach, which encodes all the seen past-future instance pairs\ninto model parameters. However, in this way, the model parameters come from all\nseen instances, which means a huge amount of irrelevant seen instances might\nalso involve in predicting the current situation, disturbing the performance.\nTo provide a more explicit link between the current situation and the seen\ninstances, we imitate the mechanism of retrospective memory in neuropsychology\nand propose MemoNet, an instance-based approach that predicts the movement\nintentions of agents by looking for similar scenarios in the training data. In\nMemoNet, we design a pair of memory banks to explicitly store representative\ninstances in the training set, acting as prefrontal cortex in the neural\nsystem, and a trainable memory addresser to adaptively search a current\nsituation with similar instances in the memory bank, acting like basal ganglia.\nDuring prediction, MemoNet recalls previous memory by using the memory\naddresser to index related instances in the memory bank. We further propose a\ntwo-step trajectory prediction system, where the first step is to leverage\nMemoNet to predict the destination and the second step is to fulfill the whole\ntrajectory according to the predicted destinations. Experiments show that the\nproposed MemoNet improves the FDE by 20.3%/10.2%/28.3% from the previous best\nmethod on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has\nthe ability to trace back to specific instances during prediction, promoting\nmore interpretability.\n","authors":["Chenxin Xu","Weibo Mao","Wenjun Zhang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2203.11474v1.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.11471v1","updated":"2022-03-22T05:42:31Z","published":"2022-03-22T05:42:31Z","title":"Ray3D: ray-based 3D human pose estimation for monocular absolute 3D\n  localization","summary":"  In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute\nhuman pose estimation with calibrated camera. Accurate and generalizable\nabsolute 3D human pose estimation from monocular 2D pose input is an ill-posed\nproblem. To address this challenge, we convert the input from pixel space to 3D\nnormalized rays. This conversion makes our approach robust to camera intrinsic\nparameter changes. To deal with the in-the-wild camera extrinsic parameter\nvariations, Ray3D explicitly takes the camera extrinsic parameters as an input\nand jointly models the distribution between the 3D pose rays and camera\nextrinsic parameters. This novel network design is the key to the outstanding\ngeneralizability of Ray3D approach. To have a comprehensive understanding of\nhow the camera intrinsic and extrinsic parameter variations affect the accuracy\nof absolute 3D key-point localization, we conduct in-depth systematic\nexperiments on three single person 3D benchmarks as well as one synthetic\nbenchmark. These experiments demonstrate that our method significantly\noutperforms existing state-of-the-art models. Our code and the synthetic\ndataset are available at https://github.com/YxZhxn/Ray3D .\n","authors":["Yu Zhan","Fenghai Li","Renliang Weng","Wongun Choi"],"pdf_url":"https://arxiv.org/pdf/2203.11471v1.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2111.10502v3","updated":"2022-03-22T05:27:57Z","published":"2021-11-20T02:58:38Z","title":"CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and\n  Scene Flow Estimation","summary":"  In this paper, we study the problem of jointly estimating the optical flow\nand scene flow from synchronized 2D and 3D data. Previous methods either employ\na complex pipeline that splits the joint task into independent stages, or fuse\n2D and 3D information in an \"early-fusion\" or \"late-fusion\" manner. Such\none-size-fits-all approaches suffer from a dilemma of failing to fully utilize\nthe characteristic of each modality or to maximize the inter-modality\ncomplementarity. To address the problem, we propose a novel end-to-end\nframework, called CamLiFlow. It consists of 2D and 3D branches with multiple\nbidirectional connections between them in specific layers. Different from\nprevious work, we apply a point-based 3D branch to better extract the geometric\nfeatures and design a symmetric learnable operator to fuse dense image features\nand sparse point features. Experiments show that CamLiFlow achieves better\nperformance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow\nbenchmark, outperforming the previous art with 1/7 parameters. Code is\navailable at https://github.com/MCG-NJU/CamLiFlow.\n","authors":["Haisong Liu","Tao Lu","Yihui Xu","Jia Liu","Wenjie Li","Lijun Chen"],"pdf_url":"https://arxiv.org/pdf/2111.10502v3.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2111.11133v9","updated":"2022-03-22T05:02:36Z","published":"2021-11-22T11:48:26Z","title":"L-Verse: Bidirectional Generation Between Image and Text","summary":"  Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n","authors":["Taehoon Kim","Gwangmo Song","Sihaeng Lee","Sangyun Kim","Yewon Seo","Soonyoung Lee","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae"],"pdf_url":"https://arxiv.org/pdf/2111.11133v9.pdf","comment":"Accepted to CVPR 2022 (Oral)"},{"id":"http://arxiv.org/abs/2007.04793v2","updated":"2022-03-22T04:22:52Z","published":"2020-07-08T00:46:43Z","title":"Statistical Shape Analysis of Brain Arterial Networks (BAN)","summary":"  Structures of brain arterial networks (BANs) - that are complex arrangements\nof individual arteries, their branching patterns, and inter-connectivities -\nplay an important role in characterizing and understanding brain physiology.\nOne would like tools for statistically analyzing the shapes of BANs, i.e.\nquantify shape differences, compare population of subjects, and study the\neffects of covariates on these shapes. This paper mathematically represents and\nstatistically analyzes BAN shapes as elastic shape graphs. Each elastic shape\ngraph is made up of nodes that are connected by a number of 3D curves, and\nedges, with arbitrary shapes. We develop a mathematical representation, a\nRiemannian metric and other geometrical tools, such as computations of\ngeodesics, means and covariances, and PCA for analyzing elastic graphs and\nBANs. This analysis is applied to BANs after separating them into four\ncomponents -- top, bottom, left, and right. This framework is then used to\ngenerate shape summaries of BANs from 92 subjects, and to study the effects of\nage and gender on shapes of BAN components. We conclude that while gender\neffects require further investigation, the age has a clear, quantifiable effect\non BAN shapes. Specifically, we find an increased variance in BAN shapes as age\nincreases.\n","authors":["Xiaoyang Guo","Aditi Basu Bal","Tom Needham","Anuj Srivastava"],"pdf_url":"https://arxiv.org/pdf/2007.04793v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2003.00287"},{"id":"http://arxiv.org/abs/2203.11453v1","updated":"2022-03-22T04:18:45Z","published":"2022-03-22T04:18:45Z","title":"DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic\n  Layouts","summary":"  Limited by the computational efficiency and accuracy, generating complex 3D\nscenes remains a challenging problem for existing generation networks. In this\nwork, we propose DepthGAN, a novel method of generating depth maps with only\nsemantic layouts as input. First, we introduce a well-designed cascade of\ntransformer blocks as our generator to capture the structural correlations in\ndepth maps, which makes a balance between global feature aggregation and local\nattention. Meanwhile, we propose a cross-attention fusion module to guide edge\npreservation efficiently in depth generation, which exploits additional\nappearance supervision information. Finally, we conduct extensive experiments\non the perspective views of the Structured3d panorama dataset and demonstrate\nthat our DepthGAN achieves superior performance both on quantitative results\nand visual effects in the depth generation task.Furthermore, 3D indoor scenes\ncan be reconstructed by our generated depth maps with reasonable structure and\nspatial coherency.\n","authors":["Yidi Li","Yiqun Wang","Zhengda Lu","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2203.11453v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.10739v2","updated":"2022-03-22T04:17:36Z","published":"2022-03-21T05:16:23Z","title":"Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation","summary":"  Sparsely annotated semantic segmentation (SASS) aims to train a segmentation\nnetwork with coarse-grained (i.e., point-, scribble-, and block-wise)\nsupervisions, where only a small proportion of pixels are labeled in each\nimage. In this paper, we propose a novel tree energy loss for SASS by providing\nsemantic guidance for unlabeled pixels. The tree energy loss represents images\nas minimum spanning trees to model both low-level and high-level pair-wise\naffinities. By sequentially applying these affinities to the network\nprediction, soft pseudo labels for unlabeled pixels are generated in a\ncoarse-to-fine manner, achieving dynamic online self-training. The tree energy\nloss is effective and easy to be incorporated into existing frameworks by\ncombining it with a traditional segmentation loss. Compared with previous SASS\nmethods, our method requires no multistage training strategies, alternating\noptimization procedures, additional supervised data, or time-consuming\npost-processing while outperforming them in all SASS settings. Code is\navailable at https://github.com/megvii-research/TreeEnergyLoss.\n","authors":["Zhiyuan Liang","Tiancai Wang","Xiangyu Zhang","Jian Sun","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2203.10739v2.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.11449v1","updated":"2022-03-22T04:07:20Z","published":"2022-03-22T04:07:20Z","title":"Leveraging Textures in Zero-shot Understanding of Fine-Grained Domains","summary":"  Textures can be used to describe the appearance of objects in a wide range of\nfine-grained domains. Textures are localized and one can often refer to their\nproperties in a manner that is independent of the object identity. Moreover,\nthere is a rich vocabulary to describe textures corresponding to properties\nsuch as their color, pattern, structure, periodicity, stochasticity, and\nothers. Motivated by this, we study the effectiveness of large-scale language\nand vision models (e.g., CLIP) at recognizing texture attributes in natural\nimages. We first conduct a systematic study of CLIP on texture datasets where\nwe find that it has good coverage for a wide range of texture terms. CLIP can\nalso handle compositional phrases that consist of color and pattern terms\n(e.g., red dots or yellow stripes). We then show how these attributes allow for\nzero-shot fine-grained categorization on existing datasets.\n","authors":["Chenyun Wu","Subhransu Maji"],"pdf_url":"https://arxiv.org/pdf/2203.11449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11447v1","updated":"2022-03-22T03:57:02Z","published":"2022-03-22T03:57:02Z","title":"Manipulating UAV Imagery for Satellite Model Training, Calibration and\n  Testing","summary":"  Modern livestock farming is increasingly data driven and frequently relies on\nefficient remote sensing to gather data over wide areas. High resolution\nsatellite imagery is one such data source, which is becoming more accessible\nfor farmers as coverage increases and cost falls. Such images can be used to\ndetect and track animals, monitor pasture changes, and understand land use.\nMany of the data driven models being applied to these tasks require ground\ntruthing at resolutions higher than satellites can provide. Simultaneously,\nthere is a lack of available aerial imagery focused on farmland changes that\noccur over days or weeks, such as herd movement. With this goal in mind, we\npresent a new multi-temporal dataset of high resolution UAV imagery which is\nartificially degraded to match satellite data quality. An empirical blurring\nmetric is used to calibrate the degradation process against actual satellite\nimagery of the area. UAV surveys were flown repeatedly over several weeks, for\nspecific farm locations. This 5cm/pixel data is sufficiently high resolution to\naccurately ground truth cattle locations, and other factors such as grass\ncover. From 33 wide area UAV surveys, 1869 patches were extracted and\nartificially degraded using an accurate satellite optical model to simulate\nsatellite data. Geographic patches from multiple time periods are aligned and\npresented as sets, providing a multi-temporal dataset that can be used for\ndetecting changes on farms. The geo-referenced images and 27,853 manually\nannotated cattle labels are made publicly available.\n","authors":["Jasper Brown","Cameron Clark","Sabrina Lomax","Khalid Rafique","Salah Sukkarieh"],"pdf_url":"https://arxiv.org/pdf/2203.11447v1.pdf","comment":"16 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2203.11442v1","updated":"2022-03-22T03:33:27Z","published":"2022-03-22T03:33:27Z","title":"Associating Objects with Scalable Transformers for Video Object\n  Segmentation","summary":"  This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computation resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\njointly and collaboratively. In detail, AOT employs an identification mechanism\nto associate multiple targets into the same high-dimensional embedding space.\nThus, we can simultaneously process multiple objects' matching and segmentation\ndecoding as efficiently as processing a single object. To sufficiently model\nmulti-object association, a Long Short-Term Transformer (LSTT) is devised to\nconstruct hierarchical matching and propagation. Based on AOT, we further\npropose a more flexible and robust framework, Associating Objects with Scalable\nTransformers (AOST), in which a scalable version of LSTT is designed to enable\nrun-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces\na better layer-wise manner to couple identification and vision embeddings. We\nconduct extensive experiments on multi-object and single-object benchmarks to\nexamine AOT series frameworks. Compared to the state-of-the-art competitors,\nour methods can maintain times of run-time efficiency with superior\nperformance. Notably, we achieve new state-of-the-art performance on three\npopular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test\n(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:\nhttps://github.com/z-x-yang/AOT.\n","authors":["Zongxin Yang","Jiaxu Miao","Xiaohan Wang","Yunchao Wei","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2203.11442v1.pdf","comment":"Extension of arXiv:2106.02638 (NeurIPS 2021)"},{"id":"http://arxiv.org/abs/2203.11441v1","updated":"2022-03-22T03:31:29Z","published":"2022-03-22T03:31:29Z","title":"Multi-Modal Learning for AU Detection Based on Multi-Head Fused\n  Transformers","summary":"  Multi-modal learning has been intensified in recent years, especially for\napplications in facial analysis and action unit detection whilst there still\nexist two main challenges in terms of 1) relevant feature learning for\nrepresentation and 2) efficient fusion for multi-modalities. Recently, there\nare a number of works have shown the effectiveness in utilizing the attention\nmechanism for AU detection, however, most of them are binding the region of\ninterest (ROI) with features but rarely apply attention between features of\neach AU. On the other hand, the transformer, which utilizes a more efficient\nself-attention mechanism, has been widely used in natural language processing\nand computer vision tasks but is not fully explored in AU detection tasks. In\nthis paper, we propose a novel end-to-end Multi-Head Fused Transformer (MFT)\nmethod for AU detection, which learns AU encoding features representation from\ndifferent modalities by transformer encoder and fuses modalities by another\nfusion transformer module. Multi-head fusion attention is designed in the\nfusion transformer module for the effective fusion of multiple modalities. Our\napproach is evaluated on two public multi-modal AU databases, BP4D, and BP4D+,\nand the results are superior to the state-of-the-art algorithms and baseline\nmodels. We further analyze the performance of AU detection from different\nmodalities.\n","authors":["Xiang Zhang","Lijun Yin"],"pdf_url":"https://arxiv.org/pdf/2203.11441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11437v1","updated":"2022-03-22T03:17:15Z","published":"2022-03-22T03:17:15Z","title":"Self-Supervised Representation Learning as Multimodal Variational\n  Inference","summary":"  This paper proposes a probabilistic extension of SimSiam, a recent\nself-supervised learning (SSL) method. SimSiam trains a model by maximizing the\nsimilarity between image representations of different augmented views of the\nsame image. Although uncertainty-aware machine learning has been getting\ngeneral like deep variational inference, SimSiam and other SSL are\ninsufficiently uncertainty-aware, which could lead to limitations on its\npotential. The proposed extension is to make SimSiam uncertainty-aware based on\nvariational inference. Our main contributions are twofold: Firstly, we clarify\nthe theoretical relationship between non-contrastive SSL and multimodal\nvariational inference. Secondly, we introduce a novel SSL called variational\ninference SimSiam (VI-SimSiam), which incorporates the uncertainty by involving\nspherical posterior distributions. Our experiment shows that VI-SimSiam\noutperforms SimSiam in classification tasks in ImageNette and ImageWoof by\nsuccessfully estimating the representation uncertainty.\n","authors":["Hiroki Nakamura","Masashi Okada","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2203.11437v1.pdf","comment":"4 pages, 4 figures, work in progress"},{"id":"http://arxiv.org/abs/2203.11433v1","updated":"2022-03-22T03:13:33Z","published":"2022-03-22T03:13:33Z","title":"Making DeepFakes more spurious: evading deep face forgery detection via\n  trace removal attack","summary":"  DeepFakes are raising significant social concerns. Although various DeepFake\ndetectors have been developed as forensic countermeasures, these detectors are\nstill vulnerable to attacks. Recently, a few attacks, principally adversarial\nattacks, have succeeded in cloaking DeepFake images to evade detection.\nHowever, these attacks have typical detector-specific designs, which require\nprior knowledge about the detector, leading to poor transferability. Moreover,\nthese attacks only consider simple security scenarios. Less is known about how\neffective they are in high-level scenarios where either the detectors or the\nattacker's knowledge varies. In this paper, we solve the above challenges with\npresenting a novel detector-agnostic trace removal attack for DeepFake\nanti-forensics. Instead of investigating the detector side, our attack looks\ninto the original DeepFake creation pipeline, attempting to remove all\ndetectable natural DeepFake traces to render the fake images more \"authentic\".\nTo implement this attack, first, we perform a DeepFake trace discovery,\nidentifying three discernible traces. Then a trace removal network (TR-Net) is\nproposed based on an adversarial learning framework involving one generator and\nmultiple discriminators. Each discriminator is responsible for one individual\ntrace representation to avoid cross-trace interference. These discriminators\nare arranged in parallel, which prompts the generator to remove various traces\nsimultaneously. To evaluate the attack efficacy, we crafted heterogeneous\nsecurity scenarios where the detectors were embedded with different levels of\ndefense and the attackers' background knowledge of data varies. The\nexperimental results show that the proposed attack can significantly compromise\nthe detection accuracy of six state-of-the-art DeepFake detectors while causing\nonly a negligible loss in visual quality to the original DeepFake samples.\n","authors":["Chi Liu","Huajie Chen","Tianqing Zhu","Jun Zhang","Wanlei Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.11433v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2109.09587v2","updated":"2022-03-22T15:43:52Z","published":"2021-09-20T14:42:39Z","title":"Recommender systems based on graph embedding techniques: A comprehensive\n  review","summary":"  As a pivotal tool to alleviate the information overload problem, recommender\nsystems aim to predict user's preferred items from millions of candidates by\nanalyzing observed user-item relations. As for alleviating the sparsity and\ncold start problems encountered by recommender systems, researchers resort to\nemploying side information or knowledge in recommendation as a strategy for\nuncovering hidden (indirect) user-item relations, aiming to enrich observed\ninformation (or data) for recommendation. However, in the face of the high\ncomplexity and large scale of side information and knowledge, this strategy\nrelies for efficient implementation on the scalability of recommendation\nmodels. Not until after the prevalence of machine learning did graph embedding\ntechniques be a concentration, which can efficiently utilize complex and\nlarge-scale data. In light of that, equipping recommender systems with graph\nembedding techniques has been widely studied these years, appearing to\noutperform conventional recommendation implemented directly based on graph\ntopological analysis. As the focus, this article retrospects graph\nembedding-based recommendation from embedding techniques for bipartite graphs,\ngeneral graphs and knowledge graphs, and proposes a general design pipeline of\nthat. In addition, after comparing several representative graph embedding-based\nrecommendation models with the most common-used conventional recommendation\nmodels on simulations, this article manifests that the conventional models can\noverall outperform the graph embedding-based ones in predicting implicit\nuser-item interactions, revealing the comparative weakness of graph\nembedding-based recommendation in these tasks. To foster future research, this\narticle proposes suggestions on making a trade-off between graph\nembedding-based recommendation and conventional recommendation in different\ntasks, and puts forward open questions.\n","authors":["Yue Deng"],"pdf_url":"https://arxiv.org/pdf/2109.09587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12524v2","updated":"2022-03-22T09:32:01Z","published":"2022-02-25T06:58:28Z","title":"MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation","summary":"  Large-scale e-commercial platforms in the real-world usually contain various\nrecommendation scenarios (domains) to meet demands of diverse customer groups.\nMulti-Domain Recommendation (MDR), which aims to jointly improve\nrecommendations on all domains, has attracted increasing attention from\npractitioners and researchers. Existing MDR methods often employ a shared\nstructure to leverage reusable features for all domains and several specific\nparts to capture domain-specific information. However, data from different\ndomains may conflict with each other and cause shared parameters to stay at a\ncompromised position on the optimization landscape. This could deteriorate the\noverall performance. Despite the specific parameters are separately learned for\neach domain, they can easily overfit on data sparsity domains. Furthermore,\ndata distribution differs across domains, making it challenging to develop a\ngeneral model that can be applied to all circumstances. To address these\nproblems, we propose a novel model agnostic learning method, namely MAMDR, for\nthe multi-domain recommendation. Specifically, we first propose a Domain\nNegotiation (DN) strategy to alleviate the conflict between domains and learn\nbetter shared parameters. Then, we develop a Domain Regularization (DR) scheme\nto improve the generalization ability of specific parameters by learning from\nother domains. Finally, we integrate these components into a unified framework\nand present MAMDR which can be applied to any model structure to perform\nmulti-domain recommendation. Extensive experiments on various real-world\ndatasets and online applications demonstrate both the effectiveness and\ngeneralizability of MAMDR.\n","authors":["Linhao Luo","Yumeng Li","Buyu Gao","Shuai Tang","Sinan Wang","Jiancheng Li","Tanchao Zhu","Jiancai Liu","Zhao Li","Binqiang Zhao","Ziyang Zheng","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2202.12524v2.pdf","comment":"This paper has been submitted to KDD 2022 ADS Track"},{"id":"http://arxiv.org/abs/2110.14747v2","updated":"2022-03-22T09:14:27Z","published":"2021-10-27T20:17:47Z","title":"Dynamic Review-based Recommenders","summary":"  Just as user preferences change with time, item reviews also reflect those\nsame preference changes. In a nutshell, if one is to sequentially incorporate\nreview content knowledge into recommender systems, one is naturally led to\ndynamical models of text. In the present work we leverage the known power of\nreviews to enhance rating predictions in a way that (i) respects the causality\nof review generation and (ii) includes, in a bidirectional fashion, the ability\nof ratings to inform language review models and vice-versa, language\nrepresentations that help predict ratings end-to-end. Moreover, our\nrepresentations are time-interval aware and thus yield a continuous-time\nrepresentation of the dynamics. We provide experiments on real-world datasets\nand show that our methodology is able to outperform several state-of-the-art\nmodels. Source code for all models can be found at [1].\n","authors":["Kostadin Cvejoski","Ramses J. Sanchez","Christian Bauckhage","Cesar Ojeda"],"pdf_url":"https://arxiv.org/pdf/2110.14747v2.pdf","comment":"6pages, Published at International Data Science Conference 2021\n  (iDSC21)"},{"id":"http://arxiv.org/abs/2203.11491v1","updated":"2022-03-22T06:56:06Z","published":"2022-03-22T06:56:06Z","title":"Making Recommender Systems Forget: Learning and Unlearning for Erasable\n  Recommendation","summary":"  Privacy laws and regulations enforce data-driven systems, e.g., recommender\nsystems, to erase the data that concern individuals. As machine learning models\npotentially memorize the training data, data erasure should also unlearn the\ndata lineage in models, which raises increasing interest in the problem of\nMachine Unlearning (MU). However, existing MU methods cannot be directly\napplied into recommendation. The basic idea of most recommender systems is\ncollaborative filtering, but existing MU methods ignore the collaborative\ninformation across users and items. In this paper, we propose a general\nerasable recommendation framework, namely LASER, which consists of Group module\nand SeqTrain module. Firstly, Group module partitions users into balanced\ngroups based on their similarity of collaborative embedding learned via\nhypergraph. Then SeqTrain module trains the model sequentially on all groups\nwith curriculum learning. Both theoretical analysis and experiments on two\nreal-world datasets demonstrate that LASER can not only achieve efficient\nunlearning, but also outperform the state-of-the-art unlearning framework in\nterms of model utility.\n","authors":["Yuyuan Li","Xiaolin Zheng","Chaochao Chen","Junlin Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08307v2","updated":"2022-03-22T21:40:55Z","published":"2022-03-15T22:51:22Z","title":"Improving Word Translation via Two-Stage Contrastive Learning","summary":"  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n","authors":["Yaoyiran Li","Fangyu Liu","Nigel Collier","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2203.08307v2.pdf","comment":"ACL 2022 Main"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2203.11933v1","updated":"2022-03-22T17:59:04Z","published":"2022-03-22T17:59:04Z","title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models\n  with Adversarial Learning","summary":"  Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n","authors":["Hugo Berg","Siobhan Mackenzie Hall","Yash Bhalgat","Wonsuk Yang","Hannah Rose Kirk","Aleksandar Shtedritski","Max Bain"],"pdf_url":"https://arxiv.org/pdf/2203.11933v1.pdf","comment":"24 pages, 10 figures. For code and trained token embeddings, see\n  https://github.com/oxai/debias-vision-lang"},{"id":"http://arxiv.org/abs/2203.11934v1","updated":"2022-03-22T17:59:04Z","published":"2022-03-22T17:59:04Z","title":"Learning from All Vehicles","summary":"  In this paper, we present a system to train driving policies from experiences\ncollected not just from the ego-vehicle, but all vehicles that it observes.\nThis system uses the behaviors of other agents to create more diverse driving\nscenarios without collecting additional data. The main difficulty in learning\nfrom other vehicles is that there is no sensor information. We use a set of\nsupervisory tasks to learn an intermediate representation that is invariant to\nthe viewpoint of the controlling vehicle. This not only provides a richer\nsignal at training time but also allows more complex reasoning during\ninference. Learning how all vehicles drive helps predict their behavior at test\ntime and can avoid collisions. We evaluate this system in closed-loop driving\nsimulations. Our system outperforms all prior methods on the public CARLA\nLeaderboard by a wide margin, improving driving score by 25 and route\ncompletion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving\nchallenge. Demo videos are available at https://dotchen.github.io/LAV/.\n","authors":["Dian Chen","Philipp Krähenbühl"],"pdf_url":"https://arxiv.org/pdf/2203.11934v1.pdf","comment":"Paper accepted to CVPR 2022; Code and data available at\n  https://github.com/dotchen/LAV"},{"id":"http://arxiv.org/abs/2203.11932v1","updated":"2022-03-22T17:58:59Z","published":"2022-03-22T17:58:59Z","title":"Dataset Distillation by Matching Training Trajectories","summary":"  Dataset distillation is the task of synthesizing a small dataset such that a\nmodel trained on the synthetic set will match the test accuracy of the model\ntrained on the full dataset. In this paper, we propose a new formulation that\noptimizes our distilled data to guide networks to a similar state as those\ntrained on real data across many training steps. Given a network, we train it\nfor several iterations on our distilled data and optimize the distilled data\nwith respect to the distance between the synthetically trained parameters and\nthe parameters trained on real data. To efficiently obtain the initial and\ntarget network parameters for large-scale datasets, we pre-compute and store\ntraining trajectories of expert networks trained on the real dataset. Our\nmethod handily outperforms existing methods and also allows us to distill\nhigher-resolution visual data.\n","authors":["George Cazenavette","Tongzhou Wang","Antonio Torralba","Alexei A. Efros","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.11932v1.pdf","comment":"CVPR 2022 website:\n  https://georgecazenavette.github.io/mtt-distillation/ code:\n  https://github.com/GeorgeCazenavette/mtt-distillation"},{"id":"http://arxiv.org/abs/2203.11931v1","updated":"2022-03-22T17:58:31Z","published":"2022-03-22T17:58:31Z","title":"MetaMorph: Learning Universal Controllers with Transformers","summary":"  Multiple domains like vision, natural language, and audio are witnessing\ntremendous progress by leveraging Transformers for large scale pre-training\nfollowed by task specific fine tuning. In contrast, in robotics we primarily\ntrain a single robot for a single task. However, modular robot systems now\nallow for the flexible combination of general-purpose building blocks into task\noptimized morphologies. However, given the exponentially large number of\npossible robot morphologies, training a controller for each new design is\nimpractical. In this work, we propose MetaMorph, a Transformer based approach\nto learn a universal controller over a modular robot design space. MetaMorph is\nbased on the insight that robot morphology is just another modality on which we\ncan condition the output of a Transformer. Through extensive experiments we\ndemonstrate that large scale pre-training on a variety of robot morphologies\nresults in policies with combinatorial generalization capabilities, including\nzero shot generalization to unseen robot morphologies. We further demonstrate\nthat our pre-trained policy can be used for sample-efficient transfer to\ncompletely new robot morphologies and tasks.\n","authors":["Agrim Gupta","Linxi Fan","Surya Ganguli","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2203.11931v1.pdf","comment":"ICLR 2022"},{"id":"http://arxiv.org/abs/2203.11926v1","updated":"2022-03-22T17:54:50Z","published":"2022-03-22T17:54:50Z","title":"Focal Modulation Networks","summary":"  In this work, we propose focal modulation network (FocalNet in short), where\nself-attention (SA) is completely replaced by a focal modulation module that is\nmore effective and efficient for modeling token interactions. Focal modulation\ncomprises three components: $(i)$ hierarchical contextualization, implemented\nusing a stack of depth-wise convolutional layers, to encode visual contexts\nfrom short to long ranges at different granularity levels, $(ii)$ gated\naggregation to selectively aggregate context features for each visual token\n(query) based on its content, and $(iii)$ modulation or element-wise affine\ntransformation to fuse the aggregated features into the query vector. Extensive\nexperiments show that FocalNets outperform the state-of-the-art SA counterparts\n(e.g., Swin Transformers) with similar time and memory cost on the tasks of\nimage classification, object detection, and semantic segmentation.\nSpecifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9%\ntop-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains\n86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\\times$224\nand 384$\\times$384, respectively. FocalNets exhibit remarkable superiority when\ntransferred to downstream tasks. For object detection with Mask R-CNN, our\nFocalNet base trained with 1$\\times$ already surpasses Swin trained with\n3$\\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet,\nFocalNet base evaluated at single-scale outperforms Swin evaluated at\nmulti-scale (50.5 v.s. 49.7). These results render focal modulation a favorable\nalternative to SA for effective and efficient visual modeling in real-world\napplications. Code is available at https://github.com/microsoft/FocalNet.\n","authors":["Jianwei Yang","Chunyuan Li","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2203.11926v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2109.02748v3","updated":"2022-03-22T17:53:38Z","published":"2021-09-06T21:27:43Z","title":"Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model\n  CLIP","summary":"  In an out-of-distribution (OOD) detection problem, samples of known\nclasses(also called in-distribution classes) are used to train a special\nclassifier. In testing, the classifier can (1) classify the test samples of\nknown classes to their respective classes and also (2) detect samples that do\nnot belong to any of the known classes (i.e., they belong to some unknown or\nOOD classes). This paper studies the problem of zero-shot\nout-of-distribution(OOD) detection, which still performs the same two tasks in\ntesting but has no training except using the given known class names. This\npaper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC\nbuilds on top of the recent advances in zero-shot classification through\nmulti-modal representation learning. It first extends the pre-trained\nlanguage-vision model CLIP by training a text-based image description generator\non top of CLIP. In testing, it uses the extended model to generate candidate\nunknown class names for each test sample and computes a confidence score based\non both the known class names and candidate unknown class names for zero-shot\nOOD detection. Experimental results on 5 benchmark datasets for OOD detection\ndemonstrate that ZOC outperforms the baselines by a large margin.\n","authors":["Sepideh Esmaeilpour","Bing Liu","Eric Robertson","Lei Shu"],"pdf_url":"https://arxiv.org/pdf/2109.02748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11924v1","updated":"2022-03-22T17:52:18Z","published":"2022-03-22T17:52:18Z","title":"On Supervised Feature Selection from High Dimensional Feature Spaces","summary":"  The application of machine learning to image and video data often yields a\nhigh dimensional feature space. Effective feature selection techniques identify\na discriminant feature subspace that lowers computational and modeling costs\nwith little performance degradation. A novel supervised feature selection\nmethodology is proposed for machine learning decisions in this work. The\nresulting tests are called the discriminant feature test (DFT) and the relevant\nfeature test (RFT) for the classification and regression problems,\nrespectively. The DFT and RFT procedures are described in detail. Furthermore,\nwe compare the effectiveness of DFT and RFT with several classic feature\nselection methods. To this end, we use deep features obtained by LeNet-5 for\nMNIST and Fashion-MNIST datasets as illustrative examples. It is shown by\nexperimental results that DFT and RFT can select a lower dimensional feature\nsubspace distinctly and robustly while maintaining high decision performance.\n","authors":["Yijing Yang","Wei Wang","Hongyu Fu","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2203.11924v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2107.04631v3","updated":"2022-03-22T17:47:15Z","published":"2021-07-09T18:59:58Z","title":"Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral\n  Images using a Hybrid Deep Neural Network","summary":"  Atmospheric correction is a fundamental task in remote sensing because\nobservations are taken either of the atmosphere or looking through the\natmosphere. Atmospheric correction errors can significantly alter the spectral\nsignature of the observations, and lead to invalid classifications or target\ndetection. This is even more crucial when working with hyperspectral data,\nwhere a precise measurement of spectral properties is required.\nState-of-the-art physics-based atmospheric correction approaches require\nextensive prior knowledge about sensor characteristics, collection geometry,\nand environmental characteristics of the scene being collected. These\napproaches are computationally expensive, prone to inaccuracy due to lack of\nsufficient environmental and collection information, and often impossible for\nreal-time applications. In this paper, a geometry-dependent hybrid neural\nnetwork is proposed for automatic atmospheric correction using multi-scan\nhyperspectral data collected from different geometries. The proposed network\ncan characterize the atmosphere without any additional meteorological data. A\ngrid-search method is also proposed to solve the temperature emissivity\nseparation problem. Results show that the proposed network has the capacity to\naccurately characterize the atmosphere and estimate target emissivity spectra\nwith a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This\nsolution can lead to accurate atmospheric correction to improve target\ndetection for real time applications.\n","authors":["Fangcao Xu","Jian Sun","Guido Cervone","Mark Salvador"],"pdf_url":"https://arxiv.org/pdf/2107.04631v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.08524v2","updated":"2022-03-22T17:41:56Z","published":"2021-11-16T14:53:19Z","title":"Non-separable Spatio-temporal Graph Kernels via SPDEs","summary":"  Gaussian processes (GPs) provide a principled and direct approach for\ninference and learning on graphs. However, the lack of justified graph kernels\nfor spatio-temporal modelling has held back their use in graph problems. We\nleverage an explicit link between stochastic partial differential equations\n(SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via\nSPDEs, and derive non-separable spatio-temporal graph kernels that capture\ninteraction across space and time. We formulate the graph kernels for the\nstochastic heat equation and wave equation. We show that by providing novel\ntools for spatio-temporal GP modelling on graphs, we outperform pre-existing\ngraph kernels in real-world applications that feature diffusion, oscillation,\nand other complicated interactions.\n","authors":["Alexander Nikitin","ST John","Arno Solin","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2111.08524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11914v1","updated":"2022-03-22T17:34:27Z","published":"2022-03-22T17:34:27Z","title":"SPRITE: A Scalable Privacy-Preserving and Verifiable Collaborative\n  Learning for Industrial IoT","summary":"  Recently collaborative learning is widely applied to model sensitive data\ngenerated in Industrial IoT (IIoT). It enables a large number of devices to\ncollectively train a global model by collaborating with a server while keeping\nthe datasets on their respective premises. However, existing approaches are\nlimited by high overheads and may also suffer from falsified aggregated results\nreturned by a malicious server. Hence, we propose a Scalable,\nPrivacy-preserving and veRIfiable collaboraTive lEarning (SPRITE) algorithm to\ntrain linear and logistic regression models for IIoT. We aim to reduce burden\nfrom resource-constrained IIoT devices and trust dependence on cloud by\nintroducing fog as a middleware. SPRITE employs threshold secret sharing to\nguarantee privacy-preservation and robustness to IIoT device dropout whereas\nverifiable additive homomorphic secret sharing to ensure verifiability during\nmodel aggregation. We prove the security of SPRITE in an honest-but-curious\nsetting where the cloud is untrustworthy. We validate SPRITE to be scalable and\nlightweight through theoretical overhead analysis and extensive testbed\nexperimentation on an IIoT use-case with two real-world industrial datasets.\nFor a large-scale industrial setup, SPRITE records 65% and 55% improved\nperformance over its competitor for linear and logistic regressions\nrespectively while reducing communication overhead for an IIoT device by 90%.\n","authors":["Jayasree Sengupta","Sushmita Ruj","Sipra Das Bit"],"pdf_url":"https://arxiv.org/pdf/2203.11914v1.pdf","comment":"Accepted for publication at The 22nd IEEE/ACM International Symposium\n  on Cluster, Cloud and Internet Computing (CCGrid 2022). 5 figures and 6\n  tables"},{"id":"http://arxiv.org/abs/2203.11903v1","updated":"2022-03-22T17:15:56Z","published":"2022-03-22T17:15:56Z","title":"Enabling faster and more reliable sonographic assessment of gestational\n  age through machine learning","summary":"  Fetal ultrasounds are an essential part of prenatal care and can be used to\nestimate gestational age (GA). Accurate GA assessment is important for\nproviding appropriate prenatal care throughout pregnancy and identifying\ncomplications such as fetal growth disorders. Since derivation of GA from\nmanual fetal biometry measurements (head, abdomen, femur) are\noperator-dependent and time-consuming, there have been a number of research\nefforts focused on using artificial intelligence (AI) models to estimate GA\nusing standard biometry images, but there is still room to improve the accuracy\nand reliability of these AI systems for widescale adoption. To improve GA\nestimates, without significant change to provider workflows, we leverage AI to\ninterpret standard plane ultrasound images as well as 'fly-to' ultrasound\nvideos, which are 5-10s videos automatically recorded as part of the standard\nof care before the still image is captured. We developed and validated three AI\nmodels: an image model using standard plane images, a video model using fly-to\nvideos, and an ensemble model (combining both image and video). All three were\nstatistically superior to standard fetal biometry-based GA estimates derived by\nexpert sonographers, the ensemble model has the lowest mean absolute error\n(MAE) compared to the clinical standard fetal biometry (mean difference: -1.51\n$\\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404\nparticipants. We showed that our models outperform standard biometry by a more\nsubstantial margin on fetuses that were small for GA. Our AI models have the\npotential to empower trained operators to estimate GA with higher accuracy\nwhile reducing the amount of time required and user variability in measurement\nacquisition.\n","authors":["Chace Lee","Angelica Willis","Christina Chen","Marcin Sieniek","Akib Uddin","Jonny Wong","Rory Pilgrim","Katherine Chou","Daniel Tse","Shravya Shetty","Ryan G. Gomes"],"pdf_url":"https://arxiv.org/pdf/2203.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11894v1","updated":"2022-03-22T17:06:07Z","published":"2022-03-22T17:06:07Z","title":"GradViT: Gradient Inversion of Vision Transformers","summary":"  In this work we demonstrate the vulnerability of vision transformers (ViTs)\nto gradient-based inversion attacks. During this attack, the original data\nbatch is reconstructed given model weights and the corresponding gradients. We\nintroduce a method, named GradViT, that optimizes random noise into naturally\nlooking images via an iterative process. The optimization objective consists of\n(i) a loss on matching the gradients, (ii) image prior in the form of distance\nto batch-normalization statistics of a pretrained CNN model, and (iii) a total\nvariation regularization on patches to guide correct recovery locations. We\npropose a unique loss scheduling function to overcome local minima during\noptimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and\nobserve unprecedentedly high fidelity and closeness to the original (hidden)\ndata. During the analysis we find that vision transformers are significantly\nmore vulnerable than previously studied CNNs due to the presence of the\nattention mechanism. Our method demonstrates new state-of-the-art results for\ngradient inversion in both qualitative and quantitative metrics. Project page\nat https://gradvit.github.io/.\n","authors":["Ali Hatamizadeh","Hongxu Yin","Holger Roth","Wenqi Li","Jan Kautz","Daguang Xu","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2203.11894v1.pdf","comment":"CVPR 2021 Accepted Paper"},{"id":"http://arxiv.org/abs/2203.11889v1","updated":"2022-03-22T17:01:07Z","published":"2022-03-22T17:01:07Z","title":"Insights From the NeurIPS 2021 NetHack Challenge","summary":"  In this report, we summarize the takeaways from the first NeurIPS 2021\nNetHack Challenge. Participants were tasked with developing a program or agent\nthat can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by\ninteracting with the NetHack Learning Environment (NLE), a scalable,\nprocedurally generated, and challenging Gym environment for reinforcement\nlearning (RL). The challenge showcased community-driven progress in AI with\nmany diverse approaches significantly beating the previously best results on\nNetHack. Furthermore, it served as a direct comparison between neural (e.g.,\ndeep RL) and symbolic AI, as well as hybrid systems, demonstrating that on\nNetHack symbolic bots currently outperform deep RL by a large margin. Lastly,\nno agent got close to winning the game, illustrating NetHack's suitability as a\nlong-term benchmark for AI research.\n","authors":["Eric Hambro","Sharada Mohanty","Dmitrii Babaev","Minwoo Byeon","Dipam Chakraborty","Edward Grefenstette","Minqi Jiang","Daejin Jo","Anssi Kanervisto","Jongmin Kim","Sungwoong Kim","Robert Kirk","Vitaly Kurin","Heinrich Küttler","Taehwon Kwon","Donghoon Lee","Vegard Mella","Nantas Nardelli","Ivan Nazarov","Nikita Ovsov","Jack Parker-Holder","Roberta Raileanu","Karolis Ramanauskas","Tim Rocktäschel","Danielle Rothermel","Mikayel Samvelyan","Dmitry Sorokin","Maciej Sypetkowski","Michał Sypetkowski"],"pdf_url":"https://arxiv.org/pdf/2203.11889v1.pdf","comment":"Under review at PMLR for the NeuRIPS 2021 Competition Workshop Track,\n  10 pages + 10 in appendices"},{"id":"http://arxiv.org/abs/2202.08510v2","updated":"2022-03-22T17:00:57Z","published":"2022-02-17T08:33:52Z","title":"A hybrid 2-stage vision transformer for artificial intelligence-assisted\n  5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic\n  tool for guiding gastric cancer treatment","summary":"  Gastric endoscopic screening is an effective way to decide appropriate\ngastric cancer (GC) treatment at an early stage, reducing GC-associated\nmortality rate. Although artificial intelligence (AI) has brought a great\npromise to assist pathologist to screen digitalized whole slide images,\nautomatic classification systems for guiding proper GC treatment based on\nclinical guideline are still lacking. We propose an AI system classifying 5\nclasses of GC histology, which can be perfectly matched to general GC treatment\nguidance. The AI system was designed to mimic the way pathologist understand\nslides through multi-scale self-attention mechanism using a 2-stage Vision\nTransformer network. The AI system performance was evaluated on 876 internal\nendoscopic slides and 336 external endoscopic slides from clinical cohort. We\nfurther evaluated practical usability of the AI system on observation of\nAI-assisted 6 pathologist performance. The AI system demonstrates clinical\ncapability by achieving class-average diagnostic sensitivity of above 85% for\nboth internal and external cohort analysis. Furthermore, AI-assisted\npathologists showed significantly improved diagnostic sensitivity by 10% within\n18% saved screening time compared to human pathologists (p-values of 0.006 and\n0.030, respectively). The reliable performance of the AI system in multi-center\ncohort testing and its clinical applicability demonstrate that AI-assisted\nendoscopic CG screening would help reduce the workload of limited pathologists.\nFurthermore, the AI system has a great potential for providing presumptive\npathologic opinion for deciding proper treatment for early GC patients.\n","authors":["Yujin Oh","Go Eun Bae","Kyung-Hee Kim","Min-Kyung Yeo","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2202.08510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.02168v4","updated":"2022-03-22T16:52:04Z","published":"2021-07-05T17:52:55Z","title":"DPPIN: A Biological Repository of Dynamic Protein-Protein Interaction\n  Network Data","summary":"  In the big data era, the relationship between entries becomes more and more\ncomplex. Effective graph (or network) representation learning and mining\nalgorithms pave the way for many applications, such as recommendation, fraud\ndetection, social search, and bioinformation retrieval. Nowadays, many graph\n(or network) algorithms have already paid attention to dynamic or temporal\nnetworks, which are more suitable than static ones for fitting the complex\nreal-world scenarios with evolving patterns of graph topology and node\nfeatures. To contribute to the network representation learning and network\nmining research community, we provide a bunch of label-adequate,\ndynamics-meaningful, and attribute-sufficient dynamic networks from the health\ndomain. To be specific, in our repository DPPIN, we totally have 12 dynamic\nnetwork datasets at different scales, and each dataset is a dynamic\nprotein-protein interaction network describing protein-level interactions of\nyeast cells. These domain-specific node features, graph evolution patterns, and\nnode and graph labels could serve as the benchmarks and constraints to help the\ntraining and learning manners of graph algorithms. All resources of this work\nare deployed and publicly available at https://github.com/DongqiFu/DPPIN.\n","authors":["Dongqi Fu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2107.02168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11872v1","updated":"2022-03-22T16:48:41Z","published":"2022-03-22T16:48:41Z","title":"Performance of long short-term memory artificial neural networks in\n  nowcasting during the COVID-19 crisis","summary":"  The COVID-19 pandemic has demonstrated the increasing need of policymakers\nfor timely estimates of macroeconomic variables. A prior UNCTAD research paper\nexamined the suitability of long short-term memory artificial neural networks\n(LSTM) for performing economic nowcasting of this nature. Here, the LSTM's\nperformance during the COVID-19 pandemic is compared and contrasted with that\nof the dynamic factor model (DFM), a commonly used methodology in the field.\nThree separate variables, global merchandise export values and volumes and\nglobal services exports, were nowcast with actual data vintages and performance\nevaluated for the second, third, and fourth quarters of 2020 and the first and\nsecond quarters of 2021. In terms of both mean absolute error and root mean\nsquare error, the LSTM obtained better performance in two-thirds of\nvariable/quarter combinations, as well as displayed more gradual forecast\nevolutions with more consistent narratives and smaller revisions. Additionally,\na methodology to introduce interpretability to LSTMs is introduced and made\navailable in the accompanying nowcast_lstm Python library, which is now also\navailable in R, MATLAB, and Julia.\n","authors":["Daniel Hopp"],"pdf_url":"https://arxiv.org/pdf/2203.11872v1.pdf","comment":"24 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2102.04152v2","updated":"2022-03-22T16:43:13Z","published":"2021-02-08T12:04:59Z","title":"EigenGame Unloaded: When playing games is better than optimizing","summary":"  We build on the recently proposed EigenGame that views eigendecomposition as\na competitive game. EigenGame's updates are biased if computed using\nminibatches of data, which hinders convergence and more sophisticated\nparallelism in the stochastic setting. In this work, we propose an unbiased\nstochastic update that is asymptotically equivalent to EigenGame, enjoys\ngreater parallelism allowing computation on datasets of larger sample sizes,\nand outperforms EigenGame in experiments. We present applications to finding\nthe principal components of massive datasets and performing spectral clustering\nof graphs. We analyze and discuss our proposed update in the context of\nEigenGame and the shift in perspective from optimization to games.\n","authors":["Ian Gemp","Brian McWilliams","Claire Vernade","Thore Graepel"],"pdf_url":"https://arxiv.org/pdf/2102.04152v2.pdf","comment":"Published in ICLR '22"},{"id":"http://arxiv.org/abs/2203.11864v1","updated":"2022-03-22T16:40:52Z","published":"2022-03-22T16:40:52Z","title":"On the (Non-)Robustness of Two-Layer Neural Networks in Different\n  Learning Regimes","summary":"  Neural networks are known to be highly sensitive to adversarial examples.\nThese may arise due to different factors, such as random initialization, or\nspurious correlations in the learning problem. To better understand these\nfactors, we provide a precise study of robustness and generalization in\ndifferent scenarios, from initialization to the end of training in different\nregimes, as well as intermediate scenarios, where initialization still plays a\nrole due to \"lazy\" training. We consider over-parameterized networks in high\ndimensions with quadratic targets and infinite samples. Our analysis allows us\nto identify new trade-offs between generalization and robustness, whereby\nrobustness can only get worse when generalization improves, and vice versa. We\nalso show how linearized lazy training regimes can worsen robustness, due to\nimproperly scaled random initialization. Our theoretical results are\nillustrated with numerical experiments.\n","authors":["Elvis Dohmatob","Alberto Bietti"],"pdf_url":"https://arxiv.org/pdf/2203.11864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11860v1","updated":"2022-03-22T16:36:36Z","published":"2022-03-22T16:36:36Z","title":"Practical tradeoffs between memory, compute, and performance in learned\n  optimizers","summary":"  Optimization plays a costly and crucial role in developing machine learning\nsystems. In learned optimizers, the few hyperparameters of commonly used\nhand-designed optimizers, e.g. Adam or SGD, are replaced with flexible\nparametric functions. The parameters of these functions are then optimized so\nthat the resulting learned optimizer minimizes a target loss on a chosen class\nof models. Learned optimizers can both reduce the number of required training\nsteps and improve the final test loss. However, they can be expensive to train,\nand once trained can be expensive to use due to computational and memory\noverhead for the optimizer itself. In this work, we identify and quantify the\ndesign features governing the memory, compute, and performance trade-offs for\nmany learned and hand-designed optimizers. We further leverage our analysis to\nconstruct a learned optimizer that is both faster and more memory efficient\nthan previous work.\n","authors":["Luke Metz","C. Daniel Freeman","James Harrison","Niru Maheswaranathan","Jascha Sohl-Dickstein"],"pdf_url":"https://arxiv.org/pdf/2203.11860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11854v1","updated":"2022-03-22T16:31:44Z","published":"2022-03-22T16:31:44Z","title":"Sionna: An Open-Source Library for Next-Generation Physical Layer\n  Research","summary":"  Sionna is a GPU-accelerated open-source library for link-level simulations\nbased on TensorFlow. It enables the rapid prototyping of complex communication\nsystem architectures and provides native support for the integration of neural\nnetworks. Sionna implements a wide breadth of carefully tested state-of-the-art\nalgorithms that can be used for benchmarking and end-to-end performance\nevaluation. This allows researchers to focus on their research, making it more\nimpactful and reproducible, while saving time implementing components outside\ntheir area of expertise. This white paper provides a brief introduction to\nSionna, explains its design principles and features, as well as future\nextensions, such as integrated ray tracing and custom CUDA kernels. We believe\nthat Sionna is a valuable tool for research on next-generation communication\nsystems, such as 6G, and we welcome contributions from our community.\n","authors":["Jakob Hoydis","Sebastian Cammerer","Fayçal Ait Aoudia","Avinash Vem","Nikolaus Binder","Guillermo Marcus","Alexander Keller"],"pdf_url":"https://arxiv.org/pdf/2203.11854v1.pdf","comment":"5 pages, 1 figure, 4 code listings"},{"id":"http://arxiv.org/abs/2203.11852v1","updated":"2022-03-22T16:30:22Z","published":"2022-03-22T16:30:22Z","title":"A Survey on Techniques for Identifying and Resolving Representation Bias\n  in Data","summary":"  The grand goal of data-driven decision-making is to help humans make\ndecisions, not only easily and at scale but also wisely, accurately, and just.\nHowever, data-driven algorithms are only as good as the data they work with,\nwhile data sets, especially social data, often miss representing minorities.\nRepresentation Bias in data can happen due to various reasons ranging from\nhistorical discrimination to selection and sampling biases in the data\nacquisition and preparation methods. One cannot expect AI-based societal\nsolutions to have equitable outcomes without addressing the representation\nbias. This paper surveys the existing literature on representation bias in the\ndata. It presents a taxonomy to categorize the studied techniques based on\nmultiple design dimensions and provide a side-by-side comparison of their\nproperties. There is still a long way to fully address representation bias\nissues in data. The authors hope that this survey motivates researchers to\napproach these challenges in the future by observing existing work within their\nrespective domains.\n","authors":["Nima Shahbazi","Yin Lin","Abolfazl Asudeh","H. V. Jagadish"],"pdf_url":"https://arxiv.org/pdf/2203.11852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.08895v3","updated":"2022-03-22T16:27:27Z","published":"2021-06-16T15:57:51Z","title":"Masked Training of Neural Networks with Partial Gradients","summary":"  State-of-the-art training algorithms for deep learning models are based on\nstochastic gradient descent (SGD). Recently, many variations have been\nexplored: perturbing parameters for better accuracy (such as in Extragradient),\nlimiting SGD updates to a subset of parameters for increased efficiency (such\nas meProp) or a combination of both (such as Dropout). However, the convergence\nof these methods is often not studied in theory. We propose a unified\ntheoretical framework to study such SGD variants -- encompassing the\naforementioned algorithms and additionally a broad variety of methods used for\ncommunication efficient training or model compression. Our insights can be used\nas a guide to improve the efficiency of such methods and facilitate\ngeneralization to new applications. As an example, we tackle the task of\njointly training networks, a version of which (limited to sub-networks) is used\nto create Slimmable Networks. By training a low-rank Transformer jointly with a\nstandard one we obtain superior performance than when it is trained separately.\n","authors":["Amirkeivan Mohtashami","Martin Jaggi","Sebastian U. Stich"],"pdf_url":"https://arxiv.org/pdf/2106.08895v3.pdf","comment":"Proceedings of the 25th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2022"},{"id":"http://arxiv.org/abs/2203.11849v1","updated":"2022-03-22T16:26:09Z","published":"2022-03-22T16:26:09Z","title":"A Girl Has A Name, And It's ... Adversarial Authorship Attribution for\n  Deobfuscation","summary":"  Recent advances in natural language processing have enabled powerful\nprivacy-invasive authorship attribution. To counter authorship attribution,\nresearchers have proposed a variety of rule-based and learning-based text\nobfuscation approaches. However, existing authorship obfuscation approaches do\nnot consider the adversarial threat model. Specifically, they are not evaluated\nagainst adversarially trained authorship attributors that are aware of\npotential obfuscation. To fill this gap, we investigate the problem of\nadversarial authorship attribution for deobfuscation. We show that\nadversarially trained authorship attributors are able to degrade the\neffectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate\nthe effectiveness of adversarial training when the attributor makes incorrect\nassumptions about whether and which obfuscator was used. While there is a a\nclear degradation in attribution accuracy, it is noteworthy that this\ndegradation is still at or above the attribution accuracy of the attributor\nthat is not adversarially trained at all. Our results underline the need for\nstronger obfuscation approaches that are resistant to deobfuscation\n","authors":["Wanyue Zhai","Jonathan Rusert","Zubair Shafiq","Padmini Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2203.11849v1.pdf","comment":"9 pages, 7 figures, 3 tables, ACL 2022"},{"id":"http://arxiv.org/abs/2203.11842v1","updated":"2022-03-22T16:09:42Z","published":"2022-03-22T16:09:42Z","title":"X-MEN: Guaranteed XOR-Maximum Entropy Constrained Inverse Reinforcement\n  Learning","summary":"  Inverse Reinforcement Learning (IRL) is a powerful way of learning from\ndemonstrations. In this paper, we address IRL problems with the availability of\nprior knowledge that optimal policies will never violate certain constraints.\nConventional approaches ignoring these constraints need many demonstrations to\nconverge. We propose XOR-Maximum Entropy Constrained Inverse Reinforcement\nLearning (X-MEN), which is guaranteed to converge to the optimal policy in\nlinear rate w.r.t. the number of learning iterations. X-MEN embeds XOR-sampling\n-- a provable sampling approach that transforms the #P complete sampling\nproblem into queries to NP oracles -- into the framework of maximum entropy\nIRL. X-MEN also guarantees the learned policy will never generate trajectories\nthat violate constraints. Empirical results in navigation demonstrate that\nX-MEN converges faster to the optimal policies compared to baseline approaches\nand always generates trajectories that satisfy multi-state combinatorial\nconstraints.\n","authors":["Fan Ding","Yeiang Xue"],"pdf_url":"https://arxiv.org/pdf/2203.11842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11836v1","updated":"2022-03-22T16:03:24Z","published":"2022-03-22T16:03:24Z","title":"A Real-time Junk Food Recognition System based on Machine Learning","summary":"  $ $As a result of bad eating habits, humanity may be destroyed. People are\nconstantly on the lookout for tasty foods, with junk foods being the most\ncommon source. As a consequence, our eating patterns are shifting, and we're\ngravitating toward junk food more than ever, which is bad for our health and\nincreases our risk of acquiring health problems. Machine learning principles\nare applied in every aspect of our lives, and one of them is object recognition\nvia image processing. However, because foods vary in nature, this procedure is\ncrucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a\nlow accuracy rate. All of these issues were defeated by the Deep Neural\nNetwork. In this work, we created a fresh dataset of 10,000 data points from 20\njunk food classifications to try to recognize junk foods. All of the data in\nthe data set was gathered using the Google search engine, which is thought to\nbe one-of-a-kind in every way. The goal was achieved using Convolution Neural\nNetwork (CNN) technology, which is well-known for image processing. We achieved\na 98.05\\% accuracy rate throughout the research, which was satisfactory. In\naddition, we conducted a test based on a real-life event, and the outcome was\nextraordinary. Our goal is to advance this research to the next level, so that\nit may be applied to a future study. Our ultimate goal is to create a system\nthat would encourage people to avoid eating junk food and to be\nhealth-conscious. \\keywords{ Machine Learning \\and junk food \\and object\ndetection \\and YOLOv3 \\and custom food dataset.}\n","authors":["Sirajum Munira Shifat","Takitazwar Parthib","Sabikunnahar Talukder Pyaasa","Nila Maitra Chaity","Niloy Kumar","Md. Kishor Morol"],"pdf_url":"https://arxiv.org/pdf/2203.11836v1.pdf","comment":"15 pages, 7 figures, accepted in ICBBDB conference"},{"id":"http://arxiv.org/abs/2203.11834v1","updated":"2022-03-22T16:01:04Z","published":"2022-03-22T16:01:04Z","title":"Improving Generalization in Federated Learning by Seeking Flat Minima","summary":"  Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2203.11834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11829v1","updated":"2022-03-22T15:56:24Z","published":"2022-03-22T15:56:24Z","title":"Provable Constrained Stochastic Convex Optimization with XOR-Projected\n  Gradient Descent","summary":"  Provably solving stochastic convex optimization problems with constraints is\nessential for various problems in science, business, and statistics. Recently\nproposed XOR-Stochastic Gradient Descent (XOR-SGD) provides a convergence rate\nguarantee solving the constraints-free version of the problem by leveraging\nXOR-Sampling. However, the task becomes more difficult when additional equality\nand inequality constraints are needed to be satisfied. Here we propose XOR-PGD,\na novel algorithm based on Projected Gradient Descent (PGD) coupled with the\nXOR sampler, which is guaranteed to solve the constrained stochastic convex\noptimization problem still in linear convergence rate by choosing proper step\nsize. We show on both synthetic stochastic inventory management and real-world\nroad network design problems that the rate of constraints satisfaction of the\nsolutions optimized by XOR-PGD is $10\\%$ more than the competing approaches in\na very large searching space. The improved XOR-PGD algorithm is demonstrated to\nbe more accurate and efficient than both XOR-SGD and SGD coupled with MCMC\nbased samplers. It is also shown to be more scalable with respect to the number\nof samples and processor cores via experiments with large dimensions.\n","authors":["Fan Ding","Yijie Wang","Jianzhu Ma","Yexiang Xue"],"pdf_url":"https://arxiv.org/pdf/2203.11829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11828v1","updated":"2022-03-22T15:54:17Z","published":"2022-03-22T15:54:17Z","title":"Explainable Landscape Analysis in Automated Algorithm Performance\n  Prediction","summary":"  Predicting the performance of an optimization algorithm on a new problem\ninstance is crucial in order to select the most appropriate algorithm for\nsolving that problem instance. For this purpose, recent studies learn a\nsupervised machine learning (ML) model using a set of problem landscape\nfeatures linked to the performance achieved by the optimization algorithm.\nHowever, these models are black-box with the only goal of achieving good\npredictive performance, without providing explanations which landscape features\ncontribute the most to the prediction of the performance achieved by the\noptimization algorithm. In this study, we investigate the expressiveness of\nproblem landscape features utilized by different supervised ML models in\nautomated algorithm performance prediction. The experimental results point out\nthat the selection of the supervised ML method is crucial, since different\nsupervised ML regression models utilize the problem landscape features\ndifferently and there is no common pattern with regard to which landscape\nfeatures are the most informative.\n","authors":["Risto Trajanov","Stefan Dimeski","Martin Popovski","Peter Korošec","Tome Eftimov"],"pdf_url":"https://arxiv.org/pdf/2203.11828v1.pdf","comment":"To appear in International Conference on the Applications of\n  Evolutionary Computation 2022 (Part of EvoStar 2022). arXiv admin note: text\n  overlap with arXiv:2110.11633"},{"id":"http://arxiv.org/abs/2111.12448v4","updated":"2022-03-22T15:52:17Z","published":"2021-11-24T11:53:33Z","title":"3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch\n  Feature Swapping for Bodies and Faces","summary":"  Learning a disentangled, interpretable, and structured latent representation\nin 3D generative models of faces and bodies is still an open problem. The\nproblem is particularly acute when control over identity features is required.\nIn this paper, we propose an intuitive yet effective self-supervised approach\nto train a 3D shape variational autoencoder (VAE) which encourages a\ndisentangled latent representation of identity features. Curating the\nmini-batch generation by swapping arbitrary features across different shapes\nallows to define a loss function leveraging known differences and similarities\nin the latent representations. Experimental results conducted on 3D meshes show\nthat state-of-the-art methods for latent disentanglement are not able to\ndisentangle identity features of faces and bodies. Our proposed method properly\ndecouples the generation of such features while maintaining good representation\nand reconstruction capabilities.\n","authors":["Simone Foti","Bongjin Koo","Danail Stoyanov","Matthew J. Clarkson"],"pdf_url":"https://arxiv.org/pdf/2111.12448v4.pdf","comment":"Accepted for publication at CVPR2022"},{"id":"http://arxiv.org/abs/2202.05928v2","updated":"2022-03-22T15:52:05Z","published":"2022-02-11T23:04:00Z","title":"Benign Overfitting without Linearity: Neural Network Classifiers Trained\n  by Gradient Descent for Noisy Linear Data","summary":"  Benign overfitting, the phenomenon where interpolating models generalize well\nin the presence of noisy data, was first observed in neural network models\ntrained with gradient descent. To better understand this empirical observation,\nwe consider the generalization error of two-layer neural networks trained to\ninterpolation by gradient descent on the logistic loss following random\ninitialization. We assume the data comes from well-separated class-conditional\nlog-concave distributions and allow for a constant fraction of the training\nlabels to be corrupted by an adversary. We show that in this setting, neural\nnetworks exhibit benign overfitting: they can be driven to zero training error,\nperfectly fitting any noisy training labels, and simultaneously achieve test\nerror close to the Bayes-optimal error. In contrast to previous work on benign\noverfitting that require linear or kernel-based predictors, our analysis holds\nin a setting where both the model and learning dynamics are fundamentally\nnonlinear.\n","authors":["Spencer Frei","Niladri S. Chatterji","Peter L. Bartlett"],"pdf_url":"https://arxiv.org/pdf/2202.05928v2.pdf","comment":"35 pages; fixed typos and clarified an assumption on the activation"},{"id":"http://arxiv.org/abs/2203.11824v1","updated":"2022-03-22T15:47:22Z","published":"2022-03-22T15:47:22Z","title":"Was that so hard? Estimating human classification difficulty","summary":"  When doctors are trained to diagnose a specific disease, they learn faster\nwhen presented with cases in order of increasing difficulty. This creates the\nneed for automatically estimating how difficult it is for doctors to classify a\ngiven case. In this paper, we introduce methods for estimating how hard it is\nfor a doctor to diagnose a case represented by a medical image, both when\nground truth difficulties are available for training, and when they are not.\nOur methods are based on embeddings obtained with deep metric learning.\nAdditionally, we introduce a practical method for obtaining ground truth human\ndifficulty for each image case in a dataset using self-assessed certainty. We\napply our methods to two different medical datasets, achieving high Kendall\nrank correlation coefficients, showing that we outperform existing methods by a\nlarge margin on our problem and data.\n","authors":["Morten Rieger Hannemose","Josefine Vilsbøll Sundgaard","Niels Kvorning Ternov","Rasmus R. Paulsen","Anders Nymark Christensen"],"pdf_url":"https://arxiv.org/pdf/2203.11824v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2109.09587v2","updated":"2022-03-22T15:43:52Z","published":"2021-09-20T14:42:39Z","title":"Recommender systems based on graph embedding techniques: A comprehensive\n  review","summary":"  As a pivotal tool to alleviate the information overload problem, recommender\nsystems aim to predict user's preferred items from millions of candidates by\nanalyzing observed user-item relations. As for alleviating the sparsity and\ncold start problems encountered by recommender systems, researchers resort to\nemploying side information or knowledge in recommendation as a strategy for\nuncovering hidden (indirect) user-item relations, aiming to enrich observed\ninformation (or data) for recommendation. However, in the face of the high\ncomplexity and large scale of side information and knowledge, this strategy\nrelies for efficient implementation on the scalability of recommendation\nmodels. Not until after the prevalence of machine learning did graph embedding\ntechniques be a concentration, which can efficiently utilize complex and\nlarge-scale data. In light of that, equipping recommender systems with graph\nembedding techniques has been widely studied these years, appearing to\noutperform conventional recommendation implemented directly based on graph\ntopological analysis. As the focus, this article retrospects graph\nembedding-based recommendation from embedding techniques for bipartite graphs,\ngeneral graphs and knowledge graphs, and proposes a general design pipeline of\nthat. In addition, after comparing several representative graph embedding-based\nrecommendation models with the most common-used conventional recommendation\nmodels on simulations, this article manifests that the conventional models can\noverall outperform the graph embedding-based ones in predicting implicit\nuser-item interactions, revealing the comparative weakness of graph\nembedding-based recommendation in these tasks. To foster future research, this\narticle proposes suggestions on making a trade-off between graph\nembedding-based recommendation and conventional recommendation in different\ntasks, and puts forward open questions.\n","authors":["Yue Deng"],"pdf_url":"https://arxiv.org/pdf/2109.09587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10736v2","updated":"2022-03-22T15:35:18Z","published":"2022-03-21T05:00:54Z","title":"The activity-weight duality in feed forward neural networks: The\n  geometric determinants of generalization","summary":"  One of the fundamental problems in machine learning is generalization. In\nneural network models with a large number of weights (parameters), many\nsolutions can be found to fit the training data equally well. The key question\nis which solution can describe testing data not in the training set. Here, we\nreport the discovery of an exact duality (equivalence) between changes in\nactivities in a given layer of neurons and changes in weights that connect to\nthe next layer of neurons in a densely connected layer in any feed forward\nneural network. The activity-weight (A-W) duality allows us to map variations\nin inputs (data) to variations of the corresponding dual weights. By using this\nmapping, we show that the generalization loss can be decomposed into a sum of\ncontributions from different eigen-directions of the Hessian matrix of the loss\nfunction at the solution in weight space. The contribution from a given\neigen-direction is the product of two geometric factors (determinants): the\nsharpness of the loss landscape and the standard deviation of the dual weights,\nwhich is found to scale with the weight norm of the solution. Our results\nprovide an unified framework, which we used to reveal how different\nregularization schemes (weight decay, stochastic gradient descent with\ndifferent batch sizes and learning rates, dropout), training data size, and\nlabeling noise affect generalization performance by controlling either one or\nboth of these two geometric determinants for generalization. These insights can\nbe used to guide development of algorithms for finding more generalizable\nsolutions in overparametrized neural networks.\n","authors":["Yu Feng","Yuhai Tu"],"pdf_url":"https://arxiv.org/pdf/2203.10736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11815v1","updated":"2022-03-22T15:35:10Z","published":"2022-03-22T15:35:10Z","title":"Clustering units in neural networks: upstream vs downstream information","summary":"  It has been hypothesized that some form of \"modular\" structure in artificial\nneural networks should be useful for learning, compositionality, and\ngeneralization. However, defining and quantifying modularity remains an open\nproblem. We cast the problem of detecting functional modules into the problem\nof detecting clusters of similar-functioning units. This begs the question of\nwhat makes two units functionally similar. For this, we consider two broad\nfamilies of methods: those that define similarity based on how units respond to\nstructured variations in inputs (\"upstream\"), and those based on how variations\nin hidden unit activations affect outputs (\"downstream\"). We conduct an\nempirical study quantifying modularity of hidden layer representations of\nsimple feedforward, fully connected networks, across a range of\nhyperparameters. For each model, we quantify pairwise associations between\nhidden units in each layer using a variety of both upstream and downstream\nmeasures, then cluster them by maximizing their \"modularity score\" using\nestablished tools from network science. We find two surprising results: first,\ndropout dramatically increased modularity, while other forms of weight\nregularization had more modest effects. Second, although we observe that there\nis usually good agreement about clusters within both upstream methods and\ndownstream methods, there is little agreement about the cluster assignments\nacross these two families of methods. This has important implications for\nrepresentation-learning, as it suggests that finding modular representations\nthat reflect structure in inputs (e.g. disentanglement) may be a distinct goal\nfrom learning modular representations that reflect structure in outputs (e.g.\ncompositionality).\n","authors":["Richard D. Lange","David S. Rolnick","Konrad P. Kording"],"pdf_url":"https://arxiv.org/pdf/2203.11815v1.pdf","comment":"12 main text pages, 4 main figures, 5 supplemental figures. Will be\n  submitted to TMLR"},{"id":"http://arxiv.org/abs/2203.11812v1","updated":"2022-03-22T15:22:31Z","published":"2022-03-22T15:22:31Z","title":"Neural System Level Synthesis: Learning over All Stabilizing Policies\n  for Nonlinear Systems","summary":"  We address the problem of designing stabilizing control policies for\nnonlinear systems in discrete-time, while minimizing an arbitrary cost\nfunction. When the system is linear and the cost is convex, the System Level\nSynthesis (SLS) approach offers an exact solution based on convex programming.\nBeyond this case, a globally optimal solution cannot be found in a tractable\nway, in general. In this paper, we develop a parametrization of all and only\nthe control policies stabilizing a given time-varying nonlinear system in terms\nof the combined effect of 1) a strongly stabilizing base controller and 2) a\nstable SLS operator to be freely designed. Based on this result, we propose a\nNeural SLS (Neur-SLS) approach guaranteeing closed-loop stability during and\nafter parameter optimization, without requiring any constraints to be\nsatisfied. We exploit recent Deep Neural Network (DNN) models based on\nRecurrent Equilibrium Networks (RENs) to learn over a rich class of nonlinear\nstable operators, and demonstrate the effectiveness of the proposed approach in\nnumerical examples.\n","authors":["Luca Furieri","Clara Lucía Galimberti","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2203.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11805v1","updated":"2022-03-22T15:16:36Z","published":"2022-03-22T15:16:36Z","title":"On Robust Classification using Contractive Hamiltonian Neural ODEs","summary":"  Deep neural networks can be fragile and sensitive to small input\nperturbations that might cause a significant change in the output. In this\npaper, we employ contraction theory to improve the robustness of neural ODEs\n(NODEs). A dynamical system is contractive if all solutions with different\ninitial conditions converge to each other asymptotically. As a consequence,\nperturbations in initial conditions become less and less relevant over time.\nSince in NODEs, the input data corresponds to the initial condition of\ndynamical systems, we show contractivity can mitigate the effect of input\nperturbations. More precisely, inspired by NODEs with Hamiltonian dynamics, we\npropose a class of contractive Hamiltonian NODEs (CH-NODEs). By properly tuning\na scalar parameter, CH-NODEs ensure contractivity by design and can be trained\nusing standard backpropagation and gradient descent algorithms. Moreover,\nCH-NODEs enjoy built-in guarantees of non-exploding gradients, which ensures a\nwell-posed training process. Finally, we demonstrate the robustness of CH-NODEs\non the MNIST image classification problem with noisy test datasets.\n","authors":["Muhammad Zakwan","Liang Xu","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2203.11805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.05734v2","updated":"2022-03-22T15:09:52Z","published":"2021-10-12T04:48:10Z","title":"Learning Efficient Multi-Agent Cooperative Visual Exploration","summary":"  We tackle the problem of cooperative visual exploration where multiple agents\nneed to jointly explore unseen regions as fast as possible based on visual\nsignals. Classical planning-based methods often suffer from expensive\ncomputation overhead at each step and a limited expressiveness of complex\ncooperation strategy. By contrast, reinforcement learning (RL) has recently\nbecome a popular paradigm for tackling this challenge due to its modeling\ncapability of arbitrarily complex strategies and minimal inference overhead. In\nthis paper, we extend the state-of-the-art single-agent visual navigation\nmethod, Active Neural SLAM (ANS), to the multi-agent setting by introducing a\nnovel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages\na transformer-based architecture, Spatial-TeamFormer, which effectively\ncaptures spatial relations and intra-agent interactions via hierarchical\nspatial self-attentions. In addition, we also implement a few multi-agent\nenhancements to process local information from each agent for an aligned\nspatial representation and more precise planning. Finally, we perform policy\ndistillation to extract a meta policy to significantly improve the\ngeneralization capability of final policy. We call this overall solution,\nMulti-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms\nclassical planning-based baselines for the first time in a photo-realistic 3D\nsimulator, Habitat. Code and videos can be found at\nhttps://sites.google.com/view/maans.\n","authors":["Chao Yu","Xinyi Yang","Jiaxuan Gao","Huazhong Yang","Yu Wang","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2110.05734v2.pdf","comment":"First three authors share equal contribution"},{"id":"http://arxiv.org/abs/2203.11793v1","updated":"2022-03-22T14:55:31Z","published":"2022-03-22T14:55:31Z","title":"A Perspective on Neural Capacity Estimation: Viability and Reliability","summary":"  Recently, several methods have been proposed for estimating the mutual\ninformation from sample data using deep neural networks and without the\nknowledge of closed-form distribution of the data. This class of estimators is\nreferred to as neural mutual information estimators (NMIE). In this paper, we\ninvestigate the performance of different NMIE proposed in the literature when\napplied to the capacity estimation problem. In particular, we study the\nperformance of mutual information neural estimator (MINE), smoothed mutual\ninformation lower-bound estimator (SMILE), and directed information neural\nestimator (DINE). For the NMIE above, capacity estimation relies on two deep\nneural networks (DNN): (i) one DNN generates samples from a distribution that\nis learned, and (ii) a DNN to estimate the MI between the channel input and the\nchannel output. We benchmark these NMIE in three scenarios: (i) AWGN channel\ncapacity estimation and (ii) channels with unknown capacity and continuous\ninputs i.e., optical intensity and peak-power constrained AWGN channel (iii)\nchannels with unknown capacity and a discrete number of mass points i.e.,\nPoisson channel. Additionally, we also (iv) consider the extension to the MAC\ncapacity problem by considering the AWGN and optical MAC models.\n","authors":["Farhad Mirkarimi","Stefano Rini"],"pdf_url":"https://arxiv.org/pdf/2203.11793v1.pdf","comment":"30 pages, 8 figures, submitted for possible journal publication.\n  arXiv admin note: text overlap with arXiv:2111.07401"},{"id":"http://arxiv.org/abs/2111.03971v2","updated":"2022-03-22T14:55:06Z","published":"2021-11-06T22:39:05Z","title":"Towards noise robust trigger-word detection with contrastive learning\n  pre-task for fast on-boarding of new trigger-words","summary":"  Trigger-word detection plays an important role as the entry point of user's\ncommunication with voice assistants. But supporting a particular word as a\ntrigger-word involves huge amount of data collection, augmentation and\nlabelling for that word. This makes supporting new trigger-words a tedious and\ntime consuming process. To combat this, we explore the use of contrastive\nlearning as a pre-training task that helps the detection model to generalize to\ndifferent words and noise conditions. We explore supervised contrastive\ntechniques and also propose a novel self-supervised training technique using\nchunked words from long sentence audios. We show that both supervised and the\nnew self-supervised contrastive pre-training techniques have comparable results\nto a traditional classification pre-training on new trigger words with less\ndata availability.\n","authors":["Sivakumar Balasubramanian","Aditya Jajodia","Gowtham Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2111.03971v2.pdf","comment":"submitted to INTERSPEECH"},{"id":"http://arxiv.org/abs/2203.11790v1","updated":"2022-03-22T14:51:44Z","published":"2022-03-22T14:51:44Z","title":"Learning Program Semantics with Code Representations: An Empirical Study","summary":"  Program semantics learning is the core and fundamental for various code\nintelligent tasks e.g., vulnerability detection, clone detection. A\nconsiderable amount of existing works propose diverse approaches to learn the\nprogram semantics for different tasks and these works have achieved\nstate-of-the-art performance. However, currently, a comprehensive and\nsystematic study on evaluating different program representation techniques\nacross diverse tasks is still missed.\n  From this starting point, in this paper, we conduct an empirical study to\nevaluate different program representation techniques. Specifically, we\ncategorize current mainstream code representation techniques into four\ncategories i.e., Feature-based, Sequence-based, Tree-based, and Graph-based\nprogram representation technique and evaluate its performance on three diverse\nand popular code intelligent tasks i.e., {Code Classification}, Vulnerability\nDetection, and Clone Detection on the public released benchmark. We further\ndesign three {research questions (RQs)} and conduct a comprehensive analysis to\ninvestigate the performance. By the extensive experimental results, we conclude\nthat (1) The graph-based representation is superior to the other selected\ntechniques across these tasks. (2) Compared with the node type information used\nin tree-based and graph-based representations, the node textual information is\nmore critical to learning the program semantics. (3) Different tasks require\nthe task-specific semantics to achieve their highest performance, however\ncombining various program semantics from different dimensions such as control\ndependency, data dependency can still produce promising results.\n","authors":["Jing Kai Siow","Shangqing Liu","Xiaofei Xie","Guozhu Meng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11790v1.pdf","comment":"Accepted in 29th edition IEEE International Conference on Software\n  Analysis, Evolution and Reengineering (SANER 2022)"},{"id":"http://arxiv.org/abs/2110.15032v5","updated":"2022-03-22T14:42:54Z","published":"2021-10-28T11:32:14Z","title":"OneFlow: Redesign the Distributed Deep Learning Framework from Scratch","summary":"  Deep learning frameworks such as TensorFlow and PyTorch provide a productive\ninterface for expressing and training a deep neural network (DNN) model on a\nsingle device or using data parallelism. Still, they may not be flexible or\nefficient enough in training emerging large models on distributed devices,\nwhich require more sophisticated parallelism beyond data parallelism. Plugins\nor wrappers have been developed to strengthen these frameworks for model or\npipeline parallelism, but they complicate the usage and implementation of\ndistributed deep learning. Aiming at a simple, neat redesign of distributed\ndeep learning frameworks for various parallelism paradigms, we present OneFlow,\na novel distributed training framework based on an SBP (split, broadcast and\npartial-value) abstraction and the actor model. SBP enables much easier\nprogramming of data parallelism and model parallelism than existing frameworks,\nand the actor model provides a succinct runtime mechanism to manage the complex\ndependencies imposed by resource constraints, data movement and computation in\ndistributed deep learning. We demonstrate the general applicability and\nefficiency of OneFlow for training various large DNN models with case studies\nand extensive experiments. The results show that OneFlow outperforms many\nwell-known customized libraries built on top of the state-of-the-art\nframeworks. The code of OneFlow is available at:\nhttps://github.com/Oneflow-Inc/oneflow.\n","authors":["Jinhui Yuan","Xinqi Li","Cheng Cheng","Juncheng Liu","Ran Guo","Shenghang Cai","Chi Yao","Fei Yang","Xiaodong Yi","Chuan Wu","Haoran Zhang","Jie Zhao"],"pdf_url":"https://arxiv.org/pdf/2110.15032v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11774v1","updated":"2022-03-22T14:39:56Z","published":"2022-03-22T14:39:56Z","title":"Estimation of speaker age and height from speech signal using bi-encoder\n  transformer mixture model","summary":"  The estimation of speaker characteristics such as age and height is a\nchallenging task, having numerous applications in voice forensic analysis. In\nthis work, we propose a bi-encoder transformer mixture model for speaker age\nand height estimation. Considering the wide differences in male and female\nvoice characteristics such as differences in formant and fundamental\nfrequencies, we propose the use of two separate transformer encoders for the\nextraction of specific voice features in the male and female gender, using\nwav2vec 2.0 as a common-level feature extractor. This architecture reduces the\ninterference effects during backpropagation and improves the generalizability\nof the model. We perform our experiments on the TIMIT dataset and significantly\noutperform the current state-of-the-art results on age estimation.\nSpecifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49\nyears for male and female age estimation, respectively. Further experiment to\nevaluate the relative importance of different phonetic types for our task\ndemonstrate that vowel sounds are the most distinguishing for age estimation.\n","authors":["Tarun Gupta","Duc-Tuan Truong","Tran The Anh","Chng Eng Siong"],"pdf_url":"https://arxiv.org/pdf/2203.11774v1.pdf","comment":"Submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.11758v1","updated":"2022-03-22T14:17:53Z","published":"2022-03-22T14:17:53Z","title":"Linear convergence of a policy gradient method for finite horizon\n  continuous time stochastic control problems","summary":"  Despite its popularity in the reinforcement learning community, a provably\nconvergent policy gradient method for general continuous space-time stochastic\ncontrol problems has been elusive. This paper closes the gap by proposing a\nproximal gradient algorithm for feedback controls of finite-time horizon\nstochastic control problems. The state dynamics are continuous time nonlinear\ndiffusions with controlled drift and possibly degenerate noise, and the\nobjectives are nonconvex in the state and nonsmooth in the control. We prove\nunder suitable conditions that the algorithm converges linearly to a stationary\npoint of the control problem, and is stable with respect to policy updates by\napproximate gradient steps. The convergence result justifies the recent\nreinforcement learning heuristics that adding entropy regularization to the\noptimization objective accelerates the convergence of policy gradient methods.\nThe proof exploits careful regularity estimates of backward stochastic\ndifferential equations.\n","authors":["Christoph Reisinger","Wolfgang Stockinger","Yufei Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11751v1","updated":"2022-03-22T14:06:26Z","published":"2022-03-22T14:06:26Z","title":"FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling\n  and Correction","summary":"  Federated learning (FL) allows multiple clients to collectively train a\nhigh-performance global model without sharing their private data. However, the\nkey challenge in federated learning is that the clients have significant\nstatistical heterogeneity among their local data distributions, which would\ncause inconsistent optimized local models on the client-side. To address this\nfundamental dilemma, we propose a novel federated learning algorithm with local\ndrift decoupling and correction (FedDC). Our FedDC only introduces lightweight\nmodifications in the local training phase, in which each client utilizes an\nauxiliary local drift variable to track the gap between the local model\nparameter and the global model parameters. The key idea of FedDC is to utilize\nthis learned local drift variable to bridge the gap, i.e., conducting\nconsistency in parameter-level. The experiment results and analysis demonstrate\nthat FedDC yields expediting convergence and better performance on various\nimage classification tasks, robust in partial participation settings, non-iid\ndata, and heterogeneous clients.\n","authors":["Liang Gao","Huazhu Fu","Li Li","Yingwen Chen","Ming Xu","Cheng-Zhong Xu"],"pdf_url":"https://arxiv.org/pdf/2203.11751v1.pdf","comment":"28 pages, 13 figures, to be published in CVPR2022"},{"id":"http://arxiv.org/abs/2111.03512v2","updated":"2022-03-22T13:57:39Z","published":"2021-11-05T14:07:06Z","title":"Data Selection for Efficient Model Update in Federated Learning","summary":"  The Federated Learning (FL) workflow of training a centralized model with\ndistributed data is growing in popularity. However, until recently, this was\nthe realm of contributing clients with similar computing capability. The fast\nexpanding IoT space and data being generated and processed at the edge are\nencouraging more effort into expanding federated learning to include\nheterogeneous systems. Previous approaches distribute light-weight models to\nclients are rely on knowledge transfer to distil the characteristic of local\ndata in partitioned updates. However, their additional knowledge exchange\ntransmitted through the network degrades the communication efficiency of FL. We\npropose to reduce the size of knowledge exchanged in these FL setups by\nclustering and selecting only the most representative bits of information from\nthe clients. The partitioned global update adopted in our work splits the\nglobal deep neural network into a lower part for generic feature extraction and\nan upper part that is more sensitive to this selected client knowledge. Our\nexperiments show that only 1.6% of the initially exchanged data can effectively\ntransfer the characteristic of the client data to the global model in our FL\napproach, using split networks. These preliminary results evolve our\nunderstanding of federated learning by demonstrating efficient training using\nstrategically selected training samples.\n","authors":["Hongrui Shi","Valentin Radu"],"pdf_url":"https://arxiv.org/pdf/2111.03512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11722v1","updated":"2022-03-22T13:31:47Z","published":"2022-03-22T13:31:47Z","title":"Convolutional Neural Network to Restore Low-Dose Digital Breast\n  Tomosynthesis Projections in a Variance Stabilization Domain","summary":"  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible\nradiation dose while maintaining sufficiently good image quality for accurate\nmedical diagnosis. In this work, we propose a convolution neural network (CNN)\nto restore low-dose (LD) DBT projections to achieve an image quality equivalent\nto a standard full-dose (FD) acquisition. The proposed network architecture\nbenefits from priors in terms of layers that were inspired by traditional\nmodel-based (MB) restoration methods, considering a model-based deep learning\napproach, where the network is trained to operate in the variance stabilization\ntransformation (VST) domain. To accurately control the network operation point,\nin terms of noise and blur of the restored image, we propose a loss function\nthat minimizes the bias and matches residual noise between the input and the\noutput. The training dataset was composed of clinical data acquired at the\nstandard FD and low-dose pairs obtained by the injection of quantum noise. The\nnetwork was tested using real DBT projections acquired with a physical\nanthropomorphic breast phantom. The proposed network achieved superior results\nin terms of the mean normalized squared error (MNSE), training time and noise\nspatial correlation compared with networks trained with traditional data-driven\nmethods. The proposed approach can be extended for other medical imaging\napplication that requires LD acquisitions.\n","authors":["Rodrigo de Barros Vimieiro","Chuang Niu","Hongming Shan","Lucas Rodrigues Borges","Ge Wang","Marcelo Andrade da Costa Vieira"],"pdf_url":"https://arxiv.org/pdf/2203.11722v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2203.11719v1","updated":"2022-03-22T13:31:09Z","published":"2022-03-22T13:31:09Z","title":"A Bayesian Approach for Shaft Centre Localisation in Journal Bearings","summary":"  It has been shown that ultrasonic techniques work well for online measuring\nof circumferential oil film thickness profile in journal bearings;\nunfortunately, they can be limited by their measuring range and unable to\ncapture details of the film all around the bearing circumference. Attempts to\nmodel the film thickness over the full range of the bearing rely on\ndeterministic approaches, which assume the observations to be true with\nabsolute certainty. Unaccounted uncertainties of the film thickness may lead to\na cascade of inaccurate predictions for subsequent calculations of hydrodynamic\nparameters. In the present work, a probabilistic framework is proposed to model\nthe film thickness with Gaussian Processes. The results are then used to\nestimate the location of the bearing shaft under various operational\nconditions. A further step in the process involves using the newly-constructed\ndataset to generate likelihood maps displaying the probable location of the\nshaft centre, given the bearing rotational speed and applied static load. The\nresults offer the possibility to visualise the confidence of the predictions\nand allow the true location to be found within an area of high probability\nwithin the bearing's bore.\n","authors":["Christopher A. Lindley","Scott Beamish","Rob Dwyer-Joyce","Nikolaos Dervilis","Keith Worden"],"pdf_url":"https://arxiv.org/pdf/2203.11719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11702v1","updated":"2022-03-22T13:12:27Z","published":"2022-03-22T13:12:27Z","title":"BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning\n  in Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) task aims to associate a piece of text\nwith a set of aspects and meanwhile infer their respective sentimental\npolarities. Up to now, the state-of-the-art approaches are built upon\nfine-tuning of various pre-trained language models. They commonly aim to learn\nthe aspect-specific representation in the corpus. Unfortunately, the aspect is\noften expressed implicitly through a set of representatives and thus renders\nimplicit mapping process unattainable unless sufficient labeled examples.\n  In this paper, we propose to jointly address aspect categorization and\naspect-based sentiment subtasks in a unified framework. Specifically, we first\nintroduce a simple but effective mechanism that collaborates the semantic and\nsyntactic information to construct auxiliary-sentences for the implicit aspect.\nThen, we encourage BERT to learn the aspect-specific representation in response\nto the automatically constructed auxiliary-sentence instead of the aspect\nitself. Finally, we empirically evaluate the performance of the proposed\nsolution by a comparative study on real benchmark datasets for both ABSA and\nTargeted-ABSA tasks. Our extensive experiments show that it consistently\nachieves state-of-the-art performance in terms of aspect categorization and\naspect-based sentiment across all datasets and the improvement margins are\nconsiderable.\n","authors":["Ahmed Murtadha","Shengfeng Pan","Bo Wen","Jianlin Su","Wenze Zhang","Yunfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11702v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2203.11700v1","updated":"2022-03-22T13:09:17Z","published":"2022-03-22T13:09:17Z","title":"Exploring Linear Feature Disentanglement For Neural Networks","summary":"  Non-linear activation functions, e.g., Sigmoid, ReLU, and Tanh, have achieved\ngreat success in neural networks (NNs). Due to the complex non-linear\ncharacteristic of samples, the objective of those activation functions is to\nproject samples from their original feature space to a linear separable feature\nspace. This phenomenon ignites our interest in exploring whether all features\nneed to be transformed by all non-linear functions in current typical NNs,\ni.e., whether there exists a part of features arriving at the linear separable\nfeature space in the intermediate layers, that does not require further\nnon-linear variation but an affine transformation instead. To validate the\nabove hypothesis, we explore the problem of linear feature disentanglement for\nneural networks in this paper. Specifically, we devise a learnable mask module\nto distinguish between linear and non-linear features. Through our designed\nexperiments we found that some features reach the linearly separable space\nearlier than the others and can be detached partly from the NNs. The explored\nmethod also provides a readily feasible pruning strategy which barely affects\nthe performance of the original model. We conduct our experiments on four\ndatasets and present promising results.\n","authors":["Tiantian He","Zhibin Li","Yongshun Gong","Yazhou Yao","Xiushan Nie","Yilong Yin"],"pdf_url":"https://arxiv.org/pdf/2203.11700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.04857v2","updated":"2022-03-22T13:08:18Z","published":"2021-11-04T23:24:22Z","title":"Model-assisted deep learning of rare extreme events from partial\n  observations","summary":"  To predict rare extreme events using deep neural networks, one encounters the\nso-called small data problem because even long-term observations often contain\nfew extreme events. Here, we investigate a model-assisted framework where the\ntraining data is obtained from numerical simulations, as opposed to\nobservations, with adequate samples from extreme events. However, to ensure the\ntrained networks are applicable in practice, the training is not performed on\nthe full simulation data; instead we only use a small subset of observable\nquantities which can be measured in practice. We investigate the feasibility of\nthis model-assisted framework on three different dynamical systems (Rossler\nattractor, FitzHugh-Nagumo model, and a turbulent fluid flow) and three\ndifferent deep neural network architectures (feedforward, long short-term\nmemory, and reservoir computing). In each case, we study the prediction\naccuracy, robustness to noise, reproducibility under repeated training, and\nsensitivity to the type of input data. In particular, we find long short-term\nmemory networks to be most robust to noise and to yield relatively accurate\npredictions, while requiring minimal fine-tuning of the hyperparameters.\n","authors":["Anna Asch","Ethan Brady","Hugo Gallardo","John Hood","Bryan Chu","Mohammad Farazmand"],"pdf_url":"https://arxiv.org/pdf/2111.04857v2.pdf","comment":"Accepted for publication in Chaos: An Interdisciplinary Journal of\n  Nonlinear Science"},{"id":"http://arxiv.org/abs/2203.11683v1","updated":"2022-03-22T12:58:03Z","published":"2022-03-22T12:58:03Z","title":"Twin Weisfeiler-Lehman: High Expressive GNNs for Graph Classification","summary":"  The expressive power of message passing GNNs is upper-bounded by\nWeisfeiler-Lehman (WL) test. To achieve high expressive GNNs beyond WL test, we\npropose a novel graph isomorphism test method, namely Twin-WL, which\nsimultaneously passes node labels and node identities rather than only passes\nnode label as WL. The identity-passing mechanism encodes complete structure\ninformation of rooted subgraph, and thus Twin-WL can offer extra power beyond\nWL at distinguishing graph structures. Based on Twin-WL, we implement two\nTwin-GNNs for graph classification via defining readout function over rooted\nsubgraph: one simply readouts the size of rooted subgraph and the other\nreadouts rich structure information of subgraph following a GNN-style. We prove\nthat the two Twin-GNNs both have higher expressive power than traditional\nmessage passing GNNs. Experiments also demonstrate the Twin-GNNs significantly\noutperform state-of-the-art methods at the task of graph classification.\n","authors":["Zhaohui Wang","Qi Cao","Huawei Shen","Bingbing Xu","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2203.11683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.06376v3","updated":"2022-03-22T12:56:00Z","published":"2021-03-10T22:54:13Z","title":"Functional Collection Programming with Semi-Ring Dictionaries","summary":"  This paper introduces semi-ring dictionaries, a powerful class of\ncompositional and purely functional collections that subsume other collection\ntypes such as sets, multisets, arrays, vectors, and matrices. We developed\nSDQL, a statically typed language that can express relational algebra with\naggregations, linear algebra, and functional collections over data such as\nrelations and matrices using semi-ring dictionaries. Furthermore, thanks to the\nalgebraic structure behind these dictionaries, SDQL unifies a wide range of\noptimizations commonly used in databases (DB) and linear algebra (LA). As a\nresult, SDQL enables efficient processing of hybrid DB and LA workloads, by\nputting together optimizations that are otherwise confined to either DB systems\nor LA frameworks. We show experimentally that a handful of DB and LA workloads\ncan take advantage of the SDQL language and optimizations. SDQL can be\ncompetitive with or outperforms a host of systems that are state of the art in\ntheir own domain: in-memory DB systems Typer and Tectorwise for (flat, not\nnested) relational data; SciPy for LA workloads; sparse tensor compiler taco;\nthe Trance nested relational engine; and the in-database machine learning\nengines LMFAO and Morpheus for hybrid DB/LA workloads over relational data.\n","authors":["Amir Shaikhha","Mathieu Huot","Jaclyn Smith","Dan Olteanu"],"pdf_url":"https://arxiv.org/pdf/2103.06376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11670v1","updated":"2022-03-22T12:41:55Z","published":"2022-03-22T12:41:55Z","title":"Improving Meta-learning for Low-resource Text Classification and\n  Generation via Memory Imitation","summary":"  Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.\n","authors":["Yingxiu Zhao","Zhiliang Tian","Huaxiu Yao","Yinhe Zheng","Dongkyu Lee","Yiping Song","Jian Sun","Nevin L. Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11670v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.11669v1","updated":"2022-03-22T12:41:42Z","published":"2022-03-22T12:41:42Z","title":"Are You Misinformed? A Study of Covid-Related Fake News in Bengali on\n  Facebook","summary":"  Our opinions and views of life can be shaped by how we perceive the opinions\nof others on social media like Facebook. This dependence has increased during\nCOVID-19 periods when we have fewer means to connect with others. However, fake\nnews related to COVID-19 has become a significant problem on Facebook. Bengali\nis the seventh most spoken language worldwide, yet we are aware of no previous\nresearch that studied the prevalence of COVID-19 related fake news in Bengali\non Facebook. In this paper, we develop machine learning models to detect fake\nnews in Bengali automatically. The best performing model is BERT, with an\nF1-score of 0.97. We apply BERT on all Facebook Bengali posts related to\nCOVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into\nthree categories: System (e.g., medical system), belief (e.g., religious\nrituals), and social (e.g., scientific awareness).\n","authors":["Protik Bose Pranto","Syed Zami-Ul-Haque Navid","Protik Dey","Gias Uddin","Anindya Iqbal"],"pdf_url":"https://arxiv.org/pdf/2203.11669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11656v1","updated":"2022-03-22T12:28:06Z","published":"2022-03-22T12:28:06Z","title":"Is Vanilla Policy Gradient Overlooked? Analyzing Deep Reinforcement\n  Learning for Hanabi","summary":"  In pursuit of enhanced multi-agent collaboration, we analyze several\non-policy deep reinforcement learning algorithms in the recently published\nHanabi benchmark. Our research suggests a perhaps counter-intuitive finding,\nwhere Proximal Policy Optimization (PPO) is outperformed by Vanilla Policy\nGradient over multiple random seeds in a simplified environment of the\nmulti-agent cooperative card game. In our analysis of this behavior we look\ninto Hanabi-specific metrics and hypothesize a reason for PPO's plateau. In\naddition, we provide proofs for the maximum length of a perfect game (71 turns)\nand any game (89 turns). Our code can be found at:\nhttps://github.com/bramgrooten/DeepRL-for-Hanabi\n","authors":["Bram Grooten","Jelle Wemmenhove","Maurice Poot","Jim Portegies"],"pdf_url":"https://arxiv.org/pdf/2203.11656v1.pdf","comment":"Accepted at ALA 2022 (Adaptive and Learning Agents Workshop at AAMAS\n  2022)"},{"id":"http://arxiv.org/abs/2203.11649v1","updated":"2022-03-22T12:06:15Z","published":"2022-03-22T12:06:15Z","title":"Performance Evaluation of Machine Learning-based Algorithm and Taguchi\n  Algorithm for the Determination of the Hardness Value of the Friction Stir\n  Welded AA 6262 Joints at a Nugget Zone","summary":"  Nowadays, industry 4.0 plays a tremendous role in the manufacturing\nindustries for increasing the amount of data and accuracy in modern\nmanufacturing systems. Thanks to artificial intelligence, particularly machine\nlearning, big data analytics have dramatically amended, and manufacturers\neasily exploit organized and unorganized data. This study utilized hybrid\noptimization algorithms to find friction stir welding and optimal hardness\nvalue at the nugget zone. A similar AA 6262 material was used and welded in a\nbutt joint configuration. Tool rotational speed (RPM), tool traverse speed\n(mm/min), and the plane depth (mm) are used as controllable parameters and\noptimized using Taguchi L9, Random Forest, and XG Boost machine learning tools.\nAnalysis of variance was also conducted at a 95% confidence interval for\nidentifying the significant parameters. The result indicated that the\ncoefficient of determination from Taguchi L9 orthogonal array is 0.91 obtained\nwhile Random Forest and XG Boost algorithm imparted 0.62 and 0.65,\nrespectively.\n","authors":["Akshansh Mishra","Eyob Messele Sefene","Gopikrishna Nidigonda","Assefa Asmare Tsegaw"],"pdf_url":"https://arxiv.org/pdf/2203.11649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11635v1","updated":"2022-03-22T11:42:25Z","published":"2022-03-22T11:42:25Z","title":"Multi-Source Domain Adaptation Based on Federated Knowledge Alignment","summary":"  Federated Learning (FL) facilitates distributed model learning to protect\nusers' privacy. In the absence of labels for a new user's data, the knowledge\ntransfer in FL allows a learned global model to adapt to the new samples\nquickly. The multi-source domain adaptation in FL aims to improve the model's\ngenerality in a target domain by learning domain-invariant features from\ndifferent clients. In this paper, we propose Federated Knowledge Alignment\n(FedKA) that aligns features from different clients and those of the target\ntask. We identify two types of negative transfer arising in multi-source domain\nadaptation of FL and demonstrate how FedKA can alleviate such negative\ntransfers with the help of a global features disentangler enhanced by embedding\nmatching. To further facilitate representation learning of the target task, we\ndevise a federated voting mechanism to provide labels for samples from the\ntarget domain via a consensus from querying local models and fine-tune the\nglobal model with these labeled samples. Extensive experiments, including an\nablation study, on an image classification task of Digit-Five and a text\nsentiment classification task of Amazon Review, show that FedKA could be\naugmented to existing FL algorithms to improve the generality of the learned\nmodel for tackling a new task.\n","authors":["Yuwei Sun","Ng Chong","Ochiai Hideya"],"pdf_url":"https://arxiv.org/pdf/2203.11635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11633v1","updated":"2022-03-22T11:40:07Z","published":"2022-03-22T11:40:07Z","title":"Semi-Targeted Model Poisoning Attack on Federated Learning via Backward\n  Error Analysis","summary":"  Model poisoning attacks on federated learning (FL) intrude in the entire\nsystem via compromising an edge model, resulting in malfunctioning of machine\nlearning models. Such compromised models are tampered with to perform\nadversary-desired behaviors. In particular, we considered a semi-targeted\nsituation where the source class is predetermined however the target class is\nnot. The goal is to cause the global classifier to misclassify data of the\nsource class. Though approaches such as label flipping have been adopted to\ninject poisoned parameters into FL, it has been shown that their performances\nare usually class-sensitive varying with different target classes applied.\nTypically, an attack can become less effective when shifting to a different\ntarget class. To overcome this challenge, we propose the Attacking\nDistance-aware Attack (ADA) to enhance a poisoning attack by finding the\noptimized target class in the feature space. Moreover, we studied a more\nchallenging situation where an adversary had limited prior knowledge about a\nclient's data. To tackle this problem, ADA deduces pair-wise distances between\ndifferent classes in the latent feature space from shared model parameters\nbased on the backward error analysis. We performed extensive empirical\nevaluations on ADA by varying the factor of attacking frequency in three\ndifferent image classification tasks. As a result, ADA succeeded in increasing\nthe attack performance by 1.8 times in the most challenging case with an\nattacking frequency of 0.01.\n","authors":["Yuwei Sun","Hideya Ochiai","Jun Sakuma"],"pdf_url":"https://arxiv.org/pdf/2203.11633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.07143v2","updated":"2022-03-22T11:36:22Z","published":"2021-09-15T08:10:23Z","title":"Spline-PINN: Approaching PDEs without Data using Fast, Physics-Informed\n  Hermite-Spline CNNs","summary":"  Partial Differential Equations (PDEs) are notoriously difficult to solve. In\ngeneral, closed-form solutions are not available and numerical approximation\nschemes are computationally expensive. In this paper, we propose to approach\nthe solution of PDEs based on a novel technique that combines the advantages of\ntwo recently emerging machine learning based approaches. First,\nphysics-informed neural networks (PINNs) learn continuous solutions of PDEs and\ncan be trained with little to no ground truth data. However, PINNs do not\ngeneralize well to unseen domains. Second, convolutional neural networks\nprovide fast inference and generalize but either require large amounts of\ntraining data or a physics-constrained loss based on finite differences that\ncan lead to inaccuracies and discretization artifacts. We leverage the\nadvantages of both of these approaches by using Hermite spline kernels in order\nto continuously interpolate a grid-based state representation that can be\nhandled by a CNN. This allows for training without any precomputed training\ndata using a physics-informed loss function only and provides fast, continuous\nsolutions that generalize to unseen domains. We demonstrate the potential of\nour method at the examples of the incompressible Navier-Stokes equation and the\ndamped wave equation. Our models are able to learn several intriguing phenomena\nsuch as Karman vortex streets, the Magnus effect, Doppler effect, interference\npatterns and wave reflections. Our quantitative assessment and an interactive\nreal-time demo show that we are narrowing the gap in accuracy of unsupervised\nML based methods to industrial CFD solvers while being orders of magnitude\nfaster.\n","authors":["Nils Wandel","Michael Weinmann","Michael Neidlin","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2109.07143v2.pdf","comment":"AAAI 2022 (Main Track)"},{"id":"http://arxiv.org/abs/2203.11629v1","updated":"2022-03-22T11:31:12Z","published":"2022-03-22T11:31:12Z","title":"On Neural Network Equivalence Checking using SMT Solvers","summary":"  Two pretrained neural networks are deemed equivalent if they yield similar\noutputs for the same inputs. Equivalence checking of neural networks is of\ngreat importance, due to its utility in replacing learning-enabled components\nwith equivalent ones, when there is need to fulfill additional requirements or\nto address security threats, as is the case for example when using knowledge\ndistillation, adversarial training etc. SMT solvers can potentially provide\nsolutions to the problem of neural network equivalence checking that will be\nsound and complete, but as it is expected any such solution is associated with\nsignificant limitations with respect to the size of neural networks to be\nchecked. This work presents a first SMT-based encoding of the equivalence\nchecking problem, explores its utility and limitations and proposes avenues for\nfuture research and improvements towards more scalable and practically\napplicable solutions. We present experimental results that shed light to the\naforementioned issues, for diverse types of neural network models (classifiers\nand regression networks) and equivalence criteria, towards a general and\napplication-independent equivalence checking approach.\n","authors":["Charis Eleftheriadis","Nikolaos Kekatos","Panagiotis Katsaros","Stavros Tripakis"],"pdf_url":"https://arxiv.org/pdf/2203.11629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.03842v5","updated":"2022-03-22T11:31:08Z","published":"2021-05-09T05:35:36Z","title":"FastCorrect: Fast Error Correction with Edit Alignment for Automatic\n  Speech Recognition","summary":"  Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n","authors":["Yichong Leng","Xu Tan","Linchen Zhu","Jin Xu","Renqian Luo","Linquan Liu","Tao Qin","Xiang-Yang Li","Ed Lin","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2105.03842v5.pdf","comment":"NeurIPS 2021. Code URL:\n  https://github.com/microsoft/NeuralSpeech/tree/master/FastCorrect"},{"id":"http://arxiv.org/abs/2203.11610v1","updated":"2022-03-22T10:55:51Z","published":"2022-03-22T10:55:51Z","title":"Diagnosis of Schizophrenia: A comprehensive evaluation","summary":"  Machine learning models have been successfully employed in the diagnosis of\nSchizophrenia disease. The impact of classification models and the feature\nselection techniques on the diagnosis of Schizophrenia have not been evaluated.\nHere, we sought to access the performance of classification models along with\ndifferent feature selection approaches on the structural magnetic resonance\nimaging data. The data consist of 72 subjects with Schizophrenia and 74 healthy\ncontrol subjects. We evaluated different classification algorithms based on\nsupport vector machine (SVM), random forest, kernel ridge regression and\nrandomized neural networks. Moreover, we evaluated T-Test, Receiver Operator\nCharacteristics (ROC), Wilcoxon, entropy, Bhattacharyya, Minimum Redundancy\nMaximum Relevance (MRMR) and Neighbourhood Component Analysis (NCA) as the\nfeature selection techniques. Based on the evaluation, SVM based models with\nGaussian kernel proved better compared to other classification models and\nWilcoxon feature selection emerged as the best feature selection approach.\nMoreover, in terms of data modality the performance on integration of the grey\nmatter and white matter proved better compared to the performance on the grey\nand white matter individually. Our evaluation showed that classification\nalgorithms along with the feature selection approaches impact the diagnosis of\nSchizophrenia disease. This indicates that proper selection of the features and\nthe classification models can improve the diagnosis of Schizophrenia.\n","authors":["M. Tanveer","Jatin Jangir","M. A. Ganaie","Iman Beheshti","M. Tabish","Nikunj Chhabra"],"pdf_url":"https://arxiv.org/pdf/2203.11610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.05113v3","updated":"2022-03-22T10:44:32Z","published":"2021-06-09T14:46:09Z","title":"More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain\n  Activity","summary":"  In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages & depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.\n","authors":["Guy Gaziv","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2106.05113v3.pdf","comment":"Code: https://github.com/WeizmannVision/SelfSuperReconst"},{"id":"http://arxiv.org/abs/2008.07912v5","updated":"2022-03-22T10:44:16Z","published":"2020-08-18T13:09:25Z","title":"Inductive logic programming at 30: a new introduction","summary":"  Inductive logic programming (ILP) is a form of machine learning. The goal of\nILP is to induce a hypothesis (a set of logical rules) that generalises\ntraining examples. As ILP turns 30, we provide a new introduction to the field.\nWe introduce the necessary logical notation and the main learning settings;\ndescribe the building blocks of an ILP system; compare several systems on\nseveral dimensions; describe four systems (Aleph, TILDE, ASPAL, and Metagol);\nhighlight key application areas; and, finally, summarise current limitations\nand directions for future research.\n","authors":["Andrew Cropper","Sebastijan Dumančić"],"pdf_url":"https://arxiv.org/pdf/2008.07912v5.pdf","comment":"Preprint of a paper accepted for JAIR"},{"id":"http://arxiv.org/abs/2203.11268v1","updated":"2022-03-22T10:39:19Z","published":"2022-03-22T10:39:19Z","title":"Domain Knowledge Aids in Signal Disaggregation; the Example of the\n  Cumulative Water Heater","summary":"  In this article we present an unsupervised low-frequency method aimed at\ndetecting and disaggregating the power used by Cumulative Water Heaters (CWH)\nin residential homes. Our model circumvents the inherent difficulty of\nunsupervised signal disaggregation by using both the shape of a power spike and\nits time of occurrence to identify the contribution of CWH reliably. Indeed,\nmany CHWs in France are configured to turn on automatically during off-peak\nhours only, and we are able to use this domain knowledge to aid peak\nidentification despite the low sampling frequency. In order to test our model,\nwe equipped a home with sensors to record the ground-truth consumption of a\nwater heater. We then apply the model to a larger dataset of energy consumption\nof Hello Watt users consisting of one month of consumption data for 5k homes at\n30-minute resolution. In this dataset we successfully identified CWHs in the\nmajority of cases where consumers declared using them. The remaining part is\nlikely due to possible misconfiguration of CWHs, since triggering them during\noff-peak hours requires specific wiring in the electrical panel of the house.\nOur model, despite its simplicity, offers promising applications: detection of\nmis-configured CWHs on off-peak contracts and slow performance degradation.\n","authors":["Alexander Belikov","Guillaume Matheron","Johan Sassi"],"pdf_url":"https://arxiv.org/pdf/2203.11268v1.pdf","comment":"16 pages, 12 figures, 2 tables"},{"id":"http://arxiv.org/abs/2203.11156v2","updated":"2022-03-22T10:25:45Z","published":"2022-03-21T17:34:18Z","title":"Operator Sketching for Deep Unrolling Networks","summary":"  In this work we propose a new paradigm for designing efficient deep unrolling\nnetworks using operator sketching. The deep unrolling networks are currently\nthe state-of-the-art solutions for imaging inverse problems. However, for\nhigh-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI\nimaging, the deep unrolling schemes typically become inefficient both in terms\nof memory and computation, due to the need of computing multiple times the\nhigh-dimensional forward and adjoint operators. Recently researchers have found\nthat such limitations can be partially addressed by stochastic unrolling with\nsubsets of operators, inspired by the success of stochastic first-order\noptimization. In this work, we propose a further acceleration upon stochastic\nunrolling, using sketching techniques to approximate products in the\nhigh-dimensional image space. The operator sketching can be jointly applied\nwith stochastic unrolling for the best acceleration and compression\nperformance. Our numerical experiments on X-ray CT image reconstruction\ndemonstrate the remarkable effectiveness of our sketched unrolling schemes.\n","authors":["Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2203.11156v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11587v1","updated":"2022-03-22T10:13:27Z","published":"2022-03-22T10:13:27Z","title":"Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue","summary":"  Context modeling plays a significant role in building multi-turn dialogue\nsystems. In order to make full use of context information, systems can use\nIncomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue\ninto single-turn by merging current utterance and context information into a\nself-contained utterance. However, previous approaches ignore the intent\nconsistency between the original query and rewritten query. The detection of\nomitted or coreferred locations in the original query can be further improved.\nIn this paper, we introduce contrastive learning and multi-task learning to\njointly model the problem. Our method benefits from carefully designed\nself-supervised objectives, which act as auxiliary tasks to capture semantics\nat both sentence-level and token-level. The experiments show that our proposed\nmodel achieves state-of-the-art performance on several public datasets.\n","authors":["Zhihao Wang","Tangjian Duan","Zihao Wang","Minghui Yang","Zujie Wen","Yongliang Wang"],"pdf_url":"https://arxiv.org/pdf/2203.11587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11579v1","updated":"2022-03-22T10:03:16Z","published":"2022-03-22T10:03:16Z","title":"Local Stochastic Factored Gradient Descent for Distributed Quantum State\n  Tomography","summary":"  We propose a distributed Quantum State Tomography (QST) protocol, named Local\nStochastic Factored Gradient Descent (Local SFGD), to learn the low-rank factor\nof a density matrix over a set of local machines. QST is the canonical\nprocedure to characterize the state of a quantum system, which we formulate as\na stochastic nonconvex smooth optimization problem. Physically, the estimation\nof a low-rank density matrix helps characterizing the amount of noise\nintroduced by quantum computation. Theoretically, we prove the local\nconvergence of Local SFGD for a general class of restricted strongly\nconvex/smooth loss functions, i.e., Local SFGD converges locally to a small\nneighborhood of the global optimum at a linear rate with a constant step size,\nwhile it locally converges exactly at a sub-linear rate with diminishing step\nsizes. With a proper initialization, local convergence results imply global\nconvergence. We validate our theoretical findings with numerical simulations of\nQST on the Greenberger-Horne-Zeilinger (GHZ) state.\n","authors":["Junhyung Lyle Kim","Mohammad Taha Toghani","César A. Uribe","Anastasios Kyrillidis"],"pdf_url":"https://arxiv.org/pdf/2203.11579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11572v1","updated":"2022-03-22T09:51:24Z","published":"2022-03-22T09:51:24Z","title":"Fast Multi-view Clustering via Ensembles: Towards Scalability,\n  Superiority, and Simplicity","summary":"  Despite significant progress, there remain three limitations to the previous\nmulti-view clustering algorithms. First, they often suffer from high\ncomputational complexity, restricting their feasibility for large-scale\ndatasets. Second, they typically fuse multi-view information via one-stage\nfusion, neglecting the possibilities in multi-stage fusions. Third,\ndataset-specific hyperparameter-tuning is frequently required, further\nundermining their practicability. In light of this, we propose a fast\nmulti-view clustering via ensembles (FastMICE) approach. Particularly, the\nconcept of random view groups is presented to capture the versatile view-wise\nrelationships, through which the hybrid early-late fusion strategy is designed\nto enable efficient multi-stage fusions. With multiple views extended to many\nview groups, three levels of diversity (w.r.t. features, anchors, and\nneighbors, respectively) are jointly leveraged for constructing the\nview-sharing bipartite graphs in the early-stage fusion. Then, a set of\ndiversified base clusterings for different view groups are obtained via fast\ngraph partitioning, which are further formulated into a unified bipartite graph\nfor final clustering in the late-stage fusion. Remarkably, FastMICE has almost\nlinear time and space complexity, and is free of dataset-specific tuning.\nExperiments on twenty multi-view datasets demonstrate its advantages in\nscalability (for extremely large datasets), superiority (in clustering\nperformance), and simplicity (to be applied) over the state-of-the-art.\n","authors":["Dong Huang","Chang-Dong Wang","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2203.11572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11570v1","updated":"2022-03-22T09:47:31Z","published":"2022-03-22T09:47:31Z","title":"Conditional Generative Data Augmentation for Clinical Audio Datasets","summary":"  In this work, we propose a novel data augmentation method for clinical audio\ndatasets based on a conditional Wasserstein Generative Adversarial Network with\nGradient Penalty (cWGAN-GP), operating on log-mel spectrograms. To validate our\nmethod, we created a clinical audio dataset which was recorded in a real-world\noperating room during Total Hip Arthroplasty (THA) procedures and contains\ntypical sounds which resemble the different phases of the intervention. We\ndemonstrate the capability of the proposed method to generate realistic\nclass-conditioned samples from the dataset distribution and show that training\nwith the generated augmented samples outperforms classical audio augmentation\nmethods in terms of classification accuracy. The performance was evaluated\nusing a ResNet-18 classifier which shows a mean per-class accuracy improvement\nof 1.51% in a 5-fold cross validation experiment using the proposed\naugmentation method. Because clinical data is often expensive to acquire, the\ndevelopment of realistic and high-quality data augmentation methods is crucial\nto improve the robustness and generalization capabilities of learning-based\nalgorithms which is especially important for safety-critical medical\napplications. Therefore, the proposed data augmentation method is an important\nstep towards improving the data bottleneck for clinical audio-based machine\nlearning systems. The code and dataset will be published upon acceptance.\n","authors":["Matthias Seibold","Armando Hoch","Mazda Farshad","Nassir Navab","Philipp Fürnstahl"],"pdf_url":"https://arxiv.org/pdf/2203.11570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10833v2","updated":"2022-03-22T09:40:22Z","published":"2022-03-21T09:48:23Z","title":"Hyperbolic Vision Transformers: Combining Improvements in Metric\n  Learning","summary":"  Metric learning aims to learn a highly discriminative model encouraging the\nembeddings of similar classes to be close in the chosen metrics and pushed\napart for dissimilar ones. The common recipe is to use an encoder to extract\nembeddings and a distance-based loss function to match the representations --\nusually, the Euclidean distance is utilized. An emerging interest in learning\nhyperbolic data embeddings suggests that hyperbolic geometry can be beneficial\nfor natural data. Following this line of work, we propose a new\nhyperbolic-based model for metric learning. At the core of our method is a\nvision transformer with output embeddings mapped to hyperbolic space. These\nembeddings are directly optimized using modified pairwise cross-entropy loss.\nWe evaluate the proposed model with six different formulations on four datasets\nachieving the new state-of-the-art performance. The source code is available at\nhttps://github.com/htdt/hyp_metric.\n","authors":["Aleksandr Ermolov","Leyla Mirvakhabova","Valentin Khrulkov","Nicu Sebe","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2203.10833v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11560v1","updated":"2022-03-22T09:32:06Z","published":"2022-03-22T09:32:06Z","title":"Modelling continual learning in humans with Hebbian context gating and\n  exponentially decaying task signals","summary":"  Humans can learn several tasks in succession with minimal mutual interference\nbut perform more poorly when trained on multiple tasks at once. The opposite is\ntrue for standard deep neural networks. Here, we propose novel computational\nconstraints for artificial neural networks, inspired by earlier work on gating\nin the primate prefrontal cortex, that capture the cost of interleaved training\nand allow the network to learn two tasks in sequence without forgetting. We\naugment standard stochastic gradient descent with two algorithmic motifs,\nso-called \"sluggish\" task units and a Hebbian training step that strengthens\nconnections between task units and hidden units that encode task-relevant\ninformation. We found that the \"sluggish\" units introduce a switch-cost during\ntraining, which biases representations under interleaved training towards a\njoint representation that ignores the contextual cue, while the Hebbian step\npromotes the formation of a gating scheme from task units to the hidden layer\nthat produces orthogonal representations which are perfectly guarded against\ninterference. Validating the model on previously published human behavioural\ndata revealed that it matches performance of participants who had been trained\non blocked or interleaved curricula, and that these performance differences\nwere driven by misestimation of the true category boundary.\n","authors":["Timo Flesch","David G. Nagy","Andrew Saxe","Christopher Summerfield"],"pdf_url":"https://arxiv.org/pdf/2203.11560v1.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2203.08657v2","updated":"2022-03-22T09:25:48Z","published":"2022-03-16T14:47:45Z","title":"Occlusion Fields: An Implicit Representation for Non-Line-of-Sight\n  Surface Reconstruction","summary":"  Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality\nthat aims to recover objects or scene parts outside the field of view from\nmeasurements of light that is indirectly scattered off a directly visible,\ndiffuse wall. Despite recent advances in acquisition and reconstruction\ntechniques, the well-posedness of the problem at large, and the recoverability\nof objects and their shapes in particular, remains an open question. The\ncommonly employed Fermat path criterion is rather conservative with this\nregard, as it classifies some surfaces as unrecoverable, although they\ncontribute to the signal.\n  In this paper, we use a simpler necessary criterion for an opaque surface\npatch to be recoverable. Such piece of surface must be directly visible from\nsome point on the wall, and it must occlude the space behind itself. Inspired\nby recent advances in neural implicit representations, we devise a new\nrepresentation and reconstruction technique for NLoS scenes that unifies the\ntreatment of recoverability with the reconstruction itself. Our approach, which\nwe validate on various synthetic and experimental datasets, exhibits\ninteresting properties. Unlike memory-inefficient volumetric representations,\nours allows to infer adaptively tessellated surfaces from time-of-flight\nmeasurements of moderate resolution. It can further recover features beyond the\nFermat path criterion, and it is robust to significant amounts of\nself-occlusion. We believe that this is the first time that these properties\nhave been achieved in one system that, as an additional benefit, is trainable\nand hence suited for data-driven approaches.\n","authors":["Javier Grau","Markus Plack","Patrick Haehn","Michael Weinmann","Matthias Hullin"],"pdf_url":"https://arxiv.org/pdf/2203.08657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11556v1","updated":"2022-03-22T09:22:18Z","published":"2022-03-22T09:22:18Z","title":"VQ-Flows: Vector Quantized Local Normalizing Flows","summary":"  Normalizing flows provide an elegant approach to generative modeling that\nallows for efficient sampling and exact density evaluation of unknown data\ndistributions. However, current techniques have significant limitations in\ntheir expressivity when the data distribution is supported on a low-dimensional\nmanifold or has a non-trivial topology. We introduce a novel statistical\nframework for learning a mixture of local normalizing flows as \"chart maps\"\nover the data manifold. Our framework augments the expressivity of recent\napproaches while preserving the signature property of normalizing flows, that\nthey admit exact density evaluation. We learn a suitable atlas of charts for\nthe data manifold via a vector quantized auto-encoder (VQ-AE) and the\ndistributions over them using a conditional flow. We validate experimentally\nthat our probabilistic framework enables existing approaches to better model\ndata distributions over complex manifolds.\n","authors":["Sahil Sidheekh","Chris B. Dock","Tushar Jain","Radu Balan","Maneesh K. Singh"],"pdf_url":"https://arxiv.org/pdf/2203.11556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11555v1","updated":"2022-03-22T09:21:14Z","published":"2022-03-22T09:21:14Z","title":"Gradient flows and randomised thresholding: sparse inversion and\n  classification","summary":"  Sparse inversion and classification problems are ubiquitous in modern data\nscience and imaging. They are often formulated as non-smooth minimisation\nproblems. In sparse inversion, we minimise, e.g., the sum of a data fidelity\nterm and an L1/LASSO regulariser. In classification, we consider, e.g., the sum\nof a data fidelity term and a non-smooth Ginzburg--Landau energy. Standard\n(sub)gradient descent methods have shown to be inefficient when approaching\nsuch problems. Splitting techniques are much more useful: here, the target\nfunction is partitioned into a sum of two subtarget functions -- each of which\ncan be efficiently optimised. Splitting proceeds by performing optimisation\nsteps alternately with respect to each of the two subtarget functions.\n  In this work, we study splitting from a stochastic continuous-time\nperspective. Indeed, we define a differential inclusion that follows one of the\ntwo subtarget function's negative subgradient at each point in time. The choice\nof the subtarget function is controlled by a binary continuous-time Markov\nprocess. The resulting dynamical system is a stochastic approximation of the\nunderlying subgradient flow. We investigate this stochastic approximation for\nan L1-regularised sparse inversion flow and for a discrete Allen-Cahn equation\nminimising a Ginzburg--Landau energy. In both cases, we study the longtime\nbehaviour of the stochastic dynamical system and its ability to approximate the\nunderlying subgradient flow at any accuracy. We illustrate our theoretical\nfindings in a simple sparse estimation problem and also in a low-dimensional\nclassification problem.\n","authors":["Jonas Latz"],"pdf_url":"https://arxiv.org/pdf/2203.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11552v1","updated":"2022-03-22T09:15:53Z","published":"2022-03-22T09:15:53Z","title":"Factual Consistency of Multilingual Pretrained Language Models","summary":"  Pretrained language models can be queried for factual knowledge, with\npotential applications in knowledge base acquisition and tasks that require\ninference. However, for that, we need to know how reliable this knowledge is,\nand recent work has shown that monolingual English language models lack\nconsistency when predicting factual knowledge, that is, they fill-in-the-blank\ndifferently for paraphrases describing the same fact. In this paper, we extend\nthe analysis of consistency to a multilingual setting. We introduce a resource,\nmParaRel, and investigate (i) whether multilingual language models such as\nmBERT and XLM-R are more consistent than their monolingual counterparts; and\n(ii) if such models are equally consistent across languages. We find that mBERT\nis as inconsistent as English BERT in English paraphrases, but that both mBERT\nand XLM-R exhibit a high degree of inconsistency in English and even more so\nfor all the other 45 languages.\n","authors":["Constanza Fierro","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2203.11552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14747v2","updated":"2022-03-22T09:14:27Z","published":"2021-10-27T20:17:47Z","title":"Dynamic Review-based Recommenders","summary":"  Just as user preferences change with time, item reviews also reflect those\nsame preference changes. In a nutshell, if one is to sequentially incorporate\nreview content knowledge into recommender systems, one is naturally led to\ndynamical models of text. In the present work we leverage the known power of\nreviews to enhance rating predictions in a way that (i) respects the causality\nof review generation and (ii) includes, in a bidirectional fashion, the ability\nof ratings to inform language review models and vice-versa, language\nrepresentations that help predict ratings end-to-end. Moreover, our\nrepresentations are time-interval aware and thus yield a continuous-time\nrepresentation of the dynamics. We provide experiments on real-world datasets\nand show that our methodology is able to outperform several state-of-the-art\nmodels. Source code for all models can be found at [1].\n","authors":["Kostadin Cvejoski","Ramses J. Sanchez","Christian Bauckhage","Cesar Ojeda"],"pdf_url":"https://arxiv.org/pdf/2110.14747v2.pdf","comment":"6pages, Published at International Data Science Conference 2021\n  (iDSC21)"},{"id":"http://arxiv.org/abs/2102.11923v2","updated":"2022-03-22T09:10:59Z","published":"2021-02-22T14:26:34Z","title":"KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural\n  Networks with Non-Zero Training Loss","summary":"  Many physical phenomena are described by Hamiltonian mechanics using an\nenergy function (the Hamiltonian). Recently, the Hamiltonian neural network,\nwhich approximates the Hamiltonian as a neural network, and its extensions have\nattracted much attention. This is a very powerful method, but its use in\ntheoretical studies remains limited. In this study, by combining the\nstatistical learning theory and Kolmogorov-Arnold-Moser (KAM) theory, we\nprovide a theoretical analysis of the behavior of Hamiltonian neural networks\nwhen the learning error is not completely zero. A Hamiltonian neural network\nwith non-zero errors can be considered as a perturbation from the true\ndynamics, and the perturbation theory of the Hamilton equation is widely known\nas the KAM theory. To apply the KAM theory, we provide a generalization error\nbound for Hamiltonian neural networks by deriving an estimate of the covering\nnumber of the gradient of the multi-layer perceptron, which is the key\ningredient of the model. This error bound gives an $L^\\infty$ bound on the\nHamiltonian that is required in the application of the KAM theory.\n","authors":["Yuhan Chen","Takashi Matsubara","Takaharu Yaguchi"],"pdf_url":"https://arxiv.org/pdf/2102.11923v2.pdf","comment":"Accepted to the thirty-sixth AAAI conference on artificial\n  intelligence (AAAI-22) as an oral presentation"},{"id":"http://arxiv.org/abs/2203.11542v1","updated":"2022-03-22T08:50:41Z","published":"2022-03-22T08:50:41Z","title":"Mask Usage Recognition using Vision Transformer with Transfer Learning\n  and Data Augmentation","summary":"  The COVID-19 pandemic has disrupted various levels of society. The use of\nmasks is essential in preventing the spread of COVID-19 by identifying an image\nof a person using a mask. Although only 23.1% of people use masks correctly,\nArtificial Neural Networks (ANN) can help classify the use of good masks to\nhelp slow the spread of the Covid-19 virus. However, it requires a large\ndataset to train an ANN that can classify the use of masks correctly.\nMaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4\nclass labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth.\nMask classification training utilizes Vision Transformers (ViT) architecture\nwith transfer learning method using pre-trained weights on ImageNet-21k, with\nrandom augmentation. In addition, the hyper-parameters of training of 20\nepochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of\n0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation\nfunction, and a Cross-Entropy loss function are used to be applied on the\ntraining of three architectures of ViT, namely Base-16, Large-16, and Huge-14.\nFurthermore, comparisons of with and without augmentation and transfer learning\nare conducted. This study found that the best classification is transfer\nlearning and augmentation using ViT Huge-14. Using this method on\nMaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training\ndata, 0.9412 on validation data, and 0.9534 on test data. This research shows\nthat training the ViT model with data augmentation and transfer learning\nimproves classification of the mask usage, even better than convolutional-based\nResidual Network (ResNet).\n","authors":["Hensel Donato Jahja","Novanto Yudistira"," Sutrisno"],"pdf_url":"https://arxiv.org/pdf/2203.11542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11540v1","updated":"2022-03-22T08:46:11Z","published":"2022-03-22T08:46:11Z","title":"Scale-out Systolic Arrays","summary":"  Multi-pod systolic arrays are emerging as the architecture of choice in DNN\ninference accelerators. Despite their potential, designing multi-pod systolic\narrays to maximize effective throughput/Watt (i.e., throughput/Watt adjusted\nwhen accounting for array utilization) poses a unique set of challenges. In\nthis work, we study three key pillars in multi-pod systolic array designs,\nnamely array granularity, interconnect, and tiling. We identify optimal array\ngranularity across workloads and show that state-of-the-art commercial\naccelerators use suboptimal array sizes for single-tenancy workloads. We, then\nevaluate the bandwidth/latency trade-offs in interconnects and show that\nButterfly networks offer a scalable topology for accelerators with a large\nnumber of pods. Finally, we introduce a novel data tiling scheme with custom\npartition size to maximize utilization in optimally sized pods. We propose\nScale-out Systolic Arrays, a multi-pod inference accelerator for both single-\nand multi-tenancy based on these three pillars. We show that SOSA exhibits\nscaling of up to 600 TeraOps/s in effective throughput for state-of-the-art DNN\ninference workloads, and outperforms state-of-the-art multi-pod accelerators by\na factor of 1.5x.\n","authors":["Ahmet Caner Yüzügüler","Canberk Sönmez","Mario Drumond","Yunho Oh","Babak Falsafi","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2203.11540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.07513v3","updated":"2022-03-22T08:35:55Z","published":"2021-11-15T03:20:23Z","title":"A Comparative Study on Basic Elements of Deep Learning Models for\n  Spatial-Temporal Traffic Forecasting","summary":"  Traffic forecasting plays a crucial role in intelligent transportation\nsystems. The spatial-temporal complexities in transportation networks make the\nproblem especially challenging. The recently suggested deep learning models\nshare basic elements such as graph convolution, graph attention, recurrent\nunits, and/or attention mechanism. In this study, we designed an in-depth\ncomparative study for four deep neural network models utilizing different basic\nelements. For base models, one RNN-based model and one attention-based model\nwere chosen from previous literature. Then, the spatial feature extraction\nlayers in the models were substituted with graph convolution and graph\nattention. To analyze the performance of each element in various environments,\nwe conducted experiments on four real-world datasets - highway speed, highway\nflow, urban speed from a homogeneous road link network, and urban speed from a\nheterogeneous road link network. The results demonstrate that the RNN-based\nmodel and the attention-based model show a similar level of performance for\nshort-term prediction, and the attention-based model outperforms the RNN in\nlonger-term predictions. The choice of graph convolution and graph attention\nmakes a larger difference in the RNN-based models. Also, our modified version\nof GMAN shows comparable performance with the original with less memory\nconsumption.\n","authors":["Yuyol Shin","Yoonjin Yoon"],"pdf_url":"https://arxiv.org/pdf/2111.07513v3.pdf","comment":"14 pages, 4 figures, 3 Tables, This paper is accepted for AAAI-22\n  Workshop: AI for Transportation"},{"id":"http://arxiv.org/abs/2112.14012v2","updated":"2022-03-22T08:32:13Z","published":"2021-12-28T06:39:14Z","title":"Solving time dependent Fokker-Planck equations via temporal normalizing\n  flow","summary":"  In this work, we propose an adaptive learning approach based on temporal\nnormalizing flows for solving time-dependent Fokker-Planck (TFP) equations. It\nis well known that solutions of such equations are probability density\nfunctions, and thus our approach relies on modelling the target solutions with\nthe temporal normalizing flows. The temporal normalizing flow is then trained\nbased on the TFP loss function, without requiring any labeled data. Being a\nmachine learning scheme, the proposed approach is mesh-free and can be easily\napplied to high dimensional problems. We present a variety of test problems to\nshow the effectiveness of the learning approach.\n","authors":["Xiaodong Feng","Li Zeng","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2112.14012v2.pdf","comment":"16pages"},{"id":"http://arxiv.org/abs/2203.11528v1","updated":"2022-03-22T08:04:38Z","published":"2022-03-22T08:04:38Z","title":"Out-of-distribution Generalization with Causal Invariant Transformations","summary":"  In real-world applications, it is important and desirable to learn a model\nthat performs well on out-of-distribution (OOD) data. Recently, causality has\nbecome a powerful tool to tackle the OOD generalization problem, with the idea\nresting on the causal mechanism that is invariant across domains of interest.\nTo leverage the generally unknown causal mechanism, existing works assume a\nlinear form of causal feature or require sufficiently many and diverse training\ndomains, which are usually restrictive in practice. In this work, we obviate\nthese assumptions and tackle the OOD problem without explicitly recovering the\ncausal feature. Our approach is based on transformations that modify the\nnon-causal feature but leave the causal part unchanged, which can be either\nobtained from prior knowledge or learned from the training data in the\nmulti-domain scenario. Under the setting of invariant causal mechanism, we\ntheoretically show that if all such transformations are available, then we can\nlearn a minimax optimal model across the domains using only single domain data.\nNoticing that knowing a complete set of these causal invariant transformations\nmay be impractical, we further show that it suffices to know only a subset of\nthese transformations. Based on the theoretical findings, a regularized\ntraining procedure is proposed to improve the OOD generalization capability.\nExtensive experimental results on both synthetic and real datasets verify the\neffectiveness of the proposed algorithm, even with only a few causal invariant\ntransformations.\n","authors":["Ruoyu Wang","Mingyang Yi","Zhitang Chen","Shengyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.11528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11526v1","updated":"2022-03-22T07:58:40Z","published":"2022-03-22T07:58:40Z","title":"Action Candidate Driven Clipped Double Q-learning for Discrete and\n  Continuous Action Tasks","summary":"  Double Q-learning is a popular reinforcement learning algorithm in Markov\ndecision process (MDP) problems. Clipped Double Q-learning, as an effective\nvariant of Double Q-learning, employs the clipped double estimator to\napproximate the maximum expected action value. Due to the underestimation bias\nof the clipped double estimator, the performance of clipped Double Q-learning\nmay be degraded in some stochastic environments. In this paper, in order to\nreduce the underestimation bias, we propose an action candidate-based clipped\ndouble estimator for Double Q-learning. Specifically, we first select a set of\nelite action candidates with high action values from one set of estimators.\nThen, among these candidates, we choose the highest valued action from the\nother set of estimators. Finally, we use the maximum value in the second set of\nestimators to clip the action value of the chosen action in the first set of\nestimators and the clipped value is used for approximating the maximum expected\naction value. Theoretically, the underestimation bias in our clipped Double\nQ-learning decays monotonically as the number of action candidates decreases.\nMoreover, the number of action candidates controls the trade-off between the\noverestimation and underestimation biases. In addition, we also extend our\nclipped Double Q-learning to continuous action tasks via approximating the\nelite continuous action candidates. We empirically verify that our algorithm\ncan more accurately estimate the maximum expected action value on some toy\nenvironments and yield good performance on several benchmark problems.\n","authors":["Haobo Jiang","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2203.11526v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2105.00704"},{"id":"http://arxiv.org/abs/2203.09204v2","updated":"2022-03-22T07:39:45Z","published":"2022-03-17T09:54:22Z","title":"Investigation of Physics-Informed Deep Learning for the Prediction of\n  Parametric, Three-Dimensional Flow Based on Boundary Data","summary":"  The placement of temperature sensitive and safety-critical components is\ncrucial in the automotive industry. It is therefore inevitable, even at the\ndesign stage of new vehicles that these components are assessed for potential\nsafety issues. However, with increasing number of design proposals, risk\nassessment quickly becomes expensive. We therefore present a parameterized\nsurrogate model for the prediction of three-dimensional flow fields in\naerothermal vehicle simulations. The proposed physics-informed neural network\n(PINN) design is aimed at learning families of flow solutions according to a\ngeometric variation. In scope of this work, we could show that our\nnondimensional, multivariate scheme can be efficiently trained to predict the\nvelocity and pressure distribution for different design scenarios and geometric\nscales. The proposed algorithm is based on a parametric minibatch training\nwhich enables the utilization of large datasets necessary for the\nthree-dimensional flow modeling. Further, we introduce a continuous resampling\nalgorithm that allows to operate on one static dataset. Every feature of our\nmethodology is tested individually and verified against conventional CFD\nsimulations. Finally, we apply our proposed method in context of an exemplary\nreal-world automotive application.\n","authors":["Philip Heger","Markus Full","Daniel Hilger","Norbert Hosters"],"pdf_url":"https://arxiv.org/pdf/2203.09204v2.pdf","comment":"Reference to code and dataset are DOIs.The DOIs will be activated\n  when article is reviewed. Until then please contact Philip Heger or Daniel\n  Hilger if you wish for code and datasets"},{"id":"http://arxiv.org/abs/2109.09974v2","updated":"2022-03-22T07:29:34Z","published":"2021-09-21T05:54:28Z","title":"Adaptive Control of SE(3) Hamiltonian Dynamics with Learned Disturbance\n  Features","summary":"  Adaptive control is a critical component of reliable robot autonomy in\nrapidly changing operational conditions. Adaptive control designs benefit from\na disturbance model, which is often unavailable in practice. This motivates the\nuse of machine learning techniques to learn disturbance features from training\ndata offline, which can subsequently be employed to compensate the disturbances\nonline. This paper develops geometric adaptive control with a learned\ndisturbance model for rigid-body systems, such as ground, aerial, and\nunderwater vehicles, that satisfy Hamilton's equations of motion over the\n$SE(3)$ manifold. Our design consists of an \\emph{offline disturbance model\nidentification stage}, using a Hamiltonian-based neural ordinary differential\nequation (ODE) network trained from state-control trajectory data, and an\n\\emph{online adaptive control stage}, estimating and compensating the\ndisturbances based on geometric tracking errors. We demonstrate our adaptive\ngeometric controller in trajectory tracking simulations of fully-actuated\npendulum and under-actuated quadrotor systems.\n","authors":["Thai Duong","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2109.09974v2.pdf","comment":"Project website: https://thaipduong.github.io/hamadapt/"},{"id":"http://arxiv.org/abs/2203.11499v1","updated":"2022-03-22T07:19:58Z","published":"2022-03-22T07:19:58Z","title":"Residual-Guided Non-Intrusive Speech Quality Assessment","summary":"  This paper proposes an approach to improve Non-Intrusive speech quality\nassessment(NI-SQA) based on the residuals between impaired speech and enhanced\nspeech. The difficulty in our task is particularly lack of information, for\nwhich the corresponding reference speech is absent. We generate an enhanced\nspeech on the impaired speech to compensate for the absence of the reference\naudio, then pair the information of residuals with the impaired speech.\nCompared to feeding the impaired speech directly into the model, residuals\ncould bring some extra helpful information from the contrast in enhancement.\nThe human ear is sensitive to certain noises but different to deep learning\nmodel. Causing the Mean Opinion Score(MOS) the model predicted is not enough to\nfit our subjective sensitive well and causes deviation. These residuals have a\nclose relationship to reference speech and then improve the ability of the deep\nlearning models to predict MOS. During the training phase, experimental results\ndemonstrate that paired with residuals can quickly obtain better evaluation\nindicators under the same conditions. Furthermore, our final results improved\n31.3 percent and 14.1 percent, respectively, in PLCC and RMSE.\n","authors":["Zhe Ye","Jiahao Chen","Diqun Yan"],"pdf_url":"https://arxiv.org/pdf/2203.11499v1.pdf","comment":"Submitted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2203.11492v1","updated":"2022-03-22T07:03:08Z","published":"2022-03-22T07:03:08Z","title":"Exploring High-Order Structure for Robust Graph Structure Learning","summary":"  Recent studies show that Graph Neural Networks (GNNs) are vulnerable to\nadversarial attack, i.e., an imperceptible structure perturbation can fool GNNs\nto make wrong predictions. Some researches explore specific properties of clean\ngraphs such as the feature smoothness to defense the attack, but the analysis\nof it has not been well-studied. In this paper, we analyze the adversarial\nattack on graphs from the perspective of feature smoothness which further\ncontributes to an efficient new adversarial defensive algorithm for GNNs. We\ndiscover that the effect of the high-order graph structure is a smoother filter\nfor processing graph structures. Intuitively, the high-order graph structure\ndenotes the path number between nodes, where larger number indicates closer\nconnection, so it naturally contributes to defense the adversarial\nperturbation. Further, we propose a novel algorithm that incorporates the\nhigh-order structural information into the graph structure learning. We perform\nexperiments on three popular benchmark datasets, Cora, Citeseer and Polblogs.\nExtensive experiments demonstrate the effectiveness of our method for defending\nagainst graph adversarial attacks.\n","authors":["Guangqian Yang","Yibing Zhan","Jinlong Li","Baosheng Yu","Liu Liu","Fengxiang He"],"pdf_url":"https://arxiv.org/pdf/2203.11492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11491v1","updated":"2022-03-22T06:56:06Z","published":"2022-03-22T06:56:06Z","title":"Making Recommender Systems Forget: Learning and Unlearning for Erasable\n  Recommendation","summary":"  Privacy laws and regulations enforce data-driven systems, e.g., recommender\nsystems, to erase the data that concern individuals. As machine learning models\npotentially memorize the training data, data erasure should also unlearn the\ndata lineage in models, which raises increasing interest in the problem of\nMachine Unlearning (MU). However, existing MU methods cannot be directly\napplied into recommendation. The basic idea of most recommender systems is\ncollaborative filtering, but existing MU methods ignore the collaborative\ninformation across users and items. In this paper, we propose a general\nerasable recommendation framework, namely LASER, which consists of Group module\nand SeqTrain module. Firstly, Group module partitions users into balanced\ngroups based on their similarity of collaborative embedding learned via\nhypergraph. Then SeqTrain module trains the model sequentially on all groups\nwith curriculum learning. Both theoretical analysis and experiments on two\nreal-world datasets demonstrate that LASER can not only achieve efficient\nunlearning, but also outperform the state-of-the-art unlearning framework in\nterms of model utility.\n","authors":["Yuyuan Li","Xiaolin Zheng","Chaochao Chen","Junlin Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11489v1","updated":"2022-03-22T06:53:42Z","published":"2022-03-22T06:53:42Z","title":"A Note on Target Q-learning For Solving Finite MDPs with A Generative\n  Oracle","summary":"  Q-learning with function approximation could diverge in the off-policy\nsetting and the target network is a powerful technique to address this issue.\nIn this manuscript, we examine the sample complexity of the associated target\nQ-learning algorithm in the tabular case with a generative oracle. We point out\na misleading claim in [Lee and He, 2020] and establish a tight analysis. In\nparticular, we demonstrate that the sample complexity of the target Q-learning\nalgorithm in [Lee and He, 2020] is $\\widetilde{\\mathcal O}(|\\mathcal\nS|^2|\\mathcal A|^2 (1-\\gamma)^{-5}\\varepsilon^{-2})$. Furthermore, we show that\nthis sample complexity is improved to $\\widetilde{\\mathcal O}(|\\mathcal\nS||\\mathcal A| (1-\\gamma)^{-5}\\varepsilon^{-2})$ if we can sequentially update\nall state-action pairs and $\\widetilde{\\mathcal O}(|\\mathcal S||\\mathcal A|\n(1-\\gamma)^{-4}\\varepsilon^{-2})$ if $\\gamma$ is further in $(1/2, 1)$.\nCompared with the vanilla Q-learning, our results conclude that the\nintroduction of a periodically-frozen target Q-function does not sacrifice the\nsample complexity.\n","authors":["Ziniu Li","Tian Xu","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2203.11489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11486v1","updated":"2022-03-22T06:33:01Z","published":"2022-03-22T06:33:01Z","title":"Approaches for Improving the Performance of Fake News Detection in\n  Bangla: Imbalance Handling and Model Stacking","summary":"  Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.\n","authors":["Md Muzakker Hossain","Zahin Awosaf","Md. Salman Hossan Prottoy","Abu Saleh Muhammod Alvy","Md. Kishor Morol"],"pdf_url":"https://arxiv.org/pdf/2203.11486v1.pdf","comment":"12 pages, 8 figures, To appear in the Proceedings of the\n  International Conference on 4th Industrial Revolution and Beyond (IC4IR),\n  10-11 December 2021, Dhaka, Bangladesh"},{"id":"http://arxiv.org/abs/2203.11473v1","updated":"2022-03-22T05:58:44Z","published":"2022-03-22T05:58:44Z","title":"Federated Class-Incremental Learning","summary":"  Federated learning (FL) has attracted growing attention via data-private\ncollaborative training on decentralized clients. However, most existing methods\nunrealistically assume object classes of the overall framework are fixed over\ntime. It makes the global model suffer from significant catastrophic forgetting\non old classes in real-world scenarios, where local clients often collect new\nclasses continuously and have very limited storage memory to store old classes.\nMoreover, new clients with unseen new classes may participate in the FL\ntraining, further aggravating the catastrophic forgetting of the global model.\nTo address these challenges, we develop a novel Global-Local Forgetting\nCompensation (GLFC) model, to learn a global class incremental model for\nalleviating the catastrophic forgetting from both local and global\nperspectives. Specifically, to address local forgetting caused by class\nimbalance at the local clients, we design a class-aware gradient compensation\nloss and a class-semantic relation distillation loss to balance the forgetting\nof old classes and distill consistent inter-class relations across tasks. To\ntackle the global forgetting brought by the non-i.i.d class imbalance across\nclients, we propose a proxy server that selects the best old global model to\nassist the local relation distillation. Moreover, a prototype gradient-based\ncommunication mechanism is developed to protect privacy. Our model outperforms\nstate-of-the-art methods by 4.4%-15.1% in terms of average accuracy on\nrepresentative benchmark datasets.\n","authors":["Jiahua Dong","Lixu Wang","Zhen Fang","Gan Sun","Shichao Xu","Xiao Wang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.11473v1.pdf","comment":"CVPR 2022, the first two authors contribute equally and they are\n  ordered alphabetically"},{"id":"http://arxiv.org/abs/2203.11472v1","updated":"2022-03-22T05:42:46Z","published":"2022-03-22T05:42:46Z","title":"BigBird: Big Data Storage and Analytics at Scale in Hybrid Cloud","summary":"  Implementing big data storage at scale is a complex and arduous task that\nrequires an advanced infrastructure. With the rise of public cloud computing,\nvarious big data management services can be readily leveraged. As a critical\npart of Twitter's \"Project Partly Cloudy\", the cold storage data and analytics\nsystems are being moved to the public cloud. This paper showcases our approach\nin designing a scalable big data storage and analytics management framework\nusing BigQuery in Google Cloud Platform while ensuring security, privacy, and\ndata protection. The paper also discusses the limitations on the public cloud\nresources and how they can be effectively overcome when designing a big data\nstorage and analytics solution at scale. Although the paper discusses the\nframework implementation in Google Cloud Platform, it can easily be applied to\nall major cloud providers.\n","authors":["Saurabh Deochake","Vrushali Channapattan","Gary Steelman"],"pdf_url":"https://arxiv.org/pdf/2203.11472v1.pdf","comment":"Journal of Big Data"},{"id":"http://arxiv.org/abs/2111.11133v9","updated":"2022-03-22T05:02:36Z","published":"2021-11-22T11:48:26Z","title":"L-Verse: Bidirectional Generation Between Image and Text","summary":"  Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n","authors":["Taehoon Kim","Gwangmo Song","Sihaeng Lee","Sangyun Kim","Yewon Seo","Soonyoung Lee","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae"],"pdf_url":"https://arxiv.org/pdf/2111.11133v9.pdf","comment":"Accepted to CVPR 2022 (Oral)"},{"id":"http://arxiv.org/abs/2105.02446v6","updated":"2022-03-22T04:53:48Z","published":"2021-05-06T05:21:42Z","title":"DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism","summary":"  Singing voice synthesis (SVS) systems are built to synthesize high-quality\nand expressive singing voice, in which the acoustic model generates the\nacoustic features (e.g., mel-spectrogram) given a music score. Previous singing\nacoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial\nnetwork (GAN) to reconstruct the acoustic features, while they suffer from\nover-smoothing and unstable training issues respectively, which hinder the\nnaturalness of synthesized singing. In this work, we propose DiffSinger, an\nacoustic model for SVS based on the diffusion probabilistic model. DiffSinger\nis a parameterized Markov chain that iteratively converts the noise into\nmel-spectrogram conditioned on the music score. By implicitly optimizing\nvariational bound, DiffSinger can be stably trained and generate realistic\noutputs. To further improve the voice quality and speed up inference, we\nintroduce a shallow diffusion mechanism to make better use of the prior\nknowledge learned by the simple loss. Specifically, DiffSinger starts\ngeneration at a shallow step smaller than the total number of diffusion steps,\naccording to the intersection of the diffusion trajectories of the ground-truth\nmel-spectrogram and the one predicted by a simple mel-spectrogram decoder.\nBesides, we propose boundary prediction methods to locate the intersection and\ndetermine the shallow step adaptively. The evaluations conducted on a Chinese\nsinging dataset demonstrate that DiffSinger outperforms state-of-the-art SVS\nwork. Extensional experiments also prove the generalization of our methods on\ntext-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io.\nCodes: https://github.com/MoonInTheRiver/DiffSinger. The old title of this\nwork: \"Diffsinger: Diffusion acoustic model for singing voice synthesis\".\n","authors":["Jinglin Liu","Chengxi Li","Yi Ren","Feiyang Chen","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2105.02446v6.pdf","comment":"SVS (DiffSinger), TTS (DiffSpeech), Shallow Diffusion Mechanism;\n  Submitted to arxiv on 6 May 2021; Accepted by AAAI 2022"},{"id":"http://arxiv.org/abs/2203.11458v1","updated":"2022-03-22T04:50:16Z","published":"2022-03-22T04:50:16Z","title":"Hierarchical Graph Representation Learning for the Prediction of\n  Drug-Target Binding Affinity","summary":"  The identification of drug-target binding affinity (DTA) has attracted\nincreasing attention in the drug discovery process due to the more specific\ninterpretation than binary interaction prediction. Recently, numerous deep\nlearning-based computational methods have been proposed to predict the binding\naffinities between drugs and targets benefiting from their satisfactory\nperformance. However, the previous works mainly focus on encoding biological\nfeatures and chemical structures of drugs and targets, with a lack of\nexploiting the essential topological information from the drug-target affinity\nnetwork. In this paper, we propose a novel hierarchical graph representation\nlearning model for the drug-target binding affinity prediction, namely\nHGRL-DTA. The main contribution of our model is to establish a hierarchical\ngraph learning architecture to incorporate the intrinsic properties of\ndrug/target molecules and the topological affinities of drug-target pairs. In\nthis architecture, we adopt a message broadcasting mechanism to integrate the\nhierarchical representations learned from the global-level affinity graph and\nthe local-level molecular graph. Besides, we design a similarity-based\nembedding map to solve the cold start problem of inferring representations for\nunseen drugs and targets. Comprehensive experimental results under different\nscenarios indicate that HGRL-DTA significantly outperforms the state-of-the-art\nmodels and shows better model generalization among all the scenarios.\n","authors":["Zhaoyang Chu","Shichao Liu","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10769v2","updated":"2022-03-22T03:55:30Z","published":"2022-03-21T07:20:41Z","title":"ASE: Anomaly Scoring Based Ensemble Learning for Imbalanced Datasets","summary":"  Nowadays, many industries have applied classification algorithms to help them\nsolve problems in their business, like finance, medicine, manufacturing\nindustry and so on. However, in real-life scenarios, positive examples only\nmake up a small part of all instances and our datasets suffer from high\nimbalance ratio which leads to poor performance of existing classification\nmodels. To solve this problem, we come up with a bagging ensemble learning\nframework based on an anomaly detection scoring system. We test out that our\nensemble learning model can dramatically improve performance of base estimators\n(e.g. Decision Tree, Multilayer perceptron, KNN) and is more efficient than\nother existing methods under a wide range of imbalance ratio, data scale and\ndata dimension.\n","authors":["Xiayu Liang","Ying Gao","Shanrong Xu"],"pdf_url":"https://arxiv.org/pdf/2203.10769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11444v1","updated":"2022-03-22T03:50:04Z","published":"2022-03-22T03:50:04Z","title":"Root-aligned SMILES for Molecular Retrosynthesis Prediction","summary":"  Retrosynthesis prediction is a fundamental problem in organic synthesis,\nwhere the task is to discover precursor molecules that can be used to\nsynthesize a target molecule. A popular paradigm of existing computational\nretrosynthesis methods formulate retrosynthesis prediction as a\nsequence-to-sequence translation problem, where the typical SMILES\nrepresentations are adopted for both reactants and products. However, the\ngeneral-purpose SMILES neglects the characteristics of retrosynthesis that 1)\nthe search space of the reactants is quite huge, and 2) the molecular graph\ntopology is largely unaltered from products to reactants, resulting in the\nsuboptimal performance of SMILES if straightforwardly applied. In this article,\nwe propose the root-aligned SMILES~(R-SMILES), which specifies a tightly\naligned one-to-one mapping between the product and the reactant SMILES, to\nnarrow the string representation discrepancy for more efficient retrosynthesis.\nAs the minimum edit distance between the input and the output is significantly\ndecreased with the proposed R-SMILES, the computational model is largely\nrelieved from learning the complex syntax and dedicated to learning the\nchemical knowledge for retrosynthesis. We compare the proposed R-SMILES with\nvarious state-of-the-art baselines on different benchmarks and show that it\nsignificantly outperforms them all, demonstrating the superiority of the\nproposed method.\n","authors":["Zipeng Zhong","Jie Song","Zunlei Feng","Tiantao Liu","Lingxiang Jia","Shaolun Liu","Min Wu","Tingjun Hou","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2203.11444v1.pdf","comment":"Main paper: 15 pages, 5 figures, and 1 table; supplementary\n  information: 4 pages, 2 figures and 3 tables"},{"id":"http://arxiv.org/abs/2104.12576v2","updated":"2022-03-22T03:39:22Z","published":"2021-04-23T03:05:11Z","title":"A Splicing Approach to Best Subset of Groups Selection","summary":"  Best subset of groups selection (BSGS) is the process of selecting a small\npart of non-overlapping groups to achieve the best interpretability on the\nresponse variable. It has attracted increasing attention and has far-reaching\napplications in practice. However, due to the computational intractability of\nBSGS in high-dimensional settings, developing efficient algorithms for solving\nBSGS remains a research hotspot. In this paper,we propose a group-splicing\nalgorithm that iteratively detects the relevant groups and excludes the\nirrelevant ones. Moreover, coupled with a novel group information criterion, we\ndevelop an adaptive algorithm to determine the optimal model size. Under mild\nconditions, it is certifiable that our algorithm can identify the optimal\nsubset of groups in polynomial time with high probability. Finally, we\ndemonstrate the efficiency and accuracy of our methods by comparing them with\nseveral state-of-the-art algorithms on both synthetic and real-world datasets.\n","authors":["Yanhang Zhang","Junxian Zhu","Jin Zhu","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2104.12576v2.pdf","comment":"49 pages, 7 figures"},{"id":"http://arxiv.org/abs/2202.02652v2","updated":"2022-03-22T03:30:59Z","published":"2022-02-05T22:48:16Z","title":"A Graph Neural Network Framework for Grid-Based Simulation","summary":"  Reservoir simulations are computationally expensive in the well control and\nwell placement optimization. Generally, numerous simulation runs (realizations)\nare needed in order to achieve the optimal well locations. In this paper, we\npropose a graph neural network (GNN) framework to build a surrogate\nfeed-forward model which replaces simulation runs to accelerate the\noptimization process. Our GNN framework includes an encoder, a process, and a\ndecoder which takes input from the processed graph data designed and generated\nfrom the simulation raw data. We train the GNN model with 6000 samples\n(equivalent to 40 well configurations) with each containing the previous step\nstate variable and the next step state variable. We test the GNN model with\nanother 6000 samples and after model tuning, both one-step prediction and\nrollout prediction achieve a close match with the simulation results. Our GNN\nframework shows great potential in the application of well-related subsurface\noptimization including oil and gas as well as carbon capture sequestration\n(CCS).\n","authors":["Haoyu Tang","Wennan Long"],"pdf_url":"https://arxiv.org/pdf/2202.02652v2.pdf","comment":"There are conflict of interests and I need to modify the paper before\n  resubmitting"}],"Multimedia":[{"id":"http://arxiv.org/abs/2203.11832v1","updated":"2022-03-22T15:59:44Z","published":"2022-03-22T15:59:44Z","title":"Cross-View Panorama Image Synthesis","summary":"  In this paper, we tackle the problem of synthesizing a ground-view panorama\nimage conditioned on a top-view aerial image, which is a challenging problem\ndue to the large gap between the two image domains with different view-points.\nInstead of learning cross-view mapping in a feedforward pass, we propose a\nnovel adversarial feedback GAN framework named PanoGAN with two key components:\nan adversarial feedback module and a dual branch discrimination strategy.\nFirst, the aerial image is fed into the generator to produce a target panorama\nimage and its associated segmentation map in favor of model training with\nlayout semantics. Second, the feature responses of the discriminator encoded by\nour adversarial feedback module are fed back to the generator to refine the\nintermediate representations, so that the generation performance is continually\nimproved through an iterative generation process. Third, to pursue\nhigh-fidelity and semantic consistency of the generated panorama image, we\npropose a pixel-segmentation alignment mechanism under the dual branch\ndiscrimiantion strategy to facilitate cooperation between the generator and the\ndiscriminator. Extensive experimental results on two challenging cross-view\nimage datasets show that PanoGAN enables high-quality panorama image generation\nwith more convincing details than state-of-the-art approaches. The source code\nand trained models are available at \\url{https://github.com/sswuai/PanoGAN}.\n","authors":["Songsong Wu","Hao Tang","Xiao-Yuan Jing","Haifeng Zhao","Jianjun Qian","Nicu Sebe","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2203.11832v1.pdf","comment":"Accepted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2203.11686v1","updated":"2022-03-22T13:01:59Z","published":"2022-03-22T13:01:59Z","title":"End-to-End Learned Block-Based Image Compression with Block-Level Masked\n  Convolutions and Asymptotic Closed Loop Training","summary":"  Learned image compression research has achieved state-of-the-art compression\nperformance with auto-encoder based neural network architectures, where the\nimage is mapped via convolutional neural networks (CNN) into a latent\nrepresentation that is quantized and processed again with CNN to obtain the\nreconstructed image. CNN operate on entire input images. On the other hand,\ntraditional state-of-the-art image and video compression methods process images\nwith a block-by-block processing approach for various reasons. Very recently,\nwork on learned image compression with block based approaches have also\nappeared, which use the auto-encoder architecture on large blocks of the input\nimage and introduce additional neural networks that perform intra/spatial\nprediction and deblocking/post-processing functions. This paper explores an\nalternative learned block-based image compression approach in which neither an\nexplicit intra prediction neural network nor an explicit deblocking neural\nnetwork is used. A single auto-encoder neural network with block-level masked\nconvolutions is used and the block size is much smaller (8x8). By using\nblock-level masked convolutions, each block is processed using reconstructed\nneighboring left and upper blocks both at the encoder and decoder. Hence, the\nmutual information between adjacent blocks is exploited during compression and\neach block is reconstructed using neighboring blocks, resolving the need for\nexplicit intra prediction and deblocking neural networks. Since the explored\nsystem is a closed loop system, a special optimization procedure, the\nasymptotic closed loop design, is used with standard stochastic gradient\ndescent based training. The experimental results indicate competitive image\ncompression performance.\n","authors":["Fatih Kamisli"],"pdf_url":"https://arxiv.org/pdf/2203.11686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1709.00944v3","updated":"2022-03-22T09:26:16Z","published":"2017-09-01T14:17:53Z","title":"Audio-Visual Speech Enhancement using Multimodal Deep Convolutional\n  Neural Network","summary":"  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus on addressing audio information only. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. In the proposed AVDCNN SE model, audio\nand visual data are first processed using individual CNNs, and then, fused into\na joint network to generate enhanced speech at the output layer. The AVDCNN\nmodel is trained in an end-to-end manner, and parameters are jointly learned\nthrough back-propagation. We evaluate enhanced speech using five objective\ncriteria. Results show that the AVDCNN yields notably better performance,\ncompared with an audio-only CNN-based SE model and two conventional SE\napproaches, confirming the effectiveness of integrating visual information into\nthe SE process.\n","authors":["Jen-Cheng Hou","Syu-Siang Wang","Ying-Hui Lai","Yu Tsao","Hsiu-Wen Chang","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/1709.00944v3.pdf","comment":"This paper is the same as arXiv:1703.10893v2. Apologies for the\n  inconvenience"},{"id":"http://arxiv.org/abs/2203.11493v1","updated":"2022-03-22T07:05:57Z","published":"2022-03-22T07:05:57Z","title":"FrameHopper: Selective Processing of Video Frames in Detection-driven\n  Real-Time Video Analytics","summary":"  Detection-driven real-time video analytics require continuous detection of\nobjects contained in the video frames using deep learning models like YOLOV3,\nEfficientDet. However, running these detectors on each and every frame in\nresource-constrained edge devices is computationally intensive. By taking the\ntemporal correlation between consecutive video frames into account, we note\nthat detection outputs tend to be overlapping in successive frames. Elimination\nof similar consecutive frames will lead to a negligible drop in performance\nwhile offering significant performance benefits by reducing overall computation\nand communication costs. The key technical questions are, therefore, (a) how to\nidentify which frames to be processed by the object detector, and (b) how many\nsuccessive frames can be skipped (called skip-length) once a frame is selected\nto be processed. The overall goal of the process is to keep the error due to\nskipping frames as small as possible. We introduce a novel error vs processing\nrate optimization problem with respect to the object detection task that\nbalances between the error rate and the fraction of frames filtering.\nSubsequently, we propose an off-line Reinforcement Learning (RL)-based\nalgorithm to determine these skip-lengths as a state-action policy of the RL\nagent from a recorded video and then deploy the agent online for live video\nstreams. To this end, we develop FrameHopper, an edge-cloud collaborative video\nanalytics framework, that runs a lightweight trained RL agent on the camera and\npasses filtered frames to the server where the object detection model runs for\na set of applications. We have tested our approach on a number of live videos\ncaptured from real-life scenarios and show that FrameHopper processes only a\nhandful of frames but produces detection results closer to the oracle solution\nand outperforms recent state-of-the-art solutions in most cases.\n","authors":["Md Adnan Arefeen","Sumaiya Tabassum Nimi","Md Yusuf Sarwar Uddin"],"pdf_url":"https://arxiv.org/pdf/2203.11493v1.pdf","comment":"Accepted in The 18th International Conference on Distributed\n  Computing in Sensor Systems (DCOSS 2022)"},{"id":"http://arxiv.org/abs/2203.11433v1","updated":"2022-03-22T03:13:33Z","published":"2022-03-22T03:13:33Z","title":"Making DeepFakes more spurious: evading deep face forgery detection via\n  trace removal attack","summary":"  DeepFakes are raising significant social concerns. Although various DeepFake\ndetectors have been developed as forensic countermeasures, these detectors are\nstill vulnerable to attacks. Recently, a few attacks, principally adversarial\nattacks, have succeeded in cloaking DeepFake images to evade detection.\nHowever, these attacks have typical detector-specific designs, which require\nprior knowledge about the detector, leading to poor transferability. Moreover,\nthese attacks only consider simple security scenarios. Less is known about how\neffective they are in high-level scenarios where either the detectors or the\nattacker's knowledge varies. In this paper, we solve the above challenges with\npresenting a novel detector-agnostic trace removal attack for DeepFake\nanti-forensics. Instead of investigating the detector side, our attack looks\ninto the original DeepFake creation pipeline, attempting to remove all\ndetectable natural DeepFake traces to render the fake images more \"authentic\".\nTo implement this attack, first, we perform a DeepFake trace discovery,\nidentifying three discernible traces. Then a trace removal network (TR-Net) is\nproposed based on an adversarial learning framework involving one generator and\nmultiple discriminators. Each discriminator is responsible for one individual\ntrace representation to avoid cross-trace interference. These discriminators\nare arranged in parallel, which prompts the generator to remove various traces\nsimultaneously. To evaluate the attack efficacy, we crafted heterogeneous\nsecurity scenarios where the detectors were embedded with different levels of\ndefense and the attackers' background knowledge of data varies. The\nexperimental results show that the proposed attack can significantly compromise\nthe detection accuracy of six state-of-the-art DeepFake detectors while causing\nonly a negligible loss in visual quality to the original DeepFake samples.\n","authors":["Chi Liu","Huajie Chen","Tianqing Zhu","Jun Zhang","Wanlei Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.11433v1.pdf","comment":null}]},"2022-03-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.12574v1","updated":"2022-03-23T17:34:35Z","published":"2022-03-23T17:34:35Z","title":"Mitigating Gender Bias in Distilled Language Models via Counterfactual\n  Role Reversal","summary":"  Language models excel at generating coherent text, and model compression\ntechniques such as knowledge distillation have enabled their use in\nresource-constrained settings. However, these models can be biased in multiple\nways, including the unfounded association of male and female genders with\ngender-neutral professions. Therefore, knowledge distillation without any\nfairness constraints may preserve or exaggerate the teacher model's biases onto\nthe distilled model. To this end, we present a novel approach to mitigate\ngender disparity in text generation by learning a fair model during knowledge\ndistillation. We propose two modifications to the base knowledge distillation\nbased on counterfactual role reversal$\\unicode{x2014}$modifying teacher\nprobabilities and augmenting the training set. We evaluate gender polarity\nacross professions in open-ended text generated from the resulting distilled\nand finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial\nreduction in gender disparity with only a minor compromise in utility. Finally,\nwe observe that language models that reduce gender polarity in language\ngeneration do not improve embedding fairness or downstream classification\nfairness.\n","authors":["Umang Gupta","Jwala Dhamala","Varun Kumar","Apurv Verma","Yada Pruksachatkun","Satyapriya Krishna","Rahul Gupta","Kai-Wei Chang","Greg Ver Steeg","Aram Galstyan"],"pdf_url":"https://arxiv.org/pdf/2203.12574v1.pdf","comment":"To appear in the Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2110.06043v7","updated":"2022-03-23T17:31:09Z","published":"2021-10-12T14:42:33Z","title":"Topic Model Supervised by Understanding Map","summary":"  Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n","authors":["Gangli Liu"],"pdf_url":"https://arxiv.org/pdf/2110.06043v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.07577v2","updated":"2022-03-23T17:10:08Z","published":"2021-10-14T17:40:08Z","title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model\n  Tuning","summary":"  Recent parameter-efficient language model tuning (PELT) methods manage to\nmatch the performance of fine-tuning with much fewer trainable parameters and\nperform especially well when training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and tasks. In light of\nmodel diversity and the difficulty of model selection, we propose a unified\nframework, UniPELT, which incorporates different PELT methods as submodules and\nlearns to activate the ones that best suit the current data or task setup via\ngating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%\ngains compared to the best individual PELT method that it incorporates and even\noutperforms fine-tuning under different setups. Moreover, UniPELT generally\nsurpasses the upper bound that takes the best performance of all its submodules\nused individually on each task, indicating that a mixture of multiple PELT\nmethods may be inherently more effective than single methods.\n","authors":["Yuning Mao","Lambert Mathias","Rui Hou","Amjad Almahairi","Hao Ma","Jiawei Han","Wen-tau Yih","Madian Khabsa"],"pdf_url":"https://arxiv.org/pdf/2110.07577v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.12536v1","updated":"2022-03-23T16:58:10Z","published":"2022-03-23T16:58:10Z","title":"Dynamically Refined Regularization for Improving Cross-corpora Hate\n  Speech Detection","summary":"  Hate speech classifiers exhibit substantial performance degradation when\nevaluated on datasets different from the source. This is due to learning\nspurious correlations between words that are not necessarily relevant to\nhateful language, and hate speech labels from the training corpus. Previous\nwork has attempted to mitigate this problem by regularizing specific terms from\npre-defined static dictionaries. While this has been demonstrated to improve\nthe generalizability of classifiers, the coverage of such methods is limited\nand the dictionaries require regular manual updates from human experts. In this\npaper, we propose to automatically identify and reduce spurious correlations\nusing attribution methods with dynamic refinement of the list of terms that\nneed to be regularized during training. Our approach is flexible and improves\nthe cross-corpora performance over previous work independently and in\ncombination with pre-defined dictionaries.\n","authors":["Tulika Bose","Nikolaos Aletras","Irina Illina","Dominique Fohr"],"pdf_url":"https://arxiv.org/pdf/2203.12536v1.pdf","comment":"Findings of ACL 2022 preprint"},{"id":"http://arxiv.org/abs/2203.12524v1","updated":"2022-03-23T16:36:24Z","published":"2022-03-23T16:36:24Z","title":"Computational historical linguistics and language diversity in South\n  Asia","summary":"  South Asia is home to a plethora of languages, many of which severely lack\naccess to new language technologies. This linguistic diversity also results in\na research environment conducive to the study of comparative, contact, and\nhistorical linguistics -- fields which necessitate the gathering of extensive\ndata from many languages. We claim that data scatteredness (rather than\nscarcity) is the primary obstacle in the development of South Asian language\ntechnology, and suggest that the study of language history is uniquely aligned\nwith surmounting this obstacle. We review recent developments in and at the\nintersection of South Asian NLP and historical-comparative linguistics,\ndescribing our and others' current efforts in this area. We also offer new\nstrategies towards breaking the data barrier.\n","authors":["Aryaman Arora","Adam Farris","Samopriya Basu","Suresh Kolichala"],"pdf_url":"https://arxiv.org/pdf/2203.12524v1.pdf","comment":"14 pages; accepted to ACL 2022 Theme Track"},{"id":"http://arxiv.org/abs/2203.12515v1","updated":"2022-03-23T16:24:21Z","published":"2022-03-23T16:24:21Z","title":"A Survey on Cross-Lingual Summarization","summary":"  Cross-lingual summarization is the task of generating a summary in one\nlanguage (e.g., English) for the given document(s) in a different language\n(e.g., Chinese). Under the globalization background, this task has attracted\nincreasing attention of the computational linguistics community. Nevertheless,\nthere still remains a lack of comprehensive review for this task. Therefore, we\npresent the first systematic critical review on the datasets, approaches and\nchallenges in this field. Specifically, we carefully organize existing datasets\nand approaches according to different construction methods and solution\nparadigms, respectively. For each type of datasets or approaches, we thoroughly\nintroduce and summarize previous efforts and further compare them with each\nother to provide deeper analyses. In the end, we also discuss promising\ndirections and offer our thoughts to facilitate future research. This survey is\nfor both beginners and experts in cross-lingual summarization, and we hope it\nwill serve as a starting point as well as a source of new ideas for researchers\nand engineers interested in this area.\n","authors":["Jiaan Wang","Fandong Meng","Duo Zheng","Yunlong Liang","Zhixu Li","Jianfeng Qu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.12515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12487v1","updated":"2022-03-23T15:29:28Z","published":"2022-03-23T15:29:28Z","title":"A Context-Aware Feature Fusion Framework for Punctuation Restoration","summary":"  To accomplish the punctuation restoration task, most existing approaches\nfocused on leveraging extra information (e.g., part-of-speech tags) or\naddressing the class imbalance problem. Recent works have widely applied the\ntransformer-based language models and significantly improved their\neffectiveness. To the best of our knowledge, an inherent issue has remained\nneglected: the attention of individual heads in the transformer will be diluted\nor powerless while feeding the long non-punctuation utterances. Since those\nprevious contexts, not the followings, are comparatively more valuable to the\ncurrent position, it's hard to achieve a good balance by independent attention.\nIn this paper, we propose a novel Feature Fusion framework based on two-type\nAttentions (FFA) to alleviate the shortage. It introduces a two-stream\narchitecture. One module involves interaction between attention heads to\nencourage the communication, and another masked attention module captures the\ndependent feature representation. Then, it aggregates two feature embeddings to\nfuse information and enhances context-awareness. The experiments on the popular\nbenchmark dataset IWSLT demonstrate that our approach is effective. Without\nadditional data, it obtains comparable performance to the current\nstate-of-the-art models.\n","authors":["Yangjun Wu","Kebin Fang","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2203.12487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12481v1","updated":"2022-03-23T15:22:34Z","published":"2022-03-23T15:22:34Z","title":"Prompt-based Pre-trained Model for Personality and Interpersonal\n  Reactivity Prediction","summary":"  This paper describes the LingJing team's method to the Workshop on\nComputational Approaches to Subjectivity, Sentiment & Social Media Analysis\n(WASSA) 2022 shared task on Personality Prediction (PER) and Reactivity Index\nPrediction (IRI). In this paper, we adopt the prompt-based method with the\npre-trained language model to accomplish these tasks. Specifically, the prompt\nis designed to provide the extra knowledge for enhancing the pre-trained model.\nData augmentation and model ensemble are adopted for obtaining better results.\nExtensive experiments are performed, which shows the effectiveness of the\nproposed method. On the final submission, our system achieves a Pearson\nCorrelation Coefficient of 0.2301 and 0.2546 on Track 3 and Track 4\nrespectively. We ranked Top-1 on both sub-tasks.\n","authors":["Bin Li","Yixuan Weng","Qiya Song","Fuyan Ma","Bin Sun","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2203.12481v1.pdf","comment":"The shared task paper described the contributions of the Workshop on\n  Computational Approaches to Subjectivity, Sentiment & Social Media Analysis\n  (WASSA) @ ACL-2022"},{"id":"http://arxiv.org/abs/2203.06667v4","updated":"2022-03-23T15:10:44Z","published":"2022-03-13T14:42:53Z","title":"Towards Visual-Prompt Temporal Answering Grounding in Medical\n  Instructional Video","summary":"  The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms other\nstate-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,\nwhich demonstrates the effectiveness of visual prompt and the text span\npredictor.\n","authors":["Bin Li","Yixuan Weng","Bin Sun","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2203.06667v4.pdf","comment":"8 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2203.12468v1","updated":"2022-03-23T15:05:18Z","published":"2022-03-23T15:05:18Z","title":"The VoicePrivacy 2022 Challenge Evaluation Plan","summary":"  For new participants - Executive summary: (1) The task is to develop a voice\nanonymization system for speech data which conceals the speaker's voice\nidentity while protecting linguistic content, paralinguistic attributes,\nintelligibility and naturalness. (2) Training, development and evaluation\ndatasets are provided in addition to 3 different baseline anonymization\nsystems, evaluation scripts, and metrics. Participants apply their developed\nanonymization systems, run evaluation scripts and submit objective evaluation\nresults and anonymized speech data to the organizers. (3) Results will be\npresented at a workshop held in conjunction with INTERSPEECH 2022 to which all\nparticipants are invited to present their challenge systems and to submit\nadditional workshop papers.\n  For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:\n(1) A stronger, semi-informed attack model in the form of an automatic speaker\nverification (ASV) system trained on anonymized (per-utterance) speech data.\n(2) Complementary metrics comprising the equal error rate (EER) as a privacy\nmetric, the word error rate (WER) as a primary utility metric, and the pitch\ncorrelation and gain of voice distinctiveness as secondary utility metrics. (3)\nA new ranking policy based upon a set of minimum target privacy requirements.\n","authors":["Natalia Tomashenko","Xin Wang","Xiaoxiao Miao","Hubert Nourtel","Pierre Champion","Massimiliano Todisco","Emmanuel Vincent","Nicholas Evans","Junichi Yamagishi","Jean François Bonastre"],"pdf_url":"https://arxiv.org/pdf/2203.12468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.07878v2","updated":"2022-03-23T14:24:16Z","published":"2020-10-15T16:55:45Z","title":"Tokenization Repair in the Presence of Spelling Errors","summary":"  We consider the following tokenization repair problem: Given a natural\nlanguage text with any combination of missing or spurious spaces, correct\nthese. Spelling errors can be present, but it's not part of the problem to\ncorrect them. For example, given: \"Tispa per isabout token izaionrep air\",\ncompute \"Tis paper is about tokenizaion repair\". We identify three key\ningredients of high-quality tokenization repair, all missing from previous\nwork: deep language models with a bidirectional component, training the models\non text with spelling errors, and making use of the space information already\npresent. Our methods also improve existing spell checkers by fixing not only\nmore tokenization errors but also more spelling errors: once it is clear which\ncharacters form a word, it is much easier for them to figure out the correct\nword. We provide six benchmarks that cover three use cases (OCR errors, text\nextraction from PDF, human errors) and the cases of partially correct space\ninformation and all spaces missing. We evaluate our methods against the best\nexisting methods and a non-trivial baseline. We provide full reproducibility\nunder https://ad.cs.uni-freiburg.de/publications .\n","authors":["Hannah Bast","Matthias Hertel","Mostafa M. Mohamed"],"pdf_url":"https://arxiv.org/pdf/2010.07878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09053v2","updated":"2022-03-23T14:08:29Z","published":"2022-03-17T03:18:46Z","title":"Reducing Position Bias in Simultaneous Machine Translation with\n  Length-Aware Framework","summary":"  Simultaneous machine translation (SiMT) starts translating while receiving\nthe streaming source inputs, and hence the source sentence is always incomplete\nduring translating. Different from the full-sentence MT using the conventional\nseq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,\nwhich forces each target word to only align with a partial source prefix to\nadapt to the incomplete source in streaming inputs. However, the source words\nin the front positions are always illusoryly considered more important since\nthey appear in more prefixes, resulting in position bias, which makes the model\npay more attention on the front source positions in testing. In this paper, we\nfirst analyze the phenomenon of position bias in SiMT, and develop a\nLength-Aware Framework to reduce the position bias by bridging the structural\ngap between SiMT and full-sentence MT. Specifically, given the streaming\ninputs, we first predict the full-sentence length and then fill the future\nsource position with positional encoding, thereby turning the streaming inputs\ninto a pseudo full-sentence. The proposed framework can be integrated into most\nexisting SiMT methods to further improve performance. Experiments on two\nrepresentative SiMT methods, including the state-of-the-art adaptive policy,\nshow that our method successfully reduces the position bias and thereby\nachieves better SiMT performance.\n","authors":["Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2203.09053v2.pdf","comment":"Accept to ACL 2022 main conference. 14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2203.12368v1","updated":"2022-03-23T12:41:38Z","published":"2022-03-23T12:41:38Z","title":"A Framework for Fast Polarity Labelling of Massive Data Streams","summary":"  Many of the existing sentiment analysis techniques are based on supervised\nlearning, and they demand the availability of valuable training datasets to\ntrain their models. When dataset freshness is critical, the annotating of high\nspeed unlabelled data streams becomes critical but remains an open problem. In\nthis paper, we propose PLStream, a novel Apache Flink-based framework for fast\npolarity labelling of massive data streams, like Twitter tweets or online\nproduct reviews. We address the associated implementation challenges and\npropose a list of techniques including both algorithmic improvements and system\noptimizations. A thorough empirical validation with two real-world workloads\ndemonstrates that PLStream is able to generate high quality labels (almost 80%\naccuracy) in the presence of high-speed continuous unlabelled data streams\n(almost 16,000 tuples/sec) without any manual efforts.\n","authors":["Huilin Wu","Mian Lu","Zhao Zheng","Shuhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.12368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13392v2","updated":"2022-03-23T12:41:12Z","published":"2022-02-27T16:30:22Z","title":"A Simple but Effective Pluggable Entity Lookup Table for Pre-trained\n  Language Models","summary":"  Pre-trained language models (PLMs) cannot well recall rich factual knowledge\nof entities exhibited in large-scale corpora, especially those rare entities.\nIn this paper, we propose to build a simple but effective Pluggable Entity\nLookup Table (PELT) on demand by aggregating the entity's output\nrepresentations of multiple occurrences in the corpora. PELT can be compatibly\nplugged as inputs to infuse supplemental entity knowledge into PLMs. Compared\nto previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation\nwith capability of acquiring knowledge from out-of-domain corpora for domain\nadaptation scenario. The experiments on knowledge-related tasks demonstrate\nthat our method, PELT, can flexibly and effectively transfer entity knowledge\nfrom related corpora into PLMs with different architectures.\n","authors":["Deming Ye","Yankai Lin","Peng Li","Maosong Sun","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2202.13392v2.pdf","comment":"Accepted to ACL 2022. The code and models are available at\n  https://github.com/thunlp/PELT"},{"id":"http://arxiv.org/abs/2109.06067v4","updated":"2022-03-23T12:38:18Z","published":"2021-09-13T15:38:13Z","title":"Packed Levitated Marker for Entity and Relation Extraction","summary":"  Recent entity and relation extraction works focus on investigating how to\nobtain a better span representation from the pre-trained encoder. However, a\nmajor limitation of existing works is that they ignore the interrelation\nbetween spans (pairs). In this work, we propose a novel span representation\napproach, named Packed Levitated Markers (PL-Marker), to consider the\ninterrelation between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a neighborhood-oriented packing\nstrategy, which considers the neighbor spans integrally to better model the\nentity boundary information. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects to model the interrelation between the\nsame-subject span pairs. The experimental results show that, with the enhanced\nmarker feature, our model advances baselines on six NER benchmarks, and obtains\na 4.1%-4.3% strict relation F1 improvement with higher speed over previous\nstate-of-the-art models on ACE04 and ACE05.\n","authors":["Deming Ye","Yankai Lin","Peng Li","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2109.06067v4.pdf","comment":"Accepted to ACL 2022. The code and models are available at\n  https://github.com/thunlp/PL-Marker"},{"id":"http://arxiv.org/abs/2202.12024v2","updated":"2022-03-23T12:13:07Z","published":"2022-02-24T11:08:02Z","title":"NoisyTune: A Little Noise Can Help You Finetune Pretrained Language\n  Models Better","summary":"  Effectively finetuning pretrained language models (PLMs) is critical for\ntheir success in downstream tasks. However, PLMs may have risks in overfitting\nthe pretraining tasks and data, which usually have gap with the target\ndownstream tasks. Such gap may be difficult for existing PLM finetuning methods\nto overcome and lead to suboptimal performance. In this paper, we propose a\nvery simple yet effective method named NoisyTune to help better finetune PLMs\non downstream tasks by adding some noise to the parameters of PLMs before\nfine-tuning. More specifically, we propose a matrix-wise perturbing method\nwhich adds different uniform noises to different parameter matrices based on\ntheir standard deviations. In this way, the varied characteristics of different\ntypes of parameters in PLMs can be considered. Extensive experiments on both\nGLUE English benchmark and XTREME multilingual benchmark show NoisyTune can\nconsistently empower the finetuning of different PLMs on different downstream\ntasks.\n","authors":["Chuhan Wu","Fangzhao Wu","Tao Qi","Yongfeng Huang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2202.12024v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2104.04736v3","updated":"2022-03-23T11:47:49Z","published":"2021-04-10T11:10:16Z","title":"Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing","summary":"  Meta-learning, or learning to learn, is a technique that can help to overcome\nresource scarcity in cross-lingual NLP problems, by enabling fast adaptation to\nnew tasks. We apply model-agnostic meta-learning (MAML) to the task of\ncross-lingual dependency parsing. We train our model on a diverse set of\nlanguages to learn a parameter initialization that can adapt quickly to new\nlanguages. We find that meta-learning with pre-training can significantly\nimprove upon the performance of language transfer and standard supervised\nlearning baselines for a variety of unseen, typologically diverse, and\nlow-resource languages, in a few-shot learning setup.\n","authors":["Anna Langedijk","Verna Dankers","Phillip Lippe","Sander Bos","Bryan Cardenas Guevara","Helen Yannakoudakis","Ekaterina Shutova"],"pdf_url":"https://arxiv.org/pdf/2104.04736v3.pdf","comment":"- Add additional results (Appendix D) - Cosmetic updates for\n  camera-ready version ACL 2022"},{"id":"http://arxiv.org/abs/2203.08394v4","updated":"2022-03-23T11:08:28Z","published":"2022-03-16T04:50:27Z","title":"Bridging the Data Gap between Training and Inference for Unsupervised\n  Neural Machine Translation","summary":"  Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.\n","authors":["Zhiwei He","Xing Wang","Rui Wang","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2203.08394v4.pdf","comment":"13 pages, ACL 2022"},{"id":"http://arxiv.org/abs/2203.01976v2","updated":"2022-03-23T10:55:13Z","published":"2022-03-03T19:35:24Z","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer\n  Among Related Languages","summary":"  Pre-trained multilingual language models such as mBERT and XLM-R have\ndemonstrated great potential for zero-shot cross-lingual transfer to low\nweb-resource languages (LRL). However, due to limited model capacity, the large\ndifference in the sizes of available monolingual corpora between high\nweb-resource languages (HRL) and LRLs does not provide enough scope of\nco-embedding the LRL with the HRL, thereby affecting downstream task\nperformance of LRLs. In this paper, we argue that relatedness among languages\nin a language family along the dimension of lexical overlap may be leveraged to\novercome some of the corpora limitations of LRLs. We propose Overlap BPE\n(OBPE), a simple yet effective modification to the BPE vocabulary generation\nalgorithm which enhances overlap across related languages. Through extensive\nexperiments on multiple NLP tasks and datasets, we observe that OBPE generates\na vocabulary that increases the representation of LRLs via tokens shared with\nHRLs. This results in improved zero-shot transfer from related HRLs to LRLs\nwithout reducing HRL representation and accuracy. Unlike previous studies that\ndismissed the importance of token-overlap, we show that in the low-resource\nrelated language setting, token overlap matters. Synthetically reducing the\noverlap to zero can cause as much as a four-fold drop in zero-shot transfer\naccuracy.\n","authors":["Vaidehi Patil","Partha Talukdar","Sunita Sarawagi"],"pdf_url":"https://arxiv.org/pdf/2203.01976v2.pdf","comment":"Accepted to appear at the ACL 2022 Main conference"},{"id":"http://arxiv.org/abs/2203.12298v1","updated":"2022-03-23T09:46:41Z","published":"2022-03-23T09:46:41Z","title":"Input-specific Attention Subnetworks for Adversarial Detection","summary":"  Self-attention heads are characteristic of Transformer models and have been\nwell studied for interpretability and pruning. In this work, we demonstrate an\naltogether different utility of attention heads, namely for adversarial\ndetection. Specifically, we propose a method to construct input-specific\nattention subnetworks (IAS) from which we extract three features to\ndiscriminate between authentic and adversarial inputs. The resultant detector\nsignificantly improves (by over 7.5%) the state-of-the-art adversarial\ndetection accuracy for the BERT encoder on 10 NLU datasets with 11 different\nadversarial attack types. We also demonstrate that our method (a) is more\naccurate for larger models which are likely to have more spurious correlations\nand thus vulnerable to adversarial attack, and (b) performs well even with\nmodest training sets of adversarial examples.\n","authors":["Emil Biju","Anirudh Sriram","Pratyush Kumar","Mitesh M Khapra"],"pdf_url":"https://arxiv.org/pdf/2203.12298v1.pdf","comment":"Accepted at Findings of ACL 2022, 14 pages, 6 Tables and 9 Figures"},{"id":"http://arxiv.org/abs/2203.12277v1","updated":"2022-03-23T08:49:29Z","published":"2022-03-23T08:49:29Z","title":"Unified Structure Generation for Universal Information Extraction","summary":"  Information extraction suffers from its varying targets, heterogeneous\nstructures, and demand-specific schemas. In this paper, we propose a unified\ntext-to-structure generation framework, namely UIE, which can universally model\ndifferent IE tasks, adaptively generate targeted structures, and\ncollaboratively learn general IE abilities from different knowledge sources.\nSpecifically, UIE uniformly encodes different extraction structures via a\nstructured extraction language, adaptively generates target extractions via a\nschema-based prompt mechanism - structural schema instructor, and captures the\ncommon IE abilities via a large-scale pre-trained text-to-structure model.\nExperiments show that UIE achieved the state-of-the-art performance on 4 IE\ntasks, 13 datasets, and on all supervised, low-resource, and few-shot settings\nfor a wide range of entity, relation, event and sentiment extraction tasks and\ntheir unification. These results verified the effectiveness, universality, and\ntransferability of UIE.\n","authors":["Yaojie Lu","Qing Liu","Dai Dai","Xinyan Xiao","Hongyu Lin","Xianpei Han","Le Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2203.12277v1.pdf","comment":"Accepted to the main conference of ACL2022"},{"id":"http://arxiv.org/abs/2203.12276v1","updated":"2022-03-23T08:47:01Z","published":"2022-03-23T08:47:01Z","title":"ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through\n  Regularized Self-Attention","summary":"  Sparse Transformer has recently attracted a lot of attention since the\nability for reducing the quadratic dependency on the sequence length. We argue\nthat two factors, information bottleneck sensitivity and inconsistency between\ndifferent attention topologies, could affect the performance of the Sparse\nTransformer. This paper proposes a well-designed model named ERNIE-Sparse. It\nconsists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to\nsequentially unify local and global information. (ii) Self-Attention\nRegularization (SAR) method, a novel regularization designed to minimize the\ndistance for transformers with different attention topologies. To evaluate the\neffectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we\nperform experiments on a multi-modal long sequence modeling task benchmark,\nLong Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse\nsignificantly outperforms a variety of strong baseline methods including the\ndense attention and other efficient sparse attention methods and achieves\nimprovements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the\neffectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text\nclassification and 2 QA downstream tasks, achieve improvements on\nclassification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%\n(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior\nperformance.\n","authors":["Yang Liu","Jiaxiang Liu","Li Chen","Yuxiang Lu","Shikun Feng","Zhida Feng","Yu Sun","Hao Tian","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12274v1","updated":"2022-03-23T08:43:52Z","published":"2022-03-23T08:43:52Z","title":"Pre-training to Match for Unified Low-shot Relation Extraction","summary":"  Low-shot relation extraction~(RE) aims to recognize novel relations with very\nfew or even no samples, which is critical in real scenario application.\nFew-shot and zero-shot RE are two representative low-shot RE tasks, which seem\nto be with similar target but require totally different underlying abilities.\nIn this paper, we propose Multi-Choice Matching Networks to unify low-shot\nrelation extraction. To fill in the gap between zero-shot and few-shot RE, we\npropose the triplet-paraphrase meta-training, which leverages triplet\nparaphrase to pre-train zero-shot label matching ability and uses meta-learning\nparadigm to learn few-shot instance summarizing ability. Experimental results\non three different low-shot RE tasks show that the proposed method outperforms\nstrong baselines by a large margin, and achieve the best performance on\nfew-shot RE leaderboard.\n","authors":["Fangchao Liu","Hongyu Lin","Xianpei Han","Boxi Cao","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12274v1.pdf","comment":"Accepted to the main conference of ACL2022"},{"id":"http://arxiv.org/abs/2203.12264v1","updated":"2022-03-23T08:20:45Z","published":"2022-03-23T08:20:45Z","title":"ECO v1: Towards Event-Centric Opinion Mining","summary":"  Events are considered as the fundamental building blocks of the world. Mining\nevent-centric opinions can benefit decision making, people communication, and\nsocial good. Unfortunately, there is little literature addressing event-centric\nopinion mining, although which significantly diverges from the well-studied\nentity-centric opinion mining in connotation, structure, and expression. In\nthis paper, we propose and formulate the task of event-centric opinion mining\nbased on event-argument structure and expression categorizing theory. We also\nbenchmark this task by constructing a pioneer corpus and designing a two-step\nbenchmark framework. Experiment results show that event-centric opinion mining\nis feasible and challenging, and the proposed task, dataset, and baselines are\nbeneficial for future studies.\n","authors":["Ruoxi Xu","Hongyu Lin","Meng Liao","Xianpei Han","Jin Xu","Wei Tan","Yingfei Sun","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12264v1.pdf","comment":"Accepted to Findings of ACL2022"},{"id":"http://arxiv.org/abs/2203.12258v1","updated":"2022-03-23T08:10:07Z","published":"2022-03-23T08:10:07Z","title":"Can Prompt Probe Pretrained Language Models? Understanding the Invisible\n  Risks from a Causal View","summary":"  Prompt-based probing has been widely used in evaluating the abilities of\npretrained language models (PLMs). Unfortunately, recent studies have\ndiscovered such an evaluation may be inaccurate, inconsistent and unreliable.\nFurthermore, the lack of understanding its inner workings, combined with its\nwide applicability, has the potential to lead to unforeseen risks for\nevaluating and applying PLMs in real-world applications. To discover,\nunderstand and quantify the risks, this paper investigates the prompt-based\nprobing from a causal view, highlights three critical biases which could induce\nbiased results and conclusions, and proposes to conduct debiasing via causal\nintervention. This paper provides valuable insights for the design of unbiased\ndatasets, better probing frameworks and more reliable evaluations of pretrained\nlanguage models. Furthermore, our conclusions also echo that we need to rethink\nthe criteria for identifying better pretrained language models. We openly\nreleased the source code and data at https://github.com/c-box/causalEval.\n","authors":["Boxi Cao","Hongyu Lin","Xianpei Han","Fangchao Liu","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12258v1.pdf","comment":"Accepted to the main conference of ACL2022"},{"id":"http://arxiv.org/abs/2203.12257v1","updated":"2022-03-23T08:07:32Z","published":"2022-03-23T08:07:32Z","title":"IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument\n  Mining Tasks","summary":"  Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.\n","authors":["Liying Cheng","Lidong Bing","Ruidan He","Qian Yu","Yan Zhang","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2203.12257v1.pdf","comment":"11 pages, 3 figures, accepted by ACL 2022"},{"id":"http://arxiv.org/abs/2203.12254v1","updated":"2022-03-23T08:04:30Z","published":"2022-03-23T08:04:30Z","title":"Chat-Capsule: A Hierarchical Capsule for Dialog-level Emotion Analysis","summary":"  Many studies on dialog emotion analysis focus on utterance-level emotion\nonly. These models hence are not optimized for dialog-level emotion detection,\ni.e. to predict the emotion category of a dialog as a whole. More importantly,\nthese models cannot benefit from the context provided by the whole dialog. In\nreal-world applications, annotations to dialog could fine-grained, including\nboth utterance-level tags (e.g. speaker type, intent category, and emotion\ncategory), and dialog-level tags (e.g. user satisfaction, and emotion curve\ncategory). In this paper, we propose a Context-based Hierarchical Attention\nCapsule~(Chat-Capsule) model, which models both utterance-level and\ndialog-level emotions and their interrelations. On a dialog dataset collected\nfrom customer support of an e-commerce platform, our model is also able to\npredict user satisfaction and emotion curve category. Emotion curve refers to\nthe change of emotions along the development of a conversation. Experiments\nshow that the proposed Chat-Capsule outperform state-of-the-art baselines on\nboth benchmark dataset and proprietary dataset. Source code will be released\nupon acceptance.\n","authors":["Yequan Wang","Xuying Meng","Yiyi Liu","Aixin Sun","Yao Wang","Yinhe Zheng","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2203.12254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.10545v3","updated":"2022-03-23T07:59:59Z","published":"2021-11-20T08:41:54Z","title":"Triples-to-Text Generation with Reinforcement Learning Based\n  Graph-augmented Neural Networks","summary":"  Considering a collection of RDF triples, the RDF-to-text generation task aims\nto generate a text description. Most previous methods solve this task using a\nsequence-to-sequence model or using a graph-based model to encode RDF triples\nand to generate a text sequence. Nevertheless, these approaches fail to clearly\nmodel the local and global structural information between and within RDF\ntriples. Moreover, the previous methods also face the non-negligible problem of\nlow faithfulness of the generated text, which seriously affects the overall\nperformance of these models. To solve these problems, we propose a model\ncombining two new graph-augmented structural neural encoders to jointly learn\nboth local and global structural information in the input RDF triples. To\nfurther improve text faithfulness, we innovatively introduce a reinforcement\nlearning (RL) reward based on information extraction (IE). We first extract\ntriples from the generated text using a pretrained IE model and regard the\ncorrect number of the extracted triples as the additional RL reward.\nExperimental results on two benchmark datasets demonstrate that our proposed\nmodel outperforms the state-of-the-art baselines, and the additional\nreinforcement learning reward does help to improve the faithfulness of the\ngenerated text.\n","authors":["Hanning Gao","Lingfei Wu","Hongyun Zhang","Zhihua Wei","Po Hu","Fangli Xu","Bo Long"],"pdf_url":"https://arxiv.org/pdf/2111.10545v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12252v1","updated":"2022-03-23T07:56:27Z","published":"2022-03-23T07:56:27Z","title":"Few-shot Named Entity Recognition with Self-describing Networks","summary":"  Few-shot NER needs to effectively capture information from limited instances\nand transfer useful knowledge from external resources. In this paper, we\npropose a self-describing mechanism for few-shot NER, which can effectively\nleverage illustrative instances and precisely transfer knowledge from external\nresources by describing both entity types and mentions using a universal\nconcept set. Specifically, we design Self-describing Networks (SDNet), a\nSeq2Seq generation model which can universally describe mentions using\nconcepts, automatically map novel entity types to concepts, and adaptively\nrecognize entities on-demand. We pre-train SDNet with large-scale corpus, and\nconduct experiments on 8 benchmarks from different domains. Experiments show\nthat SDNet achieves competitive performances on all benchmarks and achieves the\nnew state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and\nrobustness.\n","authors":["Jiawei Chen","Qing Liu","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12252v1.pdf","comment":"Accepted to the main conference of ACL2022"},{"id":"http://arxiv.org/abs/2203.12235v1","updated":"2022-03-23T07:07:11Z","published":"2022-03-23T07:07:11Z","title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions","summary":"  The syntactic categories of categorial grammar formalisms are structured\nunits made of smaller, indivisible primitives, bound together by the underlying\ngrammar's category formation rules. In the trending approach of constructive\nsupertagging, neural models are increasingly made aware of the internal\ncategory structure, which in turn enables them to more reliably predict rare\nand out-of-vocabulary categories, with significant implications for grammars\npreviously deemed too complex to find practical use. In this work, we revisit\nconstructive supertagging from a graph-theoretic perspective, and propose a\nframework based on heterogeneous dynamic graph convolutions aimed at exploiting\nthe distinctive structure of a supertagger's output space. We test our approach\non a number of categorial grammar datasets spanning different languages and\ngrammar formalisms, achieving substantial improvements over previous state of\nthe art scores. Code will be made available at\nhttps://github.com/konstantinosKokos/dynamic-graph-supertagging\n","authors":["Konstantinos Kogkalidis","Michael Moortgat"],"pdf_url":"https://arxiv.org/pdf/2203.12235v1.pdf","comment":"8 pages plus references, unpublished preprint"},{"id":"http://arxiv.org/abs/2203.12210v1","updated":"2022-03-23T05:54:37Z","published":"2022-03-23T05:54:37Z","title":"Integrating Vectorized Lexical Constraints for Neural Machine\n  Translation","summary":"  Lexically constrained neural machine translation (NMT), which controls the\ngeneration of NMT models with pre-specified constraints, is important in many\npractical scenarios. Due to the representation gap between discrete constraints\nand continuous vectors in NMT models, most existing works choose to construct\nsynthetic data or modify the decoding algorithm to impose lexical constraints,\ntreating the NMT model as a black box. In this work, we propose to open this\nblack box by directly integrating the constraints into NMT models.\nSpecifically, we vectorize source and target constraints into continuous keys\nand values, which can be utilized by the attention modules of NMT models. The\nproposed integration method is based on the assumption that the correspondence\nbetween keys and values in attention modules is naturally suitable for modeling\nconstraint pairs. Experimental results show that our method consistently\noutperforms several representative baselines on four language pairs,\ndemonstrating the superiority of integrating vectorized lexical constraints.\n","authors":["Shuo Wang","Zhixing Tan","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2203.12210v1.pdf","comment":"Accepted by ACL 2022 (main conference)"},{"id":"http://arxiv.org/abs/2203.12201v1","updated":"2022-03-23T05:27:57Z","published":"2022-03-23T05:27:57Z","title":"Towards Expressive Speaking Style Modelling with Hierarchical Context\n  Information for Mandarin Speech Synthesis","summary":"  Previous works on expressive speech synthesis mainly focus on current\nsentence. The context in adjacent sentences is neglected, resulting in\ninflexible speaking style for the same text, which lacks speech variations. In\nthis paper, we propose a hierarchical framework to model speaking style from\ncontext. A hierarchical context encoder is proposed to explore a wider range of\ncontextual information considering structural relationship in context,\nincluding inter-phrase and inter-sentence relations. Moreover, to encourage\nthis encoder to learn style representation better, we introduce a novel\ntraining strategy with knowledge distillation, which provides the target for\nencoder training. Both objective and subjective evaluations on a Mandarin\nlecture dataset demonstrate that the proposed method can significantly improve\nthe naturalness and expressiveness of the synthesized speech.\n","authors":["Shun Lei","Yixuan Zhou","Liyang Chen","Zhiyong Wu","Shiyin Kang","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2203.12201v1.pdf","comment":"Accepted by ICASSP 2022"},{"id":"http://arxiv.org/abs/2203.03825v2","updated":"2022-03-23T04:27:29Z","published":"2022-03-08T03:21:45Z","title":"Incorporating Hierarchy into Text Encoder: a Contrastive Learning\n  Approach for Hierarchical Text Classification","summary":"  Hierarchical text classification is a challenging subtask of multi-label\nclassification due to its complex label hierarchy. Existing methods encode text\nand label hierarchy separately and mix their representations for\nclassification, where the hierarchy remains unchanged for all input text.\nInstead of modeling them separately, in this work, we propose Hierarchy-guided\nContrastive Learning (HGCLR) to directly embed the hierarchy into a text\nencoder. During training, HGCLR constructs positive samples for input text\nunder the guidance of the label hierarchy. By pulling together the input text\nand its positive sample, the text encoder can learn to generate the\nhierarchy-aware text representation independently. Therefore, after training,\nthe HGCLR enhanced text encoder can dispense with the redundant hierarchy.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nHGCLR.\n","authors":["Zihan Wang","Peiyi Wang","Lianzhe Huang","Xin Sun","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2203.03825v2.pdf","comment":"ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2203.12187v1","updated":"2022-03-23T04:19:05Z","published":"2022-03-23T04:19:05Z","title":"Converse -- A Tree-Based Modular Task-Oriented Dialogue System","summary":"  Creating a system that can have meaningful conversations with humans to help\naccomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).\nIt has defined the meaning of AI since the beginning. A lot has been\naccomplished in this area recently, with voice assistant products entering our\ndaily lives and chat bot systems becoming commonplace in customer service. At\nfirst glance there seems to be no shortage of options for dialogue systems.\nHowever, the frequently deployed dialogue systems today seem to all struggle\nwith a critical weakness - they are hard to build and harder to maintain. At\nthe core of the struggle is the need to script every single turn of\ninteractions between the bot and the human user. This makes the dialogue\nsystems more difficult to maintain as the tasks become more complex and more\ntasks are added to the system. In this paper, we propose Converse, a flexible\ntree-based modular task-oriented dialogue system. Converse uses an and-or tree\nstructure to represent tasks and offers powerful multi-task dialogue\nmanagement. Converse supports task dependency and task switching, which are\nunique features compared to other open-source dialogue frameworks. At the same\ntime, Converse aims to make the bot building process easy and simple, for both\nprofessional and non-professional software developers. The code is available at\nhttps://github.com/salesforce/Converse.\n","authors":["Tian Xie","Xinyi Yang","Angela S. Lin","Feihong Wu","Kazuma Hashimoto","Jin Qu","Young Mo Kang","Wenpeng Yin","Huan Wang","Semih Yavuz","Gang Wu","Michael Jones","Richard Socher","Yingbo Zhou","Wenhao Liu","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2203.12187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12186v1","updated":"2022-03-23T04:18:30Z","published":"2022-03-23T04:18:30Z","title":"AbductionRules: Training Transformers to Explain Unexpected Inputs","summary":"  Transformers have recently been shown to be capable of reliably performing\nlogical reasoning over facts and rules expressed in natural language, but\nabductive reasoning - inference to the best explanation of an unexpected\nobservation - has been underexplored despite significant applications to\nscientific discovery, common-sense reasoning, and model interpretability.\n  We present AbductionRules, a group of natural language datasets designed to\ntrain and test generalisable abduction over natural-language knowledge bases.\nWe use these datasets to finetune pretrained Transformers and discuss their\nperformance, finding that our models learned generalisable abductive techniques\nbut also learned to exploit the structure of our data. Finally, we discuss the\nviability of this approach to abductive reasoning and ways in which it may be\nimproved in future work.\n","authors":["Nathan Young","Qiming Bao","Joshua Bensemann","Michael Witbrock"],"pdf_url":"https://arxiv.org/pdf/2203.12186v1.pdf","comment":"Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2203.12184v1","updated":"2022-03-23T04:06:01Z","published":"2022-03-23T04:06:01Z","title":"A Theoretically Grounded Benchmark for Evaluating Machine Commonsense","summary":"  Programming machines with commonsense reasoning (CSR) abilities is a\nlongstanding challenge in the Artificial Intelligence community. Current CSR\nbenchmarks use multiple-choice (and in relatively fewer cases, generative)\nquestion-answering instances to evaluate machine commonsense. Recent progress\nin transformer-based language representation models suggest that considerable\nprogress has been made on existing benchmarks. However, although tens of CSR\nbenchmarks currently exist, and are growing, it is not evident that the full\nsuite of commonsense capabilities have been systematically evaluated.\nFurthermore, there are doubts about whether language models are 'fitting' to a\nbenchmark dataset's training partition by picking up on subtle, but normatively\nirrelevant (at least for CSR), statistical features to achieve good performance\non the testing partition. To address these challenges, we propose a benchmark\ncalled Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based\non discriminative question answering, but with questions designed to evaluate\ndiverse aspects of commonsense, such as space, time, and world states. TG-CSR\nis based on a subset of commonsense categories first proposed as a viable\ntheory of commonsense by Gordon and Hobbs. The benchmark is also designed to be\nfew-shot (and in the future, zero-shot), with only a few training and\nvalidation examples provided. This report discusses the structure and\nconstruction of the benchmark. Preliminary results suggest that the benchmark\nis challenging even for advanced language representation models designed for\ndiscriminative CSR question answering tasks.\n  Benchmark access and leaderboard:\nhttps://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:\nhttps://usc-isi-i2.github.io/TGCSR/\n","authors":["Henrique Santos","Ke Shen","Alice M. Mulvehill","Yasaman Razeghi","Deborah L. McGuinness","Mayank Kejriwal"],"pdf_url":"https://arxiv.org/pdf/2203.12184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08527v2","updated":"2022-03-23T03:47:26Z","published":"2021-10-16T09:40:30Z","title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for\n  Pre-trained Language Models","summary":"  Recent work has shown pre-trained language models capture social biases from\nthe large amounts of text they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform an\nempirical survey of five recently proposed bias mitigation techniques:\nCounterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace\nProjection, Self-Debias, and SentenceDebias. We quantify the effectiveness of\neach technique using three intrinsic bias benchmarks while also measuring the\nimpact of these techniques on a model's language modeling ability, as well as\nits performance on downstream NLU tasks. We experimentally find that: (1)\nSelf-Debias is the strongest debiasing technique, obtaining improved scores on\nall bias benchmarks; (2) Current debiasing techniques perform less consistently\nwhen mitigating non-gender biases; And (3) improvements on bias benchmarks such\nas StereoSet and CrowS-Pairs by using debiasing strategies are often\naccompanied by a decrease in language modeling ability, making it difficult to\ndetermine whether the bias mitigation was effective.\n","authors":["Nicholas Meade","Elinor Poole-Dayan","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2110.08527v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12171v1","updated":"2022-03-23T03:27:56Z","published":"2022-03-23T03:27:56Z","title":"An Empirical Study of Memorization in NLP","summary":"  A recent study by Feldman (2020) proposed a long-tail theory to explain the\nmemorization behavior of deep learning models. However, memorization has not\nbeen empirically verified in the context of NLP, a gap addressed by this work.\nIn this paper, we use three different NLP tasks to check if the long-tail\ntheory holds. Our experiments demonstrate that top-ranked memorized training\ninstances are likely atypical, and removing the top-memorized training\ninstances leads to a more serious drop in test accuracy compared with removing\ntraining instances randomly. Furthermore, we develop an attribution method to\nbetter understand why a training instance is memorized. We empirically show\nthat our memorization attribution method is faithful, and share our interesting\nfinding that the top-memorized parts of a training instance tend to be features\nnegatively correlated with the class label.\n","authors":["Xiaosen Zheng","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2203.12171v1.pdf","comment":"ACL 2022. Code & data available at\n  https://github.com/xszheng2020/memorization"},{"id":"http://arxiv.org/abs/2112.09174v2","updated":"2022-03-23T02:20:19Z","published":"2021-12-16T19:56:44Z","title":"Learning Bounded Context-Free-Grammar via LSTM and the\n  Transformer:Difference and Explanations","summary":"  Long Short-Term Memory (LSTM) and Transformers are two popular neural\narchitectures used for natural language processing tasks. Theoretical results\nshow that both are Turing-complete and can represent any context-free language\n(CFL).In practice, it is often observed that Transformer models have better\nrepresentation power than LSTM. But the reason is barely understood. We study\nsuch practical differences between LSTM and Transformer and propose an\nexplanation based on their latent space decomposition patterns. To achieve this\ngoal, we introduce an oracle training paradigm, which forces the decomposition\nof the latent representation of LSTM and the Transformer and supervises with\nthe transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With\nthe forced decomposition, we show that the performance upper bounds of LSTM and\nTransformer in learning CFL are close: both of them can simulate a stack and\nperform stack operation along with state transitions. However, the absence of\nforced decomposition leads to the failure of LSTM models to capture the stack\nand stack operations, while having a marginal impact on the Transformer model.\nLastly, we connect the experiment on the prototypical PDA to a real-world\nparsing task to re-verify the conclusions\n","authors":["Hui Shi","Sicun Gao","Yuandong Tian","Xinyun Chen","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2112.09174v2.pdf","comment":"Accepted By AAAI22"},{"id":"http://arxiv.org/abs/2203.12135v1","updated":"2022-03-23T02:03:46Z","published":"2022-03-23T02:03:46Z","title":"ALT: um software para análise de legibilidade de textos em Língua\n  Portuguesa","summary":"  In the initial stage of human life, communication, seen as a process of\nsocial interaction, was always the best way to reach consensus between the\nparties. Understanding and credibility in this process are essential for the\nmutual agreement to be validated. But, how to do it so that this communication\nreaches the great mass? This is the main challenge when what is sought is the\ndissemination of information and its approval. In this context, this study\npresents the ALT software, developed from original readability metrics adapted\nto the Portuguese language, available on the web, to reduce communication\ndifficulties. The development of the software was motivated by the theory of\ncommunicative action of Habermas, which uses a multidisciplinary style to\nmeasure the credibility of the discourse in the communication channels used to\nbuild and maintain a safe and healthy relationship with the public.\n  --\n  No est\\'agio inicial da vida humana a comunica\\c{c}\\~ao, vista como um\nprocesso de intera\\c{c}\\~ao social, foi sempre o melhor caminho para o consenso\nentre as partes. O entendimento e a credibilidade nesse processo s\\~ao\nfundamentais para que o acordo m\\'utuo seja validado. Mas, como faz\\^e-lo de\nforma que essa comunica\\c{c}\\~ao alcance a grande massa? Esse \\'e o principal\ndesafio quando o que se busca \\'e a difus\\~ao da informa\\c{c}\\~ao e a sua\naprova\\c{c}\\~ao. Nesse contexto, este estudo apresenta o software ALT,\ndesenvolvido a partir de m\\'etricas de legibilidade originais adaptadas para a\nL\\'ingua Portuguesa, dispon\\'ivel na web, para reduzir as dificuldades na\ncomunica\\c{c}\\~ao. O desenvolvimento do software foi motivado pela teoria do\nagir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para\nmedir a credibilidade do discurso nos canais de comunica\\c{c}\\~ao utilizados\npara construir e manter uma rela\\c{c}\\~ao segura e saud\\'avel com o p\\'ublico.\n","authors":["Gleice Carvalho de Lima Moreno","Marco P. M. de Souza","Nelson Hein","Adriana Kroenke Hein"],"pdf_url":"https://arxiv.org/pdf/2203.12135v1.pdf","comment":"22 pages, 13 figures, in Portuguese, see software in\n  https://legibilidade.com"},{"id":"http://arxiv.org/abs/2203.12106v1","updated":"2022-03-23T00:30:28Z","published":"2022-03-23T00:30:28Z","title":"An Empirical Study on Learning and Improving the Search Objective for\n  Unsupervised Paraphrasing","summary":"  Research in unsupervised text generation has been gaining attention over the\nyears. One recent approach is local search towards a heuristically defined\nobjective, which specifies language fluency, semantic meanings, and other\ntask-specific attributes. Search in the sentence space is realized by\nword-level edit operations including insertion, replacement, and deletion.\nHowever, such objective function is manually designed with multiple components.\nAlthough previous work has shown maximizing this objective yields good\nperformance in terms of true measure of success (i.e. BLEU and iBLEU), the\nobjective landscape is considered to be non-smooth with significant noises,\nposing challenges for optimization. In this dissertation, we address the\nresearch problem of smoothing the noise in the heuristic search objective by\nlearning to model the search dynamics. Then, the learned model is combined with\nthe original objective function to guide the search in a bootstrapping fashion.\nExperimental results show that the learned models combined with the original\nsearch objective can indeed provide a smoothing effect, improving the search\nperformance by a small margin.\n","authors":["Weikai Steven Lu"],"pdf_url":"https://arxiv.org/pdf/2203.12106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.11133v10","updated":"2022-03-23T06:09:33Z","published":"2021-11-22T11:48:26Z","title":"L-Verse: Bidirectional Generation Between Image and Text","summary":"  Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n","authors":["Taehoon Kim","Gwangmo Song","Sihaeng Lee","Sangyun Kim","Yewon Seo","Soonyoung Lee","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae"],"pdf_url":"https://arxiv.org/pdf/2111.11133v10.pdf","comment":"Accepted to CVPR 2022 (Oral)"},{"id":"http://arxiv.org/abs/2203.12751v1","updated":"2022-03-23T22:40:50Z","published":"2022-03-23T22:40:50Z","title":"ThingTalk: An Extensible, Executable Representation Language for\n  Task-Oriented Dialogues","summary":"  Task-oriented conversational agents rely on semantic parsers to translate\nnatural language to formal representations. In this paper, we propose the\ndesign and rationale of the ThingTalk formal representation, and how the design\nimproves the development of transactional task-oriented agents.\n  ThingTalk is built on four core principles: (1) representing user requests\ndirectly as executable statements, covering all the functionality of the agent,\n(2) representing dialogues formally and succinctly to support accurate\ncontextual semantic parsing, (3) standardizing types and interfaces to maximize\nreuse between agents, and (4) allowing multiple, independently-developed agents\nto be composed in a single virtual assistant. ThingTalk is developed as part of\nthe Genie Framework that allows developers to quickly build transactional\nagents given a database and APIs.\n  We compare ThingTalk to existing representations: SMCalFlow, SGD, TreeDST.\nCompared to the others, the ThingTalk design is both more general and more\ncost-effective. Evaluated on the MultiWOZ benchmark, using ThingTalk and\nassociated tools yields a new state of the art accuracy of 79% turn-by-turn.\n","authors":["Monica S. Lam","Giovanni Campagna","Mehrad Moradshahi","Sina J. Semnani","Silei Xu"],"pdf_url":"https://arxiv.org/pdf/2203.12751v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2111.10139v2","updated":"2022-03-23T22:20:57Z","published":"2021-11-19T10:23:38Z","title":"More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech","summary":"  In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.\nMotivated by dubbing, VDTTS takes advantage of video frames as an additional\ninput alongside text, and generates speech that matches the video signal. We\ndemonstrate how this allows VDTTS to, unlike plain TTS models, generate speech\nthat not only has prosodic variations like natural pauses and pitch, but is\nalso synchronized to the input video. Experimentally, we show our model\nproduces well-synchronized outputs, approaching the video-speech\nsynchronization quality of the ground-truth, on several challenging benchmarks\nincluding \"in-the-wild\" content from VoxCeleb2. Supplementary demo videos\ndemonstrating video-speech synchronization, robustness to speaker ID swapping,\nand prosody, presented at the project page.\n","authors":["Michael Hassid","Michelle Tadmor Ramanovich","Brendan Shillingford","Miaosen Wang","Ye Jia","Tal Remez"],"pdf_url":"https://arxiv.org/pdf/2111.10139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12709v1","updated":"2022-03-23T20:04:14Z","published":"2022-03-23T20:04:14Z","title":"Adversarial Training for Improving Model Robustness? Look at Both\n  Prediction and Interpretation","summary":"  Neural language models show vulnerability to adversarial examples which are\nsemantically similar to their original counterparts with a few words replaced\nby their synonyms. A common way to improve model robustness is adversarial\ntraining which follows two steps-collecting adversarial examples by attacking a\ntarget model, and fine-tuning the model on the augmented dataset with these\nadversarial examples. The objective of traditional adversarial training is to\nmake a model produce the same correct predictions on an original/adversarial\nexample pair. However, the consistency between model decision-makings on two\nsimilar texts is ignored. We argue that a robust model should behave\nconsistently on original/adversarial example pairs, that is making the same\npredictions (what) based on the same reasons (how) which can be reflected by\nconsistent interpretations. In this work, we propose a novel feature-level\nadversarial training method named FLAT. FLAT aims at improving model robustness\nin terms of both predictions and interpretations. FLAT incorporates variational\nword masks in neural networks to learn global word importance and play as a\nbottleneck teaching the model to make predictions based on important words.\nFLAT explicitly shoots at the vulnerability problem caused by the mismatch\nbetween model understandings on the replaced words and their synonyms in\noriginal/adversarial example pairs by regularizing the corresponding global\nword importance scores. Experiments show the effectiveness of FLAT in improving\nthe robustness with respect to both predictions and interpretations of four\nneural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks\non four text classification tasks. The models trained via FLAT also show better\nrobustness than baseline models on unforeseen adversarial examples across\ndifferent attacks.\n","authors":["Hanjie Chen","Yangfeng Ji"],"pdf_url":"https://arxiv.org/pdf/2203.12709v1.pdf","comment":"AAAI 2022"},{"id":"http://arxiv.org/abs/2104.08718v3","updated":"2022-03-23T19:47:21Z","published":"2021-04-18T05:00:29Z","title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning","summary":"  Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n","authors":["Jack Hessel","Ari Holtzman","Maxwell Forbes","Ronan Le Bras","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2104.08718v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.08904v4","updated":"2022-03-23T19:16:55Z","published":"2022-02-17T21:35:56Z","title":"SGPT: GPT Sentence Embeddings for Semantic Search","summary":"  GPT transformers are the largest language models available, yet semantic\nsearch is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for\napplying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric\nsearch.\n  SGPT-BE produces semantically meaningful sentence embeddings by contrastive\nfine-tuning of only bias tensors and a novel pooling method. A 5.8 billion\nparameter SGPT-BE outperforms the best available sentence embeddings by 6%\nsetting a new state-of-the-art on BEIR. It outperforms the concurrently\nproposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes\n250,000 times more parameters.\n  SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1\nbillion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It\nbeats the supervised state-of-the-art on 7 datasets, but significantly loses on\nother datasets. We show how this can be alleviated by adapting the prompt.\n  SGPT-BE and SGPT-CE performance scales with model size. Yet, increased\nlatency, storage and compute costs should be considered. Code, models and\nresult files are freely available at https://github.com/Muennighoff/sgpt.\n","authors":["Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2202.08904v4.pdf","comment":"17 pages, 3 figures, 12 tables. v2 corrects a misreported nDCG@10\n  number for the SGPT-BE-5.8B model. v3 updates SGPT-BE-5.8B scores based on\n  retrained models with larger batch sizes v4 removes a superfluous table"},{"id":"http://arxiv.org/abs/2203.12644v1","updated":"2022-03-23T18:10:18Z","published":"2022-03-23T18:10:18Z","title":"Linearizing Transformer with Key-Value Memory Bank","summary":"  Transformer has brought great success to a wide range of natural language\nprocessing tasks. Nevertheless, the computational overhead of the vanilla\ntransformer scales quadratically with sequence length. Many efforts have been\nmade to develop more efficient transformer variants. A line of work (e.g.,\nLinformer) projects the input sequence into a low-rank space, achieving linear\ntime complexity. However, Linformer does not suit well for text generation\ntasks as the sequence length must be pre-specified. We propose MemSizer, an\napproach also projects the source sequence into lower dimension representation\nbut can take input with dynamic length, with a different perspective of the\nattention mechanism. MemSizer not only achieves the same linear time complexity\nbut also enjoys efficient recurrent-style autoregressive generation, which\nyields constant memory complexity and reduced computation at inference. We\ndemonstrate that MemSizer provides an improved tradeoff between efficiency and\naccuracy over the vanilla transformer and other linear variants in language\nmodeling and machine translation tasks, revealing a viable direction towards\nfurther inference efficiency improvement.\n","authors":["Yizhe Zhang","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2203.12644v1.pdf","comment":"Work in progress"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2112.12665v2","updated":"2022-03-23T17:59:15Z","published":"2021-12-23T16:02:03Z","title":"Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image\n  Segmentation using Partially Labeled Data","summary":"  Computer-assisted quantitative analysis on Giga-pixel pathology images has\nprovided a new avenue in histology examination. The innovations have been\nlargely focused on cancer pathology (i.e., tumor segmentation and\ncharacterization). In non-cancer pathology, the learning algorithms can be\nasked to examine more comprehensive tissue types simultaneously, as a\nmulti-label setting. The prior arts typically needed to train multiple\nsegmentation networks in order to match the domain-specific knowledge for\nheterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal\ntubular, distal tubular, peritubular capillaries, and arteries). In this paper,\nwe propose a dynamic single segmentation network (Omni-Seg) that learns to\nsegment multiple tissue types using partially labeled images (i.e., only one\ntissue type is labeled for each training image) for renal pathology. By\nlearning from ~150,000 patch-wise pathological images from six tissue types,\nthe proposed Omni-Seg network achieved superior segmentation accuracy and less\nresource consumption when compared to the previous the multiple-network and\nmulti-head design. In the testing stage, the proposed method obtains\n\"completely labeled\" tissue segmentation results using only \"partially labeled\"\ntraining images. The source code is available at\nhttps://github.com/ddrrnn123/Omni-Seg\n","authors":["Ruining Deng","Quan Liu","Can Cui","Zuhayr Asad","Haichun Yang","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2112.12665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12614v1","updated":"2022-03-23T17:59:02Z","published":"2022-03-23T17:59:02Z","title":"Unsupervised Salient Object Detection with Spectral Cluster Voting","summary":"  In this paper, we tackle the challenging task of unsupervised salient object\ndetection (SOD) by leveraging spectral clustering on self-supervised features.\nWe make the following contributions: (i) We revisit spectral clustering and\ndemonstrate its potential to group the pixels of salient objects; (ii) Given\nmask proposals from multiple applications of spectral clustering on image\nfeatures computed from various self-supervised models, e.g., MoCov2, SwAV,\nDINO, we propose a simple but effective winner-takes-all voting mechanism for\nselecting the salient masks, leveraging object priors based on framing and\ndistinctiveness; (iii) Using the selected object segmentation as pseudo\ngroundtruth masks, we train a salient object detector, dubbed SelfMask, which\noutperforms prior approaches on three unsupervised SOD benchmarks. Code is\npublicly available at https://github.com/NoelShin/selfmask.\n","authors":["Gyungin Shin","Samuel Albanie","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2203.12614v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.12613v1","updated":"2022-03-23T17:58:56Z","published":"2022-03-23T17:58:56Z","title":"A Hybrid Mesh-neural Representation for 3D Transparent Object\n  Reconstruction","summary":"  We propose a novel method to reconstruct the 3D shapes of transparent objects\nusing hand-held captured images under natural light conditions. It combines the\nadvantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid\nrepresentation, to simplify the capture setting used in recent contributions.\nAfter obtaining an initial shape through the multi-view silhouettes, we\nintroduce surface-based local MLPs to encode the vertex displacement field\n(VDF) for the reconstruction of surface details. The design of local MLPs\nallows to represent the VDF in a piece-wise manner using two layer MLP\nnetworks, which is beneficial to the optimization algorithm. Defining local\nMLPs on the surface instead of the volume also reduces the searching space.\nSuch a hybrid representation enables us to relax the ray-pixel correspondences\nthat represent the light path constraint to our designed ray-cell\ncorrespondences, which significantly simplifies the implementation of\nsingle-image based environment matting algorithm. We evaluate our\nrepresentation and reconstruction algorithm on several transparent objects with\nground truth models. Our experiments show that our method can produce\nhigh-quality reconstruction results superior to state-of-the-art methods using\na simplified data acquisition setup.\n","authors":["Jiamin Xu","Zihan Zhu","Hujun Bao","Wewei Xu"],"pdf_url":"https://arxiv.org/pdf/2203.12613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12612v1","updated":"2022-03-23T17:58:31Z","published":"2022-03-23T17:58:31Z","title":"StructToken : Rethinking Semantic Segmentation with Structural Prior","summary":"  In this paper, we present structure token (StructToken), a new paradigm for\nsemantic segmentation. From a perspective on semantic segmentation as per-pixel\nclassification, the previous deep learning-based methods learn the per-pixel\nrepresentation first through an encoder and a decoder head and then classify\neach pixel representation to a specific category to obtain the semantic masks.\nDifferently, we propose a structure-aware algorithm that takes structural\ninformation as prior to predict semantic masks directly without per-pixel\nclassification. Specifically, given an input image, the learnable structure\ntoken interacts with the image representations to reason the final semantic\nmasks. Three interaction approaches are explored and the results not only\noutperform the state-of-the-art methods but also contain more structural\ninformation. Experiments are conducted on three widely used datasets including\nADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could\nserve as an alternative for semantic segmentation and inspire future research.\n","authors":["Fangjian Lin","Zhanhao Liang","Junjun He","Miao Zheng","Shengwei Tian","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2203.12612v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2203.12609v1","updated":"2022-03-23T17:56:58Z","published":"2022-03-23T17:56:58Z","title":"Improving the Fairness of Chest X-ray Classifiers","summary":"  Deep learning models have reached or surpassed human-level performance in the\nfield of medical imaging, especially in disease diagnosis using chest x-rays.\nHowever, prior work has found that such classifiers can exhibit biases in the\nform of gaps in predictive performance across protected groups. In this paper,\nwe question whether striving to achieve zero disparities in predictive\nperformance (i.e. group fairness) is the appropriate fairness definition in the\nclinical setting, over minimax fairness, which focuses on maximizing the\nperformance of the worst-case group. We benchmark the performance of nine\nmethods in improving classifier fairness across these two definitions. We find,\nconsistent with prior work on non-clinical data, that methods which strive to\nachieve better worst-group performance do not outperform simple data balancing.\nWe also find that methods which achieve group fairness do so by worsening\nperformance for all groups. In light of these results, we discuss the utility\nof fairness definitions in the clinical setting, advocating for an\ninvestigation of the bias-inducing mechanisms in the underlying data generating\nprocess whenever possible.\n","authors":["Haoran Zhang","Natalie Dullerud","Karsten Roth","Lauren Oakden-Rayner","Stephen Robert Pfohl","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2203.12609v1.pdf","comment":"Published in CHIL 2022"},{"id":"http://arxiv.org/abs/2203.12602v1","updated":"2022-03-23T17:55:10Z","published":"2022-03-23T17:55:10Z","title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for\n  Self-Supervised Video Pre-Training","summary":"  Pre-training video transformers on extra large-scale datasets is generally\nrequired to achieve premier performance on relatively small datasets. In this\npaper, we show that video masked autoencoders (VideoMAE) are data-efficient\nlearners for self-supervised video pre-training (SSVP). We are inspired by the\nrecent ImageMAE and propose customized video tube masking and reconstruction.\nThese simple designs turn out to be effective for overcoming information\nleakage caused by the temporal correlation during video reconstruction. We\nobtain three important findings on SSVP: (1) An extremely high proportion of\nmasking ratio (i.e., 90% to 95%) still yields favorable performance of\nVideoMAE. The temporally redundant video content enables higher masking ratio\nthan that of images. (2) VideoMAE achieves impressive results on very small\ndatasets (i.e., around 3k-4k videos) without using any extra data. This is\npartially ascribed to the challenging task of video reconstruction to enforce\nhigh-level structure learning. (3) VideoMAE shows that data quality is more\nimportant than data quantity for SSVP. Domain shift between pre-training and\ntarget datasets are important issues in SSVP. Notably, our VideoMAE with the\nvanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on\nSomething-Something V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any\nextra data. Code will be released at https://github.com/MCG-NJU/VideoMAE.\n","authors":["Zhan Tong","Yibing Song","Jue Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12602v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2203.12601v1","updated":"2022-03-23T17:55:09Z","published":"2022-03-23T17:55:09Z","title":"R3M: A Universal Visual Representation for Robot Manipulation","summary":"  We study how visual representations pre-trained on diverse human video data\ncan enable data-efficient learning of downstream robotic manipulation tasks.\nConcretely, we pre-train a visual representation using the Ego4D human video\ndataset using a combination of time-contrastive learning, video-language\nalignment, and an L1 penalty to encourage sparse and compact representations.\nThe resulting representation, R3M, can be used as a frozen perception module\nfor downstream policy learning. Across a suite of 12 simulated robot\nmanipulation tasks, we find that R3M improves task success by over 20% compared\nto training from scratch and by over 10% compared to state-of-the-art visual\nrepresentations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika\nPanda arm to learn a range of manipulation tasks in a real, cluttered apartment\ngiven just 20 demonstrations. Code and pre-trained models are available at\nhttps://tinyurl.com/robotr3m.\n","authors":["Suraj Nair","Aravind Rajeswaran","Vikash Kumar","Chelsea Finn","Abhinav Gupta"],"pdf_url":"https://arxiv.org/pdf/2203.12601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.07508v2","updated":"2022-03-23T17:38:40Z","published":"2022-02-15T15:32:11Z","title":"Deep Constrained Least Squares for Blind Image Super-Resolution","summary":"  In this paper, we tackle the problem of blind image super-resolution(SR) with\na reformulated degradation model and two novel modules. Following the common\npractices of blind SR, our method proposes to improve both the kernel\nestimation as well as the kernel-based high-resolution image restoration. To be\nmore specific, we first reformulate the degradation model such that the\ndeblurring kernel estimation can be transferred into the low-resolution space.\nOn top of this, we introduce a dynamic deep linear filter module. Instead of\nlearning a fixed kernel for all images, it can adaptively generate deblurring\nkernel weights conditional on the input and yield a more robust kernel\nestimation. Subsequently, a deep constrained least square filtering module is\napplied to generate clean features based on the reformulation and estimated\nkernel. The deblurred feature and the low input image feature are then fed into\na dual-path structured SR network and restore the final high-resolution result.\nTo evaluate our method, we further conduct evaluations on several benchmarks,\nincluding Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed\nmethod achieves better accuracy and visual improvements against\nstate-of-the-art methods.\n","authors":["Ziwei Luo","Haibin Huang","Lei Yu","Youwei Li","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2202.07508v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2201.09968v2","updated":"2022-03-23T17:36:24Z","published":"2022-01-24T21:40:16Z","title":"ImpliCity: City Modeling from Satellite Images with Deep Implicit\n  Occupancy Fields","summary":"  High-resolution optical satellite sensors, combined with dense stereo\nalgorithms, have made it possible to reconstruct 3D city models from space.\nHowever, these models are, in practice, rather noisy and tend to miss small\ngeometric features that are clearly visible in the images. We argue that one\nreason for the limited quality may be a too early, heuristic reduction of the\ntriangulated 3D point cloud to an explicit height field or surface mesh. To\nmake full use of the point cloud and the underlying images, we introduce\nImpliCity, a neural representation of the 3D scene as an implicit, continuous\noccupancy field, driven by learned embeddings of the point cloud and a stereo\npair of ortho-photos. We show that this representation enables the extraction\nof high-quality DSMs: with image resolution 0.5$\\,$m, ImpliCity reaches a\nmedian height error of $\\approx\\,$0.7$\\,$m and outperforms competing methods,\nespecially w.r.t. building reconstruction, featuring intricate roof details,\nsmooth surfaces, and straight, regular outlines.\n","authors":["Corinne Stucker","Bingxin Ke","Yuanwen Yue","Shengyu Huang","Iro Armeni","Konrad Schindler"],"pdf_url":"https://arxiv.org/pdf/2201.09968v2.pdf","comment":"Accepted for publication in the International Annals of the\n  Photogrammetry, Remote Sensing and Spatial Information Sciences (camera-ready\n  version + supplementary material)"},{"id":"http://arxiv.org/abs/2203.12575v1","updated":"2022-03-23T17:35:50Z","published":"2022-03-23T17:35:50Z","title":"NeuMan: Neural Human Radiance Field from a Single Video","summary":"  Photorealistic rendering and reposing of humans is important for enabling\naugmented reality experiences. We propose a novel framework to reconstruct the\nhuman and the scene that can be rendered with novel human poses and views from\njust a single in-the-wild video. Given a video captured by a moving camera, we\ntrain two NeRF models: a human NeRF model and a scene NeRF model. To train\nthese models, we rely on existing methods to estimate the rough geometry of the\nhuman and the scene. Those rough geometry estimates allow us to create a\nwarping field from the observation space to the canonical pose-independent\nspace, where we train the human model in. Our method is able to learn subject\nspecific details, including cloth wrinkles and accessories, from just a 10\nseconds video clip, and to provide high quality renderings of the human under\nnovel poses, from novel views, together with the background.\n","authors":["Wei Jiang","Kwang Moo Yi","Golnoosh Samei","Oncel Tuzel","Anurag Ranjan"],"pdf_url":"https://arxiv.org/pdf/2203.12575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12570v1","updated":"2022-03-23T17:29:51Z","published":"2022-03-23T17:29:51Z","title":"Your \"Attention\" Deserves Attention: A Self-Diversified Multi-Channel\n  Attention for Facial Action Analysis","summary":"  Visual attention has been extensively studied for learning fine-grained\nfeatures in both facial expression recognition (FER) and Action Unit (AU)\ndetection. A broad range of previous research has explored how to use attention\nmodules to localize detailed facial parts (e,g. facial action units), learn\ndiscriminative features, and learn inter-class correlation. However, few\nrelated works pay attention to the robustness of the attention module itself.\nThrough experiments, we found neural attention maps initialized with different\nfeature maps yield diverse representations when learning to attend the\nidentical Region of Interest (ROI). In other words, similar to general feature\nlearning, the representational quality of attention maps also greatly affects\nthe performance of a model, which means unconstrained attention learning has\nlots of randomnesses. This uncertainty lets conventional attention learning\nfall into sub-optimal. In this paper, we propose a compact model to enhance the\nrepresentational and focusing power of neural attention maps and learn the\n\"inter-attention\" correlation for refined attention maps, which we term the\n\"Self-Diversified Multi-Channel Attention Network (SMA-Net)\". The proposed\nmethod is evaluated on two benchmark databases (BP4D and DISFA) for AU\ndetection and four databases (CK+, MMI, BU-3DFE, and BP4D+) for facial\nexpression recognition. It achieves superior performance compared to the\nstate-of-the-art methods.\n","authors":["Xiaotian Li","Zhihua Li","Huiyuan Yang","Geran Zhao","Lijun Yin"],"pdf_url":"https://arxiv.org/pdf/2203.12570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12560v1","updated":"2022-03-23T17:22:22Z","published":"2022-03-23T17:22:22Z","title":"DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic\n  Change Segmentation","summary":"  Earth observation is a fundamental tool for monitoring the evolution of land\nuse in specific areas of interest. Observing and precisely defining change, in\nthis context, requires both time-series data and pixel-wise segmentations. To\nthat end, we propose the DynamicEarthNet dataset that consists of daily,\nmulti-spectral satellite observations of 75 selected areas of interest\ndistributed over the globe with imagery from Planet Labs. These observations\nare paired with pixel-wise monthly semantic segmentation labels of 7 land use\nand land cover (LULC) classes. DynamicEarthNet is the first dataset that\nprovides this unique combination of daily measurements and high-quality labels.\nIn our experiments, we compare several established baselines that either\nutilize the daily observations as additional training data (semi-supervised\nlearning) or multiple observations at once (spatio-temporal learning) as a\npoint of reference for future research. Finally, we propose a new evaluation\nmetric SCS that addresses the specific challenges associated with time-series\nsemantic change segmentation. The data is available at:\nhttps://mediatum.ub.tum.de/1650201.\n","authors":["Aysim Toker","Lukas Kondmann","Mark Weber","Marvin Eisenberger","Andrés Camero","Jingliang Hu","Ariadna Pregel Hoderlein","Çağlar Şenaras","Timothy Davis","Daniel Cremers","Giovanni Marchisio","Xiao Xiang Zhu","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2203.12560v1.pdf","comment":"Accepted to CVPR 2022, evaluation webpage:\n  https://codalab.lisn.upsaclay.fr/competitions/2882"},{"id":"http://arxiv.org/abs/2203.12555v1","updated":"2022-03-23T17:14:03Z","published":"2022-03-23T17:14:03Z","title":"GriTS: Grid table similarity metric for table structure recognition","summary":"  In this paper, we propose a new class of evaluation metric for table\nstructure recognition, grid table similarity (GriTS). Unlike prior metrics,\nGriTS evaluates the correctness of a predicted table directly in its natural\nform as a matrix. To create a similarity measure between matrices, we\ngeneralize the two-dimensional largest common substructure (2D-LCS) problem,\nwhich is NP-hard, to the 2D most similar substructures (2D-MSS) problem and\npropose a polynomial-time heuristic for solving it. We validate empirically\nusing the PubTables-1M dataset that comparison between matrices exhibits more\ndesirable behavior than alternatives for table structure recognition\nevaluation. GriTS also unifies all three subtasks of cell topology recognition,\ncell location recognition, and cell content recognition within the same\nframework, which simplifies the evaluation and enables more meaningful\ncomparisons across different types of structure recognition approaches. Code\nwill be released at https://github.com/microsoft/table-transformer.\n","authors":["Brandon Smock","Rohith Pesala","Robin Abraham"],"pdf_url":"https://arxiv.org/pdf/2203.12555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.15111v2","updated":"2022-03-23T17:13:58Z","published":"2021-12-30T16:07:59Z","title":"Improving the Behaviour of Vision Transformers with Token-consistent\n  Stochastic Layers","summary":"  We introduce token-consistent stochastic layers in vision transformers,\nwithout causing any severe drop in performance. The added stochasticity\nimproves network calibration, robustness and strengthens privacy. We use linear\nlayers with token-consistent stochastic parameters inside the multilayer\nperceptron blocks, without altering the architecture of the transformer. The\nstochastic parameters are sampled from the uniform distribution, both during\ntraining and inference. The applied linear operations preserve the topological\nstructure, formed by the set of tokens passing through the shared multilayer\nperceptron. This operation encourages the learning of the recognition task to\nrely on the topological structures of the tokens, instead of their values,\nwhich in turn offers the desired robustness and privacy of the visual features.\nThe effectiveness of the token-consistent stochasticity is demonstrated on\nthree different applications, namely, network calibration, adversarial\nrobustness, and feature privacy, by boosting the performance of the respective\nestablished baselines.\n","authors":["Nikola Popovic","Danda Pani Paudel","Thomas Probst","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2112.15111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12531v1","updated":"2022-03-23T16:46:09Z","published":"2022-03-23T16:46:09Z","title":"Multi-label Transformer for Action Unit Detection","summary":"  Action Unit (AU) Detection is the branch of affective computing that aims at\nrecognizing unitary facial muscular movements. It is key to unlock unbiaised\ncomputational face representations and has therefore aroused great interest in\nthe past few years. One of main obstacles toward building efficient deep\nlearning based AU detection system facial images database annotated by AU\nexperts. In that extent the ABAW challenge paves the way toward better AU\ndetection as it involves a ~2M frames AU annotated dataset. In this paper, we\npresent our submission to the ABAW3 challenge. In a nutshell, we applied a\nmulti-label detection transformer that leverage multi-head attention to learn\nwhich part of the face image is the most relevant to predict each AU.\n","authors":["Gauthier Tallec","Edouard Yvinec","Arnaud Dapogny","Kevin Bailly"],"pdf_url":"https://arxiv.org/pdf/2203.12531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11113v2","updated":"2022-03-23T16:36:25Z","published":"2022-03-21T16:41:35Z","title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces","summary":"  Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n","authors":["Jia-Xing Zhong","Kaichen Zhou","Qingyong Hu","Bing Wang","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2203.11113v2.pdf","comment":"To appear at CVPR 2022 (Source Code:\n  https://github.com/jx-zhong-for-academic-purpose/Kinet )"},{"id":"http://arxiv.org/abs/1611.06301v3","updated":"2022-03-23T16:27:31Z","published":"2016-11-19T04:27:28Z","title":"Inferring Restaurant Styles by Mining Crowd Sourced Photos from\n  User-Review Websites","summary":"  When looking for a restaurant online, user uploaded photos often give people\nan immediate and tangible impression about a restaurant. Due to their\ninformativeness, such user contributed photos are leveraged by restaurant\nreview websites to provide their users an intuitive and effective search\nexperience. In this paper, we present a novel approach to inferring restaurant\ntypes or styles (ambiance, dish styles, suitability for different occasions)\nfrom user uploaded photos on user-review websites. To that end, we first\ncollect a novel restaurant photo dataset associating the user contributed\nphotos with the restaurant styles from TripAdvior. We then propose a deep\nmulti-instance multi-label learning (MIML) framework to deal with the unique\nproblem setting of the restaurant style classification task. We employ a\ntwo-step bootstrap strategy to train a multi-label convolutional neural network\n(CNN). The multi-label CNN is then used to compute the confidence scores of\nrestaurant styles for all the images associated with a restaurant. The computed\nconfidence scores are further used to train a final binary classifier for each\nrestaurant style tag. Upon training, the styles of a restaurant can be profiled\nby analyzing restaurant photos with the trained multi-label CNN and SVM models.\nExperimental evaluation has demonstrated that our crowd sourcing-based approach\ncan effectively infer the restaurant style when there are a sufficient number\nof user uploaded photos for a given restaurant.\n","authors":["Haofu Liao","Yuncheng Li","Tianran Hu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/1611.06301v3.pdf","comment":"10 pages, Accepted by IEEE BigData 2016"},{"id":"http://arxiv.org/abs/1812.03252v3","updated":"2022-03-23T16:26:27Z","published":"2018-12-08T01:42:40Z","title":"Face Completion with Semantic Knowledge and Collaborative Adversarial\n  Learning","summary":"  Unlike a conventional background inpainting approach that infers a missing\narea from image patches similar to the background, face completion requires\nsemantic knowledge about the target object for realistic outputs. Current image\ninpainting approaches utilize generative adversarial networks (GANs) to achieve\nsuch semantic understanding. However, in adversarial learning, the semantic\nknowledge is learned implicitly and hence good semantic understanding is not\nalways guaranteed. In this work, we propose a collaborative adversarial\nlearning approach to face completion to explicitly induce the training process.\nOur method is formulated under a novel generative framework called\ncollaborative GAN (collaGAN), which allows better semantic understanding of a\ntarget object through collaborative learning of multiple tasks including face\ncompletion, landmark detection, and semantic segmentation. Together with the\ncollaGAN, we also introduce an inpainting concentrated scheme such that the\nmodel emphasizes more on inpainting instead of autoencoding. Extensive\nexperiments show that the proposed designs are indeed effective and\ncollaborative adversarial learning provides better feature representations of\nthe faces. In comparison with other generative image inpainting models and\nsingle task learning methods, our solution produces superior performances on\nall tasks.\n","authors":["Haofu Liao","Gareth Funka-Lea","Yefeng Zheng","Jiebo Luo","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/1812.03252v3.pdf","comment":"To be appear in ACCV2018"},{"id":"http://arxiv.org/abs/1812.03507v3","updated":"2022-03-23T16:25:19Z","published":"2018-12-09T16:03:38Z","title":"More Knowledge is Better: Cross-Modality Volume Completion and 3D+2D\n  Segmentation for Intracardiac Echocardiography Contouring","summary":"  Using catheter ablation to treat atrial fibrillation increasingly relies on\nintracardiac echocardiography (ICE) for an anatomical delineation of the left\natrium and the pulmonary veins that enter the atrium. However, it is a\nchallenge to build an automatic contouring algorithm because ICE is noisy and\nprovides only a limited 2D view of the 3D anatomy. This work provides the first\nautomatic solution to segment the left atrium and the pulmonary veins from ICE.\nIn this solution, we demonstrate the benefit of building a cross-modality\nframework that can leverage a database of diagnostic images to supplement the\nless available interventional images. To this end, we develop a novel deep\nneural network approach that uses the (i) 3D geometrical information provided\nby a position sensor embedded in the ICE catheter and the (ii) 3D image\nappearance information from a set of computed tomography cardiac volumes. We\nevaluate the proposed approach over 11,000 ICE images collected from 150\nclinical patients. Experimental results show that our model is significantly\nbetter than a direct 2D image-to-image deep neural network segmentation,\nespecially for less-observed structures.\n","authors":["Haofu Liao","Yucheng Tang","Gareth Funka-Lea","Jiebo Luo","Shaohua Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/1812.03507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2007.00548v3","updated":"2022-03-23T16:24:44Z","published":"2020-07-01T15:17:56Z","title":"Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse\n  Surgical Instrument Usage for Context-aware Assistance","summary":"  Intra-operative anticipation of instrument usage is a necessary component for\ncontext-aware assistance in surgery, e.g. for instrument preparation or\nsemi-automation of robotic tasks. However, the sparsity of instrument\noccurrences in long videos poses a challenge. Current approaches are limited as\nthey assume knowledge on the timing of future actions or require dense temporal\nsegmentations during training and inference. We propose a novel learning task\nfor anticipation of instrument usage in laparoscopic videos that overcomes\nthese limitations. During training, only sparse instrument annotations are\nrequired and inference is done solely on image data. We train a probabilistic\nmodel to address the uncertainty associated with future events. Our approach\noutperforms several baselines and is competitive to a variant using richer\nannotations. We demonstrate the model's ability to quantify task-relevant\nuncertainties. To the best of our knowledge, we are the first to propose a\nmethod for anticipating instruments in surgery.\n","authors":["Dominik Rivoir","Sebastian Bodenstedt","Isabel Funke","Felix von Bechtolsheim","Marius Distler","Jürgen Weitz","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2007.00548v3.pdf","comment":"Accepted at MICCAI 2020"},{"id":"http://arxiv.org/abs/1812.03520v3","updated":"2022-03-23T16:23:53Z","published":"2018-12-09T16:56:14Z","title":"Skin Disease Classification versus Skin Lesion Characterization:\n  Achieving Robust Diagnosis using Multi-label Deep Neural Networks","summary":"  In this study, we investigate what a practically useful approach is in order\nto achieve robust skin disease diagnosis. A direct approach is to target the\nground truth diagnosis labels, while an alternative approach instead focuses on\ndetermining skin lesion characteristics that are more visually consistent and\ndiscernible. We argue that, for computer-aided skin disease diagnosis, it is\nboth more realistic and more useful that lesion type tags should be considered\nas the target of an automated diagnosis system such that the system can first\nachieve a high accuracy in describing skin lesions, and in turn facilitate\ndisease diagnosis using lesion characteristics in conjunction with other\nevidence. To further meet such an objective, we employ convolutional neural\nnetworks (CNNs) for both the disease-targeted and lesion-targeted\nclassifications. We have collected a large-scale and diverse dataset of 75,665\nskin disease images from six publicly available dermatology atlantes. Then we\ntrain and compare both disease-targeted and lesion-targeted classifiers,\nrespectively. For disease-targeted classification, only 27.6% top-1 accuracy\nand 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of\n0.42. In contrast, for lesion-targeted classification, we can achieve a much\nhigher mAP of 0.70.\n","authors":["Haofu Liao","Yuncheng Li","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/1812.03520v3.pdf","comment":null},{"id":"http://arxiv.org/abs/1812.03527v3","updated":"2022-03-23T16:20:32Z","published":"2018-12-09T17:19:54Z","title":"A Deep Multi-task Learning Approach to Skin Lesion Classification","summary":"  Skin lesion identification is a key step toward dermatological diagnosis.\nWhen describing a skin lesion, it is very important to note its body site\ndistribution as many skin diseases commonly affect particular parts of the\nbody. To exploit the correlation between skin lesions and their body site\ndistributions, in this study, we investigate the possibility of improving skin\nlesion classification using the additional context information provided by body\nlocation. Specifically, we build a deep multi-task learning (MTL) framework to\njointly optimize skin lesion classification and body location classification\n(the latter is used as an inductive bias). Our MTL framework uses the\nstate-of-the-art ImageNet pretrained model with specialized loss functions for\nthe two related tasks. Our experiments show that the proposed MTL based method\nperforms more robustly than its standalone (single-task) counterpart.\n","authors":["Haofu Liao","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/1812.03527v3.pdf","comment":"AAAI 2017 Joint Workshop on Health Intelligence W3PHIAI 2017 (W3PHI &\n  HIAI), San Francisco, CA, 2017"},{"id":"http://arxiv.org/abs/2203.12514v1","updated":"2022-03-23T16:18:51Z","published":"2022-03-23T16:18:51Z","title":"Refine-Net: Normal Refinement Neural Network for Noisy Point Clouds","summary":"  Point normal, as an intrinsic geometric property of 3D objects, not only\nserves conventional geometric tasks such as surface consolidation and\nreconstruction, but also facilitates cutting-edge learning-based techniques for\nshape analysis and generation. In this paper, we propose a normal refinement\nnetwork, called Refine-Net, to predict accurate normals for noisy point clouds.\nTraditional normal estimation wisdom heavily depends on priors such as surface\nshapes or noise distributions, while learning-based solutions settle for single\ntypes of hand-crafted features. Differently, our network is designed to refine\nthe initial normal of each point by extracting additional information from\nmultiple feature representations. To this end, several feature modules are\ndeveloped and incorporated into Refine-Net by a novel connection module.\nBesides the overall network architecture of Refine-Net, we propose a new\nmulti-scale fitting patch selection scheme for the initial normal estimation,\nby absorbing geometry domain knowledge. Also, Refine-Net is a generic normal\nestimation framework: 1) point normals obtained from other methods can be\nfurther refined, and 2) any feature module related to the surface geometric\nstructures can be potentially integrated into the framework. Qualitative and\nquantitative evaluations demonstrate the clear superiority of Refine-Net over\nthe state-of-the-arts on both synthetic and real-scanned datasets. Our code is\navailable at https://github.com/hrzhou2/refinenet.\n","authors":["Haoran Zhou","Honghua Chen","Yingkui Zhang","Mingqiang Wei","Haoran Xie","Jun Wang","Tong Lu","Jing Qin","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.12514v1.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/1907.00294v4","updated":"2022-03-23T16:18:11Z","published":"2019-06-29T23:30:10Z","title":"Generative Mask Pyramid Network for CT/CBCT Metal Artifact Reduction\n  with Joint Projection-Sinogram Correction","summary":"  A conventional approach to computed tomography (CT) or cone beam CT (CBCT)\nmetal artifact reduction is to replace the X-ray projection data within the\nmetal trace with synthesized data. However, existing projection or sinogram\ncompletion methods cannot always produce anatomically consistent information to\nfill the metal trace, and thus, when the metallic implant is large, significant\nsecondary artifacts are often introduced. In this work, we propose to replace\nmetal artifact affected regions with anatomically consistent content through\njoint projection-sinogram correction as well as adversarial learning. To handle\nthe metallic implants of diverse shapes and large sizes, we also propose a\nnovel mask pyramid network that enforces the mask information across the\nnetwork's encoding layers and a mask fusion loss that reduces early saturation\nof adversarial training. Our experimental results show that the proposed\nprojection-sinogram correction designs are effective and our method recovers\ninformation from the metal traces better than the state-of-the-art methods.\n","authors":["Haofu Liao","Wei-An Lin","Zhimin Huo","Levon Vogelsang","William J. Sehnert","S. Kevin Zhou","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/1907.00294v4.pdf","comment":"This paper is accepted to MICCAI 2019"},{"id":"http://arxiv.org/abs/2110.04158v3","updated":"2022-03-23T15:43:39Z","published":"2021-10-08T14:29:02Z","title":"Explainability-Aware One Point Attack for Point Cloud Neural Networks","summary":"  With the proposition of neural networks for point clouds, deep learning has\nstarted to shine in the field of 3D object recognition while researchers have\nshown an increased interest to investigate the reliability of point cloud\nnetworks by adversarial attacks. However, most of the existing studies aim to\ndeceive humans or defense algorithms, while the few that address the operation\nprinciples of the models themselves remain flawed in terms of critical point\nselection. In this work, we propose two adversarial methods: One Point Attack\n(OPA) and Critical Traversal Attack (CTA), which incorporate the explainability\ntechnologies and aim to explore the intrinsic operating principle of point\ncloud networks and their sensitivity against critical points perturbations. Our\nresults show that popular point cloud networks can be deceived with almost\n$100\\%$ success rate by shifting only one point from the input instance. In\naddition, we show the interesting impact of different point attribution\ndistributions on the adversarial robustness of point cloud networks. Finally,\nwe discuss how our approaches facilitate the explainability study for point\ncloud networks. To the best of our knowledge, this is the first\npoint-cloud-based adversarial approach concerning explainability. Our code is\navailable at https://github.com/Explain3D/Exp-One-Point-Atk-PC.\n","authors":["Hanxiao Tan","Helena Kotthaus"],"pdf_url":"https://arxiv.org/pdf/2110.04158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12485v1","updated":"2022-03-23T15:25:31Z","published":"2022-03-23T15:25:31Z","title":"CroMo: Cross-Modal Learning for Monocular Depth Estimation","summary":"  Learning-based depth estimation has witnessed recent progress in multiple\ndirections; from self-supervision using monocular video to supervised methods\noffering highest accuracy. Complementary to supervision, further boosts to\nperformance and robustness are gained by combining information from multiple\nsignals. In this paper we systematically investigate key trade-offs associated\nwith sensor and modality design choices as well as related model training\nstrategies. Our study leads us to a new method, capable of connecting\nmodality-specific advantages from polarisation, Time-of-Flight and\nstructured-light inputs. We propose a novel pipeline capable of estimating\ndepth from monocular polarisation for which we evaluate various training\nsignals. The inversion of differentiable analytic models thereby connects scene\ngeometry with polarisation and ToF signals and enables self-supervised and\ncross-modal learning. In the absence of existing multimodal datasets, we\nexamine our approach with a custom-made multi-modal camera rig and collect\nCroMo; the first dataset to consist of synchronized stereo polarisation,\nindirect ToF and structured-light depth, captured at video rates. Extensive\nexperiments on challenging video scenes confirm both qualitative and\nquantitative pipeline advantages where we are able to outperform competitive\nmonocular depth estimation method.\n","authors":["Yannick Verdié","Jifei Song","Barnabé Mas","Benjamin Busam","Aleš Leonardis","Steven McDonagh"],"pdf_url":"https://arxiv.org/pdf/2203.12485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12482v1","updated":"2022-03-23T15:23:24Z","published":"2022-03-23T15:23:24Z","title":"A Deep Learning Framework to Reconstruct Face under Mask","summary":"  While deep learning-based image reconstruction methods have shown significant\nsuccess in removing objects from pictures, they have yet to achieve acceptable\nresults for attributing consistency to gender, ethnicity, expression, and other\ncharacteristics like the topological structure of the face. The purpose of this\nwork is to extract the mask region from a masked image and rebuild the area\nthat has been detected. This problem is complex because (i) it is difficult to\ndetermine the gender of an image hidden behind a mask, which causes the network\nto become confused and reconstruct the male face as a female or vice versa;\n(ii) we may receive images from multiple angles, making it extremely difficult\nto maintain the actual shape, topological structure of the face and a natural\nimage; and (iii) there are problems with various mask forms because, in some\ncases, the area of the mask cannot be anticipated precisely; certain parts of\nthe mask remain on the face after completion. To solve this complex task, we\nsplit the problem into three phases: landmark detection, object detection for\nthe targeted mask area, and inpainting the addressed mask region. To begin, to\nsolve the first problem, we have used gender classification, which detects the\nactual gender behind a mask, then we detect the landmark of the masked facial\nimage. Second, we identified the non-face item, i.e., the mask, and used the\nMask R-CNN network to create the binary mask of the observed mask area.\nThirdly, we developed an inpainting network that uses anticipated landmarks to\ncreate realistic images. To segment the mask, this article uses a mask R-CNN\nand offers a binary segmentation map for identifying the mask area.\nAdditionally, we generated the image utilizing landmarks as structural guidance\nthrough a GAN-based network. The studies presented in this paper use the FFHQ\nand CelebA datasets.\n","authors":["Gourango Modak","Shuvra Smaran Das","Md. Ajharul Islam Miraj","Md. Kishor Morol"],"pdf_url":"https://arxiv.org/pdf/2203.12482v1.pdf","comment":"6 pages, 9 figures, 2022 7th Conference on Data Science and Machine\n  Learning Applications (CDMA)"},{"id":"http://arxiv.org/abs/2203.12476v1","updated":"2022-03-23T15:16:29Z","published":"2022-03-23T15:16:29Z","title":"Adaptively Re-weighting Multi-Loss Untrained Transformer for Sparse-View\n  Cone-Beam CT Reconstruction","summary":"  Cone-Beam Computed Tomography (CBCT) has been proven useful in diagnosis, but\nhow to shorten scanning time with lower radiation dosage and how to efficiently\nreconstruct 3D image remain as the main issues for clinical practice. The\nrecent development of tomographic image reconstruction on sparse-view\nmeasurements employs deep neural networks in a supervised way to tackle such\nissues, whereas the success of model training requires quantity and quality of\nthe given paired measurements/images. We propose a novel untrained Transformer\nto fit the CBCT inverse solver without training data. It is mainly comprised of\nan untrained 3D Transformer of billions of network weights and a multi-level\nloss function with variable weights. Unlike conventional deep neural networks\n(DNNs), there is no requirement of training steps in our approach. Upon\nobserving the hardship of optimising Transformer, the variable weights within\nthe loss function are designed to automatically update together with the\niteration process, ultimately stabilising its optimisation. We evaluate the\nproposed approach on two publicly available datasets: SPARE and Walnut. The\nresults show a significant performance improvement on image quality metrics\nwith streak artefact reduction in the visualisation. We also provide a clinical\nreport by an experienced radiologist to assess our reconstructed images in a\ndiagnosis point of view. The source code and the optimised models are available\nfrom the corresponding author on request at the moment.\n","authors":["Minghui Wu","Yangdi Xu","Yingying Xu","Guangwei Wu","Qingqing Chen","Hongxiang Lin"],"pdf_url":"https://arxiv.org/pdf/2203.12476v1.pdf","comment":"12 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2203.06667v4","updated":"2022-03-23T15:10:44Z","published":"2022-03-13T14:42:53Z","title":"Towards Visual-Prompt Temporal Answering Grounding in Medical\n  Instructional Video","summary":"  The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms other\nstate-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,\nwhich demonstrates the effectiveness of visual prompt and the text span\npredictor.\n","authors":["Bin Li","Yixuan Weng","Bin Sun","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2203.06667v4.pdf","comment":"8 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2203.12469v1","updated":"2022-03-23T15:05:23Z","published":"2022-03-23T15:05:23Z","title":"3D Adapted Random Forest Vision (3DARFV) for Untangling\n  Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency\n  at the Utmost Accuracy","summary":"  Planetary exploration depends heavily on 3D image data to characterize the\nstatic and dynamic properties of the rock and environment. Analyzing 3D images\nrequires many computations, causing efficiency to suffer lengthy processing\ntime alongside large energy consumption. High-Performance Computing (HPC)\nprovides apparent efficiency at the expense of energy consumption. However, for\nremote explorations, the conveyed surveillance and the robotized sensing need\nfaster data analysis with ultimate accuracy to make real-time decisions. In\nsuch environments, access to HPC and energy is limited. Therefore, we realize\nthat reducing the number of computations to optimal and maintaining the desired\naccuracy leads to higher efficiency. This paper demonstrates the semantic\nsegmentation capability of a probabilistic decision tree algorithm, 3D Adapted\nRandom Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at\nthe utmost accuracy.\n","authors":["Omar Alfarisi","Zeyar Aung","Qingfeng Huang","Ashraf Al-Khateeb","Hamed Alhashmi","Mohamed Abdelsalam","Salem Alzaabi","Haifa Alyazeedi","Anthony Tzes"],"pdf_url":"https://arxiv.org/pdf/2203.12469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06312v2","updated":"2022-03-23T15:03:53Z","published":"2022-02-13T13:41:15Z","title":"Adversarial Fine-tuning for Backdoor Defense: Connecting Backdoor\n  Attacks to Adversarial Attacks","summary":"  Deep neural networks (DNNs) are known to be vulnerable to both backdoor\nattacks as well as adversarial attacks. In the literature, these two types of\nattacks are commonly treated as distinct problems and solved separately, since\nthey belong to training-time and inference-time attacks respectively. However,\nin this paper we find an intriguing connection between them: for a model\nplanted with backdoors, we observe that its adversarial examples have similar\nbehaviors as its triggered samples, i.e., both activate the same subset of DNN\nneurons. It indicates that planting a backdoor into a model will significantly\naffect the model's adversarial examples. Based on this observations, we design\na new Adversarial Fine-Tuning (AFT) algorithm to defend against backdoor\nattacks. We empirically show that, against 5 state-of-the-art backdoor attacks,\nour AFT can effectively erase the backdoor triggers without obvious performance\ndegradation on clean samples and significantly outperforms existing defense\nmethods.\n","authors":["Bingxu Mu","Zhenxing Niu","Le Wang","Xue Wang","Rong Jin","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2202.06312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2003.05597v4","updated":"2022-03-23T14:58:55Z","published":"2020-03-12T03:23:54Z","title":"On the Arbitrary-Oriented Object Detection: Classification based\n  Approaches Revisited","summary":"  Arbitrary-oriented object detection has been a building block for rotation\nsensitive tasks. We first show that the boundary problem suffered in existing\ndominant regression-based rotation detectors, is caused by angular periodicity\nor corner ordering, according to the parameterization protocol. We also show\nthat the root cause is that the ideal predictions can be out of the defined\nrange. Accordingly, we transform the angular prediction task from a regression\nproblem to a classification one. For the resulting circularly distributed angle\nclassification problem, we first devise a Circular Smooth Label technique to\nhandle the periodicity of angle and increase the error tolerance to adjacent\nangles. To reduce the excessive model parameters by Circular Smooth Label, we\nfurther design a Densely Coded Labels, which greatly reduces the length of the\nencoding. Finally, we further develop an object heading detection module, which\ncan be useful when the exact heading orientation information is needed e.g. for\nship and plane heading detection. We release our OHD-SJTU dataset and OHDet\ndetector for heading detection. Extensive experimental results on three\nlarge-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU,\nand face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show\nthe effectiveness of our approach.\n","authors":["Xue Yang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2003.05597v4.pdf","comment":"19 pages, 16 figures, 18 tables, journal version of CSL (ECCV2020)\n  and DCL (CVPR2021), accepted by IJCV2022"},{"id":"http://arxiv.org/abs/2203.12459v1","updated":"2022-03-23T14:54:29Z","published":"2022-03-23T14:54:29Z","title":"Activation-Based Sampling for Pixel- to Image-Level Aggregation in\n  Weakly-Supervised Segmentation","summary":"  Classification networks can be used to localize and segment objects in images\nby means of class activation maps (CAMs). However, without pixel-level\nannotations, they are known to (1) mainly focus on discriminative regions, and\n(2) to produce diffuse CAMs without well-defined prediction contours. In this\nwork, we approach both problems with two contributions for improving CAM\nlearning. First, we incorporate importance sampling based on the class-wise\nprobability mass function induced by the CAMs to produce stochastic image-level\nclass predictions. This results in CAMs which activate over a larger extent of\nthe objects. Second, we formulate a feature similarity loss term which aims to\nmatch the prediction contours with edges in the image. As a third contribution,\nwe conduct experiments on the PASCAL VOC and MS-COCO benchmark datasets to\ndemonstrate that these modifications significantly increase the performance in\nterms of contour accuracy, while being comparable to current state-of-the-art\nmethods in terms of region similarity.\n","authors":["Arvi Jonnarth","Michael Felsberg","Yushan Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.12459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12454v1","updated":"2022-03-23T14:51:00Z","published":"2022-03-23T14:51:00Z","title":"MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation\n  with Limited Source Labels","summary":"  The success of deep convolutional neural networks (DCNNs) benefits from high\nvolumes of annotated data. However, annotating medical images is laborious,\nexpensive, and requires human expertise, which induces the label scarcity\nproblem. Especially when encountering the domain shift, the problem becomes\nmore serious. Although deep unsupervised domain adaptation (UDA) can leverage\nwell-established source domain annotations and abundant target domain data to\nfacilitate cross-modality image segmentation and also mitigate the label\npaucity problem on the target domain, the conventional UDA methods suffer from\nsevere performance degradation when source domain annotations are scarce. In\nthis paper, we explore a challenging UDA setting - limited source domain\nannotations. We aim to investigate how to efficiently leverage unlabeled data\nfrom the source and target domains with limited source annotations for\ncross-modality image segmentation. To achieve this, we propose a new\nlabel-efficient UDA framework, termed MT-UDA, in which the student model\ntrained with limited source labels learns from unlabeled data of both domains\nby two teacher models respectively in a semi-supervised manner. More\nspecifically, the student model not only distills the intra-domain semantic\nknowledge by encouraging prediction consistency but also exploits the\ninter-domain anatomical information by enforcing structural consistency.\nConsequently, the student model can effectively integrate the underlying\nknowledge beneath available data resources to mitigate the impact of source\nlabel scarcity and yield improved cross-modality segmentation performance. We\nevaluate our method on MM-WHS 2017 dataset and demonstrate that our approach\noutperforms the state-of-the-art methods by a large margin under the\nsource-label scarcity scenario.\n","authors":["Ziyuan Zhao","Kaixin Xu","Shumeng Li","Zeng Zeng","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2203.12454v1.pdf","comment":"Accept by MICCAI 2021, code at:\n  https://github.com/jacobzhaoziyuan/MT-UDA"},{"id":"http://arxiv.org/abs/2203.12446v1","updated":"2022-03-23T14:40:20Z","published":"2022-03-23T14:40:20Z","title":"SMEMO: Social Memory for Trajectory Forecasting","summary":"  Effective modeling of human interactions is of utmost importance when\nforecasting behaviors such as future trajectories. Each individual, with its\nmotion, influences surrounding agents since everyone obeys to social\nnon-written rules such as collision avoidance or group following. In this paper\nwe model such interactions, which constantly evolve through time, by looking at\nthe problem from an algorithmic point of view, i.e. as a data manipulation\ntask. We present a neural network based on an end-to-end trainable working\nmemory, which acts as an external storage where information about each agent\ncan be continuously written, updated and recalled. We show that our method is\ncapable of learning explainable cause-effect relationships between motions of\ndifferent agents, obtaining state-of-the-art results on multiple trajectory\nforecasting datasets.\n","authors":["Francesco Marchetti","Federico Becattini","Lorenzo Seidenari","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2203.12446v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2203.12428v1","updated":"2022-03-23T14:07:39Z","published":"2022-03-23T14:07:39Z","title":"An Attention-based Method for Action Unit Detection at the 3rd ABAW\n  Competition","summary":"  Facial Action Coding System is an approach for modeling the complexity of\nhuman emotional expression. Automatic action unit (AU) detection is a crucial\nresearch area in human-computer interaction. This paper describes our\nsubmission to the third Affective Behavior Analysis in-the-wild (ABAW)\ncompetition 2022. We proposed a method for detecting facial action units in the\nvideo. At the first stage, a lightweight CNN-based feature extractor is\nemployed to extract the feature map from each video frame. Then, an attention\nmodule is applied to refine the attention map. The attention encoded vector is\nderived using a weighted sum of the feature map and the attention scores later.\nFinally, the sigmoid function is used at the output layer to make the\nprediction suitable for multi-label AUs detection. We achieved a macro F1 score\nof 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline\nmodel.\n","authors":["Duy Le Hoai","Eunchae Lim","Eunbin Choi","Sieun Kim","Sudarshan Pant","Guee-Sang Lee","Soo-Huyng Kim","Hyung-Jeong Yang"],"pdf_url":"https://arxiv.org/pdf/2203.12428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.08644v2","updated":"2022-03-23T14:06:40Z","published":"2021-11-16T17:28:46Z","title":"UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection","summary":"  Detecting abnormal events in video is commonly framed as a one-class\nclassification task, where training videos contain only normal events, while\ntest videos encompass both normal and abnormal events. In this scenario,\nanomaly detection is an open-set problem. However, some studies assimilate\nanomaly detection to action recognition. This is a closed-set scenario that\nfails to test the capability of systems at detecting new anomaly types. To this\nend, we propose UBnormal, a new supervised open-set benchmark composed of\nmultiple virtual scenes for video anomaly detection. Unlike existing data sets,\nwe introduce abnormal events annotated at the pixel level at training time, for\nthe first time enabling the use of fully-supervised learning methods for\nabnormal event detection. To preserve the typical open-set formulation, we make\nsure to include disjoint sets of anomaly types in our training and test\ncollections of videos. To our knowledge, UBnormal is the first video anomaly\ndetection benchmark to allow a fair head-to-head comparison between one-class\nopen-set models and supervised closed-set models, as shown in our experiments.\nMoreover, we provide empirical evidence showing that UBnormal can enhance the\nperformance of a state-of-the-art anomaly detection framework on two prominent\ndata sets, Avenue and ShanghaiTech. Our benchmark is freely available at\nhttps://github.com/lilygeorgescu/UBnormal.\n","authors":["Andra Acsintoae","Andrei Florescu","Mariana-Iuliana Georgescu","Tudor Mare","Paul Sumedrea","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2111.08644v2.pdf","comment":"Accepted at CVPR 2022. Paper + supplementary (15 pages, 9 figures)"},{"id":"http://arxiv.org/abs/2201.02001v3","updated":"2022-03-23T13:51:06Z","published":"2022-01-06T10:20:24Z","title":"TransVPR: Transformer-based place recognition with multi-level attention\n  aggregation","summary":"  Visual place recognition is a challenging task for applications such as\nautonomous driving navigation and mobile robot localization. Distracting\nelements presenting in complex scenes often lead to deviations in the\nperception of visual place. To address this problem, it is crucial to integrate\ninformation from only task-relevant regions into image representations. In this\npaper, we introduce a novel holistic place recognition model, TransVPR, based\non vision Transformers. It benefits from the desirable property of the\nself-attention operation in Transformers which can naturally aggregate\ntask-relevant features. Attentions from multiple levels of the Transformer,\nwhich focus on different regions of interest, are further combined to generate\na global image representation. In addition, the output tokens from Transformer\nlayers filtered by the fused attention mask are considered as key-patch\ndescriptors, which are used to perform spatial matching to re-rank the\ncandidates retrieved by the global image features. The whole model allows\nend-to-end training with a single objective and image-level supervision.\nTransVPR achieves state-of-the-art performance on several real-world benchmarks\nwhile maintaining low computational time and storage requirements.\n","authors":["Ruotong Wang","Yanqing Shen","Weiliang Zuo","Sanping Zhou","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2201.02001v3.pdf","comment":"Accepted in CVPR 2022"},{"id":"http://arxiv.org/abs/2203.07967v3","updated":"2022-03-23T13:46:13Z","published":"2022-03-15T14:52:52Z","title":"Intrinsic Neural Fields: Learning Functions on Manifolds","summary":"  Neural fields have gained significant attention in the computer vision\ncommunity due to their excellent performance in novel view synthesis, geometry\nreconstruction, and generative modeling. Some of their advantages are a sound\ntheoretic foundation and an easy implementation in current deep learning\nframeworks. While neural fields have been applied to signals on manifolds,\ne.g., for texture reconstruction, their representation has been limited to\nextrinsically embedding the shape into Euclidean space. The extrinsic embedding\nignores known intrinsic manifold properties and is inflexible wrt. transfer of\nthe learned function. To overcome these limitations, this work introduces\nintrinsic neural fields, a novel and versatile representation for neural fields\non manifolds. Intrinsic neural fields combine the advantages of neural fields\nwith the spectral properties of the Laplace-Beltrami operator. We show\ntheoretically that intrinsic neural fields inherit many desirable properties of\nthe extrinsic neural field framework but exhibit additional intrinsic\nqualities, like isometry invariance. In experiments, we show intrinsic neural\nfields can reconstruct high-fidelity textures from images with state-of-the-art\nquality and are robust to the discretization of the underlying manifold. We\ndemonstrate the versatility of intrinsic neural fields by tackling various\napplications: texture transfer between deformed shapes & different shapes,\ntexture reconstruction from real-world images with view dependence, and\ndiscretization-agnostic learning on meshes and point clouds.\n","authors":["Lukas Koestler","Daniel Grittner","Michael Moeller","Daniel Cremers","Zorah Lähner"],"pdf_url":"https://arxiv.org/pdf/2203.07967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12412v1","updated":"2022-03-23T13:44:15Z","published":"2022-03-23T13:44:15Z","title":"U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture\n  Search","summary":"  Optimizing resource utilization in target platforms is key to achieving high\nperformance during DNN inference. While optimizations have been proposed for\ninference latency, memory footprint, and energy consumption, prior\nhardware-aware neural architecture search (NAS) methods have omitted resource\nutilization, preventing DNNs to take full advantage of the target inference\nplatforms. Modeling resource utilization efficiently and accurately is\nchallenging, especially for widely-used array-based inference accelerators such\nas Google TPU. In this work, we propose a novel hardware-aware NAS framework\nthat does not only optimize for task accuracy and inference latency, but also\nfor resource utilization. We also propose and validate a new computational\nmodel for resource utilization in inference accelerators. By using the proposed\nNAS framework and the proposed resource utilization model, we achieve 2.8 - 4x\nspeedup for DNN inference compared to prior hardware-aware NAS methods while\nattaining similar or improved accuracy in image classification on CIFAR-10 and\nImagenet-100 datasets.\n","authors":["Ahmet Caner Yüzügüler","Nikolaos Dimitriadis","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2203.12412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13475v3","updated":"2022-03-23T13:21:59Z","published":"2021-11-26T12:44:54Z","title":"QMagFace: Simple and Accurate Quality-Aware Face Recognition","summary":"  Face recognition systems have to deal with large variabilities (such as\ndifferent poses, illuminations, and expressions) that might lead to incorrect\nmatching decisions. These variabilities can be measured in terms of face image\nquality which is defined over the utility of a sample for recognition. Previous\nworks on face recognition either do not employ this valuable information or\nmake use of non-inherently fit quality estimates. In this work, we propose a\nsimple and effective face recognition solution (QMagFace) that combines a\nquality-aware comparison score with a recognition model based on a\nmagnitude-aware angular margin loss. The proposed approach includes\nmodel-specific face image qualities in the comparison process to enhance the\nrecognition performance under unconstrained circumstances. Exploiting the\nlinearity between the qualities and their comparison scores induced by the\nutilized loss, our quality-aware comparison function is simple and highly\ngeneralizable. The experiments conducted on several face recognition databases\nand benchmarks demonstrate that the introduced quality-awareness leads to\nconsistent improvements in the recognition performance. Moreover, the proposed\nQMagFace approach performs especially well under challenging circumstances,\nsuch as cross-pose, cross-age, or cross-quality. Consequently, it leads to\nstate-of-the-art performances on several face recognition benchmarks, such as\n98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace\nis publicly available\n","authors":["Philipp Terhörst","Malte Ihlefeld","Marco Huber","Naser Damer","Florian Kirchbuchner","Kiran Raja","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2111.13475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11754v2","updated":"2022-03-23T13:15:17Z","published":"2022-03-22T14:10:16Z","title":"Exploring and Evaluating Image Restoration Potential in Dynamic Scenes","summary":"  In dynamic scenes, images often suffer from dynamic blur due to superposition\nof motions or low signal-noise ratio resulted from quick shutter speed when\navoiding motions. Recovering sharp and clean results from the captured images\nheavily depends on the ability of restoration methods and the quality of the\ninput. Although existing research on image restoration focuses on developing\nmodels for obtaining better restored results, fewer have studied to evaluate\nhow and which input image leads to superior restored quality. In this paper, to\nbetter study an image's potential value that can be explored for restoration,\nwe propose a novel concept, referring to image restoration potential (IRP).\nSpecifically, We first establish a dynamic scene imaging dataset containing\ncomposite distortions and applied image restoration processes to validate the\nrationality of the existence to IRP. Based on this dataset, we investigate\nseveral properties of IRP and propose a novel deep model to accurately predict\nIRP values. By gradually distilling and selective fusing the degradation\nfeatures, the proposed model shows its superiority in IRP prediction. Thanks to\nthe proposed model, we are then able to validate how various image restoration\nrelated applications are benefited from IRP prediction. We show the potential\nusages of IRP as a filtering principle to select valuable frames, an auxiliary\nguidance to improve restoration models, and even an indicator to optimize\ncamera settings for capturing better images under dynamic scenarios.\n","authors":["Cheng Zhang","Shaolin Su","Yu Zhu","Qingsen Yan","Jinqiu Sun","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.11754v2.pdf","comment":"Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2203.12387v1","updated":"2022-03-23T13:02:41Z","published":"2022-03-23T13:02:41Z","title":"On the (Limited) Generalization of MasterFace Attacks and Its Relation\n  to the Capacity of Face Representations","summary":"  A MasterFace is a face image that can successfully match against a large\nportion of the population. Since their generation does not require access to\nthe information of the enrolled subjects, MasterFace attacks represent a\npotential security risk for widely-used face recognition systems. Previous\nworks proposed methods for generating such images and demonstrated that these\nattacks can strongly compromise face recognition. However, previous works\nfollowed evaluation settings consisting of older recognition models, limited\ncross-dataset and cross-model evaluations, and the use of low-scale testing\ndata. This makes it hard to state the generalizability of these attacks. In\nthis work, we comprehensively analyse the generalizability of MasterFace\nattacks in empirical and theoretical investigations. The empirical\ninvestigations include the use of six state-of-the-art FR models, cross-dataset\nand cross-model evaluation protocols, and utilizing testing datasets of\nsignificantly higher size and variance. The results indicate a low\ngeneralizability when MasterFaces are training on a different face recognition\nmodel than the one used for testing. In these cases, the attack performance is\nsimilar to zero-effort imposter attacks. In the theoretical investigations, we\ndefine and estimate the face capacity and the maximum MasterFace coverage under\nthe assumption that identities in the face space are well separated. The\ncurrent trend of increasing the fairness and generalizability in face\nrecognition indicates that the vulnerability of future systems might further\ndecrease. We conclude that MasterFaces should not be seen as a threat to face\nrecognition systems but, on the contrary, seen as a tool to understand and\nenhance the robustness of face recognition models.\n","authors":["Philipp Terhörst","Florian Bierbaum","Marco Huber","Naser Damer","Florian Kirchbuchner","Kiran Raja","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2203.12387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09855v2","updated":"2022-03-23T12:41:52Z","published":"2022-03-18T10:48:22Z","title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion","summary":"  In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\nCodes and pre-trained models are available at\nhttps://github.com/anonymoustbd/MMMPT.\n","authors":["Zhiqiang Yan","Xiang Li","Kun Wang","Zhenyu Zhang","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2203.09855v2.pdf","comment":"26 pages, 13 figures"},{"id":"http://arxiv.org/abs/2111.12503v3","updated":"2022-03-23T12:41:40Z","published":"2021-11-24T13:58:20Z","title":"Extracting Triangular 3D Models, Materials, and Lighting From Images","summary":"  We present an efficient method for joint optimization of topology, materials\nand lighting from multi-view image observations. Unlike recent multi-view\nreconstruction approaches, which typically produce entangled 3D representations\nencoded in neural networks, we output triangle meshes with spatially-varying\nmaterials and environment lighting that can be deployed in any traditional\ngraphics engine unmodified. We leverage recent work in differentiable\nrendering, coordinate-based networks to compactly represent volumetric\ntexturing, alongside differentiable marching tetrahedrons to enable\ngradient-based optimization directly on the surface mesh. Finally, we introduce\na differentiable formulation of the split sum approximation of environment\nlighting to efficiently recover all-frequency lighting. Experiments show our\nextracted models used in advanced scene editing, material decomposition, and\nhigh quality view interpolation, all running at interactive rates in\ntriangle-based renderers (rasterizers and path tracers). Project website:\nhttps://nvlabs.github.io/nvdiffrec/ .\n","authors":["Jacob Munkberg","Jon Hasselgren","Tianchang Shen","Jun Gao","Wenzheng Chen","Alex Evans","Thomas Müller","Sanja Fidler"],"pdf_url":"https://arxiv.org/pdf/2111.12503v3.pdf","comment":"Project website: https://nvlabs.github.io/nvdiffrec/"},{"id":"http://arxiv.org/abs/2203.12367v1","updated":"2022-03-23T12:38:50Z","published":"2022-03-23T12:38:50Z","title":"Transformer-based Multimodal Information Fusion for Facial Expression\n  Analysis","summary":"  Facial expression analysis has been a crucial research problem in the\ncomputer vision area. With the recent development of deep learning techniques\nand large-scale in-the-wild annotated datasets, facial expression analysis is\nnow aimed at challenges in real world settings. In this paper, we introduce our\nsubmission to CVPR2022 Competition on Affective Behavior Analysis in-the-wild\n(ABAW) that defines four competition tasks, including expression\nclassification, action unit detection, valence-arousal estimation, and a\nmulti-task-learning. The available multimodal information consist of spoken\nwords, speech prosody, and visual expression in videos. Our work proposes four\nunified transformer-based network frameworks to create the fusion of the above\nmultimodal information. The preliminary results on the official Aff-Wild2\ndataset are reported and demonstrate the effectiveness of our proposed method.\n","authors":["Wei Zhang","Zhimeng Zhang","Feng Qiu","Suzhen Wang","Bowen Ma","Hao Zeng","Rudong An","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2203.12367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12350v1","updated":"2022-03-23T12:02:10Z","published":"2022-03-23T12:02:10Z","title":"Hyper-Spectral Imaging for Overlapping Plastic Flakes Segmentation","summary":"  Given the hyper-spectral imaging unique potentials in grasping the polymer\ncharacteristics of different materials, it is commonly used in sorting\nprocedures. In a practical plastic sorting scenario, multiple plastic flakes\nmay overlap which depending on their characteristics, the overlap can be\nreflected in their spectral signature. In this work, we use hyper-spectral\nimaging for the segmentation of three types of plastic flakes and their\npossible overlapping combinations. We propose an intuitive and simple\nmulti-label encoding approach, bitfield encoding, to account for the\noverlapping regions. With our experiments, we show that the bitfield encoding\nimproves over the baseline single-label approach and we further demonstrate its\npotential in predicting multiple labels for overlapping classes even when the\nmodel is only trained with non-overlapping classes.\n","authors":["Guillem Martinez","Maya Aghaei","Martin Dijkstra","Bhalaji Nagarajan","Femke Jaarsma","Jaap van de Loosdrecht","Petia Radeva","Klaas Dijkstra"],"pdf_url":"https://arxiv.org/pdf/2203.12350v1.pdf","comment":"Submitted to ICIP2022"},{"id":"http://arxiv.org/abs/2203.12346v1","updated":"2022-03-23T11:56:25Z","published":"2022-03-23T11:56:25Z","title":"Robust Text Line Detection in Historical Documents: Learning and\n  Evaluation Methods","summary":"  Text line segmentation is one of the key steps in historical document\nunderstanding. It is challenging due to the variety of fonts, contents, writing\nstyles and the quality of documents that have degraded through the years.\n  In this paper, we address the limitations that currently prevent people from\nbuilding line segmentation models with a high generalization capacity. We\npresent a study conducted using three state-of-the-art systems Doc-UFCN,\ndhSegment and ARU-Net and show that it is possible to build generic models\ntrained on a wide variety of historical document datasets that can correctly\nsegment diverse unseen pages. This paper also highlights the importance of the\nannotations used during training: each existing dataset is annotated\ndifferently. We present a unification of the annotations and show its positive\nimpact on the final text recognition results. In this end, we present a\ncomplete evaluation strategy using standard pixel-level metrics, object-level\nones and introducing goal-oriented metrics.\n","authors":["Mélodie Boillet","Christopher Kermorvant","Thierry Paquet"],"pdf_url":"https://arxiv.org/pdf/2203.12346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12344v1","updated":"2022-03-23T11:53:41Z","published":"2022-03-23T11:53:41Z","title":"How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs","summary":"  We aim to understand how actions are performed and identify subtle\ndifferences, such as 'fold firmly' vs. 'fold gently'. To this end, we propose a\nmethod which recognizes adverbs across different actions. However, such\nfine-grained annotations are difficult to obtain and their long-tailed nature\nmakes it challenging to recognize adverbs in rare action-adverb compositions.\nOur approach therefore uses semi-supervised learning with multiple adverb\npseudo-labels to leverage videos with only action labels. Combined with\nadaptive thresholding of these pseudo-adverbs we are able to make efficient use\nof the available data while tackling the long-tailed distribution.\nAdditionally, we gather adverb annotations for three existing video retrieval\ndatasets, which allows us to introduce the new tasks of recognizing adverbs in\nunseen action-adverb compositions and unseen domains. Experiments demonstrate\nthe effectiveness of our method, which outperforms prior work in recognizing\nadverbs and semi-supervised works adapted for adverb recognition. We also show\nhow adverbs can relate fine-grained actions.\n","authors":["Hazel Doughty","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2203.12344v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.10463v2","updated":"2022-03-23T11:45:37Z","published":"2022-03-20T06:06:43Z","title":"Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural\n  Networks","summary":"  In this paper, we propose a new adapter network for adapting a pre-trained\ndeep neural network to a target domain with minimal computation. The proposed\nmodel, unidirectional thin adapter (UDTA), helps the classifier adapt to new\ndata by providing auxiliary features that complement the backbone network. UDTA\ntakes outputs from multiple layers of the backbone as input features but does\nnot transmit any feature to the backbone. As a result, UDTA can learn without\ncomputing the gradient of the backbone, which saves computation for training\nsignificantly. In addition, since UDTA learns the target task without modifying\nthe backbone, a single backbone can adapt to multiple tasks by learning only\nUDTAs separately. In experiments on five fine-grained classification datasets\nconsisting of a small number of samples, UDTA significantly reduced computation\nand training time required for backpropagation while showing comparable or even\nimproved accuracy compared with conventional adapter models.\n","authors":["Han Gyel Sun","Hyunjae Ahn","HyunGyu Lee","Injung Kim"],"pdf_url":"https://arxiv.org/pdf/2203.10463v2.pdf","comment":"9 pages and 7 figures"},{"id":"http://arxiv.org/abs/2203.12341v1","updated":"2022-03-23T11:43:29Z","published":"2022-03-23T11:43:29Z","title":"Towards Semi-Supervised Deep Facial Expression Recognition with An\n  Adaptive Confidence Margin","summary":"  Only parts of unlabeled data are selected to train models for most\nsemi-supervised learning methods, whose confidence scores are usually higher\nthan the pre-defined threshold (i.e., the confidence margin). We argue that the\nrecognition performance should be further improved by making full use of all\nunlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM)\nto fully leverage all unlabeled data for semi-supervised deep facial expression\nrecognition. All unlabeled samples are partitioned into two subsets by\ncomparing their confidence scores with the adaptively learned confidence margin\nat each training epoch: (1) subset I including samples whose confidence scores\nare no lower than the margin; (2) subset II including samples whose confidence\nscores are lower than the margin. For samples in subset I, we constrain their\npredictions to match pseudo labels. Meanwhile, samples in subset II participate\nin the feature-level contrastive objective to learn effective facial expression\nfeatures. We extensively evaluate Ada-CM on four challenging datasets, showing\nthat our method achieves state-of-the-art performance, especially surpassing\nfully-supervised baselines in a semi-supervised manner. Ablation study further\nproves the effectiveness of our method. The source code is available at\nhttps://github.com/hangyu94/Ada-CM.\n","authors":["Hangyu Li","Nannan Wang","Xi Yang","Xiaoyu Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2203.12341v1.pdf","comment":"Accepted for CVPR2022"},{"id":"http://arxiv.org/abs/2203.12338v1","updated":"2022-03-23T11:33:27Z","published":"2022-03-23T11:33:27Z","title":"Real-time Object Detection for Streaming Perception","summary":"  Autonomous driving requires the model to perceive the environment and (re)act\nwithin a low latency for safety. While past works ignore the inevitable changes\nin the environment after processing, streaming perception is proposed to\njointly evaluate the latency and accuracy into a single metric for video online\nperception. In this paper, instead of searching trade-offs between accuracy and\nspeed like previous works, we point out that endowing real-time models with the\nability to predict the future is the key to dealing with this problem. We build\na simple and effective framework for streaming perception. It equips a novel\nDualFlow Perception module (DFP), which includes dynamic and static flows to\ncapture the moving trend and basic detection feature for streaming prediction.\nFurther, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to\ngenerate adaptive weights for objects with different moving speeds. Our simple\nmethod achieves competitive performance on Argoverse-HD dataset and improves\nthe AP by 4.9% compared to the strong baseline, validating its effectiveness.\nOur code will be made available at https://github.com/yancie-yjr/StreamYOLO.\n","authors":["Jinrong Yang","Songtao Liu","Zeming Li","Xiaoping Li","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12338v1.pdf","comment":"CVPR 2022 Accepted Paper"},{"id":"http://arxiv.org/abs/2203.12337v1","updated":"2022-03-23T11:30:34Z","published":"2022-03-23T11:30:34Z","title":"Binary Morphological Neural Network","summary":"  In the last ten years, Convolutional Neural Networks (CNNs) have formed the\nbasis of deep-learning architectures for most computer vision tasks. However,\nthey are not necessarily optimal. For example, mathematical morphology is known\nto be better suited to deal with binary images. In this work, we create a\nmorphological neural network that handles binary inputs and outputs. We propose\ntheir construction inspired by CNNs to formulate layers adapted to such images\nby replacing convolutions with erosions and dilations. We give explainable\ntheoretical results on whether or not the resulting learned networks are indeed\nmorphological operators. We present promising experimental results designed to\nlearn basic binary operators, and we have made our code publicly available\nonline.\n","authors":["Theodore Aouad","Hugues Talbot"],"pdf_url":"https://arxiv.org/pdf/2203.12337v1.pdf","comment":"Preprint of a submission to ICIP 2022. 7 pages. 4 figures"},{"id":"http://arxiv.org/abs/2203.12335v1","updated":"2022-03-23T11:24:44Z","published":"2022-03-23T11:24:44Z","title":"DR.VIC: Decomposition and Reasoning for Video Individual Counting","summary":"  Pedestrian counting is a fundamental tool for understanding pedestrian\npatterns and crowd flow analysis. Existing works (e.g., image-level pedestrian\ncounting, crossline crowd counting et al.) either only focus on the image-level\ncounting or are constrained to the manual annotation of lines. In this work, we\npropose to conduct the pedestrian counting from a new perspective - Video\nIndividual Counting (VIC), which counts the total number of individual\npedestrians in the given video (a person is only counted once). Instead of\nrelying on the Multiple Object Tracking (MOT) techniques, we propose to solve\nthe problem by decomposing all pedestrians into the initial pedestrians who\nexisted in the first frame and the new pedestrians with separate identities in\neach following frame. Then, an end-to-end Decomposition and Reasoning Network\n(DRNet) is designed to predict the initial pedestrian count with the density\nestimation method and reason the new pedestrian's count of each frame with the\ndifferentiable optimal transport. Extensive experiments are conducted on two\ndatasets with congested pedestrians and diverse scenes, demonstrating the\neffectiveness of our method over baselines with great superiority in counting\nthe individual pedestrians. Code: https://github.com/taohan10200/DRNet.\n","authors":["Tao Han","Lei Bai","Junyu Gao","Qi Wang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2203.12335v1.pdf","comment":"Accepted by CVPR 2022. [camera ready with supplement]"},{"id":"http://arxiv.org/abs/2111.11843v4","updated":"2022-03-23T11:07:28Z","published":"2021-11-23T13:15:56Z","title":"U-shape Transformer for Underwater Image Enhancement","summary":"  The light absorption and scattering of underwater impurities lead to poor\nunderwater imaging quality. The existing data-driven based underwater image\nenhancement (UIE) techniques suffer from the lack of a large-scale dataset\ncontaining various underwater scenes and high-fidelity reference images.\nBesides, the inconsistent attenuation in different color channels and space\nareas is not fully considered for boosted enhancement. In this work, we\nconstructed a large-scale underwater image (LSUI) dataset including 5004 image\npairs, and reported an U-shape Transformer network where the transformer model\nis for the first time introduced to the UIE task. The U-shape Transformer is\nintegrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)\nmodule and a spatial-wise global feature modeling transformer (SGFMT) module,\nwhich reinforce the network's attention to the color channels and space areas\nwith more serious attenuation. Meanwhile, in order to further improve the\ncontrast and saturation, a novel loss function combining RGB, LAB and LCH color\nspaces is designed following the human vision principle. The extensive\nexperiments on available datasets validate the state-of-the-art performance of\nthe reported technique with more than 2dB superiority.\n","authors":["Lintao Peng","Chunli Zhu","Liheng Bian"],"pdf_url":"https://arxiv.org/pdf/2111.11843v4.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2203.11660v2","updated":"2022-03-23T10:55:05Z","published":"2022-03-22T12:35:20Z","title":"Channel Self-Supervision for Online Knowledge Distillation","summary":"  Recently, researchers have shown an increased interest in the online\nknowledge distillation. Adopting an one-stage and end-to-end training fashion,\nonline knowledge distillation uses aggregated intermediated predictions of\nmultiple peer models for training. However, the absence of a powerful teacher\nmodel may result in the homogeneity problem between group peers, affecting the\neffectiveness of group distillation adversely. In this paper, we propose a\nnovel online knowledge distillation method, \\textbf{C}hannel\n\\textbf{S}elf-\\textbf{S}upervision for Online Knowledge Distillation (CSS),\nwhich structures diversity in terms of input, target, and network to alleviate\nthe homogenization problem. Specifically, we construct a dual-network\nmulti-branch structure and enhance inter-branch diversity through\nself-supervised learning, adopting the feature-level transformation and\naugmenting the corresponding labels. Meanwhile, the dual network structure has\na larger space of independent parameters to resist the homogenization problem\nduring distillation. Extensive quantitative experiments on CIFAR-100 illustrate\nthat our method provides greater diversity than OKDDip and we also give pretty\nperformance improvement, even over the state-of-the-art such as PCL. The\nresults on three fine-grained datasets (StanfordDogs, StanfordCars,\nCUB-200-211) also show the significant generalization capability of our\napproach.\n","authors":["Shixiao Fan","Xuan Cheng","Xiaomin Wang","Chun Yang","Pan Deng","Minghui Liu","Jiali Deng","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2203.11660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12321v1","updated":"2022-03-23T10:46:33Z","published":"2022-03-23T10:46:33Z","title":"Autofocus for Event Cameras","summary":"  Focus control (FC) is crucial for cameras to capture sharp images in\nchallenging real-world scenarios. The autofocus (AF) facilitates the FC by\nautomatically adjusting the focus settings. However, due to the lack of\neffective AF methods for the recently introduced event cameras, their FC still\nrelies on naive AF like manual focus adjustments, leading to poor adaptation in\nchallenging real-world conditions. In particular, the inherent differences\nbetween event and frame data in terms of sensing modality, noise, temporal\nresolutions, etc., bring many challenges in designing an effective AF method\nfor event cameras. To address these challenges, we develop a novel event-based\nautofocus framework consisting of an event-specific focus measure called event\nrate (ER) and a robust search strategy called event-based golden search (EGS).\nTo verify the performance of our method, we have collected an event-based\nautofocus dataset (EAD) containing well-synchronized frames, events, and focal\npositions in a wide variety of challenging scenes with severe lighting and\nmotion conditions. The experiments on this dataset and additional real-world\nscenarios demonstrated the superiority of our method over state-of-the-art\napproaches in terms of efficiency and accuracy.\n","authors":["Shijie Lin","Yinqiang Zhang","Lei Yu","Bin Zhou","Xiaowei Luo","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2203.12321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.11936v2","updated":"2022-03-23T10:34:19Z","published":"2021-09-24T12:54:42Z","title":"Towards Autonomous Crop-Agnostic Visual Navigation in Arable Fields","summary":"  Autonomous navigation of a robot in agricultural fields is essential for\nevery task from crop monitoring to weed management and fertilizer application.\nMany current approaches rely on accurate GPS, however, such technology is\nexpensive and also prone to failure (e.g. through lack of coverage). As such,\nautonomous navigation through sensors that can interpret their environment\n(such as cameras) is important to achieve the goal of autonomy in agriculture.\nIn this paper, we introduce a purely vision-based navigation scheme that is\nable to reliably guide the robot through row-crop fields without manual\nintervention. Independent of any global localization or mapping, this approach\nis able to accurately follow the crop-rows and switch between the rows, only\nusing onboard cameras. With the help of a novel crop-row detection and a novel\ncrop-row switching technique, our navigation scheme can be deployed in a wide\nrange of fields with different canopy types in various growth stages with\nlimited parameter tuning, creating a crop agnostic navigation approach. We have\nextensively evaluated our approach in three different fields under various\nillumination conditions using our agricultural robotic platform (BonnBot-I).\nFor navigation, our approach is evaluated on five crop types and achieves an\naverage navigation accuracy of 3.82cm relative to manual teleoperation.\n","authors":["Alireza Ahmadi","Michael Halstead","Chris McCool"],"pdf_url":"https://arxiv.org/pdf/2109.11936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09581v2","updated":"2022-03-23T10:26:42Z","published":"2021-12-17T15:52:46Z","title":"Watermarking Images in Self-Supervised Latent Spaces","summary":"  We revisit watermarking techniques based on pre-trained deep networks, in the\nlight of self-supervised approaches. We present a way to embed both marks and\nbinary messages into their latent spaces, leveraging data augmentation at\nmarking time. Our method can operate at any resolution and creates watermarks\nrobust to a broad range of transformations (rotations, crops, JPEG, contrast,\netc). It significantly outperforms the previous zero-bit methods, and its\nperformance on multi-bit watermarking is on par with state-of-the-art\nencoder-decoder architectures trained end-to-end for watermarking. The code is\navailable at github.com/facebookresearch/ssl_watermarking\n","authors":["Pierre Fernandez","Alexandre Sablayrolles","Teddy Furon","Hervé Jégou","Matthijs Douze"],"pdf_url":"https://arxiv.org/pdf/2112.09581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12311v1","updated":"2022-03-23T10:22:03Z","published":"2022-03-23T10:22:03Z","title":"Self-supervised HDR Imaging from Motion and Exposure Cues","summary":"  Recent High Dynamic Range (HDR) techniques extend the capabilities of current\ncameras where scenes with a wide range of illumination can not be accurately\ncaptured with a single low-dynamic-range (LDR) image. This is generally\naccomplished by capturing several LDR images with varying exposure values whose\ninformation is then incorporated into a merged HDR image. While such approaches\nwork well for static scenes, dynamic scenes pose several challenges, mostly\nrelated to the difficulty of finding reliable pixel correspondences.\nData-driven approaches tackle the problem by learning an end-to-end mapping\nwith paired LDR-HDR training data, but in practice generating such HDR\nground-truth labels for dynamic scenes is time-consuming and requires complex\nprocedures that assume control of certain dynamic elements of the scene (e.g.\nactor pose) and repeatable lighting conditions (stop-motion capturing). In this\nwork, we propose a novel self-supervised approach for learnable HDR estimation\nthat alleviates the need for HDR ground-truth labels. We propose to leverage\nthe internal statistics of LDR images to create HDR pseudo-labels. We\nseparately exploit static and well-exposed parts of the input images, which in\nconjunction with synthetic illumination clipping and motion augmentation\nprovide high quality training examples. Experimental results show that the HDR\nmodels trained using our proposed self-supervision approach achieve performance\ncompetitive with those trained under full supervision, and are to a large\nextent superior to previous methods that equally do not require any\nsupervision.\n","authors":["Michal Nazarczuk","Sibi Catley-Chandar","Ales Leonardis","Eduardo Pérez Pellitero"],"pdf_url":"https://arxiv.org/pdf/2203.12311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12304v1","updated":"2022-03-23T10:01:35Z","published":"2022-03-23T10:01:35Z","title":"Domain-Generalized Textured Surface Anomaly Detection","summary":"  Anomaly detection aims to identify abnormal data that deviates from the\nnormal ones, while typically requiring a sufficient amount of normal data to\ntrain the model for performing this task. Despite the success of recent anomaly\ndetection methods, performing anomaly detection in an unseen domain remain a\nchallenging task. In this paper, we address the task of domain-generalized\ntextured surface anomaly detection. By observing normal and abnormal surface\ndata across multiple source domains, our model is expected to be generalized to\nan unseen textured surface of interest, in which only a small number of normal\ndata can be observed during testing. Although with only image-level labels\nobserved in the training data, our patch-based meta-learning model exhibits\npromising generalization ability: not only can it generalize to unseen image\ndomains, but it can also localize abnormal regions in the query image. Our\nexperiments verify that our model performs favorably against state-of-the-art\nanomaly detection and domain generalization approaches in various settings.\n","authors":["Shang-Fu Chen","Yu-Min Liu","Chia-Ching Lin","Trista Pei-Chun Chen","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12304v1.pdf","comment":"Accepted by IEEE International Conference on Multimedia and Expo\n  (ICME) 2022"},{"id":"http://arxiv.org/abs/2203.12301v1","updated":"2022-03-23T09:48:59Z","published":"2022-03-23T09:48:59Z","title":"Lane detection with Position Embedding","summary":"  Recently, lane detection has made great progress in autonomous driving. RESA\n(REcurrent Feature-Shift Aggregator) is based on image segmentation. It\npresents a novel module to enrich lane feature after preliminary feature\nextraction with an ordinary CNN. For Tusimple dataset, there is not too\ncomplicated scene and lane has more prominent spatial features. On the basis of\nRESA, we introduce the method of position embedding to enhance the spatial\nfeatures. The experimental results show that this method has achieved the best\naccuracy 96.93% on Tusimple dataset.\n","authors":["Jun Xie","Jiacheng Han","Dezhen Qi","Feng Chen","Kaer Huang","Jianwei Shuai"],"pdf_url":"https://arxiv.org/pdf/2203.12301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.00788v2","updated":"2022-03-23T09:23:59Z","published":"2020-11-02T07:36:13Z","title":"Representation Decomposition for Image Manipulation and Beyond","summary":"  Representation disentanglement aims at learning interpretable features, so\nthat the output can be recovered or manipulated accordingly. While existing\nworks like infoGAN and AC-GAN exist, they choose to derive disjoint attribute\ncode for feature disentanglement, which is not applicable for existing/trained\ngenerative models. In this paper, we propose a decomposition-GAN (dec-GAN),\nwhich is able to achieve the decomposition of an existing latent representation\ninto content and attribute features. Guided by the classifier pre-trained on\nthe attributes of interest, our dec-GAN decomposes the attributes of interest\nfrom the latent representation, while data recovery and feature consistency\nobjectives enforce the learning of our proposed method. Our experiments on\nmultiple image datasets confirm the effectiveness and robustness of our dec-GAN\nover recent representation disentanglement models.\n","authors":["Shang-Fu Chen","Jia-Wei Yan","Ya-Fan Su","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2011.00788v2.pdf","comment":"Published at IEEE International Conference in Image Processing (ICIP)\n  2021"},{"id":"http://arxiv.org/abs/2203.12290v1","updated":"2022-03-23T09:20:30Z","published":"2022-03-23T09:20:30Z","title":"Cell segmentation from telecentric bright-field transmitted light\n  microscopic images using a Residual Attention U-Net: a case study on HeLa\n  line","summary":"  Living cell segmentation from bright-field light microscopic images is\nchallenging due to the image complexity and temporal changes in the living\ncells. Recently developed deep learning (DL)-based methods became popular in\nmedical and microscopic image segmentation tasks due to their success and\npromising outcomes. The main objective of this paper is to develop a deep\nlearning, UNet-based method to segment the living cells of the HeLa line in\nbright-field transmitted light microscopy. To find the most suitable\narchitecture for our datasets, we have proposed a residual attention U-Net and\ncompared it with an attention and a simple U-Net architecture. The attention\nmechanism highlights the remarkable features and suppresses activations in the\nirrelevant image regions. The residual mechanism overcomes with vanishing\ngradient problem. The Mean-IoU score for our datasets reaches 0.9505, 0.9524,\nand 0.9530 for the simple, attention, and residual attention U-Net,\nrespectively. We achieved the most accurate semantic segmentation results in\nthe Mean-IoU and Dice metrics by applying the residual and attention mechanisms\ntogether. The watershed method applied to this best - Residual Attention -\nsemantic segmentation result gave the segmentation with the specific\ninformation for each cell.\n","authors":["Ali Ghaznavi","Renata Rychtarikova","Mohammadmehdi Saberioon","Dalibor Stys"],"pdf_url":"https://arxiv.org/pdf/2203.12290v1.pdf","comment":"28 pages, 7 figures"},{"id":"http://arxiv.org/abs/2203.06907v3","updated":"2022-03-23T09:14:57Z","published":"2022-03-14T08:01:14Z","title":"Hierarchical Memory Learning for Fine-Grained Scene Graph Generation","summary":"  As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the\ndataset due to the crowd-sourced labeling, and the long-tail problem is also\npronounced. Given this tricky situation, many existing SGG methods treat the\npredicates equally and learn the model under the supervision of\nmixed-granularity predicates in one stage, leading to relatively coarse\npredictions. In order to alleviate the negative impact of the suboptimum\nmixed-granularity annotation and long-tail effect problems, this paper proposes\na novel Hierarchical Memory Learning (HML) framework to learn the model from\nsimple to complex, which is similar to the human beings' hierarchical memory\nlearning process. After the autonomous partition of coarse and fine predicates,\nthe model is first trained on the coarse predicates and then learns the fine\npredicates. In order to realize this hierarchical learning pattern, this paper,\nfor the first time, formulates the HML framework using the new Concept\nReconstruction (CR) and Model Reconstruction (MR) constraints. It is worth\nnoticing that the HML framework can be taken as one general optimization\nstrategy to improve various SGG models, and significant improvement can be\nachieved on the SGG benchmark (i.e., Visual Genome).\n","authors":["Youming Deng","Yansheng Li","Yongjun Zhang","Xiang Xiang","Jian Wang","Jingdong Chen","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2203.06907v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2005.02641v2","updated":"2022-03-23T09:05:22Z","published":"2020-05-06T08:05:08Z","title":"Incremental Few-Shot Object Detection for Robotics","summary":"  Incremental few-shot learning is highly expected for practical robotics\napplications. On one hand, robot is desired to learn new tasks quickly and\nflexibly using only few annotated training samples; on the other hand, such new\nadditional tasks should be learned in a continuous and incremental manner\nwithout forgetting the previous learned knowledge dramatically. In this work,\nwe propose a novel Class-Incremental Few-Shot Object Detection (CI-FSOD)\nframework that enables deep object detection network to perform effective\ncontinual learning from just few-shot samples without re-accessing the previous\ntraining data. We achieve this by equipping the widely-used Faster-RCNN\ndetector with three elegant components. Firstly, to best preserve performance\non the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES)\narchitecture which decouples the representation learning of base and novel\ncategories into different spaces. Secondly, to mitigate the catastrophic\nforgetting on the accumulated novel classes, we propose a Sequential Model\nFusion (SMF) method, which is able to achieve long-term memory without\nadditional storage cost. Thirdly, to promote inter-task class separation in\nfeature space, we propose a novel regularization technique that extends the\nclassification boundary further away from the previous classes to avoid\nmisclassification. Overall, our framework is simple yet effective and\noutperforms the previous SOTA with a significant margin of 2.4 points in AP\nperformance.\n","authors":["Yiting Li","Haiyue Zhu","Sichao Tian","Fan Feng","Jun Ma","Chek Sing Teo","Cheng Xiang","Prahlad Vadakkepat","Tong Heng Lee"],"pdf_url":"https://arxiv.org/pdf/2005.02641v2.pdf","comment":"ICRA 2022"},{"id":"http://arxiv.org/abs/2112.00485v2","updated":"2022-03-23T08:41:26Z","published":"2021-12-01T13:23:00Z","title":"Learning Transformer Features for Image Quality Assessment","summary":"  Objective image quality evaluation is a challenging task, which aims to\nmeasure the quality of a given image automatically. According to the\navailability of the reference images, there are Full-Reference and No-Reference\nIQA tasks, respectively. Most deep learning approaches use regression from deep\nfeatures extracted by Convolutional Neural Networks. For the FR task, another\noption is conducting a statistical comparison on deep features. For all these\nmethods, non-local information is usually neglected. In addition, the\nrelationship between FR and NR tasks is less explored. Motivated by the recent\nsuccess of transformers in modeling contextual information, we propose a\nunified IQA framework that utilizes CNN backbone and transformer encoder to\nextract features. The proposed framework is compatible with both FR and NR\nmodes and allows for a joint training scheme. Evaluation experiments on three\nstandard IQA datasets, i.e., LIVE, CSIQ and TID2013, and KONIQ-10K, show that\nour proposed model can achieve state-of-the-art FR performance. In addition,\ncomparable NR performance is achieved in extensive experiments, and the results\nshow that the NR performance can be leveraged by the joint training scheme.\n","authors":["Chao Zeng","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2112.00485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12273v1","updated":"2022-03-23T08:40:42Z","published":"2022-03-23T08:40:42Z","title":"DAN: a Segmentation-free Document Attention Network for Handwritten\n  Document Recognition","summary":"  Unconstrained handwritten document recognition is a challenging computer\nvision task. It is traditionally handled by a two-step approach combining line\nsegmentation followed by text line recognition. For the first time, we propose\nan end-to-end segmentation-free architecture for the task of handwritten\ndocument recognition: the Document Attention Network. In addition to the text\nrecognition, the model is trained to label text parts using begin and end tags\nin an XML-like fashion. This model is made up of an FCN encoder for feature\nextraction and a stack of transformer decoder layers for a recurrent\ntoken-by-token prediction process. It takes whole text documents as input and\nsequentially outputs characters, as well as logical layout tokens. Contrary to\nthe existing segmentation-based approaches, the model is trained without using\nany segmentation label. We achieve competitive results on the READ dataset at\npage level, as well as double-page level with a CER of 3.53% and 3.69%,\nrespectively. We also provide results for the RIMES dataset at page level,\nreaching 4.54% of CER.\n  We provide all source code and pre-trained model weights at\nhttps://github.com/FactoDeepLearning/DAN.\n","authors":["Denis Coquenet","Clément Chatelain","Thierry Paquet"],"pdf_url":"https://arxiv.org/pdf/2203.12273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12270v1","updated":"2022-03-23T08:37:04Z","published":"2022-03-23T08:37:04Z","title":"Event-Based Dense Reconstruction Pipeline","summary":"  Event cameras are a new type of sensors that are different from traditional\ncameras. Each pixel is triggered asynchronously by event. The trigger event is\nthe change of the brightness irradiated on the pixel. If the increment or\ndecrement of brightness is higher than a certain threshold, an event is output.\nCompared with traditional cameras, event cameras have the advantages of high\ndynamic range and no motion blur. Since events are caused by the apparent\nmotion of intensity edges, the majority of 3D reconstructed maps consist only\nof scene edges, i.e., semi-dense maps, which is not enough for some\napplications. In this paper, we propose a pipeline to realize event-based dense\nreconstruction. First, deep learning is used to reconstruct intensity images\nfrom events. And then, structure from motion (SfM) is used to estimate camera\nintrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)\nis used to complete dense reconstruction.\n","authors":["Kun Xiao","Guohui Wang","Yi Chen","Jinghong Nan","Yongfeng Xie"],"pdf_url":"https://arxiv.org/pdf/2203.12270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12247v1","updated":"2022-03-23T07:43:44Z","published":"2022-03-23T07:43:44Z","title":"Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition","summary":"  We introduce Ev-TTA, a simple, effective test-time adaptation algorithm for\nevent-based object recognition. While event cameras are proposed to provide\nmeasurements of scenes with fast motions or drastic illumination changes, many\nexisting event-based recognition algorithms suffer from performance\ndeterioration under extreme conditions due to significant domain shifts. Ev-TTA\nmitigates the severe domain gaps by fine-tuning the pre-trained classifiers\nduring the test phase using loss functions inspired by the spatio-temporal\ncharacteristics of events. Since the event data is a temporal stream of\nmeasurements, our loss function enforces similar predictions for adjacent\nevents to quickly adapt to the changed environment online. Also, we utilize the\nspatial correlations between two polarities of events to handle noise under\nextreme illumination, where different polarities of events exhibit distinctive\nnoise distributions. Ev-TTA demonstrates a large amount of performance gain on\na wide range of event-based object recognition tasks without extensive\nadditional training. Our formulation can be successfully applied regardless of\ninput representations and further extended into regression tasks. We expect\nEv-TTA to provide the key technique to deploy event-based vision algorithms in\nchallenging real-world applications where significant domain shift is\ninevitable.\n","authors":["Junho Kim","Inwoo Hwang","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2203.12247v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2112.03552v3","updated":"2022-03-23T07:39:04Z","published":"2021-12-07T07:56:50Z","title":"Bootstrapping ViTs: Towards Liberating Vision Transformers from\n  Pre-training","summary":"  Recently, vision Transformers (ViTs) are developing rapidly and starting to\nchallenge the domination of convolutional neural networks (CNNs) in the realm\nof computer vision (CV). With the general-purpose Transformer architecture\nreplacing the hard-coded inductive biases of convolution, ViTs have surpassed\nCNNs, especially in data-sufficient circumstances. However, ViTs are prone to\nover-fit on small datasets and thus rely on large-scale pre-training, which\nexpends enormous time. In this paper, we strive to liberate ViTs from\npre-training by introducing CNNs' inductive biases back to ViTs while\npreserving their network architectures for higher upper bound and setting up\nmore suitable optimization objectives. To begin with, an agent CNN is designed\nbased on the given ViT with inductive biases. Then a bootstrapping training\nalgorithm is proposed to jointly optimize the agent and ViT with weight\nsharing, during which the ViT learns inductive biases from the intermediate\nfeatures of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k\nwith limited training data have shown encouraging results that the inductive\nbiases help ViTs converge significantly faster and outperform conventional CNNs\nwith even fewer parameters. Our code is publicly available at\nhttps://github.com/zhfeing/Bootstrapping-ViTs-pytorch.\n","authors":["Haofei Zhang","Jiarui Duan","Mengqi Xue","Jie Song","Li Sun","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2112.03552v3.pdf","comment":"Accepted as a conference paper by CVPR2022"},{"id":"http://arxiv.org/abs/2203.12244v1","updated":"2022-03-23T07:33:37Z","published":"2022-03-23T07:33:37Z","title":"Scale-Equivalent Distillation for Semi-Supervised Object Detection","summary":"  Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on\nself-training, i.e., generating hard pseudo-labels by a teacher model on\nunlabeled data as supervisory signals. Although they achieved certain success,\nthe limited labeled data in semi-supervised learning scales up the challenges\nof object detection. We analyze the challenges these methods meet with the\nempirical experiment results. We find that the massive False Negative samples\nand inferior localization precision lack consideration. Besides, the large\nvariance of object sizes and class imbalance (i.e., the extreme ratio between\nbackground and object) hinder the performance of prior arts. Further, we\novercome these challenges by introducing a novel approach, Scale-Equivalent\nDistillation (SED), which is a simple yet effective end-to-end knowledge\ndistillation framework robust to large object size variance and class\nimbalance. SED has several appealing benefits compared to the previous works.\n(1) SED imposes a consistency regularization to handle the large scale variance\nproblem. (2) SED alleviates the noise problem from the False Negative samples\nand inferior localization precision. (3) A re-weighting strategy can implicitly\nscreen the potential foreground regions of the unlabeled data to reduce the\neffect of class imbalance. Extensive experiments show that SED consistently\noutperforms the recent state-of-the-art methods on different datasets with\nsignificant margins. For example, it surpasses the supervised counterpart by\nmore than 10 mAP when using 5% and 10% labeled data on MS-COCO.\n","authors":["Qiushan Guo","Yao Mu","Jianyu Chen","Tianqi Wang","Yizhou Yu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2203.12244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12241v1","updated":"2022-03-23T07:29:39Z","published":"2022-03-23T07:29:39Z","title":"A Method of Data Augmentation to Train a Small Area Fingerprint\n  Recognition Deep Neural Network with a Normal Fingerprint Database","summary":"  Fingerprints are popular among the biometric based systems due to ease of\nacquisition, uniqueness and availability. Nowadays it is used in smart phone\nsecurity, digital payment and digital locker. The traditional fingerprint\nmatching methods based on minutiae are mainly applicable for large-area\nfingerprint and the accuracy rate would reduce significantly when dealing with\nsmall-area fingerprint from smart phone. There are many attempts to using deep\nlearning for small-area fingerprint recognition, and there are many successes.\nBut training deep neural network needs a lot of datasets for training. There is\nno well-known dataset for small-area, so we have to make datasets ourselves. In\nthis paper, we propose a method of data augmentation to train a small-area\nfingerprint recognition deep neural network with a normal fingerprint database\n(such as FVC2002) and verify it via tests. The experimental results showed the\nefficiency of our method.\n","authors":["JuSong Kim"],"pdf_url":"https://arxiv.org/pdf/2203.12241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.04237v2","updated":"2022-03-23T07:24:13Z","published":"2022-02-09T02:46:26Z","title":"Learning Robust Convolutional Neural Networks with Relevant Feature\n  Focusing via Explanations","summary":"  Existing image recognition techniques based on convolutional neural networks\n(CNNs) basically assume that the training and test datasets are sampled from\ni.i.d distributions. However, this assumption is easily broken in the real\nworld because of the distribution shift that occurs when the co-occurrence\nrelations between objects and backgrounds in input images change. Under this\ntype of distribution shift, CNNs learn to focus on features that are not\ntask-relevant, such as backgrounds from the training data, and degrade their\naccuracy on the test data. To tackle this problem, we propose relevant feature\nfocusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via\nexplanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc\nexplanation modules, it can be easily applied to off-the-shelf CNNs.\nFurthermore, ReFF requires no additional inference cost at test time because it\nis only used for regularization while training. We demonstrate that CNNs\ntrained with ReFF focus on features relevant to the target task and that ReFF\nimproves the test-time accuracy.\n","authors":["Kazuki Adachi","Shin'ya Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2202.04237v2.pdf","comment":"Accepted by ICME 2022"},{"id":"http://arxiv.org/abs/2203.12236v1","updated":"2022-03-23T07:12:39Z","published":"2022-03-23T07:12:39Z","title":"A Multi-Characteristic Learning Method with Micro-Doppler Signatures for\n  Pedestrian Identification","summary":"  The identification of pedestrians using radar micro-Doppler signatures has\nbecome a hot topic in recent years. In this paper, we propose a\nmulti-characteristic learning (MCL) model with clusters to jointly learn\ndiscrepant pedestrian micro-Doppler signatures and fuse the knowledge learned\nfrom each cluster into final decisions. Time-Doppler spectrogram (TDS) and\nsignal statistical features extracted from FMCW radar, as two categories of\nmicro-Doppler signatures, are used in MCL to learn the micro-motion information\ninside pedestrians' free walking patterns. The experimental results show that\nour model achieves a higher accuracy rate and is more stable for pedestrian\nidentification than other studies, which make our model more practical.\n","authors":["Yu Xiang","Yu Huang","Haodong Xu","Guangbo Zhang","Wenyong Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.02054v2","updated":"2022-03-23T07:10:20Z","published":"2021-09-05T12:18:53Z","title":"Sensor Data Augmentation by Resampling for Contrastive Learning in Human\n  Activity Recognition","summary":"  While deep learning has contributed to the advancement of sensor-based Human\nActivity Recognition (HAR), it is usually a costly and challenging supervised\ntask with the needs of a large amount of labeled data. To alleviate this issue,\ncontrastive learning has been applied for sensor-based HAR. Data augmentation\nis an essential part of contrastive learning and has a significant impact on\nthe performance of downstream tasks. However, current popular augmentation\nmethods do not achieve competitive performance in contrastive learning for\nsensor-based HAR. Motivated by this issue, we propose a new sensor data\naugmentation method by resampling, which simulates more realistic activity data\nby varying the sampling frequency to maximize the coverage of the sampling\nspace. In addition, we extend MoCo, a popular contrastive learning framework,\nto MoCoHAR for HAR. The resampling augmentation method will be evaluated on two\ncontrastive learning frameworks, SimCLRHAR and MoCoHAR, using UCI-HAR,\nMotionSensor, and USC-HAD datasets. The experiment results show that the\nresampling augmentation method outperforms all state-of-the-art methods under a\nsmall amount of labeled data, on SimCLRHAR and MoCoHAR, with mean F1-score as\nthe evaluation metric. The results also demonstrate that not all data\naugmentation methods have positive effects in the contrastive learning\nframework.\n","authors":["Jinqiang Wang","Tao Zhu","Jingyuan Gan","Liming Chen","Huansheng Ning","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2109.02054v2.pdf","comment":"13 pages,5 figures"},{"id":"http://arxiv.org/abs/2104.13450v6","updated":"2022-03-23T06:55:18Z","published":"2021-04-27T19:51:39Z","title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and\n  Extracting Them from 2D Renderings","summary":"  Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n","authors":["Innfarn Yoo","Huiwen Chang","Xiyang Luo","Ondrej Stava","Ce Liu","Peyman Milanfar","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2104.13450v6.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12230v1","updated":"2022-03-23T06:54:16Z","published":"2022-03-23T06:54:16Z","title":"Negative Selection by Clustering for Contrastive Learning in Human\n  Activity Recognition","summary":"  Contrastive learning has been applied to Human Activity Recognition (HAR)\nbased on sensor data owing to its ability to achieve performance comparable to\nsupervised learning with a large amount of unlabeled data and a small amount of\nlabeled data. The pre-training task for contrastive learning is generally\ninstance discrimination, which specifies that each instance belongs to a single\nclass, but this will consider the same class of samples as negative examples.\nSuch a pre-training task is not conducive to human activity recognition tasks,\nwhich are mainly classification tasks. To address this problem, we follow\nSimCLR to propose a new contrastive learning framework that negative selection\nby clustering in HAR, which is called ClusterCLHAR. Compared with SimCLR, it\nredefines the negative pairs in the contrastive loss function by using\nunsupervised clustering methods to generate soft labels that mask other samples\nof the same cluster to avoid regarding them as negative samples. We evaluate\nClusterCLHAR on three benchmark datasets, USC-HAD, MotionSense, and UCI-HAR,\nusing mean F1-score as the evaluation metric. The experiment results show that\nit outperforms all the state-of-the-art methods applied to HAR in\nself-supervised learning and semi-supervised learning.\n","authors":["Jinqiang Wang","Tao Zhu","Liming Chen","Huansheng Ning","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2203.12230v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.12224v1","updated":"2022-03-23T06:24:31Z","published":"2022-03-23T06:24:31Z","title":"Efficient Few-Shot Object Detection via Knowledge Inheritance","summary":"  Few-shot object detection (FSOD), which aims at learning a generic detector\nthat can adapt to unseen tasks with scarce training samples, has witnessed\nconsistent improvement recently. However, most existing methods ignore the\nefficiency issues, e.g., high computational complexity and slow adaptation\nspeed. Notably, efficiency has become an increasingly important evaluation\nmetric for few-shot techniques due to an emerging trend toward embedded AI. To\nthis end, we present an efficient pretrain-transfer framework (PTF) baseline\nwith no computational increment, which achieves comparable results with\nprevious state-of-the-art (SOTA) methods. Upon this baseline, we devise an\ninitializer named knowledge inheritance (KI) to reliably initialize the novel\nweights for the box classifier, which effectively facilitates the knowledge\ntransfer process and boosts the adaptation speed. Within the KI initializer, we\npropose an adaptive length re-scaling (ALR) strategy to alleviate the vector\nlength inconsistency between the predicted novel weights and the pretrained\nbase weights. Finally, our approach not only achieves the SOTA results across\nthree public benchmarks, i.e., PASCAL VOC, COCO and LVIS, but also exhibits\nhigh efficiency with 1.8-9.0x faster adaptation speed against the other methods\non COCO/LVIS benchmark during few-shot transfer. To our best knowledge, this is\nthe first work to consider the efficiency problem in FSOD. We hope to motivate\na trend toward powerful yet efficient few-shot technique development. The codes\nare publicly available at https://github.com/Ze-Yang/Efficient-FSOD.\n","authors":["Ze Yang","Chi Zhang","Ruibo Li","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2203.12224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.00933v2","updated":"2022-03-23T06:13:10Z","published":"2021-12-02T02:12:03Z","title":"PartImageNet: A Large, High-Quality Dataset of Parts","summary":"  It is natural to represent objects in terms of their parts. This has the\npotential to improve the performance of algorithms for object recognition and\nsegmentation but can also help for downstream tasks like activity recognition.\nResearch on part-based models, however, is hindered by the lack of datasets\nwith per-pixel part annotations. This is partly due to the difficulty and high\ncost of annotating object parts so it has rarely been done except for humans\n(where there exists a big literature on part-based models). To help address\nthis problem, we propose PartImageNet, a large, high-quality dataset with part\nsegmentation annotations. It consists of $158$ classes from ImageNet with\napproximately $24,000$ images. PartImageNet is unique because it offers\npart-level annotations on a general set of classes including non-rigid,\narticulated objects, while having an order of magnitude larger size compared to\nexisting part datasets (excluding datasets of humans). It can be utilized for\nmany vision tasks including Object Segmentation, Semantic Part Segmentation,\nFew-shot Learning and Part Discovery. We conduct comprehensive experiments\nwhich study these tasks and set up a set of baselines. The dataset and scripts\nare released at https://github.com/TACJu/PartImageNet.\n","authors":["Ju He","Shuo Yang","Shaokang Yang","Adam Kortylewski","Xiaoding Yuan","Jie-Neng Chen","Shuai Liu","Cheng Yang","Qihang Yu","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2112.00933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12217v1","updated":"2022-03-23T06:06:54Z","published":"2022-03-23T06:06:54Z","title":"Training-free Transformer Architecture Search","summary":"  Recently, Vision Transformer (ViT) has achieved remarkable success in several\ncomputer vision tasks. The progresses are highly relevant to the architecture\ndesign, then it is worthwhile to propose Transformer Architecture Search (TAS)\nto search for better ViTs automatically. However, current TAS methods are\ntime-consuming and existing zero-cost proxies in CNN do not generalize well to\nthe ViT search space according to our experimental observations. In this paper,\nfor the first time, we investigate how to conduct TAS in a training-free manner\nand devise an effective training-free TAS (TF-TAS) scheme. Firstly, we observe\nthat the properties of multi-head self-attention (MSA) and multi-layer\nperceptron (MLP) in ViTs are quite different and that the synaptic diversity of\nMSA affects the performance notably. Secondly, based on the observation, we\ndevise a modular strategy in TF-TAS that evaluates and ranks ViT architectures\nfrom two theoretical perspectives: synaptic diversity and synaptic saliency,\ntermed as DSS-indicator. With DSS-indicator, evaluation results are strongly\ncorrelated with the test accuracies of ViT models. Experimental results\ndemonstrate that our TF-TAS achieves a competitive performance against the\nstate-of-the-art manually or automatically design ViT architectures, and it\npromotes the searching efficiency in ViT search space greatly: from about $24$\nGPU days to less than $0.5$ GPU days. Moreover, the proposed DSS-indicator\noutperforms the existing cutting-edge zero-cost approaches (e.g., TE-score and\nNASWOT).\n","authors":["Qinqin Zhou","Kekai Sheng","Xiawu Zheng","Ke Li","Xing Sun","Yonghong Tian","Jie Chen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2203.12217v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12215v1","updated":"2022-03-23T06:04:11Z","published":"2022-03-23T06:04:11Z","title":"Physics-Driven Deep Learning for Computational Magnetic Resonance\n  Imaging","summary":"  Physics-driven deep learning methods have emerged as a powerful tool for\ncomputational magnetic resonance imaging (MRI) problems, pushing reconstruction\nperformance to new limits. This article provides an overview of the recent\ndevelopments in incorporating physics information into learning-based MRI\nreconstruction. We consider inverse problems with both linear and non-linear\nforward models for computational MRI, and review the classical approaches for\nsolving these. We then focus on physics-driven deep learning approaches,\ncovering physics-driven loss functions, plug-and-play methods, generative\nmodels, and unrolled networks. We highlight domain-specific challenges such as\nreal- and complex-valued building blocks of neural networks, and translational\napplications in MRI with linear and non-linear forward models. Finally, we\ndiscuss common issues and open challenges, and draw connections to the\nimportance of physics-driven learning when combined with other downstream tasks\nin the medical imaging pipeline.\n","authors":["Kerstin Hammernik","Thomas Küstner","Burhaneddin Yaman","Zhengnan Huang","Daniel Rueckert","Florian Knoll","Mehmet Akçakaya"],"pdf_url":"https://arxiv.org/pdf/2203.12215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12208v1","updated":"2022-03-23T05:52:23Z","published":"2022-03-23T05:52:23Z","title":"Self-supervised Learning of Adversarial Example: Towards Good\n  Generalizations for Deepfake Detection","summary":"  Recent studies in deepfake detection have yielded promising results when the\ntraining and testing face forgeries are from the same dataset. However, the\nproblem remains challenging when one tries to generalize the detector to\nforgeries created by unseen methods in the training dataset. This work\naddresses the generalizable deepfake detection from a simple principle: a\ngeneralizable representation should be sensitive to diverse types of forgeries.\nFollowing this principle, we propose to enrich the \"diversity\" of forgeries by\nsynthesizing augmented forgeries with a pool of forgery configurations and\nstrengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\nthe forgery configurations. To effectively explore the large forgery\naugmentation space, we further propose to use the adversarial training strategy\nto dynamically synthesize the most challenging forgeries to the current model.\nThrough extensive experiments, we show that the proposed strategies are\nsurprisingly effective (see Figure 1), and they could achieve superior\nperformance than the current state-of-the-art methods. Code is available at\n\\url{https://github.com/liangchen527/SLADD}.\n","authors":["Liang Chen","Yong Zhang","Yibing Song","Lingqiao Liu","Jue Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12208v1.pdf","comment":"To appear in CVPR 2022"},{"id":"http://arxiv.org/abs/1811.11849v4","updated":"2022-03-23T05:41:56Z","published":"2018-11-28T21:35:23Z","title":"Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on\n  Crowd Videos","summary":"  Group-level emotion recognition (ER) is a growing research area as the\ndemands for assessing crowds of all sizes are becoming an interest in both the\nsecurity arena as well as social media. This work extends the earlier ER\ninvestigations, which focused on either group-level ER on single images or\nwithin a video, by fully investigating group-level expression recognition on\ncrowd videos. In this paper, we propose an effective deep feature level fusion\nmechanism to model the spatial-temporal information in the crowd videos. In our\napproach, the fusing process is performed on the deep feature domain by a\ngenerative probabilistic model, Non-Volume Preserving Fusion (NVPF), that\nmodels spatial information relationships. Furthermore, we extend our proposed\nspatial NVPF approach to the spatial-temporal NVPF approach to learn the\ntemporal information between frames. To demonstrate the robustness and\neffectiveness of each component in the proposed approach, three experiments\nwere conducted: (i) evaluation on AffectNet database to benchmark the proposed\nEmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to\nbenchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)\nexamine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos\n(GECV) dataset composed of 627 videos collected from publicly available\nsources. GECV dataset is a collection of videos containing crowds of people.\nEach video is labeled with emotion categories at three levels: individual\nfaces, group of people, and the entire video frame.\n","authors":["Kha Gia Quach","Ngan Le","Chi Nhan Duong","Ibsa Jalata","Kaushik Roy","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/1811.11849v4.pdf","comment":"In press at Patter Recognition Journal"},{"id":"http://arxiv.org/abs/2203.12204v1","updated":"2022-03-23T05:36:02Z","published":"2022-03-23T05:36:02Z","title":"Interpretable Prediction of Lung Squamous Cell Carcinoma Recurrence With\n  Self-supervised Learning","summary":"  Lung squamous cell carcinoma (LSCC) has a high recurrence and metastasis\nrate. Factors influencing recurrence and metastasis are currently unknown and\nthere are no distinct histopathological or morphological features indicating\nthe risks of recurrence and metastasis in LSCC. Our study focuses on the\nrecurrence prediction of LSCC based on H&E-stained histopathological\nwhole-slide images (WSI). Due to the small size of LSCC cohorts in terms of\npatients with available recurrence information, standard end-to-end learning\nwith various convolutional neural networks for this task tends to overfit.\nAlso, the predictions made by these models are hard to interpret.\nHistopathology WSIs are typically very large and are therefore processed as a\nset of smaller tiles. In this work, we propose a novel conditional\nself-supervised learning (SSL) method to learn representations of WSI at the\ntile level first, and leverage clustering algorithms to identify the tiles with\nsimilar histopathological representations. The resulting representations and\nclusters from self-supervision are used as features of a survival model for\nrecurrence prediction at the patient level. Using two publicly available\ndatasets from TCGA and CPTAC, we show that our LSCC recurrence prediction\nsurvival model outperforms both LSCC pathological stage-based approach and\nmachine learning baselines such as multiple instance learning. The proposed\nmethod also enables us to explain the recurrence histopathological risk factors\nvia the derived clusters. This can help pathologists derive new hypotheses\nregarding morphological features associated with LSCC recurrence.\n","authors":["Weicheng Zhu","Carlos Fernandez-Granda","Narges Razavian"],"pdf_url":"https://arxiv.org/pdf/2203.12204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12198v1","updated":"2022-03-23T05:19:06Z","published":"2022-03-23T05:19:06Z","title":"Deep Frequency Filtering for Domain Generalization","summary":"  Improving the generalization capability of Deep Neural Networks (DNNs) is\ncritical for their practical uses, which has been a longstanding challenge.\nSome theoretical studies have revealed that DNNs have preferences to different\nfrequency components in the learning process and indicated that this may affect\nthe robustness of learned features. In this paper, we propose Deep Frequency\nFiltering (DFF) for learning domain-generalizable features, which is the first\nendeavour to explicitly modulate frequency components of different transfer\ndifficulties across domains during training. To achieve this, we perform Fast\nFourier Transform (FFT) on feature maps at different layers, then adopt a\nlight-weight module to learn the attention masks from frequency representations\nafter FFT to enhance transferable frequency components while suppressing the\ncomponents not conductive to generalization. Further, we empirically compare\ndifferent types of attention for implementing our conceptualized DFF. Extensive\nexperiments demonstrate the effectiveness of the proposed DFF and show that\napplying DFF on a plain baseline outperforms the state-of-the-art methods on\ndifferent domain generalization tasks, including close-set classification and\nopen-set retrieval.\n","authors":["Shiqi Lin","Zhizheng Zhang","Zhipeng Huang","Yan Lu","Cuiling Lan","Peng Chu","Quanzeng You","Jiang Wang","Zicheng Liu","Amey Parulkar","Viraj Navkal","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2203.12198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12197v1","updated":"2022-03-23T05:14:50Z","published":"2022-03-23T05:14:50Z","title":"Biceph-Net: A robust and lightweight framework for the diagnosis of\n  Alzheimer's disease using 2D-MRI scans and deep similarity learning","summary":"  Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the\nsignificant causes of death in the elderly population. Many deep learning\ntechniques have been proposed to diagnose AD using Magnetic Resonance Imaging\n(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is\nchallenging as the inter-slice information gets lost. To this end, we propose a\nnovel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D\nMRI scans that model both the intra-slice and inter-slice information.\nBiceph-Net has been experimentally shown to perform similar to other\nSpatio-temporal neural networks while being computationally more efficient.\nBiceph-Net is also superior in performance compared to vanilla 2D convolutional\nneural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has\nan inbuilt neighbourhood-based model interpretation feature that can be\nexploited to understand the classification decision taken by the network.\nBiceph-Net experimentally achieves a test accuracy of 100% in the\nclassification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive\nImpairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.\n","authors":["A. H. Rashid","A. Gupta","J. Gupta","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2203.12197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09550v2","updated":"2022-03-23T05:02:02Z","published":"2022-03-17T18:16:52Z","title":"Multi-similarity based Hyperrelation Network for few-shot segmentation","summary":"  Few-shot semantic segmentation aims at recognizing the object regions of\nunseen categories with only a few annotated examples as supervision. The key to\nfew-shot segmentation is to establish a robust semantic relationship between\nthe support and query images and to prevent overfitting. In this paper, we\npropose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle\nthe few-shot semantic segmentation problem. In MSHNet, we propose a new\nGenerative Prototype Similarity (GPS), which together with cosine similarity\ncan establish a strong semantic relation between the support and query images.\nThe locally generated prototype similarity based on global feature is logically\ncomplementary to the global cosine similarity based on local feature, and the\nrelationship between the query image and the supported image can be expressed\nmore comprehensively by using the two similarities simultaneously. In addition,\nwe propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge\nmulti-layer, multi-shot and multi-similarity hyperrelational features. MSHNet\nis built on the basis of similarity rather than specific category features,\nwhich can achieve more general unity and effectively reduce overfitting. On two\nbenchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet\nachieves new state-of-the-art performances on 1-shot and 5-shot semantic\nsegmentation tasks.\n","authors":["Xiangwen Shi","Zhe Cui","Shaobing Zhang","Miao Cheng","Lian He","Xianghong Tang"],"pdf_url":"https://arxiv.org/pdf/2203.09550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12193v1","updated":"2022-03-23T04:53:26Z","published":"2022-03-23T04:53:26Z","title":"Self-Supervised Robust Scene Flow Estimation via the Alignment of\n  Probability Density Functions","summary":"  In this paper, we present a new self-supervised scene flow estimation\napproach for a pair of consecutive point clouds. The key idea of our approach\nis to represent discrete point clouds as continuous probability density\nfunctions using Gaussian mixture models. Scene flow estimation is therefore\nconverted into the problem of recovering motion from the alignment of\nprobability density functions, which we achieve using a closed-form expression\nof the classic Cauchy-Schwarz divergence. Unlike existing\nnearest-neighbor-based approaches that use hard pairwise correspondences, our\nproposed approach establishes soft and implicit point correspondences between\npoint clouds and generates more robust and accurate scene flow in the presence\nof missing correspondences and outliers. Comprehensive experiments show that\nour method makes noticeable gains over the Chamfer Distance and the Earth\nMover's Distance in real-world environments and achieves state-of-the-art\nperformance among self-supervised learning methods on FlyingThings3D and KITTI,\neven outperforming some supervised methods with ground truth annotations.\n","authors":["Pan He","Patrick Emami","Sanjay Ranka","Anand Rangarajan"],"pdf_url":"https://arxiv.org/pdf/2203.12193v1.pdf","comment":"Published in AAAI Conference on Artificial Intelligence (2022)"},{"id":"http://arxiv.org/abs/2203.12192v1","updated":"2022-03-23T04:50:50Z","published":"2022-03-23T04:50:50Z","title":"Learning to Censor by Noisy Sampling","summary":"  Point clouds are an increasingly ubiquitous input modality and the raw signal\ncan be efficiently processed with recent progress in deep learning. This signal\nmay, often inadvertently, capture sensitive information that can leak semantic\nand geometric properties of the scene which the data owner does not want to\nshare. The goal of this work is to protect sensitive information when learning\nfrom point clouds; by censoring the sensitive information before the point\ncloud is released for downstream tasks. Specifically, we focus on preserving\nutility for perception tasks while mitigating attribute leakage attacks. The\nkey motivating insight is to leverage the localized saliency of perception\ntasks on point clouds to provide good privacy-utility trade-offs. We realize\nthis through a mechanism called Censoring by Noisy Sampling (CBNS), which is\ncomposed of two modules: i) Invariant Sampler: a differentiable point-cloud\nsampler which learns to remove points invariant to utility and ii) Noisy\nDistorter: which learns to distort sampled points to decouple the sensitive\ninformation from utility, and mitigate privacy leakage. We validate the\neffectiveness of CBNS through extensive comparisons with state-of-the-art\nbaselines and sensitivity analyses of key design choices. Results show that\nCBNS achieves superior privacy-utility trade-offs on multiple datasets.\n","authors":["Ayush Chopra","Abhinav Java","Abhishek Singh","Vivek Sharma","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2203.12192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09301v2","updated":"2022-03-23T04:45:36Z","published":"2022-03-17T13:03:06Z","title":"One-Shot Adaptation of GAN in Just One CLIP","summary":"  There are many recent research efforts to fine-tune a pre-trained generator\nwith a few target images to generate images of a novel domain. Unfortunately,\nthese methods often suffer from overfitting or under-fitting when fine-tuned\nwith a single target image. To address this, here we present a novel\nsingle-shot GAN adaptation method through unified CLIP space manipulations.\nSpecifically, our model employs a two-step training strategy: reference image\nsearch in the source generator using a CLIP-guided latent optimization,\nfollowed by generator fine-tuning with a novel loss function that imposes CLIP\nspace consistency between the source and adapted generators. To further improve\nthe adapted model to produce spatially consistent samples with respect to the\nsource generator, we also propose contrastive regularization for patchwise\nrelationships in the CLIP space. Experimental results show that our model\ngenerates diverse outputs with the target texture and outperforms the baseline\nmodels both qualitatively and quantitatively. Furthermore, we show that our\nCLIP space manipulation strategy allows more effective attribute editing.\n","authors":["Gihyun Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2203.09301v2.pdf","comment":"Image compressed version"},{"id":"http://arxiv.org/abs/2203.02281v2","updated":"2022-03-23T04:39:12Z","published":"2022-03-03T07:49:21Z","title":"A Comprehensive Review of Computer Vision in Sports: Open Issues, Future\n  Trends and Research Directions","summary":"  Recent developments in video analysis of sports and computer vision\ntechniques have achieved significant improvements to enable a variety of\ncritical operations. To provide enhanced information, such as detailed complex\nanalysis in sports like soccer, basketball, cricket, badminton, etc., studies\nhave focused mainly on computer vision techniques employed to carry out\ndifferent tasks. This paper presents a comprehensive review of sports video\nanalysis for various applications high-level analysis such as detection and\nclassification of players, tracking player or ball in sports and predicting the\ntrajectories of player or ball, recognizing the teams strategies, classifying\nvarious events in sports. The paper further discusses published works in a\nvariety of application-specific tasks related to sports and the present\nresearchers views regarding them. Since there is a wide research scope in\nsports for deploying computer vision techniques in various sports, some of the\npublicly available datasets related to a particular sport have been provided.\nThis work reviews a detailed discussion on some of the artificial\nintelligence(AI)applications in sports vision, GPU-based work stations, and\nembedded platforms. Finally, this review identifies the research directions,\nprobable challenges, and future trends in the area of visual recognition in\nsports.\n","authors":["Banoth Thulasya Naik","Mohammad Farukh Hashmi","Neeraj Dhanraj Bokde"],"pdf_url":"https://arxiv.org/pdf/2203.02281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.00260v2","updated":"2022-03-23T04:25:48Z","published":"2021-12-01T03:36:58Z","title":"Ranking Distance Calibration for Cross-Domain Few-Shot Learning","summary":"  Recent progress in few-shot learning promotes a more realistic cross-domain\nsetting, where the source and target datasets are from different domains. Due\nto the domain gap and disjoint label spaces between source and target datasets,\ntheir shared knowledge is extremely limited. This encourages us to explore more\ninformation in the target domain rather than to overly elaborate training\nstrategies on the source domain as in many existing methods. Hence, we start\nfrom a generic representation pre-trained by a cross-entropy loss and a\nconventional distance-based classifier, along with an image retrieval view, to\nemploy a re-ranking process for calibrating a target distance matrix by\ndiscovering the reciprocal k-nearest neighbours within the task. Assuming the\npre-trained representation is biased towards the source, we construct a\nnon-linear subspace to minimise task-irrelevant features therewithin while keep\nmore transferrable discriminative information by a hyperbolic tangent\ntransformation. The calibrated distance in this target-aware non-linear\nsubspace is complementary to that in the pre-trained representation. To impose\nsuch distance calibration information onto the pre-trained representation, a\nKullback-Leibler divergence loss is employed to gradually guide the model\ntowards the calibrated distance-based distribution. Extensive evaluations on\neight target domains show that this target ranking calibration process can\nimprove conventional distance-based classifiers in few-shot learning.\n","authors":["Pan Li","Shaogang Gong","Chengjie Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2112.00260v2.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/2104.04891v2","updated":"2022-03-23T04:07:45Z","published":"2021-04-11T01:29:50Z","title":"SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point\n  Clouds","summary":"  Labelling point clouds fully is highly time-consuming and costly. As larger\npoint cloud datasets with billions of points become more common, we ask whether\nthe full annotation is even necessary, demonstrating that existing baselines\ndesigned under a fully annotated assumption only degrade slightly even when\nfaced with 1% random point annotations. However, beyond this point, e.g., at\n0.1% annotations, segmentation accuracy is unacceptably low. We observe that,\nas point clouds are samples of the 3D world, the distribution of points in a\nlocal neighborhood is relatively homogeneous, exhibiting strong semantic\nsimilarity. Motivated by this, we propose a new weak supervision method to\nimplicitly augment highly sparse supervision signals. Extensive experiments\ndemonstrate the proposed Semantic Query Network (SQN) achieves promising\nperformance on seven large-scale open datasets under weak supervision schemes,\nwhile requiring only 0.1% randomly annotated points for training, greatly\nreducing annotation cost and effort. The code is available at\nhttps://github.com/QingyongHu/SQN.\n","authors":["Qingyong Hu","Bo Yang","Guangchi Fang","Yulan Guo","Ales Leonardis","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2104.04891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.08918v4","updated":"2022-03-23T04:07:45Z","published":"2021-11-17T06:01:17Z","title":"Local Texture Estimator for Implicit Representation Function","summary":"  Recent works with an implicit neural function shed light on representing\nimages in arbitrary resolution. However, a standalone multi-layer perceptron\nshows limited performance in learning high-frequency components. In this paper,\nwe propose a Local Texture Estimator (LTE), a dominant-frequency estimator for\nnatural images, enabling an implicit function to capture fine details while\nreconstructing images in a continuous manner. When jointly trained with a deep\nsuper-resolution (SR) architecture, LTE is capable of characterizing image\ntextures in 2D Fourier space. We show that an LTE-based neural function\nachieves favorable performance against existing deep SR methods within an\narbitrary-scale factor. Furthermore, we demonstrate that our implementation\ntakes the shortest running time compared to previous works.\n","authors":["Jaewon Lee","Kyong Hwan Jin"],"pdf_url":"https://arxiv.org/pdf/2111.08918v4.pdf","comment":"CVPR 2022 camera-ready version"},{"id":"http://arxiv.org/abs/2202.08227v2","updated":"2022-03-23T03:54:50Z","published":"2022-02-16T18:12:14Z","title":"Ditto: Building Digital Twins of Articulated Objects from Interaction","summary":"  Digitizing physical objects into the virtual world has the potential to\nunlock new research and applications in embodied AI and mixed reality. This\nwork focuses on recreating interactive digital twins of real-world articulated\nobjects, which can be directly imported into virtual environments. We introduce\nDitto to learn articulation model estimation and 3D geometry reconstruction of\nan articulated object through interactive perception. Given a pair of visual\nobservations of an articulated object before and after interaction, Ditto\nreconstructs part-level geometry and estimates the articulation model of the\nobject. We employ implicit neural representations for joint geometry and\narticulation modeling. Our experiments show that Ditto effectively builds\ndigital twins of articulated objects in a category-agnostic way. We also apply\nDitto to real-world objects and deploy the recreated digital twins in physical\nsimulation. Code and additional results are available at\nhttps://ut-austin-rpl.github.io/Ditto\n","authors":["Zhenyu Jiang","Cheng-Chun Hsu","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2202.08227v2.pdf","comment":"14 pages, 7 figures; Code and additional results are available at\n  https://ut-austin-rpl.github.io/Ditto"},{"id":"http://arxiv.org/abs/2203.12178v1","updated":"2022-03-23T03:43:12Z","published":"2022-03-23T03:43:12Z","title":"Unifying Motion Deblurring and Frame Interpolation with Events","summary":"  Slow shutter speed and long exposure time of frame-based cameras often cause\nvisual blur and loss of inter-frame information, degenerating the overall\nquality of captured videos. To this end, we present a unified framework of\nevent-based motion deblurring and frame interpolation for blurry video\nenhancement, where the extremely low latency of events is leveraged to\nalleviate motion blur and facilitate intermediate frame prediction.\nSpecifically, the mapping relation between blurry frames and sharp latent\nimages is first predicted by a learnable double integral network, and a fusion\nnetwork is then proposed to refine the coarse results via utilizing the\ninformation from consecutive blurry inputs and the concurrent events. By\nexploring the mutual constraints among blurry frames, latent images, and event\nstreams, we further propose a self-supervised learning framework to enable\nnetwork training with real-world blurry videos and events. Extensive\nexperiments demonstrate that our method compares favorably against the\nstate-of-the-art approaches and achieves remarkable performance on both\nsynthetic and real-world datasets.\n","authors":["Xiang Zhang","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2203.12178v1.pdf","comment":"Paper accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2201.10703v2","updated":"2022-03-23T03:42:34Z","published":"2022-01-26T01:48:37Z","title":"Anomaly Detection via Reverse Distillation from One-Class Embedding","summary":"  Knowledge distillation (KD) achieves promising results on the challenging\nproblem of unsupervised anomaly detection (AD).The representation discrepancy\nof anomalies in the teacher-student (T-S) model provides essential evidence for\nAD. However, using similar or identical architectures to build the teacher and\nstudent models in previous studies hinders the diversity of anomalous\nrepresentations. To tackle this problem, we propose a novel T-S model\nconsisting of a teacher encoder and a student decoder and introduce a simple\nyet effective \"reverse distillation\" paradigm accordingly. Instead of receiving\nraw images directly, the student network takes teacher model's one-class\nembedding as input and targets to restore the teacher's multiscale\nrepresentations. Inherently, knowledge distillation in this study starts from\nabstract, high-level presentations to low-level features. In addition, we\nintroduce a trainable one-class bottleneck embedding (OCBE) module in our T-S\nmodel. The obtained compact embedding effectively preserves essential\ninformation on normal patterns, but abandons anomaly perturbations. Extensive\nexperimentation on AD and one-class novelty detection benchmarks shows that our\nmethod surpasses SOTA performance, demonstrating our proposed approach's\neffectiveness and generalizability.\n","authors":["Hanqiu Deng","Xingyu Li"],"pdf_url":"https://arxiv.org/pdf/2201.10703v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2203.07697v4","updated":"2022-03-23T03:38:07Z","published":"2022-03-15T07:30:27Z","title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose\n  Estimation","summary":"  In this paper, we present a novel Distribution-Aware Single-stage (DAS) model\nfor tackling the challenging multi-person 3D pose estimation problem. Different\nfrom existing top-down and bottom-up methods, the proposed DAS model\nsimultaneously localizes person positions and their corresponding body joints\nin the 3D camera space in a one-pass manner. This leads to a simplified\npipeline with enhanced efficiency. In addition, DAS learns the true\ndistribution of body joints for the regression of their positions, rather than\nmaking a simple Laplacian or Gaussian assumption as previous works. This\nprovides valuable priors for model prediction and thus boosts the\nregression-based scheme to achieve competitive performance with volumetric-base\nones. Moreover, DAS exploits a recursive update strategy for progressively\napproaching to regression target, alleviating the optimization difficulty and\nfurther lifting the regression performance. DAS is implemented with a fully\nConvolutional Neural Network and end-to-end learnable. Comprehensive\nexperiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior\nefficiency of the proposed DAS model, specifically 1.5x speedup over previous\nbest model, and its stat-of-the-art accuracy for multi-person 3D pose\nestimation.\n","authors":["Zitian Wang","Xuecheng Nie","Xiaochao Qu","Yunpeng Chen","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2203.07697v4.pdf","comment":"To appear in CVPR 2022. Code will be released"},{"id":"http://arxiv.org/abs/2203.12175v1","updated":"2022-03-23T03:37:44Z","published":"2022-03-23T03:37:44Z","title":"Adaptive Transformers for Robust Few-shot Cross-domain Face\n  Anti-spoofing","summary":"  While recent face anti-spoofing methods perform well under the intra-domain\nsetups, an effective approach needs to account for much larger appearance\nvariations of images acquired in complex scenes with different sensors for\nrobust performance. In this paper, we present adaptive vision transformers\n(ViT) for robust cross-domain face anti-spoofing. Specifically, we adopt ViT as\na backbone to exploit its strength to account for long-range dependencies among\npixels. We further introduce the ensemble adapters module and feature-wise\ntransformation layers in the ViT to adapt to different domains for robust\nperformance with a few samples. Experiments on several benchmark datasets show\nthat the proposed models achieve both robust and competitive performance\nagainst the state-of-the-art methods.\n","authors":["Hsin-Ping Huang","Deqing Sun","Yaojie Liu","Wen-Sheng Chu","Taihong Xiao","Jinwei Yuan","Hartwig Adam","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2203.12175v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2203.12465v1","updated":"2022-03-23T14:58:43Z","published":"2022-03-23T14:58:43Z","title":"Multi-agent Searching System for Medical Information","summary":"  In the paper is proposed a model of multi-agent security system for searching\na medical information in Internet. The advantages when using mobile agent are\ndescribed, so that to perform searching in Internet. Nowadays, multi-agent\nsystems found their application into distribution of decisions. For modeling\nthe proposed multi-agent medical system is used JADE. Finally, the results when\nusing mobile agent are generated that could reflect performance when working\nwith BIG DATA. The proposed system is having also relatively high precision\n96%.\n","authors":["Mariya Evtimova-Gardair"],"pdf_url":"https://arxiv.org/pdf/2203.12465v1.pdf","comment":"Volume 16, 2019, pp.140-145"},{"id":"http://arxiv.org/abs/2203.12451v1","updated":"2022-03-23T14:43:14Z","published":"2022-03-23T14:43:14Z","title":"Deep Multi-View Learning for Tire Recommendation","summary":"  We are constantly using recommender systems, often without even noticing.\nThey build a profile of our person in order to recommend the content we will\nmost likely be interested in. The data representing the users, their\ninteractions with the system or the products may come from different sources\nand be of a various nature. Our goal is to use a multi-view learning approach\nto improve our recommender system and improve its capacity to manage multi-view\ndata. We propose a comparative study between several state-of-the-art\nmulti-view models applied to our industrial data. Our study demonstrates the\nrelevance of using multi-view learning within recommender systems.\n","authors":["Thomas Ranvier","Kilian Bourhis","Khalid Benabdeslem","Bruno Canitia"],"pdf_url":"https://arxiv.org/pdf/2203.12451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.07404v2","updated":"2022-03-23T12:14:36Z","published":"2021-04-15T12:08:30Z","title":"Two Birds with One Stone: Unified Model Learning for Both Recall and\n  Ranking in News Recommendation","summary":"  Recall and ranking are two critical steps in personalized news\nrecommendation. Most existing news recommender systems conduct personalized\nnews recall and ranking separately with different models. However, maintaining\nmultiple models leads to high computational cost and poses great challenge to\nmeeting the online latency requirement of news recommender systems. In order to\nhandle this problem, in this paper we propose UniRec, a unified method for\nrecall and ranking in news recommendation. In our method, we first infer user\nembedding for ranking from the historical news click behaviors of a user using\na user encoder model. Then we derive the user embedding for recall from the\nobtained user embedding for ranking by using it as the attention query to\nselect a set of basis user embeddings which encode different general user\ninterests and synthesize them into a user embedding for recall. The extensive\nexperiments on benchmark dataset demonstrate that our method can improve both\nefficiency and effectiveness for recall and ranking in news recommendation.\n","authors":["Chuhan Wu","Fangzhao Wu","Tao Qi","Yongfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2104.07404v2.pdf","comment":"ACL 2022 Findings"},{"id":"http://arxiv.org/abs/2104.07407v2","updated":"2022-03-23T12:06:42Z","published":"2021-04-15T12:11:50Z","title":"MM-Rec: Multimodal News Recommendation","summary":"  Accurate news representation is critical for news recommendation. Most of\nexisting news representation methods learn news representations only from news\ntexts while ignore the visual information in news like images. In fact, users\nmay click news not only because of the interest in news titles but also due to\nthe attraction of news images. Thus, images are useful for representing news\nand predicting user behaviors. In this paper, we propose a multimodal news\nrecommendation method, which can incorporate both textual and visual\ninformation of news to learn multimodal news representations. We first extract\nregion-of-interests (ROIs) from news images via object detection. Then we use a\npre-trained visiolinguistic model to encode both news texts and news image ROIs\nand model their inherent relatedness using co-attentional Transformers. In\naddition, we propose a crossmodal candidate-aware attention network to select\nrelevant historical clicked news for accurate user modeling by measuring the\ncrossmodal relatedness between clicked news and candidate news. Experiments\nvalidate that incorporating multimodal news information can effectively improve\nnews recommendation.\n","authors":["Chuhan Wu","Fangzhao Wu","Tao Qi","Yongfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2104.07407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12267v1","updated":"2022-03-23T08:29:46Z","published":"2022-03-23T08:29:46Z","title":"PEAR: Personalized Re-ranking with Contextualized Transformer for\n  Recommendation","summary":"  The goal of recommender systems is to provide ordered item lists to users\nthat best match their interests. As a critical task in the recommendation\npipeline, re-ranking has received increasing attention in recent years. In\ncontrast to conventional ranking models that score each item individually,\nre-ranking aims to explicitly model the mutual influences among items to\nfurther refine the ordering of items given an initial ranking list. In this\npaper, we present a personalized re-ranking model (dubbed PEAR) based on\ncontextualized transformer. PEAR makes several major improvements over the\nexisting methods. Specifically, PEAR not only captures feature-level and\nitem-level interactions, but also models item contexts from both the initial\nranking list and the historical clicked item list. In addition to item-level\nranking score prediction, we also augment the training of PEAR with a\nlist-level classification task to assess users' satisfaction on the whole\nranking list. Experimental results on both public and production datasets have\nshown the superior effectiveness of PEAR compared to the previous re-ranking\nmodels.\n","authors":["Yi Li","Jieming Zhu","Weiwen Liu","Liangcai Su","Guohao Cai","Qi Zhang","Ruiming Tang","Xi Xiao","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2203.12267v1.pdf","comment":"Accepted by WWW 2022"},{"id":"http://arxiv.org/abs/2202.08904v4","updated":"2022-03-23T19:16:55Z","published":"2022-02-17T21:35:56Z","title":"SGPT: GPT Sentence Embeddings for Semantic Search","summary":"  GPT transformers are the largest language models available, yet semantic\nsearch is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for\napplying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric\nsearch.\n  SGPT-BE produces semantically meaningful sentence embeddings by contrastive\nfine-tuning of only bias tensors and a novel pooling method. A 5.8 billion\nparameter SGPT-BE outperforms the best available sentence embeddings by 6%\nsetting a new state-of-the-art on BEIR. It outperforms the concurrently\nproposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes\n250,000 times more parameters.\n  SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1\nbillion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It\nbeats the supervised state-of-the-art on 7 datasets, but significantly loses on\nother datasets. We show how this can be alleviated by adapting the prompt.\n  SGPT-BE and SGPT-CE performance scales with model size. Yet, increased\nlatency, storage and compute costs should be considered. Code, models and\nresult files are freely available at https://github.com/Muennighoff/sgpt.\n","authors":["Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2202.08904v4.pdf","comment":"17 pages, 3 figures, 12 tables. v2 corrects a misreported nDCG@10\n  number for the SGPT-BE-5.8B model. v3 updates SGPT-BE-5.8B scores based on\n  retrained models with larger batch sizes v4 removes a superfluous table"},{"id":"http://arxiv.org/abs/2203.12663v1","updated":"2022-03-23T18:41:36Z","published":"2022-03-23T18:41:36Z","title":"CorpusVis: Visual Analysis of Digital Sheet Music Collections","summary":"  Manually investigating sheet music collections is challenging for music\nanalysts due to the magnitude and complexity of underlying features,\nstructures, and contextual information. However, applying sophisticated\nalgorithmic methods would require advanced technical expertise that analysts do\nnot necessarily have. Bridging this gap, we contribute CorpusVis, an\ninteractive visual workspace, enabling scalable and multi-faceted analysis. Our\nproposed visual analytics dashboard provides access to computational methods,\ngenerating varying perspectives on the same data. The proposed application uses\nmetadata including composers, type, epoch, and low-level features, such as\npitch, melody, and rhythm. To evaluate our approach, we conducted a pair\nanalytics study with nine participants. The qualitative results show that\nCorpusVis supports users in performing exploratory and confirmatory analysis,\nleading them to new insights and findings. In addition, based on three\nexemplary workflows, we demonstrate how to apply our approach to different\ntasks, such as exploring musical features or comparing composers.\n","authors":["Matthias Miller","Julius Rauscher","Daniel A. Keim","Mennatallah El-Assady"],"pdf_url":"https://arxiv.org/pdf/2203.12663v1.pdf","comment":"12 pages, 9 figures, Computer Graphics Forum 2022, EuroVis Conference\n  (Full Papers)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2203.12616v1","updated":"2022-03-23T17:59:45Z","published":"2022-03-23T17:59:45Z","title":"Unsupervised Pre-Training on Patient Population Graphs for Patient-Level\n  Predictions","summary":"  Pre-training has shown success in different areas of machine learning, such\nas Computer Vision (CV), Natural Language Processing (NLP) and medical imaging.\nHowever, it has not been fully explored for clinical data analysis. Even though\nan immense amount of Electronic Health Record (EHR) data is recorded, data and\nlabels can be scarce if the data is collected in small hospitals or deals with\nrare diseases. In such scenarios, pre-training on a larger set of EHR data\ncould improve the model performance. In this paper, we apply unsupervised\npre-training to heterogeneous, multi-modal EHR data for patient outcome\nprediction. To model this data, we leverage graph deep learning over population\ngraphs. We first design a network architecture based on graph transformer\ndesigned to handle various input feature types occurring in EHR data, like\ncontinuous, discrete, and time-series features, allowing better multi-modal\ndata fusion. Further, we design pre-training methods based on masked imputation\nto pre-train our network before fine-tuning on different end tasks.\nPre-training is done in a fully unsupervised fashion, which lays the groundwork\nfor pre-training on large public datasets with different tasks and similar\nmodalities in the future. We test our method on two medical datasets of patient\nrecords, TADPOLE and MIMIC-III, including imaging and non-imaging features and\ndifferent prediction tasks. We find that our proposed graph based pre-training\nmethod helps in modeling the data at a population level and further improves\nperformance on the fine tuning tasks in terms of AUC on average by 4.15% for\nMIMIC and 7.64% for TADPOLE.\n","authors":["Chantal Pellegrini","Anees Kazi","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2203.12616v1.pdf","comment":"10 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2203.12610v1","updated":"2022-03-23T17:57:01Z","published":"2022-03-23T17:57:01Z","title":"AI Poincaré 2.0: Machine Learning Conservation Laws from\n  Differential Equations","summary":"  We present a machine learning algorithm that discovers conservation laws from\ndifferential equations, both numerically (parametrized as neural networks) and\nsymbolically, ensuring their functional independence (a non-linear\ngeneralization of linear independence). Our independence module can be viewed\nas a nonlinear generalization of singular value decomposition. Our method can\nreadily handle inductive biases for conservation laws. We validate it with\nexamples including the 3-body problem, the KdV equation and nonlinear\nSchr\\\"odinger equation.\n","authors":["Ziming Liu","Varun Madhavan","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2203.12610v1.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2203.12609v1","updated":"2022-03-23T17:56:58Z","published":"2022-03-23T17:56:58Z","title":"Improving the Fairness of Chest X-ray Classifiers","summary":"  Deep learning models have reached or surpassed human-level performance in the\nfield of medical imaging, especially in disease diagnosis using chest x-rays.\nHowever, prior work has found that such classifiers can exhibit biases in the\nform of gaps in predictive performance across protected groups. In this paper,\nwe question whether striving to achieve zero disparities in predictive\nperformance (i.e. group fairness) is the appropriate fairness definition in the\nclinical setting, over minimax fairness, which focuses on maximizing the\nperformance of the worst-case group. We benchmark the performance of nine\nmethods in improving classifier fairness across these two definitions. We find,\nconsistent with prior work on non-clinical data, that methods which strive to\nachieve better worst-group performance do not outperform simple data balancing.\nWe also find that methods which achieve group fairness do so by worsening\nperformance for all groups. In light of these results, we discuss the utility\nof fairness definitions in the clinical setting, advocating for an\ninvestigation of the bias-inducing mechanisms in the underlying data generating\nprocess whenever possible.\n","authors":["Haoran Zhang","Natalie Dullerud","Karsten Roth","Lauren Oakden-Rayner","Stephen Robert Pfohl","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2203.12609v1.pdf","comment":"Published in CHIL 2022"},{"id":"http://arxiv.org/abs/2203.12601v1","updated":"2022-03-23T17:55:09Z","published":"2022-03-23T17:55:09Z","title":"R3M: A Universal Visual Representation for Robot Manipulation","summary":"  We study how visual representations pre-trained on diverse human video data\ncan enable data-efficient learning of downstream robotic manipulation tasks.\nConcretely, we pre-train a visual representation using the Ego4D human video\ndataset using a combination of time-contrastive learning, video-language\nalignment, and an L1 penalty to encourage sparse and compact representations.\nThe resulting representation, R3M, can be used as a frozen perception module\nfor downstream policy learning. Across a suite of 12 simulated robot\nmanipulation tasks, we find that R3M improves task success by over 20% compared\nto training from scratch and by over 10% compared to state-of-the-art visual\nrepresentations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika\nPanda arm to learn a range of manipulation tasks in a real, cluttered apartment\ngiven just 20 demonstrations. Code and pre-trained models are available at\nhttps://tinyurl.com/robotr3m.\n","authors":["Suraj Nair","Aravind Rajeswaran","Vikash Kumar","Chelsea Finn","Abhinav Gupta"],"pdf_url":"https://arxiv.org/pdf/2203.12601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12592v1","updated":"2022-03-23T17:54:20Z","published":"2022-03-23T17:54:20Z","title":"Your Policy Regularizer is Secretly an Adversary","summary":"  Policy regularization methods such as maximum entropy regularization are\nwidely used in reinforcement learning to improve the robustness of a learned\npolicy. In this paper, we show how this robustness arises from hedging against\nworst-case perturbations of the reward function, which are chosen from a\nlimited set by an imagined adversary. Using convex duality, we characterize\nthis robust set of adversarial reward perturbations under KL- and\n{\\alpha}-divergence regularization, which includes Shannon and Tsallis entropy\nregularization as special cases. Importantly, generalization guarantees can be\ngiven within this robust set. We provide detailed discussion of the worst-case\nreward perturbations, and present intuitive empirical examples to illustrate\nthis robustness and its relationship with generalization. Finally, we discuss\nhow our analysis complements and extends previous results on adversarial reward\nrobustness and path consistency optimality conditions.\n","authors":["Rob Brekelmans","Tim Genewein","Jordi Grau-Moya","Grégoire Delétang","Markus Kunesch","Shane Legg","Pedro Ortega"],"pdf_url":"https://arxiv.org/pdf/2203.12592v1.pdf","comment":"10 pages main text"},{"id":"http://arxiv.org/abs/2203.12577v1","updated":"2022-03-23T17:37:43Z","published":"2022-03-23T17:37:43Z","title":"Minimax Regret for Cascading Bandits","summary":"  Cascading bandits model the task of learning to rank $K$ out of $L$ items\nover $n$ rounds of partial feedback. For this model, the minimax (i.e.,\ngap-free) regret is poorly understood; in particular, the best known lower and\nupper bounds are $\\Omega(\\sqrt{nL/K})$ and $\\tilde{O}(\\sqrt{nLK})$,\nrespectively. We improve the lower bound to $\\Omega(\\sqrt{nL})$ and show\nCascadeKL-UCB (which ranks items by their KL-UCB indices) attains it up to log\nterms. Surprisingly, we also show CascadeUCB1 (which ranks via UCB1) can suffer\nsuboptimal $\\Omega(\\sqrt{nLK})$ regret. This sharply contrasts with standard\n$L$-armed bandits, where the corresponding algorithms both achieve the minimax\nregret $\\sqrt{nL}$ (up to log terms), and the main advantage of KL-UCB is only\nto improve constants in the gap-dependent bounds. In essence, this contrast\noccurs because Pinsker's inequality is tight for hard problems in the $L$-armed\ncase but loose (by a factor of $K$) in the cascading case.\n","authors":["Daniel Vial","Sujay Sanghavi","Sanjay Shakkottai","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2203.12577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12574v1","updated":"2022-03-23T17:34:35Z","published":"2022-03-23T17:34:35Z","title":"Mitigating Gender Bias in Distilled Language Models via Counterfactual\n  Role Reversal","summary":"  Language models excel at generating coherent text, and model compression\ntechniques such as knowledge distillation have enabled their use in\nresource-constrained settings. However, these models can be biased in multiple\nways, including the unfounded association of male and female genders with\ngender-neutral professions. Therefore, knowledge distillation without any\nfairness constraints may preserve or exaggerate the teacher model's biases onto\nthe distilled model. To this end, we present a novel approach to mitigate\ngender disparity in text generation by learning a fair model during knowledge\ndistillation. We propose two modifications to the base knowledge distillation\nbased on counterfactual role reversal$\\unicode{x2014}$modifying teacher\nprobabilities and augmenting the training set. We evaluate gender polarity\nacross professions in open-ended text generated from the resulting distilled\nand finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial\nreduction in gender disparity with only a minor compromise in utility. Finally,\nwe observe that language models that reduce gender polarity in language\ngeneration do not improve embedding fairness or downstream classification\nfairness.\n","authors":["Umang Gupta","Jwala Dhamala","Varun Kumar","Apurv Verma","Yada Pruksachatkun","Satyapriya Krishna","Rahul Gupta","Kai-Wei Chang","Greg Ver Steeg","Aram Galstyan"],"pdf_url":"https://arxiv.org/pdf/2203.12574v1.pdf","comment":"To appear in the Findings of ACL 2022"},{"id":"http://arxiv.org/abs/2203.12569v1","updated":"2022-03-23T17:29:17Z","published":"2022-03-23T17:29:17Z","title":"A Top-down Supervised Learning Approach to Hierarchical Multi-label\n  Classification in Networks","summary":"  Node classification is the task of inferring or predicting missing node\nattributes from information available for other nodes in a network. This paper\npresents a general prediction model to hierarchical multi-label classification\n(HMC), where the attributes to be inferred can be specified as a strict poset.\nIt is based on a top-down classification approach that addresses hierarchical\nmulti-label classification with supervised learning by building a local\nclassifier per class. The proposed model is showcased with a case study on the\nprediction of gene functions for Oryza sativa Japonica, a variety of rice. It\nis compared to the Hierarchical Binomial-Neighborhood, a probabilistic model,\nby evaluating both approaches in terms of prediction performance and\ncomputational cost. The results in this work support the working hypothesis\nthat the proposed model can achieve good levels of prediction efficiency, while\nscaling up in relation to the state of the art.\n","authors":["Miguel Romero","Jorge Finke","Camilo Rocha"],"pdf_url":"https://arxiv.org/pdf/2203.12569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12555v1","updated":"2022-03-23T17:14:03Z","published":"2022-03-23T17:14:03Z","title":"GriTS: Grid table similarity metric for table structure recognition","summary":"  In this paper, we propose a new class of evaluation metric for table\nstructure recognition, grid table similarity (GriTS). Unlike prior metrics,\nGriTS evaluates the correctness of a predicted table directly in its natural\nform as a matrix. To create a similarity measure between matrices, we\ngeneralize the two-dimensional largest common substructure (2D-LCS) problem,\nwhich is NP-hard, to the 2D most similar substructures (2D-MSS) problem and\npropose a polynomial-time heuristic for solving it. We validate empirically\nusing the PubTables-1M dataset that comparison between matrices exhibits more\ndesirable behavior than alternatives for table structure recognition\nevaluation. GriTS also unifies all three subtasks of cell topology recognition,\ncell location recognition, and cell content recognition within the same\nframework, which simplifies the evaluation and enables more meaningful\ncomparisons across different types of structure recognition approaches. Code\nwill be released at https://github.com/microsoft/table-transformer.\n","authors":["Brandon Smock","Rohith Pesala","Robin Abraham"],"pdf_url":"https://arxiv.org/pdf/2203.12555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.07577v2","updated":"2022-03-23T17:10:08Z","published":"2021-10-14T17:40:08Z","title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model\n  Tuning","summary":"  Recent parameter-efficient language model tuning (PELT) methods manage to\nmatch the performance of fine-tuning with much fewer trainable parameters and\nperform especially well when training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and tasks. In light of\nmodel diversity and the difficulty of model selection, we propose a unified\nframework, UniPELT, which incorporates different PELT methods as submodules and\nlearns to activate the ones that best suit the current data or task setup via\ngating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%\ngains compared to the best individual PELT method that it incorporates and even\noutperforms fine-tuning under different setups. Moreover, UniPELT generally\nsurpasses the upper bound that takes the best performance of all its submodules\nused individually on each task, indicating that a mixture of multiple PELT\nmethods may be inherently more effective than single methods.\n","authors":["Yuning Mao","Lambert Mathias","Rui Hou","Amjad Almahairi","Hao Ma","Jiawei Han","Wen-tau Yih","Madian Khabsa"],"pdf_url":"https://arxiv.org/pdf/2110.07577v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2111.02994v4","updated":"2022-03-23T17:08:49Z","published":"2021-11-04T16:45:15Z","title":"Towards an Understanding of Default Policies in Multitask Policy\n  Optimization","summary":"  Much of the recent success of deep reinforcement learning has been driven by\nregularized policy optimization (RPO) algorithms with strong performance across\nmultiple domains. In this family of methods, agents are trained to maximize\ncumulative reward while penalizing deviation in behavior from some reference,\nor default policy. In addition to empirical success, there is a strong\ntheoretical foundation for understanding RPO methods applied to single tasks,\nwith connections to natural gradient, trust region, and variational approaches.\nHowever, there is limited formal understanding of desirable properties for\ndefault policies in the multitask setting, an increasingly important domain as\nthe field shifts towards training more generally capable agents. Here, we take\na first step towards filling this gap by formally linking the quality of the\ndefault policy to its effect on optimization. Using these results, we then\nderive a principled RPO algorithm for multitask learning with strong\nperformance guarantees.\n","authors":["Ted Moskovitz","Michael Arbel","Jack Parker-Holder","Aldo Pacchiano"],"pdf_url":"https://arxiv.org/pdf/2111.02994v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12546v1","updated":"2022-03-23T17:07:53Z","published":"2022-03-23T17:07:53Z","title":"Constrained Clustering and Multiple Kernel Learning without Pairwise\n  Constraint Relaxation","summary":"  Clustering under pairwise constraints is an important knowledge discovery\ntool that enables the learning of appropriate kernels or distance metrics to\nimprove clustering performance. These pairwise constraints, which come in the\nform of must-link and cannot-link pairs, arise naturally in many applications\nand are intuitive for users to provide. However, the common practice of\nrelaxing discrete constraints to a continuous domain to ease optimization when\nlearning kernels or metrics can harm generalization, as information which only\nencodes linkage is transformed to informing distances. We introduce a new\nconstrained clustering algorithm that jointly clusters data and learns a kernel\nin accordance with the available pairwise constraints. To generalize well, our\nmethod is designed to maximize constraint satisfaction without relaxing\npairwise constraints to a continuous domain where they inform distances. We\nshow that the proposed method outperforms existing approaches on a large number\nof diverse publicly available datasets, and we discuss how our method can scale\nto handling large data.\n","authors":["Benedikt Boecking","Vincent Jeanselme","Artur Dubrawski"],"pdf_url":"https://arxiv.org/pdf/2203.12546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12533v1","updated":"2022-03-23T16:50:53Z","published":"2022-03-23T16:50:53Z","title":"Pathways: Asynchronous Distributed Dataflow for ML","summary":"  We present the design of a new large scale orchestration layer for\naccelerators. Our system, Pathways, is explicitly designed to enable\nexploration of new systems and ML research ideas, while retaining state of the\nart performance for current models. Pathways uses a sharded dataflow graph of\nasynchronous operators that consume and produce futures, and efficiently\ngang-schedules heterogeneous parallel computations on thousands of accelerators\nwhile coordinating data transfers over their dedicated interconnects. Pathways\nmakes use of a novel asynchronous distributed dataflow design that lets the\ncontrol plane execute in parallel despite dependencies in the data plane. This\ndesign, with careful engineering, allows Pathways to adopt a single-controller\nmodel that makes it easier to express complex new parallelism patterns. We\ndemonstrate that Pathways can achieve performance parity (~100% accelerator\nutilization) with state-of-the-art systems when running SPMD computations over\n2048 TPUs, while also delivering throughput comparable to the SPMD case for\nTransformer models that are pipelined across 16 stages, or sharded across two\nislands of accelerators connected over a data center network.\n","authors":["Paul Barham","Aakanksha Chowdhery","Jeff Dean","Sanjay Ghemawat","Steven Hand","Dan Hurt","Michael Isard","Hyeontaek Lim","Ruoming Pang","Sudip Roy","Brennan Saeta","Parker Schuh","Ryan Sepassi","Laurent El Shafey","Chandramohan A. Thekkath","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2203.12533v1.pdf","comment":"MLSys 2022"},{"id":"http://arxiv.org/abs/2112.06517v3","updated":"2022-03-23T16:43:08Z","published":"2021-12-13T09:48:54Z","title":"Top $K$ Ranking for Multi-Armed Bandit with Noisy Evaluations","summary":"  We consider a multi-armed bandit setting where, at the beginning of each\nround, the learner receives noisy independent, and possibly biased,\n\\emph{evaluations} of the true reward of each arm and it selects $K$ arms with\nthe objective of accumulating as much reward as possible over $T$ rounds. Under\nthe assumption that at each round the true reward of each arm is drawn from a\nfixed distribution, we derive different algorithmic approaches and theoretical\nguarantees depending on how the evaluations are generated. First, we show a\n$\\widetilde{O}(T^{2/3})$ regret in the general case when the observation\nfunctions are a genearalized linear function of the true rewards. On the other\nhand, we show that an improved $\\widetilde{O}(\\sqrt{T})$ regret can be derived\nwhen the observation functions are noisy linear functions of the true rewards.\nFinally, we report an empirical validation that confirms our theoretical\nfindings, provides a thorough comparison to alternative approaches, and further\nsupports the interest of this setting in practice.\n","authors":["Evrard Garcelon","Vashist Avadhanula","Alessandro Lazaric","Matteo Pirotta"],"pdf_url":"https://arxiv.org/pdf/2112.06517v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12529v1","updated":"2022-03-23T16:43:07Z","published":"2022-03-23T16:43:07Z","title":"A Deep Learning Approach to Probabilistic Forecasting of Weather","summary":"  We discuss an approach to probabilistic forecasting based on two chained\nmachine-learning steps: a dimensional reduction step that learns a reduction\nmap of predictor information to a low-dimensional space in a manner designed to\npreserve information about forecast quantities; and a density estimation step\nthat uses the probabilistic machine learning technique of normalizing flows to\ncompute the joint probability density of reduced predictors and forecast\nquantities. This joint density is then renormalized to produce the conditional\nforecast distribution. In this method, probabilistic calibration testing plays\nthe role of a regularization procedure, preventing overfitting in the second\nstep, while effective dimensional reduction from the first step is the source\nof forecast sharpness. We verify the method using a 22-year 1-hour cadence time\nseries of Weather Research and Forecasting (WRF) simulation data of surface\nwind on a grid.\n","authors":["Nick Rittler","Carlo Graziani","Jiali Wang","Rao Kotamarthi"],"pdf_url":"https://arxiv.org/pdf/2203.12529v1.pdf","comment":"12 pages, 5 figures. Submitted to Artificial Intelligence for Earth\n  Systems"},{"id":"http://arxiv.org/abs/2203.11113v2","updated":"2022-03-23T16:36:25Z","published":"2022-03-21T16:41:35Z","title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces","summary":"  Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n","authors":["Jia-Xing Zhong","Kaichen Zhou","Qingyong Hu","Bing Wang","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2203.11113v2.pdf","comment":"To appear at CVPR 2022 (Source Code:\n  https://github.com/jx-zhong-for-academic-purpose/Kinet )"},{"id":"http://arxiv.org/abs/2203.12522v1","updated":"2022-03-23T16:31:53Z","published":"2022-03-23T16:31:53Z","title":"Semi-Supervised Graph Learning Meets Dimensionality Reduction","summary":"  Semi-supervised learning (SSL) has recently received increased attention from\nmachine learning researchers. By enabling effective propagation of known labels\nin graph-based deep learning (GDL) algorithms, SSL is poised to become an\nincreasingly used technique in GDL in the coming years. However, there are\ncurrently few explorations in the graph-based SSL literature on exploiting\nclassical dimensionality reduction techniques for improved label propagation.\nIn this work, we investigate the use of dimensionality reduction techniques\nsuch as PCA, t-SNE, and UMAP to see their effect on the performance of graph\nneural networks (GNNs) designed for semi-supervised propagation of node labels.\nOur study makes use of benchmark semi-supervised GDL datasets such as the Cora\nand Citeseer datasets to allow meaningful comparisons of the representations\nlearned by each algorithm when paired with a dimensionality reduction\ntechnique. Our comprehensive benchmarks and clustering visualizations\nquantitatively and qualitatively demonstrate that, under certain conditions,\nemploying a priori and a posteriori dimensionality reduction to GNN inputs and\noutputs, respectively, can simultaneously improve the effectiveness of\nsemi-supervised node label propagation and node clustering. Our source code is\nfreely available on GitHub.\n","authors":["Alex Morehead","Watchanan Chantapakul","Jianlin Cheng"],"pdf_url":"https://arxiv.org/pdf/2203.12522v1.pdf","comment":"8 pages, 8 figures, and 5 tables. Submitted to the 2022 International\n  Joint Conference on Neural Networks (IJCNN 2022). Source code is available at\n  https://github.com/amorehead/SSL-With-DR-And-GNNs"},{"id":"http://arxiv.org/abs/2201.07577v2","updated":"2022-03-23T16:31:26Z","published":"2022-01-19T12:59:04Z","title":"Models for information propagation on graphs","summary":"  We propose and unify classes of different models for information propagation\nover graphs. In a first class, propagation is modeled as a wave which emanates\nfrom a set of known nodes at an initial time, to all other unknown nodes at\nlater times with an ordering determined by the arrival time of the information\nwave front. A second class of models is based on the notion of a travel time\nalong paths between nodes. The time of information propagation from an initial\nknown set of nodes to a node is defined as the minimum of a generalized travel\ntime over subsets of all admissible paths. A final class is given by imposing a\nlocal equation of an eikonal form at each unknown node, with boundary\nconditions at the known nodes. The solution value of the local equation at a\nnode is coupled to those of neighbouring nodes with lower values. We provide\nprecise formulations of the model classes and prove equivalences between them.\nMotivated by the connection between first arrival time model and the eikonal\nequation in the continuum setting, we derive mean field limits for graphs based\non uniform grids in Euclidean space under grid refinement. For a specific\nparameter setting, we demonstrate that the solution on the grid approximates\nthe Euclidean distance, and illustrate the use of front propagation on graphs\nto semi-supervised learning.\n","authors":["Oliver R. A. Dunbar","Charles M. Elliott","Lisa Maria Kreusser"],"pdf_url":"https://arxiv.org/pdf/2201.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08727v2","updated":"2022-03-23T16:28:06Z","published":"2021-10-17T05:16:58Z","title":"Graph-less Neural Networks: Teaching Old MLPs New Tricks via\n  Distillation","summary":"  Graph Neural Networks (GNNs) are popular for graph machine learning and have\nshown great results on wide node classification tasks. Yet, they are less\npopular for practical deployments in the industry owing to their scalability\nchallenges incurred by data dependency. Namely, GNN inference depends on\nneighbor nodes multiple hops away from the target, and fetching them burdens\nlatency-constrained applications. Existing inference acceleration methods like\npruning and quantization can speed up GNNs by reducing\nMultiplication-and-ACcumulation (MAC) operations, but the improvements are\nlimited given the data dependency is not resolved. Conversely, multi-layer\nperceptrons (MLPs) have no graph dependency and infer much faster than GNNs,\neven though they are less accurate than GNNs for node classification in\ngeneral. Motivated by these complementary strengths and weaknesses, we bring\nGNNs and MLPs together via knowledge distillation (KD). Our work shows that the\nperformance of MLPs can be improved by large margins with GNN KD. We call the\ndistilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference\ngraph dependency. We show that GLNNs with competitive accuracy infer faster\nthan GNNs by 146X-273X and faster than other acceleration methods by 14X-27X.\nUnder a production setting involving both transductive and inductive\npredictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by\n12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows\nwhen and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN\nas a handy choice for latency-constrained applications.\n","authors":["Shichang Zhang","Yozen Liu","Yizhou Sun","Neil Shah"],"pdf_url":"https://arxiv.org/pdf/2110.08727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12513v1","updated":"2022-03-23T16:17:22Z","published":"2022-03-23T16:17:22Z","title":"Sampling Theorems for Unsupervised Learning in Linear Inverse Problems","summary":"  Solving a linear inverse problem requires knowledge about the underlying\nsignal model. In many applications, this model is a priori unknown and has to\nbe learned from data. However, it is impossible to learn the model using\nobservations obtained via a single incomplete measurement operator, as there is\nno information outside the range of the inverse operator, resulting in a\nchicken-and-egg problem: to learn the model we need reconstructed signals, but\nto reconstruct the signals we need to know the model. Two ways to overcome this\nlimitation are using multiple measurement operators or assuming that the signal\nmodel is invariant to a certain group action. In this paper, we present\nnecessary and sufficient sampling conditions for learning the signal model from\npartial measurements which only depend on the dimension of the model, and the\nnumber of operators or properties of the group action that the model is\ninvariant to. As our results are agnostic of the learning algorithm, they shed\nlight into the fundamental limitations of learning from incomplete data and\nhave implications in a wide range set of practical algorithms, such as\ndictionary learning, matrix completion and deep neural networks.\n","authors":["Julián Tachella","Dongdong Chen","Mike Davies"],"pdf_url":"https://arxiv.org/pdf/2203.12513v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2201.12151"},{"id":"http://arxiv.org/abs/2103.09927v2","updated":"2022-03-23T16:16:58Z","published":"2021-03-17T21:49:21Z","title":"Encrypted Linear Contextual Bandit","summary":"  Contextual bandit is a general framework for online learning in sequential\ndecision-making problems that has found application in a wide range of domains,\nincluding recommendation systems, online advertising, and clinical trials.\n  A critical aspect of bandit methods is that they require to observe the\ncontexts --i.e., individual or group-level data-- and rewards in order to solve\nthe sequential problem. The large deployment in industrial applications has\nincreased interest in methods that preserve the users' privacy. In this paper,\nwe introduce a privacy-preserving bandit framework based on homomorphic\nencryption{\\color{violet} which allows computations using encrypted data}. The\nalgorithm \\textit{only} observes encrypted information (contexts and rewards)\nand has no ability to decrypt it. Leveraging the properties of homomorphic\nencryption, we show that despite the complexity of the setting, it is possible\nto solve linear contextual bandits over encrypted data with a\n$\\widetilde{O}(d\\sqrt{T})$ regret bound in any linear contextual bandit\nproblem, while keeping data encrypted.\n","authors":["Evrard Garcelon","Vianney Perchet","Matteo Pirotta"],"pdf_url":"https://arxiv.org/pdf/2103.09927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.01937v3","updated":"2022-03-23T16:16:25Z","published":"2021-03-02T18:53:20Z","title":"Neural Production Systems: Learning Rule-Governed Visual Dynamics","summary":"  Visual environments are structured, consisting of distinct objects or\nentities. These entities have properties -- both visible and latent -- that\ndetermine the manner in which they interact with one another. To partition\nimages into entities, deep-learning researchers have proposed structural\ninductive biases such as slot-based architectures. To model interactions among\nentities, equivariant graph neural nets (GNNs) are used, but these are not\nparticularly well suited to the task for two reasons. First, GNNs do not\npredispose interactions to be sparse, as relationships among independent\nentities are likely to be. Second, GNNs do not factorize knowledge about\ninteractions in an entity-conditional manner. As an alternative, we take\ninspiration from cognitive science and resurrect a classic approach, production\nsystems, which consist of a set of rule templates that are applied by binding\nplaceholder variables in the rules to specific entities. Rules are scored on\ntheir match to entities, and the best fitting rules are applied to update\nentity properties. In a series of experiments, we demonstrate that this\narchitecture achieves a flexible, dynamic flow of control and serves to\nfactorize entity-specific and rule-based information. This disentangling of\nknowledge achieves robust future-state prediction in rich visual environments,\noutperforming state-of-the-art methods using GNNs, and allows for the\nextrapolation from simple (few object) environments to more complex\nenvironments.\n","authors":["Anirudh Goyal","Aniket Didolkar","Nan Rosemary Ke","Charles Blundell","Philippe Beaudoin","Nicolas Heess","Michael Mozer","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2103.01937v3.pdf","comment":"NeurIPS'21"},{"id":"http://arxiv.org/abs/2112.02048v3","updated":"2022-03-23T16:04:23Z","published":"2021-12-03T17:56:10Z","title":"Graph Neural Networks for Charged Particle Tracking on FPGAs","summary":"  The determination of charged particle trajectories in collisions at the CERN\nLarge Hadron Collider (LHC) is an important but challenging problem, especially\nin the high interaction density conditions expected during the future\nhigh-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a\ntype of geometric deep learning algorithm that has successfully been applied to\nthis task by embedding tracker data as a graph -- nodes represent hits, while\nedges represent possible track segments -- and classifying the edges as true or\nfake track segments. However, their study in hardware- or software-based\ntrigger applications has been limited due to their large computational cost. In\nthis paper, we introduce an automated translation workflow, integrated into a\nbroader tool called $\\texttt{hls4ml}$, for converting GNNs into firmware for\nfield-programmable gate arrays (FPGAs). We use this translation tool to\nimplement GNNs for charged particle tracking, trained using the TrackML\nchallenge dataset, on FPGAs with designs targeting different graph sizes, task\ncomplexites, and latency/throughput requirements. This work could enable the\ninclusion of charged particle tracking GNNs at the trigger level for HL-LHC\nexperiments.\n","authors":["Abdelrahman Elabd","Vesal Razavimaleki","Shi-Yu Huang","Javier Duarte","Markus Atkinson","Gage DeZoort","Peter Elmer","Scott Hauck","Jin-Xuan Hu","Shih-Chieh Hsu","Bo-Cheng Lai","Mark Neubauer","Isobel Ojalvo","Savannah Thais","Matthew Trahms"],"pdf_url":"https://arxiv.org/pdf/2112.02048v3.pdf","comment":"28 pages, 17 figures, 1 table, published version"},{"id":"http://arxiv.org/abs/2203.12505v1","updated":"2022-03-23T16:03:55Z","published":"2022-03-23T16:03:55Z","title":"A Spatial-Temporal Attention Multi-Graph Convolution Network for\n  Ride-Hailing Demand Prediction Based on Periodicity with Offset","summary":"  Ride-hailing service is becoming a leading part in urban transportation. To\nimprove the efficiency of ride-hailing service, accurate prediction of\ntransportation demand is a fundamental challenge. In this paper, we tackle this\nproblem from both aspects of network structure and data-set formulation. For\nnetwork design, we propose a spatial-temporal attention multi-graph convolution\nnetwork (STA-MGCN). A spatial-temporal layer in STA-MGCN is developed to\ncapture the temporal correlations by temporal attention mechanism and temporal\ngate convolution, and the spatial correlations by multigraph convolution. A\nfeature cluster layer is introduced to learn latent regional functions and to\nreduce the computation burden. For the data-set formulation, we develop a novel\napproach which considers the transportation feature of periodicity with offset.\nInstead of only using history data during the same time period, the history\norder demand in forward and backward neighboring time periods from yesterday\nand last week are also included. Extensive experiments on the three real-world\ndatasets of New-York, Chicago and Chengdu show that the proposed algorithm\nachieves the state-of-the-art performance for ride-hailing demand prediction.\n","authors":["Dong Xing","Chenguang Zhao","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.04797v3","updated":"2022-03-23T15:59:36Z","published":"2021-04-10T15:52:39Z","title":"Coupling streaming AI and HPC ensembles to achieve 100-1000x faster\n  biomolecular simulations","summary":"  The use of ML methods to dynamically steer ensemble-based simulations\npromises significant improvements in the performance of scientific\napplications. We present DeepDriveMD, a tool for a range of prototypical\nML-driven HPC simulation scenarios, and use it to quantify improvements in the\nscientific performance of ML-driven ensemble-based applications. We discuss its\ndesign and characterize its performance. Motivated by the potential for further\nscientific improvements and applicability to more sophisticated physical\nsystems, we extend the design of DeepDriveMD to support stream-based\ncommunication between simulations and learning methods. It demonstrates a 100x\nspeedup to fold proteins, and performs 1.6x more simulations per unit time,\nimproving resource utilization compared to the sequential framework.\nExperiments are performed on leadership-class platforms, at scales of up to\nO(1000) nodes, and for production workloads. We establish DeepDriveMD as a\nhigh-performance framework for ML-driven HPC simulation scenarios, that\nsupports diverse simulation and ML back-ends, and which enables new scientific\ninsights by improving length- and time-scale accessed.\n","authors":["Alexander Brace","Igor Yakushin","Heng Ma","Anda Trifan","Todd Munson","Ian Foster","Arvind Ramanathan","Hyungro Lee","Matteo Turilli","Shantenu Jha"],"pdf_url":"https://arxiv.org/pdf/2104.04797v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09466v2","updated":"2022-03-23T15:58:39Z","published":"2021-12-17T12:07:04Z","title":"An overview of active learning methods for insurance with fairness\n  appreciation","summary":"  This paper addresses and solves some challenges in the adoption of machine\nlearning in insurance with the democratization of model deployment. The first\nchallenge is reducing the labelling effort (hence focusing on the data quality)\nwith the help of active learning, a feedback loop between the model inference\nand an oracle: as in insurance the unlabeled data is usually abundant, active\nlearning can become a significant asset in reducing the labelling cost. For\nthat purpose, this paper sketches out various classical active learning\nmethodologies before studying their empirical impact on both synthetic and real\ndatasets. Another key challenge in insurance is the fairness issue in model\ninferences. We will introduce and integrate a post-processing fairness for\nmulti-class tasks in this active learning framework to solve these two issues.\nFinally numerical experiments on unfair datasets highlight that the proposed\nsetup presents a good compromise between model precision and fairness.\n","authors":["Romuald Elie","Caroline Hillairet","François Hu","Marc Juillard"],"pdf_url":"https://arxiv.org/pdf/2112.09466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12497v1","updated":"2022-03-23T15:50:12Z","published":"2022-03-23T15:50:12Z","title":"Quantum-enhanced Markov chain Monte Carlo","summary":"  Sampling from complicated probability distributions is a hard computational\nproblem arising in many fields, including statistical physics, optimization,\nand machine learning. Quantum computers have recently been used to sample from\ncomplicated distributions that are hard to sample from classically, but which\nseldom arise in applications. Here we introduce a quantum algorithm to sample\nfrom distributions that pose a bottleneck in several applications, which we\nimplement on a superconducting quantum processor. The algorithm performs Markov\nchain Monte Carlo (MCMC), a popular iterative sampling technique, to sample\nfrom the Boltzmann distribution of classical Ising models. In each step, the\nquantum processor explores the model in superposition to propose a random move,\nwhich is then accepted or rejected by a classical computer and returned to the\nquantum processor, ensuring convergence to the desired Boltzmann distribution.\nWe find that this quantum algorithm converges in fewer iterations than common\nclassical MCMC alternatives on relevant problem instances, both in simulations\nand experiments. It therefore opens a new path for quantum computers to solve\nuseful--not merely difficult--problems in the near term.\n","authors":["David Layden","Guglielmo Mazzola","Ryan V. Mishmash","Mario Motta","Pawel Wocjan","Jin-Sung Kim","Sarah Sheldon"],"pdf_url":"https://arxiv.org/pdf/2203.12497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.04735v2","updated":"2022-03-23T15:26:15Z","published":"2022-01-12T23:16:37Z","title":"Planning in Observable POMDPs in Quasipolynomial Time","summary":"  Partially Observable Markov Decision Processes (POMDPs) are a natural and\ngeneral model in reinforcement learning that take into account the agent's\nuncertainty about its current state. In the literature on POMDPs, it is\ncustomary to assume access to a planning oracle that computes an optimal policy\nwhen the parameters are known, even though the problem is known to be\ncomputationally hard. Almost all existing planning algorithms either run in\nexponential time, lack provable performance guarantees, or require placing\nstrong assumptions on the transition dynamics under every possible policy. In\nthis work, we revisit the planning problem and ask: are there natural and\nwell-motivated assumptions that make planning easy?\n  Our main result is a quasipolynomial-time algorithm for planning in\n(one-step) observable POMDPs. Specifically, we assume that well-separated\ndistributions on states lead to well-separated distributions on observations,\nand thus the observations are at least somewhat informative in each step.\nCrucially, this assumption places no restrictions on the transition dynamics of\nthe POMDP; nevertheless, it implies that near-optimal policies admit\nquasi-succinct descriptions, which is not true in general (under standard\nhardness assumptions). Our analysis is based on new quantitative bounds for\nfilter stability -- i.e. the rate at which an optimal filter for the latent\nstate forgets its initialization. Furthermore, we prove matching hardness for\nplanning in observable POMDPs under the Exponential Time Hypothesis.\n","authors":["Noah Golowich","Ankur Moitra","Dhruv Rohatgi"],"pdf_url":"https://arxiv.org/pdf/2201.04735v2.pdf","comment":"52 pages"},{"id":"http://arxiv.org/abs/2203.12482v1","updated":"2022-03-23T15:23:24Z","published":"2022-03-23T15:23:24Z","title":"A Deep Learning Framework to Reconstruct Face under Mask","summary":"  While deep learning-based image reconstruction methods have shown significant\nsuccess in removing objects from pictures, they have yet to achieve acceptable\nresults for attributing consistency to gender, ethnicity, expression, and other\ncharacteristics like the topological structure of the face. The purpose of this\nwork is to extract the mask region from a masked image and rebuild the area\nthat has been detected. This problem is complex because (i) it is difficult to\ndetermine the gender of an image hidden behind a mask, which causes the network\nto become confused and reconstruct the male face as a female or vice versa;\n(ii) we may receive images from multiple angles, making it extremely difficult\nto maintain the actual shape, topological structure of the face and a natural\nimage; and (iii) there are problems with various mask forms because, in some\ncases, the area of the mask cannot be anticipated precisely; certain parts of\nthe mask remain on the face after completion. To solve this complex task, we\nsplit the problem into three phases: landmark detection, object detection for\nthe targeted mask area, and inpainting the addressed mask region. To begin, to\nsolve the first problem, we have used gender classification, which detects the\nactual gender behind a mask, then we detect the landmark of the masked facial\nimage. Second, we identified the non-face item, i.e., the mask, and used the\nMask R-CNN network to create the binary mask of the observed mask area.\nThirdly, we developed an inpainting network that uses anticipated landmarks to\ncreate realistic images. To segment the mask, this article uses a mask R-CNN\nand offers a binary segmentation map for identifying the mask area.\nAdditionally, we generated the image utilizing landmarks as structural guidance\nthrough a GAN-based network. The studies presented in this paper use the FFHQ\nand CelebA datasets.\n","authors":["Gourango Modak","Shuvra Smaran Das","Md. Ajharul Islam Miraj","Md. Kishor Morol"],"pdf_url":"https://arxiv.org/pdf/2203.12482v1.pdf","comment":"6 pages, 9 figures, 2022 7th Conference on Data Science and Machine\n  Learning Applications (CDMA)"},{"id":"http://arxiv.org/abs/2110.10342v2","updated":"2022-03-23T15:13:50Z","published":"2021-10-20T02:25:25Z","title":"Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and\n  Beyond","summary":"  In distributed learning, local SGD (also known as federated averaging) and\nits simple baseline minibatch SGD are widely studied optimization methods. Most\nexisting analyses of these methods assume independent and unbiased gradient\nestimates obtained via with-replacement sampling. In contrast, we study\nshuffling-based variants: minibatch and local Random Reshuffling, which draw\nstochastic gradients without replacement and are thus closer to practice. For\nsmooth functions satisfying the Polyak-{\\L}ojasiewicz condition, we obtain\nconvergence bounds (in the large epoch regime) which show that these\nshuffling-based variants converge faster than their with-replacement\ncounterparts. Moreover, we prove matching lower bounds showing that our\nconvergence analysis is tight. Finally, we propose an algorithmic modification\ncalled synchronized shuffling that leads to convergence rates faster than our\nlower bounds in near-homogeneous settings.\n","authors":["Chulhee Yun","Shashank Rajput","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2110.10342v2.pdf","comment":"ICLR 2022 camera-ready (selected for an oral presentation); 76 pages,\n  3 figures"},{"id":"http://arxiv.org/abs/2203.12469v1","updated":"2022-03-23T15:05:23Z","published":"2022-03-23T15:05:23Z","title":"3D Adapted Random Forest Vision (3DARFV) for Untangling\n  Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency\n  at the Utmost Accuracy","summary":"  Planetary exploration depends heavily on 3D image data to characterize the\nstatic and dynamic properties of the rock and environment. Analyzing 3D images\nrequires many computations, causing efficiency to suffer lengthy processing\ntime alongside large energy consumption. High-Performance Computing (HPC)\nprovides apparent efficiency at the expense of energy consumption. However, for\nremote explorations, the conveyed surveillance and the robotized sensing need\nfaster data analysis with ultimate accuracy to make real-time decisions. In\nsuch environments, access to HPC and energy is limited. Therefore, we realize\nthat reducing the number of computations to optimal and maintaining the desired\naccuracy leads to higher efficiency. This paper demonstrates the semantic\nsegmentation capability of a probabilistic decision tree algorithm, 3D Adapted\nRandom Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at\nthe utmost accuracy.\n","authors":["Omar Alfarisi","Zeyar Aung","Qingfeng Huang","Ashraf Al-Khateeb","Hamed Alhashmi","Mohamed Abdelsalam","Salem Alzaabi","Haifa Alyazeedi","Anthony Tzes"],"pdf_url":"https://arxiv.org/pdf/2203.12469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12459v1","updated":"2022-03-23T14:54:29Z","published":"2022-03-23T14:54:29Z","title":"Activation-Based Sampling for Pixel- to Image-Level Aggregation in\n  Weakly-Supervised Segmentation","summary":"  Classification networks can be used to localize and segment objects in images\nby means of class activation maps (CAMs). However, without pixel-level\nannotations, they are known to (1) mainly focus on discriminative regions, and\n(2) to produce diffuse CAMs without well-defined prediction contours. In this\nwork, we approach both problems with two contributions for improving CAM\nlearning. First, we incorporate importance sampling based on the class-wise\nprobability mass function induced by the CAMs to produce stochastic image-level\nclass predictions. This results in CAMs which activate over a larger extent of\nthe objects. Second, we formulate a feature similarity loss term which aims to\nmatch the prediction contours with edges in the image. As a third contribution,\nwe conduct experiments on the PASCAL VOC and MS-COCO benchmark datasets to\ndemonstrate that these modifications significantly increase the performance in\nterms of contour accuracy, while being comparable to current state-of-the-art\nmethods in terms of region similarity.\n","authors":["Arvi Jonnarth","Michael Felsberg","Yushan Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.12459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12451v1","updated":"2022-03-23T14:43:14Z","published":"2022-03-23T14:43:14Z","title":"Deep Multi-View Learning for Tire Recommendation","summary":"  We are constantly using recommender systems, often without even noticing.\nThey build a profile of our person in order to recommend the content we will\nmost likely be interested in. The data representing the users, their\ninteractions with the system or the products may come from different sources\nand be of a various nature. Our goal is to use a multi-view learning approach\nto improve our recommender system and improve its capacity to manage multi-view\ndata. We propose a comparative study between several state-of-the-art\nmulti-view models applied to our industrial data. Our study demonstrates the\nrelevance of using multi-view learning within recommender systems.\n","authors":["Thomas Ranvier","Kilian Bourhis","Khalid Benabdeslem","Bruno Canitia"],"pdf_url":"https://arxiv.org/pdf/2203.12451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.02786v2","updated":"2022-03-23T14:08:07Z","published":"2021-05-05T12:06:34Z","title":"LGGNet: Learning from Local-Global-Graph Representations for\n  Brain-Computer Interface","summary":"  Neuropsychological studies suggest that co-operative activities among\ndifferent brain functional areas drive high-level cognitive processes. To learn\nthe brain activities within and among different functional areas of the brain,\nwe propose LGGNet, a novel neurologically inspired graph neural network, to\nlearn local-global-graph representations of electroencephalography (EEG) for\nBrain-Computer Interface (BCI). The input layer of LGGNet comprises a series of\ntemporal convolutions with multi-scale 1D convolutional kernels and\nkernel-level attentive fusion. It captures temporal dynamics of EEG which then\nserves as input to the proposed local and global graph-filtering layers. Using\na defined neurophysiologically meaningful set of local and global graphs,\nLGGNet models the complex relations within and among functional areas of the\nbrain. Under the robust nested cross-validation settings, the proposed method\nis evaluated on three publicly available datasets for four types of cognitive\nclassification tasks, namely, the attention, fatigue, emotion, and preference\nclassification tasks. LGGNet is compared with state-of-the-art methods, such as\nDeepConvNet, EEGNet, R2G-STNN, TSception, and HRNN. The results show that\nLGGNet outperforms these methods, and the improvements are statistically\nsignificant (p<0.05) in most cases. The results show that bringing neuroscience\nprior knowledge into neural network design yields an improvement of\nclassification performance. The source code can be found at\nhttps://github.com/yi-ding-cs/LGG\n","authors":["Yi Ding","Neethu Robinson","Qiuhao Zeng","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2105.02786v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2111.08644v2","updated":"2022-03-23T14:06:40Z","published":"2021-11-16T17:28:46Z","title":"UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection","summary":"  Detecting abnormal events in video is commonly framed as a one-class\nclassification task, where training videos contain only normal events, while\ntest videos encompass both normal and abnormal events. In this scenario,\nanomaly detection is an open-set problem. However, some studies assimilate\nanomaly detection to action recognition. This is a closed-set scenario that\nfails to test the capability of systems at detecting new anomaly types. To this\nend, we propose UBnormal, a new supervised open-set benchmark composed of\nmultiple virtual scenes for video anomaly detection. Unlike existing data sets,\nwe introduce abnormal events annotated at the pixel level at training time, for\nthe first time enabling the use of fully-supervised learning methods for\nabnormal event detection. To preserve the typical open-set formulation, we make\nsure to include disjoint sets of anomaly types in our training and test\ncollections of videos. To our knowledge, UBnormal is the first video anomaly\ndetection benchmark to allow a fair head-to-head comparison between one-class\nopen-set models and supervised closed-set models, as shown in our experiments.\nMoreover, we provide empirical evidence showing that UBnormal can enhance the\nperformance of a state-of-the-art anomaly detection framework on two prominent\ndata sets, Avenue and ShanghaiTech. Our benchmark is freely available at\nhttps://github.com/lilygeorgescu/UBnormal.\n","authors":["Andra Acsintoae","Andrei Florescu","Mariana-Iuliana Georgescu","Tudor Mare","Paul Sumedrea","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2111.08644v2.pdf","comment":"Accepted at CVPR 2022. Paper + supplementary (15 pages, 9 figures)"},{"id":"http://arxiv.org/abs/2109.13236v2","updated":"2022-03-23T14:05:45Z","published":"2021-09-27T12:51:24Z","title":"FedIPR: Ownership Verification for Federated Deep Neural Network Models","summary":"  Federated learning models are collaboratively developed upon valuable\ntraining data owned by multiple parties. During the development and deployment\nof federated models, they are exposed to risks including illegal copying,\nre-distribution, misuse and/or free-riding. To address these risks, the\nownership verification of federated learning models is a prerequisite that\nprotects federated learning model intellectual property rights (IPR) i.e.,\nFedIPR. We propose a novel federated deep neural network (FedDNN) ownership\nverification scheme that allows private watermarks to be embedded and verified\nto claim legitimate IPR of FedDNN models. In the proposed scheme, each client\nindependently verifies the existence of the model watermarks and claims\nrespective ownership of the federated model without disclosing neither private\ntraining data nor private watermark information. The effectiveness of embedded\nwatermarks is theoretically justified by the rigorous analysis of conditions\nunder which watermarks can be privately embedded and detected by multiple\nclients. Moreover, extensive experimental results on computer vision and\nnatural language processing tasks demonstrate that varying bit-length\nwatermarks can be embedded and reliably detected without compromising original\nmodel performances. Our watermarking scheme is also resilient to various\nfederated training settings and robust against removal attacks.\n","authors":["Bowen Li","Lixin Fan","Hanlin Gu","Jie Li","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2109.13236v2.pdf","comment":"17 pages, under review"},{"id":"http://arxiv.org/abs/2111.00213v2","updated":"2022-03-23T13:50:15Z","published":"2021-10-30T09:26:45Z","title":"Adjacency constraint for efficient hierarchical reinforcement learning","summary":"  Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising\napproach for scaling up reinforcement learning (RL) techniques. However, it\noften suffers from training inefficiency as the action space of the high-level,\ni.e., the goal space, is large. Searching in a large goal space poses\ndifficulty for both high-level subgoal generation and low-level policy\nlearning. In this paper, we show that this problem can be effectively\nalleviated by restricting the high-level action space from the whole goal space\nto a $k$-step adjacent region of the current state using an adjacency\nconstraint. We theoretically prove that in a deterministic Markov Decision\nProcess (MDP), the proposed adjacency constraint preserves the optimal\nhierarchical policy, while in a stochastic MDP the adjacency constraint induces\na bounded state-value suboptimality determined by the MDP's transition\nstructure. We further show that this constraint can be practically implemented\nby training an adjacency network that can discriminate between adjacent and\nnon-adjacent subgoals. Experimental results on discrete and continuous control\ntasks including challenging simulated robot locomotion and manipulation tasks\nshow that incorporating the adjacency constraint significantly boosts the\nperformance of state-of-the-art goal-conditioned HRL approaches.\n","authors":["Tianren Zhang","Shangqi Guo","Tian Tan","Xiaolin Hu","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2111.00213v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2006.11485"},{"id":"http://arxiv.org/abs/2012.14906v4","updated":"2022-03-23T13:50:12Z","published":"2020-12-29T18:59:14Z","title":"Synthesizing Decentralized Controllers with Graph Neural Networks and\n  Imitation Learning","summary":"  Dynamical systems consisting of a set of autonomous agents face the challenge\nof having to accomplish a global task, relying only on local information. While\ncentralized controllers are readily available, they face limitations in terms\nof scalability and implementation, as they do not respect the distributed\ninformation structure imposed by the network system of agents. Given the\ndifficulties in finding optimal decentralized controllers, we propose a novel\nframework using graph neural networks (GNNs) to \\emph{learn} these controllers.\nGNNs are well-suited for the task since they are naturally distributed\narchitectures and exhibit good scalability and transferability properties. We\nshow that GNNs learn appropriate decentralized controllers by means of\nimitation learning, leverage their permutation invariance properties to\nsuccessfully scale to larger teams and transfer to unseen scenarios at\ndeployment time. The problems of flocking and multi-agent path planning are\nexplored to illustrate the potential of GNNs in learning decentralized\ncontrollers.\n","authors":["Fernando Gama","Qingbiao Li","Ekaterina Tolstaya","Amanda Prorok","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2012.14906v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.07006v4","updated":"2022-03-23T13:49:53Z","published":"2021-04-14T17:38:40Z","title":"Fast quantum state reconstruction via accelerated non-convex programming","summary":"  We propose a new quantum state reconstruction method that combines ideas from\ncompressed sensing, non-convex optimization, and acceleration methods. The\nalgorithm, called Momentum-Inspired Factored Gradient Descent (\\texttt{MiFGD}),\nextends the applicability of quantum tomography for larger systems. Despite\nbeing a non-convex method, \\texttt{MiFGD} converges \\emph{provably} close to\nthe true density matrix at an accelerated linear rate, in the absence of\nexperimental and statistical noise, and under common assumptions. With this\nmanuscript, we present the method, prove its convergence property and provide\nFrobenius norm bound guarantees with respect to the true density matrix. From a\npractical point of view, we benchmark the algorithm performance with respect to\nother existing methods, in both synthetic and real experiments performed on an\nIBM's quantum processing unit. We find that the proposed algorithm performs\norders of magnitude faster than state of the art approaches, with the same or\nbetter accuracy. In both synthetic and real experiments, we observed accurate\nand robust reconstruction, despite experimental and statistical noise in the\ntomographic data. Finally, we provide a ready-to-use code for state tomography\nof multi-qubit systems.\n","authors":["Junhyung Lyle Kim","George Kollias","Amir Kalev","Ken X. Wei","Anastasios Kyrillidis"],"pdf_url":"https://arxiv.org/pdf/2104.07006v4.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2203.07967v3","updated":"2022-03-23T13:46:13Z","published":"2022-03-15T14:52:52Z","title":"Intrinsic Neural Fields: Learning Functions on Manifolds","summary":"  Neural fields have gained significant attention in the computer vision\ncommunity due to their excellent performance in novel view synthesis, geometry\nreconstruction, and generative modeling. Some of their advantages are a sound\ntheoretic foundation and an easy implementation in current deep learning\nframeworks. While neural fields have been applied to signals on manifolds,\ne.g., for texture reconstruction, their representation has been limited to\nextrinsically embedding the shape into Euclidean space. The extrinsic embedding\nignores known intrinsic manifold properties and is inflexible wrt. transfer of\nthe learned function. To overcome these limitations, this work introduces\nintrinsic neural fields, a novel and versatile representation for neural fields\non manifolds. Intrinsic neural fields combine the advantages of neural fields\nwith the spectral properties of the Laplace-Beltrami operator. We show\ntheoretically that intrinsic neural fields inherit many desirable properties of\nthe extrinsic neural field framework but exhibit additional intrinsic\nqualities, like isometry invariance. In experiments, we show intrinsic neural\nfields can reconstruct high-fidelity textures from images with state-of-the-art\nquality and are robust to the discretization of the underlying manifold. We\ndemonstrate the versatility of intrinsic neural fields by tackling various\napplications: texture transfer between deformed shapes & different shapes,\ntexture reconstruction from real-world images with view dependence, and\ndiscretization-agnostic learning on meshes and point clouds.\n","authors":["Lukas Koestler","Daniel Grittner","Michael Moeller","Daniel Cremers","Zorah Lähner"],"pdf_url":"https://arxiv.org/pdf/2203.07967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12412v1","updated":"2022-03-23T13:44:15Z","published":"2022-03-23T13:44:15Z","title":"U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture\n  Search","summary":"  Optimizing resource utilization in target platforms is key to achieving high\nperformance during DNN inference. While optimizations have been proposed for\ninference latency, memory footprint, and energy consumption, prior\nhardware-aware neural architecture search (NAS) methods have omitted resource\nutilization, preventing DNNs to take full advantage of the target inference\nplatforms. Modeling resource utilization efficiently and accurately is\nchallenging, especially for widely-used array-based inference accelerators such\nas Google TPU. In this work, we propose a novel hardware-aware NAS framework\nthat does not only optimize for task accuracy and inference latency, but also\nfor resource utilization. We also propose and validate a new computational\nmodel for resource utilization in inference accelerators. By using the proposed\nNAS framework and the proposed resource utilization model, we achieve 2.8 - 4x\nspeedup for DNN inference compared to prior hardware-aware NAS methods while\nattaining similar or improved accuracy in image classification on CIFAR-10 and\nImagenet-100 datasets.\n","authors":["Ahmet Caner Yüzügüler","Nikolaos Dimitriadis","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2203.12412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12407v1","updated":"2022-03-23T13:33:02Z","published":"2022-03-23T13:33:02Z","title":"Verification of safety critical control policies using kernel methods","summary":"  Hamilton-Jacobi reachability methods for safety-critical control have been\nwell studied, but the safety guarantees derived rely on the accuracy of the\nnumerical computation. Thus, it is crucial to understand and account for any\ninaccuracies that occur due to uncertainty in the underlying dynamics and\nenvironment as well as the induced numerical errors. To this end, we propose a\nframework for modeling the error of the value function inherent in\nHamilton-Jacobi reachability using a Gaussian process. The derived safety\ncontroller can be used in conjuncture with arbitrary controllers to provide a\nsafe hybrid control law. The marginal likelihood of the Gaussian process then\nprovides a confidence metric used to determine switches between a least\nrestrictive controller and a safety controller. We test both the prediction as\nwell as the correction capabilities of the presented method in a classical\npursuit-evasion example.\n","authors":["Nikolaus Vertovec","Sina Ober-Blöbaum","Kostas Margellos"],"pdf_url":"https://arxiv.org/pdf/2203.12407v1.pdf","comment":"Paper published in 2022 European Control Conference (ECC), 6 pages, 4\n  figures"},{"id":"http://arxiv.org/abs/2203.12377v1","updated":"2022-03-23T12:52:49Z","published":"2022-03-23T12:52:49Z","title":"Dynamically-Scaled Deep Canonical Correlation Analysis","summary":"  Canonical Correlation Analysis (CCA) is a method for feature extraction of\ntwo views by finding maximally correlated linear projections of them. Several\nvariants of CCA have been introduced in the literature, in particular, variants\nbased on deep neural networks for learning highly correlated nonlinear\ntransformations of two views. As these models are parameterized conventionally,\ntheir learnable parameters remain independent of the inputs after the training\nprocess, which may limit their capacity for learning highly correlated\nrepresentations. We introduce a novel dynamic scaling method for training an\ninput-dependent canonical correlation model. In our deep-CCA models, the\nparameters of the last layer are scaled by a second neural network that is\nconditioned on the model's input, resulting in a parameterization that is\ndependent on the input samples. We evaluate our model on multiple datasets and\ndemonstrate that the learned representations are more correlated in comparison\nto the conventionally-parameterized CCA-based models and also obtain preferable\nretrieval results. Our code is available at\nhttps://github.com/tomerfr/DynamicallyScaledDeepCCA.\n","authors":["Tomer Friedlander","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2203.12377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12369v1","updated":"2022-03-23T12:42:28Z","published":"2022-03-23T12:42:28Z","title":"MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data","summary":"  Training of speech enhancement systems often does not incorporate knowledge\nof human perception and thus can lead to unnatural sounding results.\nIncorporating psychoacoustically motivated speech perception metrics as part of\nmodel training via a predictor network has recently gained interest. However,\nthe performance of such predictors is limited by the distribution of metric\nscores that appear in the training data.In this work, we propose MetricGAN+/-\n(an extension of MetricGAN+, one such metric-motivated system) which introduces\nan additional network - a \"de-generator\" which attempts to improve the\nrobustness of the prediction network (and by extension of the generator) by\nensuring observation of a wider range of metric scores in training.\nExperimental results on the VoiceBank-DEMAND dataset show relative improvement\nin PESQ score of $3.8\\%$ ($3.05$ vs $3.22$ PESQ score), as well as better\ngeneralisation to unseen noise and speech.\n","authors":["George Close","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2203.12369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12363v1","updated":"2022-03-23T12:35:59Z","published":"2022-03-23T12:35:59Z","title":"Ethereum Fraud Detection with Heterogeneous Graph Neural Networks","summary":"  While transactions with cryptocurrencies such as Ethereum are becoming more\nprevalent, fraud and other criminal transactions are not uncommon. Graph\nanalysis algorithms and machine learning techniques detect suspicious\ntransactions that lead to phishing in large transaction networks. Many graph\nneural network (GNN) models have been proposed to apply deep learning\ntechniques to graph structures. Although there is research on phishing\ndetection using GNN models in the Ethereum transaction network, models that\naddress the scale of the number of vertices and edges and the imbalance of\nlabels have not yet been studied. In this paper, we compared the model\nperformance of GNN models on the actual Ethereum transaction network dataset\nand phishing reported label data to exhaustively compare and verify which GNN\nmodels and hyperparameters produce the best accuracy. Specifically, we\nevaluated the model performance of representative homogeneous GNN models which\nconsider single-type nodes and edges and heterogeneous GNN models which support\ndifferent types of nodes and edges. We showed that heterogeneous models had\nbetter model performance than homogeneous models. In particular, the RGCN model\nachieved the best performance in the overall metrics.\n","authors":["Hiroki Kanezashi","Toyotaro Suzumura","Xin Liu","Takahiro Hirofuchi"],"pdf_url":"https://arxiv.org/pdf/2203.12363v1.pdf","comment":"9 pages, 14 figures"},{"id":"http://arxiv.org/abs/2108.05818v3","updated":"2022-03-23T12:32:49Z","published":"2021-08-12T15:58:12Z","title":"PatrickStar: Parallel Training of Pre-trained Models via Chunk-based\n  Memory Management","summary":"  The pre-trained model (PTM) is revolutionizing Artificial intelligence (AI)\ntechnology. It can learn general language features on massive data and then be\nfine-tuned on task-specific data. Unfortunately, the computing hardware\nrequirement of PTM training is prohibitively expensive, which makes it a game\nfor a small proportion of people in the AI community. Therefore, we proposed a\nsystem called PatrickStar to lower the hardware requirements of PTMs and make\nthem accessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory\nspace to store the model data. Different from existing works, we first manage\nthe model data in a fine-grained manner by organizing them in memory chunks and\ndynamically distributing them in the heterogeneous memory space. Guided by the\nruntime memory statistics collected in a warm-up iteration, chunks are\norchestrated efficiently in heterogeneous memory and generate lower CPU-GPU\ndata transmission volume. Symbiosis with the Zero Redundancy Optimizer,\nPatrickStar scales to multiple GPUs using data parallelism, with lower\ncommunication bandwidth requirements and more efficient bandwidth utilization.\nThe system can train tasks on bigger models and larger batch sizes, which\nexisting works cannot complete. Experimental results show that PatrickStar\ntrains a 12 billion parameters GPT model, 1.5x as large as the model scale\nlimit of the SOTA works, on an 8xV100 and 240GB CPU memory node, and also\nachieves significantly higher computing efficiency than SOTA. Even on a $700\npersonal computer, it can train a 0.7 billion parameter GPT model. Our code is\npublicly available.\n","authors":["Jiarui Fang","Yang Yu","Zilin Zhu","Shenggui Li","Yang You","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2108.05818v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2203.09566v2","updated":"2022-03-23T12:00:45Z","published":"2022-03-17T19:09:38Z","title":"Leveraging Adversarial Examples to Quantify Membership Information\n  Leakage","summary":"  The use of personal data for training machine learning systems comes with a\nprivacy threat and measuring the level of privacy of a model is one of the\nmajor challenges in machine learning today. Identifying training data based on\na trained model is a standard way of measuring the privacy risks induced by the\nmodel. We develop a novel approach to address the problem of membership\ninference in pattern recognition models, relying on information provided by\nadversarial examples. The strategy we propose consists of measuring the\nmagnitude of a perturbation necessary to build an adversarial example. Indeed,\nwe argue that this quantity reflects the likelihood of belonging to the\ntraining data. Extensive numerical experiments on multivariate data and an\narray of state-of-the-art target models show that our method performs\ncomparable or even outperforms state-of-the-art strategies, but without\nrequiring any additional training samples.\n","authors":["Ganesh Del Grosso","Hamid Jalalzai","Georg Pichler","Catuscia Palamidessi","Pablo Piantanida"],"pdf_url":"https://arxiv.org/pdf/2203.09566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.09338v2","updated":"2022-03-23T11:59:03Z","published":"2021-07-20T08:48:42Z","title":"Stein Variational Gradient Descent with Multiple Kernel","summary":"  Stein variational gradient descent (SVGD) and its variants have shown\npromising successes in approximate inference for complex distributions. In\npractice, we notice that the kernel used in SVGD-based methods has a decisive\neffect on the empirical performance. Radial basis function (RBF) kernel with\nmedian heuristics is a common choice in previous approaches, but unfortunately\nthis has proven to be sub-optimal. Inspired by the paradigm of Multiple Kernel\nLearning (MKL), our solution to this flaw is using a combination of multiple\nkernels to approximate the optimal kernel, rather than a single one which may\nlimit the performance and flexibility. Specifically, we first extend Kernelized\nStein Discrepancy (KSD) to its multiple kernels view called Multiple Kernelized\nStein Discrepancy (MKSD) and then leverage MKSD to construct a general\nalgorithm Multiple Kernel SVGD (MK-SVGD). Further, MKSVGD can automatically\nassign a weight to each kernel without any other parameters, which means that\nour method not only gets rid of optimal kernel dependence but also maintains\ncomputational efficiency. Experiments on various tasks and models demonstrate\nthat our proposed method consistently matches or outperforms the competing\nmethods.\n","authors":["Qingzhong Ai","Shiyu Liu","Lirong He","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2107.09338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12337v1","updated":"2022-03-23T11:30:34Z","published":"2022-03-23T11:30:34Z","title":"Binary Morphological Neural Network","summary":"  In the last ten years, Convolutional Neural Networks (CNNs) have formed the\nbasis of deep-learning architectures for most computer vision tasks. However,\nthey are not necessarily optimal. For example, mathematical morphology is known\nto be better suited to deal with binary images. In this work, we create a\nmorphological neural network that handles binary inputs and outputs. We propose\ntheir construction inspired by CNNs to formulate layers adapted to such images\nby replacing convolutions with erosions and dilations. We give explainable\ntheoretical results on whether or not the resulting learned networks are indeed\nmorphological operators. We present promising experimental results designed to\nlearn basic binary operators, and we have made our code publicly available\nonline.\n","authors":["Theodore Aouad","Hugues Talbot"],"pdf_url":"https://arxiv.org/pdf/2203.12337v1.pdf","comment":"Preprint of a submission to ICIP 2022. 7 pages. 4 figures"},{"id":"http://arxiv.org/abs/2203.12330v1","updated":"2022-03-23T11:15:36Z","published":"2022-03-23T11:15:36Z","title":"Towards explaining the generalization gap in neural networks using\n  topological data analysis","summary":"  Understanding how neural networks generalize on unseen data is crucial for\ndesigning more robust and reliable models. In this paper, we study the\ngeneralization gap of neural networks using methods from topological data\nanalysis. For this purpose, we compute homological persistence diagrams of\nweighted graphs constructed from neuron activation correlations after a\ntraining phase, aiming to capture patterns that are linked to the\ngeneralization capacity of the network. We compare the usefulness of different\nnumerical summaries from persistence diagrams and show that a combination of\nsome of them can accurately predict and partially explain the generalization\ngap without the need of a test set. Evaluation on two computer vision\nrecognition tasks (CIFAR10 and SVHN) shows competitive generalization gap\nprediction when compared against state-of-the-art methods.\n","authors":["Rubén Ballester","Xavier Arnal Clemente","Carles Casacuberta","Meysam Madadi","Ciprian A. Corneanu","Sergio Escalera"],"pdf_url":"https://arxiv.org/pdf/2203.12330v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2203.12329v1","updated":"2022-03-23T11:14:40Z","published":"2022-03-23T11:14:40Z","title":"The BP Dependency Function: a Generic Measure of Dependence between\n  Random Variables","summary":"  Measuring and quantifying dependencies between random variables (RV's) can\ngive critical insights into a data-set. Typical questions are: `Do underlying\nrelationships exist?', `Are some variables redundant?', and `Is some target\nvariable $Y$ highly or weakly dependent on variable $X$?' Interestingly,\ndespite the evident need for a general-purpose measure of dependency between\nRV's, common practice of data analysis is that most data analysts use the\nPearson correlation coefficient (PCC) to quantify dependence between RV's,\nwhile it is well-recognized that the PCC is essentially a measure for linear\ndependency only. Although many attempts have been made to define more generic\ndependency measures, there is yet no consensus on a standard, general-purpose\ndependency function. In fact, several ideal properties of a dependency function\nhave been proposed, but without much argumentation. Motivated by this, in this\npaper we will discuss and revise the list of desired properties and propose a\nnew dependency function that meets all these requirements. This general-purpose\ndependency function provides data analysts a powerful means to quantify the\nlevel of dependence between variables. To this end, we also provide Python code\nto determine the dependency function for use in practice.\n","authors":["Guus Berkelmans","Joris Pries","Sandjai Bhulai","Rob van der Mei"],"pdf_url":"https://arxiv.org/pdf/2203.12329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12314v1","updated":"2022-03-23T10:27:41Z","published":"2022-03-23T10:27:41Z","title":"Wider or Deeper Neural Network Architecture for Acoustic Scene\n  Classification with Mismatched Recording Devices","summary":"  In this paper, we present a robust and low complexity system for Acoustic\nScene Classification (ASC), the task of identifying the scene of an audio\nrecording. We first construct an ASC baseline system in which a novel\ninception-residual-based network architecture is proposed to deal with the\nmismatched recording device issue. To further improve the performance but still\nsatisfy the low complexity model, we apply two techniques: ensemble of multiple\nspectrograms and channel reduction on the ASC baseline system. By conducting\nextensive experiments on the benchmark DCASE 2020 Task 1A Development dataset,\nwe achieve the best model performing an accuracy of 69.9% and a low complexity\nof 2.4M trainable parameters, which is competitive to the state-of-the-art ASC\nsystems and potential for real-life applications on edge devices.\n","authors":["Lam Pham","Khoa Dinh","Dat Ngo","Hieu Tang","Alexander Schindler"],"pdf_url":"https://arxiv.org/pdf/2203.12314v1.pdf","comment":"This paper was submitted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2112.09581v2","updated":"2022-03-23T10:26:42Z","published":"2021-12-17T15:52:46Z","title":"Watermarking Images in Self-Supervised Latent Spaces","summary":"  We revisit watermarking techniques based on pre-trained deep networks, in the\nlight of self-supervised approaches. We present a way to embed both marks and\nbinary messages into their latent spaces, leveraging data augmentation at\nmarking time. Our method can operate at any resolution and creates watermarks\nrobust to a broad range of transformations (rotations, crops, JPEG, contrast,\netc). It significantly outperforms the previous zero-bit methods, and its\nperformance on multi-bit watermarking is on par with state-of-the-art\nencoder-decoder architectures trained end-to-end for watermarking. The code is\navailable at github.com/facebookresearch/ssl_watermarking\n","authors":["Pierre Fernandez","Alexandre Sablayrolles","Teddy Furon","Hervé Jégou","Matthijs Douze"],"pdf_url":"https://arxiv.org/pdf/2112.09581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.12731v2","updated":"2022-03-23T10:09:29Z","published":"2021-02-25T08:45:06Z","title":"Improving Approximate Optimal Transport Distances using Quantization","summary":"  Optimal transport (OT) is a popular tool in machine learning to compare\nprobability measures geometrically, but it comes with substantial computational\nburden. Linear programming algorithms for computing OT distances scale\ncubically in the size of the input, making OT impractical in the large-sample\nregime. We introduce a practical algorithm, which relies on a quantization\nstep, to estimate OT distances between measures given cheap sample access. We\nalso provide a variant of our algorithm to improve the performance of\napproximate solvers, focusing on those for entropy-regularized transport. We\ngive theoretical guarantees on the benefits of this quantization step and\ndisplay experiments showing that it behaves well in practice, providing a\npractical approximation algorithm that can be used as a drop-in replacement for\nexisting OT estimators.\n","authors":["Gaspard Beugnot","Aude Genevay","Kristjan Greenewald","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2102.12731v2.pdf","comment":"Published in the proceedings of the Conference on Uncertainty in\n  Artificial Intelligence 2021 (UAI)"},{"id":"http://arxiv.org/abs/2203.12299v1","updated":"2022-03-23T09:46:44Z","published":"2022-03-23T09:46:44Z","title":"NavDreams: Towards Camera-Only RL Navigation Among Humans","summary":"  Autonomously navigating a robot in everyday crowded spaces requires solving\ncomplex perception and planning challenges. When using only monocular image\nsensor data as input, classical two-dimensional planning approaches cannot be\nused. While images present a significant challenge when it comes to perception\nand planning, they also allow capturing potentially important details, such as\ncomplex geometry, body movement, and other visual cues. In order to\nsuccessfully solve the navigation task from only images, algorithms must be\nable to model the scene and its dynamics using only this channel of\ninformation. We investigate whether the world model concept, which has shown\nstate-of-the-art results for modeling and learning policies in Atari games as\nwell as promising results in 2D LiDAR-based crowd navigation, can also be\napplied to the camera-based navigation problem. To this end, we create\nsimulated environments where a robot must navigate past static and moving\nhumans without colliding in order to reach its goal. We find that\nstate-of-the-art methods are able to achieve success in solving the navigation\nproblem, and can generate dream-like predictions of future image-sequences\nwhich show consistent geometry and moving persons. We are also able to show\nthat policy performance in our high-fidelity sim2real simulation scenario\ntransfers to the real world by testing the policy on a real robot. We make our\nsimulator, models and experiments available at\nhttps://github.com/danieldugas/NavDreams.\n","authors":["Daniel Dugas","Olov Andersson","Roland Siegwart","Jen Jen Chung"],"pdf_url":"https://arxiv.org/pdf/2203.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12298v1","updated":"2022-03-23T09:46:41Z","published":"2022-03-23T09:46:41Z","title":"Input-specific Attention Subnetworks for Adversarial Detection","summary":"  Self-attention heads are characteristic of Transformer models and have been\nwell studied for interpretability and pruning. In this work, we demonstrate an\naltogether different utility of attention heads, namely for adversarial\ndetection. Specifically, we propose a method to construct input-specific\nattention subnetworks (IAS) from which we extract three features to\ndiscriminate between authentic and adversarial inputs. The resultant detector\nsignificantly improves (by over 7.5%) the state-of-the-art adversarial\ndetection accuracy for the BERT encoder on 10 NLU datasets with 11 different\nadversarial attack types. We also demonstrate that our method (a) is more\naccurate for larger models which are likely to have more spurious correlations\nand thus vulnerable to adversarial attack, and (b) performs well even with\nmodest training sets of adversarial examples.\n","authors":["Emil Biju","Anirudh Sriram","Pratyush Kumar","Mitesh M Khapra"],"pdf_url":"https://arxiv.org/pdf/2203.12298v1.pdf","comment":"Accepted at Findings of ACL 2022, 14 pages, 6 Tables and 9 Figures"},{"id":"http://arxiv.org/abs/2201.06780v2","updated":"2022-03-23T09:46:25Z","published":"2022-01-18T07:21:31Z","title":"Asymptotic self-similar blow up profile for 3-D Euler via\n  physics-informed neural networks","summary":"  We develop a new numerical framework, employing physics-informed neural\nnetworks, to find a smooth self-similar solution for the Boussinesq equations.\nThe solution in addition corresponds to an asymptotic self-similar profile for\nthe 3-dimensional Euler equations in the presence of a cylindrical boundary. In\nparticular, the solution represents a precise description of the Luo-Hou\nblow-up scenario [G. Luo, T. Hou, Proc. Natl. Acad. Sci. 111(36): 12968-12973,\n2014] for 3-dimensional Euler. To the best of the authors' knowledge, the\nsolution is the first truly multi-dimensional smooth backwards self-similar\nprofile found for an equation from fluid mechanics. The new numerical framework\nis shown to be both robust and readily adaptable to other equations.\n","authors":["Yongji Wang","Ching-Yao Lai","Javier Gómez-Serrano","Tristan Buckmaster"],"pdf_url":"https://arxiv.org/pdf/2201.06780v2.pdf","comment":"13 pages, 5 figures. Some sign conventions have been changed and\n  improved computations were performed"},{"id":"http://arxiv.org/abs/2203.12297v1","updated":"2022-03-23T09:45:12Z","published":"2022-03-23T09:45:12Z","title":"Increasing the accuracy and resolution of precipitation forecasts using\n  deep generative models","summary":"  Accurately forecasting extreme rainfall is notoriously difficult, but is also\never more crucial for society as climate change increases the frequency of such\nextremes. Global numerical weather prediction models often fail to capture\nextremes, and are produced at too low a resolution to be actionable, while\nregional, high-resolution models are hugely expensive both in computation and\nlabour. In this paper we explore the use of deep generative models to\nsimultaneously correct and downscale (super-resolve) global ensemble forecasts\nover the Continental US. Specifically, using fine-grained radar observations as\nour ground truth, we train a conditional Generative Adversarial Network --\ncoined CorrectorGAN -- via a custom training procedure and augmented loss\nfunction, to produce ensembles of high-resolution, bias-corrected forecasts\nbased on coarse, global precipitation forecasts in addition to other relevant\nmeteorological fields. Our model outperforms an interpolation baseline, as well\nas super-resolution-only and CNN-based univariate methods, and approaches the\nperformance of an operational regional high-resolution model across an array of\nestablished probabilistic metrics. Crucially, CorrectorGAN, once trained,\nproduces predictions in seconds on a single machine. These results raise\nexciting questions about the necessity of regional models, and whether\ndata-driven downscaling and correction methods can be transferred to data-poor\nregions that so far have had no access to high-resolution forecasts.\n","authors":["Ilan Price","Stephan Rasp"],"pdf_url":"https://arxiv.org/pdf/2203.12297v1.pdf","comment":"To appear in AISTATS 2022"},{"id":"http://arxiv.org/abs/2203.11528v2","updated":"2022-03-23T09:28:12Z","published":"2022-03-22T08:04:38Z","title":"Out-of-distribution Generalization with Causal Invariant Transformations","summary":"  In real-world applications, it is important and desirable to learn a model\nthat performs well on out-of-distribution (OOD) data. Recently, causality has\nbecome a powerful tool to tackle the OOD generalization problem, with the idea\nresting on the causal mechanism that is invariant across domains of interest.\nTo leverage the generally unknown causal mechanism, existing works assume a\nlinear form of causal feature or require sufficiently many and diverse training\ndomains, which are usually restrictive in practice. In this work, we obviate\nthese assumptions and tackle the OOD problem without explicitly recovering the\ncausal feature. Our approach is based on transformations that modify the\nnon-causal feature but leave the causal part unchanged, which can be either\nobtained from prior knowledge or learned from the training data in the\nmulti-domain scenario. Under the setting of invariant causal mechanism, we\ntheoretically show that if all such transformations are available, then we can\nlearn a minimax optimal model across the domains using only single domain data.\nNoticing that knowing a complete set of these causal invariant transformations\nmay be impractical, we further show that it suffices to know only a subset of\nthese transformations. Based on the theoretical findings, a regularized\ntraining procedure is proposed to improve the OOD generalization capability.\nExtensive experimental results on both synthetic and real datasets verify the\neffectiveness of the proposed algorithm, even with only a few causal invariant\ntransformations.\n","authors":["Ruoyu Wang","Mingyang Yi","Zhitang Chen","Shengyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.11528v2.pdf","comment":"accepted by cvpr2022"},{"id":"http://arxiv.org/abs/2203.12281v1","updated":"2022-03-23T09:03:54Z","published":"2022-03-23T09:03:54Z","title":"Efficient Fully Distributed Federated Learning with Adaptive Local Links","summary":"  Nowadays, data-driven, machine and deep learning approaches have provided\nunprecedented performance in various complex tasks, including image\nclassification and object detection, and in a variety of application areas,\nlike autonomous vehicles, medical imaging and wireless communications.\nTraditionally, such approaches have been deployed, along with the involved\ndatasets, on standalone devices. Recently, a shift has been observed towards\nthe so-called Edge Machine Learning, in which centralized architectures are\nadopted that allow multiple devices with local computational and storage\nresources to collaborate with the assistance of a centralized server. The\nwell-known federated learning approach is able to utilize such architectures by\nallowing the exchange of only parameters with the server, while keeping the\ndatasets private to each contributing device. In this work, we propose a fully\ndistributed, diffusion-based learning algorithm that does not require a central\nserver and propose an adaptive combination rule for the cooperation of the\ndevices. By adopting a classification task on the MNIST dataset, the efficacy\nof the proposed algorithm over corresponding counterparts is demonstrated via\nthe reduction of the number of collaboration rounds required to achieve an\nacceptable accuracy level in non- IID dataset scenarios.\n","authors":["Evangelos Georgatos","Christos Mavrokefalidis","Kostas Berberidis"],"pdf_url":"https://arxiv.org/pdf/2203.12281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12267v1","updated":"2022-03-23T08:29:46Z","published":"2022-03-23T08:29:46Z","title":"PEAR: Personalized Re-ranking with Contextualized Transformer for\n  Recommendation","summary":"  The goal of recommender systems is to provide ordered item lists to users\nthat best match their interests. As a critical task in the recommendation\npipeline, re-ranking has received increasing attention in recent years. In\ncontrast to conventional ranking models that score each item individually,\nre-ranking aims to explicitly model the mutual influences among items to\nfurther refine the ordering of items given an initial ranking list. In this\npaper, we present a personalized re-ranking model (dubbed PEAR) based on\ncontextualized transformer. PEAR makes several major improvements over the\nexisting methods. Specifically, PEAR not only captures feature-level and\nitem-level interactions, but also models item contexts from both the initial\nranking list and the historical clicked item list. In addition to item-level\nranking score prediction, we also augment the training of PEAR with a\nlist-level classification task to assess users' satisfaction on the whole\nranking list. Experimental results on both public and production datasets have\nshown the superior effectiveness of PEAR compared to the previous re-ranking\nmodels.\n","authors":["Yi Li","Jieming Zhu","Weiwen Liu","Liangcai Su","Guohao Cai","Qi Zhang","Ruiming Tang","Xi Xiao","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2203.12267v1.pdf","comment":"Accepted by WWW 2022"},{"id":"http://arxiv.org/abs/2203.12265v1","updated":"2022-03-23T08:21:10Z","published":"2022-03-23T08:21:10Z","title":"Node Representation Learning in Graph via Node-to-Neighbourhood Mutual\n  Information Maximization","summary":"  The key towards learning informative node representations in graphs lies in\nhow to gain contextual information from the neighbourhood. In this work, we\npresent a simple-yet-effective self-supervised node representation learning\nstrategy via directly maximizing the mutual information between the hidden\nrepresentations of nodes and their neighbourhood, which can be theoretically\njustified by its link to graph smoothing. Following InfoNCE, our framework is\noptimized via a surrogate contrastive loss, where the positive selection\nunderpins the quality and efficiency of representation learning. To this end,\nwe propose a topology-aware positive sampling strategy, which samples positives\nfrom the neighbourhood by considering the structural dependencies between nodes\nand thus enables positive selection upfront. In the extreme case when only one\npositive is sampled, we fully avoid expensive neighbourhood aggregation. Our\nmethods achieve promising performance on various node classification datasets.\nIt is also worth mentioning by applying our loss function to MLP based node\nencoders, our methods can be orders of faster than existing solutions. Our\ncodes and supplementary materials are available at\nhttps://github.com/dongwei156/n2n.\n","authors":["Wei Dong","Junsheng Wu","Yi Luo","Zongyuan Ge","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12265v1.pdf","comment":"Paper is accepted to CVPR 2022. Codes and supplementary materials are\n  available at https://github.com/dongwei156/n2n"},{"id":"http://arxiv.org/abs/2109.02914v2","updated":"2022-03-23T08:11:08Z","published":"2021-09-07T07:56:15Z","title":"Scale-invariant representation of machine learning","summary":"  The success of machine learning has resulted from its structured\nrepresentation of data. Similar data have close internal representations as\ncompressed codes for classification or emerged labels for clustering. We\nobserve that the frequency of internal codes or labels follows power laws in\nboth supervised and unsupervised learning models. This scale-invariant\ndistribution implies that machine learning largely compresses frequent typical\ndata, and simultaneously, differentiates many atypical data as outliers. In\nthis study, we derive the process by which these power laws can naturally arise\nin machine learning. In terms of information theory, the scale-invariant\nrepresentation corresponds to a maximally uncertain data grouping among\npossible representations that guarantee a given learning accuracy.\n","authors":["Sungyeop Lee","Junghyo Jo"],"pdf_url":"https://arxiv.org/pdf/2109.02914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.00916v3","updated":"2022-03-23T07:45:52Z","published":"2021-10-27T08:25:26Z","title":"A multi-task learning-based optimization approach for finding diverse\n  sets of material microstructures with desired properties and its application\n  to texture optimization","summary":"  The optimization along the chain processing-structure-properties-performance\nis one of the core objectives in data-driven materials science. In this sense,\nprocesses are supposed to manufacture workpieces with targeted material\nmicrostructures. These microstructures are defined by the material properties\nof interest and identifying them is a question of materials design. In the\npresent paper, we addresse this issue and introduce a generic multi-task\nlearning-based optimization approach. The approach enables the identification\nof sets of highly diverse microstructures for given desired properties and\ncorresponding tolerances. Basically, the approach consists of an optimization\nalgorithm that interacts with a machine learning model that combines multi-task\nlearning with siamese neural networks. The resulting model (1) relates\nmicrostructures and properties, (2) estimates the likelihood of a\nmicrostructure of being producible, and (3) performs a distance preserving\nmicrostructure feature extraction in order to generate a lower dimensional\nlatent feature space to enable efficient optimization. The proposed approach is\napplied on a crystallographic texture optimization problem for rolled steel\nsheets given desired properties.\n","authors":["Tarek Iraki","Lukas Morand","Johannes Dornheim","Norbert Link","Dirk Helm"],"pdf_url":"https://arxiv.org/pdf/2111.00916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12246v1","updated":"2022-03-23T07:41:35Z","published":"2022-03-23T07:41:35Z","title":"New Distinguishers for Negation-Limited Weak Pseudorandom Functions","summary":"  We show how to distinguish circuits with $\\log k$ negations (a.k.a\n$k$-monotone functions) from uniformly random functions in\n$\\exp\\left(\\tilde{O}\\left(n^{1/3}k^{2/3}\\right)\\right)$ time using random\nsamples. The previous best distinguisher, due to the learning algorithm by\nBlais, Cannone, Oliveira, Servedio, and Tan (RANDOM'15), requires\n$\\exp\\big(\\tilde{O}(n^{1/2} k)\\big)$ time.\n  Our distinguishers are based on Fourier analysis on \\emph{slices of the\nBoolean cube}. We show that some \"middle\" slices of negation-limited circuits\nhave strong low-degree Fourier concentration and then we apply a variation of\nthe classic Linial, Mansour, and Nisan \"Low-Degree algorithm\" (JACM'93) on\nslices. Our techniques also lead to a slightly improved weak learner for\nnegation limited circuits under the uniform distribution.\n","authors":["Zhihuai Chen","Siyao Guo","Qian Li","Chengyu Lin","Xiaoming Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12246v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2202.04237v2","updated":"2022-03-23T07:24:13Z","published":"2022-02-09T02:46:26Z","title":"Learning Robust Convolutional Neural Networks with Relevant Feature\n  Focusing via Explanations","summary":"  Existing image recognition techniques based on convolutional neural networks\n(CNNs) basically assume that the training and test datasets are sampled from\ni.i.d distributions. However, this assumption is easily broken in the real\nworld because of the distribution shift that occurs when the co-occurrence\nrelations between objects and backgrounds in input images change. Under this\ntype of distribution shift, CNNs learn to focus on features that are not\ntask-relevant, such as backgrounds from the training data, and degrade their\naccuracy on the test data. To tackle this problem, we propose relevant feature\nfocusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via\nexplanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc\nexplanation modules, it can be easily applied to off-the-shelf CNNs.\nFurthermore, ReFF requires no additional inference cost at test time because it\nis only used for regularization while training. We demonstrate that CNNs\ntrained with ReFF focus on features relevant to the target task and that ReFF\nimproves the test-time accuracy.\n","authors":["Kazuki Adachi","Shin'ya Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2202.04237v2.pdf","comment":"Accepted by ICME 2022"},{"id":"http://arxiv.org/abs/2203.12236v1","updated":"2022-03-23T07:12:39Z","published":"2022-03-23T07:12:39Z","title":"A Multi-Characteristic Learning Method with Micro-Doppler Signatures for\n  Pedestrian Identification","summary":"  The identification of pedestrians using radar micro-Doppler signatures has\nbecome a hot topic in recent years. In this paper, we propose a\nmulti-characteristic learning (MCL) model with clusters to jointly learn\ndiscrepant pedestrian micro-Doppler signatures and fuse the knowledge learned\nfrom each cluster into final decisions. Time-Doppler spectrogram (TDS) and\nsignal statistical features extracted from FMCW radar, as two categories of\nmicro-Doppler signatures, are used in MCL to learn the micro-motion information\ninside pedestrians' free walking patterns. The experimental results show that\nour model achieves a higher accuracy rate and is more stable for pedestrian\nidentification than other studies, which make our model more practical.\n","authors":["Yu Xiang","Yu Huang","Haodong Xu","Guangbo Zhang","Wenyong Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12235v1","updated":"2022-03-23T07:07:11Z","published":"2022-03-23T07:07:11Z","title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions","summary":"  The syntactic categories of categorial grammar formalisms are structured\nunits made of smaller, indivisible primitives, bound together by the underlying\ngrammar's category formation rules. In the trending approach of constructive\nsupertagging, neural models are increasingly made aware of the internal\ncategory structure, which in turn enables them to more reliably predict rare\nand out-of-vocabulary categories, with significant implications for grammars\npreviously deemed too complex to find practical use. In this work, we revisit\nconstructive supertagging from a graph-theoretic perspective, and propose a\nframework based on heterogeneous dynamic graph convolutions aimed at exploiting\nthe distinctive structure of a supertagger's output space. We test our approach\non a number of categorial grammar datasets spanning different languages and\ngrammar formalisms, achieving substantial improvements over previous state of\nthe art scores. Code will be made available at\nhttps://github.com/konstantinosKokos/dynamic-graph-supertagging\n","authors":["Konstantinos Kogkalidis","Michael Moortgat"],"pdf_url":"https://arxiv.org/pdf/2203.12235v1.pdf","comment":"8 pages plus references, unpublished preprint"},{"id":"http://arxiv.org/abs/2104.03583v2","updated":"2022-03-23T07:00:50Z","published":"2021-04-08T07:52:48Z","title":"Query Driven-Graph Neural Networks for Community Search: From\n  Non-Attributed, Attributed, to Interactive Attributed","summary":"  Given one or more query vertices, Community Search (CS) aims to find densely\nintra-connected and loosely inter-connected structures containing query\nvertices. Attributed Community Search (ACS), a related problem, is more\nchallenging since it finds communities with both cohesive structures and\nhomogeneous vertex attributes. However, most methods for the CS task rely on\ninflexible pre-defined structures and studies for ACS treat each attribute\nindependently. Moreover, the most popular ACS strategies decompose ACS into two\nseparate sub-problems, i.e., the CS task and subsequent attribute filtering\ntask. However, in real-world graphs, the community structure and the vertex\nattributes are closely correlated to each other. This correlation is vital for\nthe ACS problem. In this paper, we propose Graph Neural Network models for both\nCS and ACS problems, i.e., Query Driven-GNN and Attributed Query Driven-GNN. In\nQD-GNN, we combine the local query-dependent structure and global graph\nembedding. In order to extend QD-GNN to handle attributes, we model vertex\nattributes as a bipartite graph and capture the relation between attributes by\nconstructing GNNs on this bipartite graph. With a Feature Fusion operator,\nAQD-GNN processes the structure and attribute simultaneously and predicts\ncommunities according to each attributed query. Experiments on real-world\ngraphs with ground-truth communities demonstrate that the proposed models\noutperform existing CS and ACS algorithms in terms of both efficiency and\neffectiveness. More recently, an interactive setting for CS is proposed that\nallows users to adjust the predicted communities. We further verify our\napproaches under the interactive setting and extend to the attributed context.\nOur method achieves 2.37% and 6.29% improvements in F1-score than the\nstate-of-the-art model without attributes and with attributes respectively.\n","authors":["Yuli Jiang","Yu Rong","Hong Cheng","Xin Huang","Kangfei Zhao","Junzhou Huang"],"pdf_url":"https://arxiv.org/pdf/2104.03583v2.pdf","comment":"PVLDB 2022"},{"id":"http://arxiv.org/abs/2104.13450v6","updated":"2022-03-23T06:55:18Z","published":"2021-04-27T19:51:39Z","title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and\n  Extracting Them from 2D Renderings","summary":"  Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n","authors":["Innfarn Yoo","Huiwen Chang","Xiyang Luo","Ondrej Stava","Ce Liu","Peyman Milanfar","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2104.13450v6.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12221v1","updated":"2022-03-23T06:21:53Z","published":"2022-03-23T06:21:53Z","title":"Modality Competition: What Makes Joint Training of Multi-modal Network\n  Fail in Deep Learning? (Provably)","summary":"  Despite the remarkable success of deep multi-modal learning in practice, it\nhas not been well-explained in theory. Recently, it has been observed that the\nbest uni-modal network outperforms the jointly trained multi-modal network,\nwhich is counter-intuitive since multiple signals generally bring more\ninformation. This work provides a theoretical explanation for the emergence of\nsuch performance gap in neural networks for the prevalent joint training\nframework. Based on a simplified data distribution that captures the realistic\nproperty of multi-modal data, we prove that for the multi-modal late-fusion\nnetwork with (smoothed) ReLU activation trained jointly by gradient descent,\ndifferent modalities will compete with each other. The encoder networks will\nlearn only a subset of modalities. We refer to this phenomenon as modality\ncompetition. The losing modalities, which fail to be discovered, are the\norigins where the sub-optimality of joint training comes from. Experimentally,\nwe illustrate that modality competition matches the intrinsic behavior of\nlate-fusion joint training.\n","authors":["Yu Huang","Junyang Lin","Chang Zhou","Hongxia Yang","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2203.12221v1.pdf","comment":"41 pages, 2 figures"},{"id":"http://arxiv.org/abs/2203.12215v1","updated":"2022-03-23T06:04:11Z","published":"2022-03-23T06:04:11Z","title":"Physics-Driven Deep Learning for Computational Magnetic Resonance\n  Imaging","summary":"  Physics-driven deep learning methods have emerged as a powerful tool for\ncomputational magnetic resonance imaging (MRI) problems, pushing reconstruction\nperformance to new limits. This article provides an overview of the recent\ndevelopments in incorporating physics information into learning-based MRI\nreconstruction. We consider inverse problems with both linear and non-linear\nforward models for computational MRI, and review the classical approaches for\nsolving these. We then focus on physics-driven deep learning approaches,\ncovering physics-driven loss functions, plug-and-play methods, generative\nmodels, and unrolled networks. We highlight domain-specific challenges such as\nreal- and complex-valued building blocks of neural networks, and translational\napplications in MRI with linear and non-linear forward models. Finally, we\ndiscuss common issues and open challenges, and draw connections to the\nimportance of physics-driven learning when combined with other downstream tasks\nin the medical imaging pipeline.\n","authors":["Kerstin Hammernik","Thomas Küstner","Burhaneddin Yaman","Zhengnan Huang","Daniel Rueckert","Florian Knoll","Mehmet Akçakaya"],"pdf_url":"https://arxiv.org/pdf/2203.12215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.01380v3","updated":"2022-03-23T05:52:16Z","published":"2020-12-02T18:23:48Z","title":"Deep Graph Neural Networks with Shallow Subgraph Samplers","summary":"  While Graph Neural Networks (GNNs) are powerful models for learning\nrepresentations on graphs, most state-of-the-art models do not have significant\naccuracy gain beyond two to three layers. Deep GNNs fundamentally need to\naddress: 1). expressivity challenge due to oversmoothing, and 2). computation\nchallenge due to neighborhood explosion. We propose a simple \"deep GNN, shallow\nsampler\" design principle to improve both the GNN accuracy and efficiency -- to\ngenerate representation of a target node, we use a deep GNN to pass messages\nonly within a shallow, localized subgraph. A properly sampled subgraph may\nexclude irrelevant or even noisy nodes, and still preserve the critical\nneighbor features and graph structures. The deep GNN then smooths the\ninformative local signals to enhance feature learning, rather than\noversmoothing the global graph signals into just \"white noise\". We\ntheoretically justify why the combination of deep GNNs with shallow samplers\nyields the best learning performance. We then propose various sampling\nalgorithms and neural architecture extensions to achieve good empirical\nresults. On the largest public graph dataset, ogbn-papers100M, we achieve\nstate-of-the-art accuracy with an order of magnitude reduction in hardware\ncost.\n","authors":["Hanqing Zeng","Muhan Zhang","Yinglong Xia","Ajitesh Srivastava","Andrey Malevich","Rajgopal Kannan","Viktor Prasanna","Long Jin","Ren Chen"],"pdf_url":"https://arxiv.org/pdf/2012.01380v3.pdf","comment":"The complete version of this paper is accepted to NeurIPS 2021,\n  available on arXiv under the new title \"Decoupling the depth and scope of\n  graph neural networks\" (arXiv:2201.07858). This version, \"Deep graph neural\n  networks with shallow subgraph samplers\", is a short version and we withdraw\n  it to avoid confusion. Please always refer to arXiv:2201.07858"},{"id":"http://arxiv.org/abs/2203.12197v1","updated":"2022-03-23T05:14:50Z","published":"2022-03-23T05:14:50Z","title":"Biceph-Net: A robust and lightweight framework for the diagnosis of\n  Alzheimer's disease using 2D-MRI scans and deep similarity learning","summary":"  Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the\nsignificant causes of death in the elderly population. Many deep learning\ntechniques have been proposed to diagnose AD using Magnetic Resonance Imaging\n(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is\nchallenging as the inter-slice information gets lost. To this end, we propose a\nnovel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D\nMRI scans that model both the intra-slice and inter-slice information.\nBiceph-Net has been experimentally shown to perform similar to other\nSpatio-temporal neural networks while being computationally more efficient.\nBiceph-Net is also superior in performance compared to vanilla 2D convolutional\nneural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has\nan inbuilt neighbourhood-based model interpretation feature that can be\nexploited to understand the classification decision taken by the network.\nBiceph-Net experimentally achieves a test accuracy of 100% in the\nclassification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive\nImpairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.\n","authors":["A. H. Rashid","A. Gupta","J. Gupta","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2203.12197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.10535v2","updated":"2022-03-23T05:07:24Z","published":"2021-06-19T17:11:42Z","title":"Learning and Generalization in Overparameterized Normalizing Flows","summary":"  In supervised learning, it is known that overparameterized neural networks\nwith one hidden layer provably and efficiently learn and generalize, when\ntrained using stochastic gradient descent with a sufficiently small learning\nrate and suitable initialization. In contrast, the benefit of\noverparameterization in unsupervised learning is not well understood.\nNormalizing flows (NFs) constitute an important class of models in unsupervised\nlearning for sampling and density estimation. In this paper, we theoretically\nand empirically analyze these models when the underlying neural network is a\none-hidden-layer overparametrized network. Our main contributions are two-fold:\n(1) On the one hand, we provide theoretical and empirical evidence that for\nconstrained NFs (this class of NFs underlies many NF constructions) with the\none-hidden-layer network, overparametrization hurts training. (2) On the other\nhand, we prove that unconstrained NFs, a recently introduced model, can\nefficiently learn any reasonable data distribution under minimal assumptions\nwhen the underlying network is overparametrized and has one hidden-layer.\n","authors":["Kulin Shah","Amit Deshpande","Navin Goyal"],"pdf_url":"https://arxiv.org/pdf/2106.10535v2.pdf","comment":"75 pages, Accepted in AISTATS 2022"},{"id":"http://arxiv.org/abs/2203.12192v1","updated":"2022-03-23T04:50:50Z","published":"2022-03-23T04:50:50Z","title":"Learning to Censor by Noisy Sampling","summary":"  Point clouds are an increasingly ubiquitous input modality and the raw signal\ncan be efficiently processed with recent progress in deep learning. This signal\nmay, often inadvertently, capture sensitive information that can leak semantic\nand geometric properties of the scene which the data owner does not want to\nshare. The goal of this work is to protect sensitive information when learning\nfrom point clouds; by censoring the sensitive information before the point\ncloud is released for downstream tasks. Specifically, we focus on preserving\nutility for perception tasks while mitigating attribute leakage attacks. The\nkey motivating insight is to leverage the localized saliency of perception\ntasks on point clouds to provide good privacy-utility trade-offs. We realize\nthis through a mechanism called Censoring by Noisy Sampling (CBNS), which is\ncomposed of two modules: i) Invariant Sampler: a differentiable point-cloud\nsampler which learns to remove points invariant to utility and ii) Noisy\nDistorter: which learns to distort sampled points to decouple the sensitive\ninformation from utility, and mitigate privacy leakage. We validate the\neffectiveness of CBNS through extensive comparisons with state-of-the-art\nbaselines and sensitivity analyses of key design choices. Results show that\nCBNS achieves superior privacy-utility trade-offs on multiple datasets.\n","authors":["Ayush Chopra","Abhinav Java","Abhishek Singh","Vivek Sharma","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2203.12192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12191v1","updated":"2022-03-23T04:48:38Z","published":"2022-03-23T04:48:38Z","title":"An Adaptive Gradient Method with Energy and Momentum","summary":"  We introduce a novel algorithm for gradient-based optimization of stochastic\nobjective functions. The method may be seen as a variant of SGD with momentum\nequipped with an adaptive learning rate automatically adjusted by an 'energy'\nvariable. The method is simple to implement, computationally efficient, and\nwell suited for large-scale machine learning problems. The method exhibits\nunconditional energy stability for any size of the base learning rate. We\nprovide a regret bound on the convergence rate under the online convex\noptimization framework. We also establish the energy-dependent convergence rate\nof the algorithm to a stationary point in the stochastic non-convex setting. In\naddition, a sufficient condition is provided to guarantee a positive lower\nthreshold for the energy variable. Our experiments demonstrate that the\nalgorithm converges fast while generalizing better than or as well as SGD with\nmomentum in training deep neural networks, and compares also favorably to Adam.\n","authors":["Hailiang Liu","Xuping Tian"],"pdf_url":"https://arxiv.org/pdf/2203.12191v1.pdf","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.09301v2","updated":"2022-03-23T04:45:36Z","published":"2022-03-17T13:03:06Z","title":"One-Shot Adaptation of GAN in Just One CLIP","summary":"  There are many recent research efforts to fine-tune a pre-trained generator\nwith a few target images to generate images of a novel domain. Unfortunately,\nthese methods often suffer from overfitting or under-fitting when fine-tuned\nwith a single target image. To address this, here we present a novel\nsingle-shot GAN adaptation method through unified CLIP space manipulations.\nSpecifically, our model employs a two-step training strategy: reference image\nsearch in the source generator using a CLIP-guided latent optimization,\nfollowed by generator fine-tuning with a novel loss function that imposes CLIP\nspace consistency between the source and adapted generators. To further improve\nthe adapted model to produce spatially consistent samples with respect to the\nsource generator, we also propose contrastive regularization for patchwise\nrelationships in the CLIP space. Experimental results show that our model\ngenerates diverse outputs with the target texture and outperforms the baseline\nmodels both qualitatively and quantitatively. Furthermore, we show that our\nCLIP space manipulation strategy allows more effective attribute editing.\n","authors":["Gihyun Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2203.09301v2.pdf","comment":"Image compressed version"},{"id":"http://arxiv.org/abs/2203.11444v2","updated":"2022-03-23T04:35:56Z","published":"2022-03-22T03:50:04Z","title":"Root-aligned SMILES for Molecular Retrosynthesis Prediction","summary":"  Retrosynthesis prediction is a fundamental problem in organic synthesis,\nwhere the task is to discover precursor molecules that can be used to\nsynthesize a target molecule. A popular paradigm of existing computational\nretrosynthesis methods formulate retrosynthesis prediction as a\nsequence-to-sequence translation problem, where the typical SMILES\nrepresentations are adopted for both reactants and products. However, the\ngeneral-purpose SMILES neglects the characteristics of retrosynthesis that 1)\nthe search space of the reactants is quite huge, and 2) the molecular graph\ntopology is largely unaltered from products to reactants, resulting in the\nsuboptimal performance of SMILES if straightforwardly applied. In this article,\nwe propose the root-aligned SMILES~(R-SMILES), which specifies a tightly\naligned one-to-one mapping between the product and the reactant SMILES, to\nnarrow the string representation discrepancy for more efficient retrosynthesis.\nAs the minimum edit distance between the input and the output is significantly\ndecreased with the proposed R-SMILES, the computational model is largely\nrelieved from learning the complex syntax and dedicated to learning the\nchemical knowledge for retrosynthesis. We compare the proposed R-SMILES with\nvarious state-of-the-art baselines on different benchmarks and show that it\nsignificantly outperforms them all, demonstrating the superiority of the\nproposed method.\n","authors":["Zipeng Zhong","Jie Song","Zunlei Feng","Tiantao Liu","Lingxiang Jia","Shaolun Yao","Min Wu","Tingjun Hou","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2203.11444v2.pdf","comment":"Main paper: 15 pages, 5 figures, and 1 table; supplementary\n  information: 4 pages, 2 figures and 3 tables. Code repository:\n  https://github.com/otori-bird/retrosynthesis"},{"id":"http://arxiv.org/abs/2203.01382v2","updated":"2022-03-23T04:35:28Z","published":"2022-03-02T19:57:32Z","title":"Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data\n  Programming","summary":"  Weak Supervision (WS) techniques allow users to efficiently create large\ntraining datasets by programmatically labeling data with heuristic sources of\nsupervision. While the success of WS relies heavily on the provided labeling\nheuristics, the process of how these heuristics are created in practice has\nremained under-explored. In this work, we formalize the development process of\nlabeling heuristics as an interactive procedure, built around the existing\nworkflow where users draw ideas from a selected set of development data for\ndesigning the heuristic sources. With the formalism, we study two core problems\nof how to strategically select the development data to guide users in\nefficiently creating informative heuristics, and how to exploit the information\nwithin the development process to contextualize and better learn from the\nresultant heuristics. Building upon two novel methodologies that effectively\ntackle the respective problems considered, we present Nemo, an end-to-end\ninteractive system that improves the overall productivity of WS learning\npipeline by an average 20% (and up to 47% in one task) compared to the\nprevailing WS approach.\n","authors":["Cheng-Yu Hsieh","Jieyu Zhang","Alexander Ratner"],"pdf_url":"https://arxiv.org/pdf/2203.01382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.00260v2","updated":"2022-03-23T04:25:48Z","published":"2021-12-01T03:36:58Z","title":"Ranking Distance Calibration for Cross-Domain Few-Shot Learning","summary":"  Recent progress in few-shot learning promotes a more realistic cross-domain\nsetting, where the source and target datasets are from different domains. Due\nto the domain gap and disjoint label spaces between source and target datasets,\ntheir shared knowledge is extremely limited. This encourages us to explore more\ninformation in the target domain rather than to overly elaborate training\nstrategies on the source domain as in many existing methods. Hence, we start\nfrom a generic representation pre-trained by a cross-entropy loss and a\nconventional distance-based classifier, along with an image retrieval view, to\nemploy a re-ranking process for calibrating a target distance matrix by\ndiscovering the reciprocal k-nearest neighbours within the task. Assuming the\npre-trained representation is biased towards the source, we construct a\nnon-linear subspace to minimise task-irrelevant features therewithin while keep\nmore transferrable discriminative information by a hyperbolic tangent\ntransformation. The calibrated distance in this target-aware non-linear\nsubspace is complementary to that in the pre-trained representation. To impose\nsuch distance calibration information onto the pre-trained representation, a\nKullback-Leibler divergence loss is employed to gradually guide the model\ntowards the calibrated distance-based distribution. Extensive evaluations on\neight target domains show that this target ranking calibration process can\nimprove conventional distance-based classifiers in few-shot learning.\n","authors":["Pan Li","Shaogang Gong","Chengjie Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2112.00260v2.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/1906.01437v8","updated":"2022-03-23T04:19:53Z","published":"2019-06-01T05:33:05Z","title":"On the Efficiency of Entropic Regularized Algorithms for Optimal\n  Transport","summary":"  We present several new complexity results for the entropic regularized\nalgorithms that approximately solve the optimal transport (OT) problem between\ntwo discrete probability measures with at most $n$ atoms. First, we improve the\ncomplexity bound of a greedy variant of Sinkhorn, known as \\textit{Greenkhorn},\nfrom $\\widetilde{O}(n^2\\varepsilon^{-3})$ to\n$\\widetilde{O}(n^2\\varepsilon^{-2})$. Notably, our result can match the best\nknown complexity bound of Sinkhorn and help clarify why Greenkhorn\nsignificantly outperforms Sinkhorn in practice in terms of row/column updates\nas observed by~\\citet{Altschuler-2017-Near}. Second, we propose a new\nalgorithm, which we refer to as \\textit{APDAMD} and which generalizes an\nadaptive primal-dual accelerated gradient descent (APDAGD)\nalgorithm~\\citep{Dvurechensky-2018-Computational} with a prespecified mirror\nmapping $\\phi$. We prove that APDAMD achieves the complexity bound of\n$\\widetilde{O}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$ in which $\\delta>0$ stands\nfor the regularity of $\\phi$. In addition, we show by a counterexample that the\ncomplexity bound of $\\widetilde{O}(\\min\\{n^{9/4}\\varepsilon^{-1},\nn^2\\varepsilon^{-2}\\})$ proved for APDAGD before is invalid and give a refined\ncomplexity bound of $\\widetilde{O}(n^{5/2}\\varepsilon^{-1})$. Further, we\ndevelop a \\textit{deterministic} accelerated variant of Sinkhorn via appeal to\nestimated sequence and prove the complexity bound of\n$\\widetilde{O}(n^{7/3}\\varepsilon^{-4/3})$. As such, we see that accelerated\nvariant of Sinkhorn outperforms Sinkhorn and Greenkhorn in terms of\n$1/\\varepsilon$ and APDAGD and accelerated alternating minimization\n(AAM)~\\citep{Guminov-2021-Combination} in terms of $n$. Finally, we conduct the\nexperiments on synthetic and real data and the numerical results show the\nefficiency of Greenkhorn, APDAMD and accelerated Sinkhorn in practice.\n","authors":["Tianyi Lin","Nhat Ho","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/1906.01437v8.pdf","comment":"A preliminary version [arXiv:1901.06482] of this paper, with a subset\n  of the results that are presented here, was presented at ICML 2019"},{"id":"http://arxiv.org/abs/2110.08527v2","updated":"2022-03-23T03:47:26Z","published":"2021-10-16T09:40:30Z","title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for\n  Pre-trained Language Models","summary":"  Recent work has shown pre-trained language models capture social biases from\nthe large amounts of text they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform an\nempirical survey of five recently proposed bias mitigation techniques:\nCounterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace\nProjection, Self-Debias, and SentenceDebias. We quantify the effectiveness of\neach technique using three intrinsic bias benchmarks while also measuring the\nimpact of these techniques on a model's language modeling ability, as well as\nits performance on downstream NLU tasks. We experimentally find that: (1)\nSelf-Debias is the strongest debiasing technique, obtaining improved scores on\nall bias benchmarks; (2) Current debiasing techniques perform less consistently\nwhen mitigating non-gender biases; And (3) improvements on bias benchmarks such\nas StereoSet and CrowS-Pairs by using debiasing strategies are often\naccompanied by a decrease in language modeling ability, making it difficult to\ndetermine whether the bias mitigation was effective.\n","authors":["Nicholas Meade","Elinor Poole-Dayan","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2110.08527v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12160v1","updated":"2022-03-23T03:07:10Z","published":"2022-03-23T03:07:10Z","title":"An Emulation Framework for Fire Front Spread","summary":"  Forecasting bushfire spread is an important element in fire prevention and\nresponse efforts. Empirical observations of bushfire spread can be used to\nestimate fire response under certain conditions. These observations form\nrate-of-spread models, which can be used to generate simulations. We use\nmachine learning to drive the emulation approach for bushfires and show that\nemulation has the capacity to closely reproduce simulated fire-front data. We\npresent a preliminary emulator approach with the capacity for fast emulation of\ncomplex simulations. Large numbers of predictions can then be generated as part\nof ensemble estimation techniques, which provide more robust and reliable\nforecasts of stochastic systems.\n","authors":["Andrew Bolt","Joel Janek Dabrowski","Carolyn Huston","Petra Kuhnert"],"pdf_url":"https://arxiv.org/pdf/2203.12160v1.pdf","comment":"Machine Learning and the Physical Sciences Workshop, NeurIPS, 2021"},{"id":"http://arxiv.org/abs/2112.09899v2","updated":"2022-03-23T03:05:19Z","published":"2021-12-18T10:51:13Z","title":"Improving Subgraph Recognition with Variational Graph Information\n  Bottleneck","summary":"  Subgraph recognition aims at discovering a compressed substructure of a graph\nthat is most informative to the graph property. It can be formulated by\noptimizing Graph Information Bottleneck (GIB) with a mutual information\nestimator. However, GIB suffers from training instability since the mutual\ninformation of graph data is intrinsically difficult to estimate. This paper\nintroduces a noise injection method to compress the information in the\nsubgraphs, which leads to a novel Variational Graph Information Bottleneck\n(VGIB) framework. VGIB allows a tractable variational approximation to its\nobjective under mild assumptions. Therefore, VGIB enjoys more stable and\nefficient training process - we find that VGIB converges 10 times faster than\nGIB with improved performances in practice. Extensive experiments on graph\ninterpretation, explainability of Graph Neural Networks, and graph\nclassification show that VGIB finds better subgraphs than existing methods.\n","authors":["Junchi Yu","Jie Cao","Ran He"],"pdf_url":"https://arxiv.org/pdf/2112.09899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12147v1","updated":"2022-03-23T02:46:26Z","published":"2022-03-23T02:46:26Z","title":"3D-EDM: Early Detection Model for 3D-Printer Faults","summary":"  With the advent of 3D printers in different price ranges and sizes, they are\nno longer just for professionals. However, it is still challenging to use a 3D\nprinter perfectly. Especially, in the case of the Fused Deposition Method, it\nis very difficult to perform with accurate calibration. Previous studies have\nsuggested that these problems can be detected using sensor data and image data\nwith machine learning methods. However, there are difficulties to apply the\nproposed method due to extra installation of additional sensors. Considering\nactual use in the future, we focus on generating the lightweight early\ndetection model with easily collectable data. Proposed early detection model\nthrough Convolutional Neural Network shows significant fault classification\naccuracy with 96.72% for the binary classification task, and 93.38% for\nmulti-classification task respectively. By this research, we hope that general\nusers of 3D printers can use the printer accurately.\n","authors":["Harim Jeong","Joo Hun Yoo"],"pdf_url":"https://arxiv.org/pdf/2203.12147v1.pdf","comment":"Accepted by KSII The 13th International Conference on\n  Internet(ICONI)2021. Copyright 2021 KSII"},{"id":"http://arxiv.org/abs/2203.12145v1","updated":"2022-03-23T02:42:08Z","published":"2022-03-23T02:42:08Z","title":"Out of Distribution Detection, Generalization, and Robustness Triangle\n  with Maximum Probability Theorem","summary":"  Maximum Probability Framework, powered by Maximum Probability Theorem, is a\nrecent theoretical development, aiming to formally define probabilistic models,\nguiding development of objective functions, and regularization of probabilistic\nmodels. MPT uses the probability distribution that the models assume on random\nvariables to provide an upper bound on probability of the model. We apply MPT\nto challenging out-of-distribution (OOD) detection problems in computer vision\nby incorporating MPT as a regularization scheme in training of CNNs and their\nenergy based variants. We demonstrate the effectiveness of the proposed method\non 1080 trained models, with varying hyperparameters, and conclude that MPT\nbased regularization strategy both stabilizes and improves the generalization\nand robustness of base models in addition to improved OOD performance on\nCIFAR10, CIFAR100 and MNIST datasets.\n","authors":["Amir Emad Marvasti","Ehsan Emad Marvasti","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2203.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.10611v4","updated":"2022-03-23T02:08:49Z","published":"2021-04-21T16:09:56Z","title":"Programmable 3D snapshot microscopy with Fourier convolutional networks","summary":"  3D snapshot microscopy enables fast volumetric imaging by capturing a 3D\nvolume in a single 2D camera image and performing computational reconstruction.\nFast volumetric imaging has a variety of biological applications such as whole\nbrain imaging of rapid neural activity in larval zebrafish. The optimal\nmicroscope design for this optical 3D-to-2D encoding is both sample- and\ntask-dependent, with no general solution known. Deep learning based decoders\ncan be combined with a differentiable simulation of an optical encoder for\nend-to-end optimization of both the deep learning decoder and optical encoder.\nThis technique has been used to engineer local optical encoders for other\nproblems such as depth estimation, 3D particle localization, and lensless\nphotography. However, 3D snapshot microscopy is known to require a highly\nnon-local optical encoder which existing UNet-based decoders are not able to\nengineer. We show that a neural network architecture based on global kernel\nFourier convolutional neural networks can efficiently decode information from\nmultiple depths in a volume, globally encoded across a 3D snapshot image. We\nshow in simulation that our proposed networks succeed in engineering and\nreconstructing optical encoders for 3D snapshot microscopy where the existing\nstate-of-the-art UNet architecture fails. We also show that our networks\noutperform the state-of-the-art learned reconstruction algorithms for a\ncomputational photography dataset collected on a prototype lensless camera\nwhich also uses a highly non-local optical encoding.\n","authors":["Diptodip Deb","Zhenfei Jiao","Alex B. Chen","Michael Broxton","Misha B. Ahrens","Kaspar Podgorski","Srinivas C. Turaga"],"pdf_url":"https://arxiv.org/pdf/2104.10611v4.pdf","comment":"Add memory usage tables"},{"id":"http://arxiv.org/abs/2203.12136v1","updated":"2022-03-23T02:03:47Z","published":"2022-03-23T02:03:47Z","title":"Wasserstein Distributionally Robust Optimization via Wasserstein\n  Barycenters","summary":"  In many applications in statistics and machine learning, the availability of\ndata samples from multiple sources has become increasingly prevalent. On the\nother hand, in distributionally robust optimization, we seek data-driven\ndecisions which perform well under the most adverse distribution from a nominal\ndistribution constructed from data samples within a certain distance of\nprobability distributions. However, it remains unclear how to achieve such\ndistributional robustness when data samples from multiple sources are\navailable. In this paper, we propose constructing the nominal distribution in\nWasserstein distributionally robust optimization problems through the notion of\nWasserstein barycenter as an aggregation of data samples from multiple sources.\nUnder specific choices of the loss function, the proposed formulation admits a\ntractable reformulation as a finite convex program, with powerful finite-sample\nand asymptotic guarantees. We illustrate our proposed method through concrete\nexamples with nominal distributions of location-scatter families and\ndistributionally robust maximum likelihood estimation.\n","authors":["Tim Tsz-Kit Lau","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2203.12136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12131v1","updated":"2022-03-23T01:50:24Z","published":"2022-03-23T01:50:24Z","title":"Should Machine Learning Models Report to Us When They Are Clueless?","summary":"  The right to AI explainability has consolidated as a consensus in the\nresearch community and policy-making. However, a key component of\nexplainability has been missing: extrapolation, which describes the extent to\nwhich AI models can be clueless when they encounter unfamiliar samples (i.e.,\nsamples outside a convex hull of their training sets, as we will explain down\nbelow). We report that AI models extrapolate outside their range of familiar\ndata, frequently and without notifying the users and stakeholders. Knowing\nwhether a model has extrapolated or not is a fundamental insight that should be\nincluded in explaining AI models in favor of transparency and accountability.\nInstead of dwelling on the negatives, we offer ways to clear the roadblocks in\npromoting AI transparency. Our analysis commentary accompanying practical\nclauses useful to include in AI regulations such as the National AI Initiative\nAct in the US and the AI Act by the European Commission.\n","authors":["Roozbeh Yousefzadeh","Xuenan Cao"],"pdf_url":"https://arxiv.org/pdf/2203.12131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12130v1","updated":"2022-03-23T01:47:33Z","published":"2022-03-23T01:47:33Z","title":"Pixel VQ-VAEs for Improved Pixel Art Representation","summary":"  Machine learning has had a great deal of success in image processing.\nHowever, the focus of this work has largely been on realistic images, ignoring\nmore niche art styles such as pixel art. Additionally, many traditional machine\nlearning models that focus on groups of pixels do not work well with pixel art,\nwhere individual pixels are important. We propose the Pixel VQ-VAE, a\nspecialized VQ-VAE model that learns representations of pixel art. We show that\nit outperforms other models in both the quality of embeddings as well as\nperformance on downstream tasks.\n","authors":["Akash Saravanan","Matthew Guzdial"],"pdf_url":"https://arxiv.org/pdf/2203.12130v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2203.10425v2","updated":"2022-03-23T01:22:01Z","published":"2022-03-20T01:04:38Z","title":"A Study on Robustness to Perturbations for Representations of\n  Environmental Sound","summary":"  Audio applications involving environmental sound analysis increasingly use\ngeneral-purpose audio representations, also known as embeddings, for transfer\nlearning. Recently, Holistic Evaluation of Audio Representations (HEAR)\nevaluated twenty-nine embedding models on nineteen diverse tasks. However, the\nevaluation's effectiveness depends on the variation already captured within a\ngiven dataset. Therefore, for a given data domain, it is unclear how the\nrepresentations would be affected by the variations caused by myriad\nmicrophones' range and acoustic conditions -- commonly known as channel\neffects. We aim to extend HEAR to evaluate invariance to channel effects in\nthis work. To accomplish this, we imitate channel effects by injecting\nperturbations to the audio signal and measure the shift in the new (perturbed)\nembeddings with three distance measures, making the evaluation domain-dependent\nbut not task-dependent. Combined with the downstream performance, it helps us\nmake a more informed prediction of how robust the embeddings are to the channel\neffects. We evaluate two embeddings -- YAMNet, and OpenL$^3$ on monophonic\n(UrbanSound8K) and polyphonic (SONYC UST) datasets. We show that one distance\nmeasure does not suffice in such task-independent evaluation. Although\nFr\\'echet Audio Distance (FAD) correlates with the trend of the performance\ndrop in the downstream task most accurately, we show that we need to study this\nin conjunction with the other distances to get a clear understanding of the\noverall effect of the perturbation. In terms of the embedding performance, we\nfind OpenL$^3$ to be more robust to YAMNet, which aligns with the HEAR\nevaluation.\n","authors":["Sangeeta Srivastava","Ho-Hsiang Wu","Joao Rulff","Magdalena Fuentes","Mark Cartwright","Claudio Silva","Anish Arora","Juan Pablo Bello"],"pdf_url":"https://arxiv.org/pdf/2203.10425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12120v1","updated":"2022-03-23T01:20:54Z","published":"2022-03-23T01:20:54Z","title":"Matrix Completion with Heterogonous Cost","summary":"  Matrix completion problem has been studied extensively under the condition\nthat each entry has uniform observation cost. And the problem has been explored\nwithing adaptive or nonadaptive, exact or estimation categories. In this paper,\nwe propose a method that approaches to problem in a different category that,\nprice of checking different entries varies accross the matrix. We study under\ntwo type of cost model, first one is each column has different cost, but\nwithing a column, every entry has different cost of observation. Also, another\ncost model is each entry within the matrix are different no matter if they are\nin the same column or row.\n","authors":["Ilqar Ramazanli"],"pdf_url":"https://arxiv.org/pdf/2203.12120v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2002.02431"},{"id":"http://arxiv.org/abs/2203.12117v1","updated":"2022-03-23T01:06:04Z","published":"2022-03-23T01:06:04Z","title":"NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty","summary":"  A robust body of reinforcement learning techniques have been developed to\nsolve complex sequential decision making problems. However, these methods\nassume that train and evaluation tasks come from similarly or identically\ndistributed environments. This assumption does not hold in real life where\nsmall novel changes to the environment can make a previously learned policy\nfail or introduce simpler solutions that might never be found. To that end we\nexplore the concept of {\\em novelty}, defined in this work as the sudden change\nto the mechanics or properties of environment. We provide an ontology of for\nnovelties most relevant to sequential decision making, which distinguishes\nbetween novelties that affect objects versus actions, unary properties versus\nnon-unary relations, and the distribution of solutions to a task. We introduce\nNovGrid, a novelty generation framework built on MiniGrid, acting as a toolkit\nfor rapidly developing and evaluating novelty-adaptation-enabled reinforcement\nlearning techniques. Along with the core NovGrid we provide exemplar novelties\naligned with our ontology and instantiate them as novelty templates that can be\napplied to many MiniGrid-compliant environments. Finally, we present a set of\nmetrics built into our framework for the evaluation of\nnovelty-adaptation-enabled machine-learning techniques, and show\ncharacteristics of a baseline RL model using these metrics.\n","authors":["Jonathan Balloch","Zhiyu Lin","Mustafa Hussain","Aarun Srinivas","Robert Wright","Xiangyu Peng","Julia Kim","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2203.12117v1.pdf","comment":"7 pages, 4 figures, AAAI Spring Symposium 2022 on Designing\n  Artificial Intelligence for Open Worlds (Long Oral)"},{"id":"http://arxiv.org/abs/2203.12114v1","updated":"2022-03-23T00:59:35Z","published":"2022-03-23T00:59:35Z","title":"An Optical Controlling Environment and Reinforcement Learning Benchmarks","summary":"  Deep reinforcement learning has the potential to address various scientific\nproblems. In this paper, we implement an optics simulation environment for\nreinforcement learning based controllers. The environment incorporates\nnonconvex and nonlinear optical phenomena as well as more realistic\ntime-dependent noise. Then we provide the benchmark results of several\nstate-of-the-art reinforcement learning algorithms on the proposed simulation\nenvironment. In the end, we discuss the difficulty of controlling the\nreal-world optical environment with reinforcement learning algorithms.\n","authors":["Abulikemu Abuduweili","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2203.12114v1.pdf","comment":"https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking"},{"id":"http://arxiv.org/abs/2007.15751v6","updated":"2022-03-23T00:54:33Z","published":"2020-07-30T21:38:56Z","title":"From calibration to parameter learning: Harnessing the scaling effects\n  of big data in geoscientific modeling","summary":"  The behaviors and skills of models in many geosciences (e.g., hydrology and\necosystem sciences) strongly depend on spatially-varying parameters that need\ncalibration. A well-calibrated model can reasonably propagate information from\nobservations to unobserved variables via model physics, but traditional\ncalibration is highly inefficient and results in non-unique solutions. Here we\npropose a novel differentiable parameter learning (dPL) framework that\nefficiently learns a global mapping between inputs (and optionally responses)\nand parameters. Crucially, dPL exhibits beneficial scaling curves not\npreviously demonstrated to geoscientists: as training data increases, dPL\nachieves better performance, more physical coherence, and better\ngeneralizability (across space and uncalibrated variables), all with\norders-of-magnitude lower computational cost. We demonstrate examples that\nlearned from soil moisture and streamflow, where dPL drastically outperformed\nexisting evolutionary and regionalization methods, or required only ~12.5% of\nthe training data to achieve similar performance. The generic scheme promotes\nthe integration of deep learning and process-based models, without mandating\nreimplementation.\n","authors":["Wen-Ping Tsai","Dapeng Feng","Ming Pan","Hylke Beck","Kathryn Lawson","Yuan Yang","Jiangtao Liu","Chaopeng Shen"],"pdf_url":"https://arxiv.org/pdf/2007.15751v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.14307v2","updated":"2022-03-23T00:53:38Z","published":"2021-12-28T22:04:03Z","title":"Ensemble Recognition in Reproducing Kernel Hilbert Spaces through\n  Aggregated Measurements","summary":"  In this paper, we study the problem of learning dynamical properties of\nensemble systems from their collective behaviors using statistical approaches\nin reproducing kernel Hilbert space (RKHS). Specifically, we provide a\nframework to identify and cluster multiple ensemble systems through computing\nthe maximum mean discrepancy (MMD) between their aggregated measurements in an\nRKHS, without any prior knowledge of the system dynamics of ensembles. Then,\nleveraging the gradient flow of the newly proposed notion of aggregated Markov\nparameters, we present a systematic framework to recognize and identify an\nensemble systems using their linear approximations. Finally, we demonstrate\nthat the proposed approaches can be extended to cluster multiple unknown\nensembles in RKHS using their aggregated measurements. Numerical experiments\nshow that our approach is reliable and robust to ensembles with different types\nof system dynamics.\n","authors":["Wei Miao","Gong Cheng","Jr-Shin Li"],"pdf_url":"https://arxiv.org/pdf/2112.14307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.10121v3","updated":"2022-03-23T00:44:50Z","published":"2020-02-24T08:59:34Z","title":"The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed\n  Bandit with Many Arms","summary":"  We study a Bayesian $k$-armed bandit problem in many-armed regime, when $k\n\\geq \\sqrt{T}$, with $T$ the time horizon. We first show that subsampling is\ncritical for designing optimal policies. Specifically, the standard UCB\nalgorithm is sub-optimal while a subsampled UCB (SS-UCB), which samples\n$\\Theta(\\sqrt{T})$ arms and executes UCB on that subset, is rate-optimal.\nDespite theoretically optimal regret, SS-UCB numerically performs worse than a\ngreedy algorithm that pulls the current empirically best arm each time. These\nempirical insights hold in a contextual setting as well, using simulations on\nreal data. These results suggest a new form of free exploration in the\nmany-armed regime that benefits greedy algorithms. We theoretically show that\nthis source of free exploration is deeply connected to the distribution of a\ntail event for the prior distribution of arm rewards. This is a fundamentally\ndistinct phenomenon from free exploration due to variation in covariates, as\ndiscussed in the recent literature on contextual bandits. Building on this\nresult, we prove that the subsampled greedy algorithm is rate-optimal for\nBernoulli bandits in many armed regime, and achieves sublinear regret with more\ngeneral distributions. Taken together, our results suggest that practitioners\nmay benefit from using greedy algorithms in the many-armed regime.\n","authors":["Mohsen Bayati","Nima Hamidi","Ramesh Johari","Khashayar Khosravi"],"pdf_url":"https://arxiv.org/pdf/2002.10121v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12106v1","updated":"2022-03-23T00:30:28Z","published":"2022-03-23T00:30:28Z","title":"An Empirical Study on Learning and Improving the Search Objective for\n  Unsupervised Paraphrasing","summary":"  Research in unsupervised text generation has been gaining attention over the\nyears. One recent approach is local search towards a heuristically defined\nobjective, which specifies language fluency, semantic meanings, and other\ntask-specific attributes. Search in the sentence space is realized by\nword-level edit operations including insertion, replacement, and deletion.\nHowever, such objective function is manually designed with multiple components.\nAlthough previous work has shown maximizing this objective yields good\nperformance in terms of true measure of success (i.e. BLEU and iBLEU), the\nobjective landscape is considered to be non-smooth with significant noises,\nposing challenges for optimization. In this dissertation, we address the\nresearch problem of smoothing the noise in the heuristic search objective by\nlearning to model the search dynamics. Then, the learned model is combined with\nthe original objective function to guide the search in a bootstrapping fashion.\nExperimental results show that the learned models combined with the original\nsearch objective can indeed provide a smoothing effect, improving the search\nperformance by a small margin.\n","authors":["Weikai Steven Lu"],"pdf_url":"https://arxiv.org/pdf/2203.12106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12104v1","updated":"2022-03-23T00:04:27Z","published":"2022-03-23T00:04:27Z","title":"Fast on-line signature recognition based on VQ with time modeling","summary":"  This paper proposes a multi-section vector quantization approach for on-line\nsignature recognition. We have used the MCYT database, which consists of 330\nusers and 25 skilled forgeries per person performed by 5 different impostors.\nThis database is larger than those typically used in the literature.\nNevertheless, we also provide results from the SVC database.\n  Our proposed system outperforms the winner of SVC with a reduced\ncomputational requirement, which is around 47 times lower than DTW. In\naddition, our system improves the database storage requirements due to vector\ncompression, and is more privacy-friendly as it is not possible to recover the\noriginal signature using the codebooks. Experimental results with MCYT provide\na 99.76% identification rate and 2.46% EER (skilled forgeries and individual\nthreshold). Experimental results with SVC are 100% of identification rate and\n0% (individual threshold) and 0.31% (general threshold) when using a\ntwo-section VQ approach.\n","authors":["Juan-Manuel Pascual-Gaspar","Marcos Faundez-Zanuy","Carlos Vivaracho"],"pdf_url":"https://arxiv.org/pdf/2203.12104v1.pdf","comment":"23 pages, published in Engineering Applications of Artificial\n  Intelligence, Volume 24, Issue 2, 2011, Pages 368-377, ISSN 0952-1976"},{"id":"http://arxiv.org/abs/2203.12774v1","updated":"2022-03-23T23:53:39Z","published":"2022-03-23T23:53:39Z","title":"Learning Efficient Exploration through Human Seeded Rapidly-exploring\n  Random Trees","summary":"  Modern day computer games have extremely large state and action spaces. To\ndetect bugs in these games' models, human testers play the games repeatedly to\nexplore the game and find errors in the games. Such game play is exhaustive and\ntime consuming. Moreover, since robotics simulators depend on similar methods\nof model specification and debugging, the problem of finding errors in the\nmodel is of interest for the robotics community to ensure robot behaviors and\ninteractions are consistent in simulators. Previous methods have used\nreinforcement learning and search based methods including Rapidly-exploring\nRandom Trees (RRT) to explore a game's state-action space to find bugs.\nHowever, such search and exploration based methods are not efficient at\nexploring the state-action space without a pre-defined heuristic. In this work\nwe attempt to combine a human-tester's expertise in solving games, and the\nexhaustiveness of RRT to search a game's state space efficiently with high\ncoverage. This paper introduces human-seeded RRT (HS-RRT) and\nbehavior-cloning-assisted RRT (CA-RRT) in testing the number of game states\nsearched and the time taken to explore those game states. We compare our\nmethods to an existing weighted RRT baseline for game exploration testing\nstudied. We find HS-RRT and CA-RRT both explore more game states in fewer tree\nexpansions/iterations when compared to the existing baseline. In each test,\nCA-RRT reached more states on average in the same number of iterations as RRT.\nIn our tested environments, CA-RRT was able to reach the same number of states\nas RRT by more than 5000 fewer iterations on average, almost a 50% reduction.\n","authors":["Max Zuo","Logan Schick","Matthew Gombolay","Nakul Gopalan"],"pdf_url":"https://arxiv.org/pdf/2203.12774v1.pdf","comment":"HRI 2022 Workshop - MLHRC"},{"id":"http://arxiv.org/abs/2203.00628v2","updated":"2022-03-23T23:26:25Z","published":"2022-03-01T17:16:41Z","title":"A Neural Ordinary Differential Equation Model for Visualizing Deep\n  Neural Network Behaviors in Multi-Parametric MRI based Glioma Segmentation","summary":"  Purpose: To develop a neural ordinary differential equation (ODE) model for\nvisualizing deep neural network (DNN) behavior during multi-parametric MRI\n(mp-MRI) based glioma segmentation as a method to enhance deep learning\nexplainability. Methods: By hypothesizing that deep feature extraction can be\nmodeled as a spatiotemporally continuous process, we designed a novel deep\nlearning model, neural ODE, in which deep feature extraction was governed by an\nODE without explicit expression. The dynamics of 1) MR images after\ninteractions with DNN and 2) segmentation formation can be visualized after\nsolving ODE. An accumulative contribution curve (ACC) was designed to\nquantitatively evaluate the utilization of each MRI by DNN towards the final\nsegmentation results. The proposed neural ODE model was demonstrated using 369\nglioma patients with a 4-modality mp-MRI protocol: T1, contrast-enhanced T1\n(T1-Ce), T2, and FLAIR. Three neural ODE models were trained to segment\nenhancing tumor (ET), tumor core (TC), and whole tumor (WT). The key MR\nmodalities with significant utilization by DNN were identified based on ACC\nanalysis. Segmentation results by DNN using only the key MR modalities were\ncompared to the ones using all 4 MR modalities. Results: All neural ODE models\nsuccessfully illustrated image dynamics as expected. ACC analysis identified\nT1-Ce as the only key modality in ET and TC segmentations, while both FLAIR and\nT2 were key modalities in WT segmentation. Compared to the U-Net results using\nall 4 MR modalities, Dice coefficient of ET (0.784->0.775), TC (0.760->0.758),\nand WT (0.841->0.837) using the key modalities only had minimal differences\nwithout significance. Conclusion: The neural ODE model offers a new tool for\noptimizing the deep learning model inputs with enhanced explainability. The\npresented methodology can be generalized to other medical image-related deep\nlearning applications.\n","authors":["Zhenyu Yang","Zongsheng Hu","Hangjie Ji","Kyle Lafata","Scott Floyd","Fang-Fang Yin","Chunhao Wang"],"pdf_url":"https://arxiv.org/pdf/2203.00628v2.pdf","comment":"30 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2203.12758v1","updated":"2022-03-23T23:02:16Z","published":"2022-03-23T23:02:16Z","title":"Mokey: Enabling Narrow Fixed-Point Inference for Out-of-the-Box\n  Floating-Point Transformer Models","summary":"  Increasingly larger and better Transformer models keep advancing\nstate-of-the-art accuracy and capability for Natural Language Processing\napplications. These models demand more computational power, storage, and\nenergy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit\nfloating-point transformer models by quantizing all values to 4-bit indexes\ninto dictionaries of representative 16-bit fixed-point centroids. Mokey does\nnot need fine-tuning, an essential feature as often the training resources or\ndatasets are not available to many. Exploiting the range of values that\nnaturally occur in transformer models, Mokey selects centroid values to also\nfit an exponential curve. This unique feature enables Mokey to replace the bulk\nof the original multiply-accumulate operations with narrow 3b fixed-point\nadditions resulting in an area- and energy-efficient hardware accelerator\ndesign. Over a set of state-of-the-art transformer models, the Mokey\naccelerator delivers an order of magnitude improvements in energy efficiency\nover a Tensor Cores-based accelerator while improving performance by at least\n$4\\times$ and as much as $15\\times$ depending on the model and on-chip\nbuffering capacity. Optionally, Mokey can be used as a memory compression\nassist for any other accelerator, transparently stashing wide floating-point or\nfixed-point activations or weights into narrow 4-bit indexes. Mokey proves\nsuperior to prior state-of-the-art quantization methods for Transformers.\n","authors":["Ali Hadi Zadeh","Mostafa Mahmoud","Ameer Abdelhadi","Andreas Moshovos"],"pdf_url":"https://arxiv.org/pdf/2203.12758v1.pdf","comment":"Accepted at the 49th IEEE/ACM International Symposium on Computer\n  Architecture (ISCA '22)"},{"id":"http://arxiv.org/abs/2203.12748v1","updated":"2022-03-23T22:20:29Z","published":"2022-03-23T22:20:29Z","title":"Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in\n  Deep Metric Learning","summary":"  Deep metric learning (DML) enables learning with less supervision through its\nemphasis on the similarity structure of representations. There has been much\nwork on improving generalization of DML in settings like zero-shot retrieval,\nbut little is known about its implications for fairness. In this paper, we are\nthe first to evaluate state-of-the-art DML methods trained on imbalanced data,\nand to show the negative impact these representations have on minority subgroup\nperformance when used for downstream tasks. In this work, we first define\nfairness in DML through an analysis of three properties of the representation\nspace -- inter-class alignment, intra-class alignment, and uniformity -- and\npropose finDML, the fairness in non-balanced DML benchmark to characterize\nrepresentation fairness. Utilizing finDML, we find bias in DML representations\nto propagate to common downstream classification tasks. Surprisingly, this bias\nis propagated even when training data in the downstream task is re-balanced. To\naddress this problem, we present Partial Attribute De-correlation (PARADE) to\nde-correlate feature representations from sensitive attributes and reduce\nperformance gaps between subgroups in both embedding space and downstream\nmetrics.\n","authors":["Natalie Dullerud","Karsten Roth","Kimia Hamidieh","Nicolas Papernot","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2203.12748v1.pdf","comment":"Published as a conference paper at ICLR 2022"},{"id":"http://arxiv.org/abs/2109.10919v2","updated":"2022-03-23T22:19:03Z","published":"2021-09-22T18:00:03Z","title":"An Exploration of Learnt Representations of W Jets","summary":"  I present a Variational Autoencoder (VAE) trained on collider physics data\n(specifically boosted $W$ jets), with reconstruction error given by an\napproximation to the Earth Movers Distance (EMD) between input and output jets.\nThis VAE learns a concrete representation of the data manifold, with\nsemantically meaningful and interpretable latent space directions which are\nhierarchically organized in terms of their relation to physical EMD scales in\nthe underlying physical generative process. A hyperparameter $\\beta$ controls\nthe resolution at which the VAE is sensitive to structures in the data\nmanifold. The variation of the latent space structure with $\\beta$, and the\nscaling of some VAE properties, provide insight into scale dependent structure\nof the dataset and its information complexity. I introduce two measures of the\ndimensionality of the learnt representation that are calculated from this\nscaling.\n","authors":["Jack H. Collins"],"pdf_url":"https://arxiv.org/pdf/2109.10919v2.pdf","comment":"Added fig 5 and altered fig 3. Added citations"},{"id":"http://arxiv.org/abs/2203.12742v1","updated":"2022-03-23T21:58:45Z","published":"2022-03-23T21:58:45Z","title":"Accelerating Bayesian Optimization for Biological Sequence Design with\n  Denoising Autoencoders","summary":"  Bayesian optimization is a gold standard for query-efficient continuous\noptimization. However, its adoption for drug and antibody sequence design has\nbeen hindered by the discrete, high-dimensional nature of the decision\nvariables. We develop a new approach (LaMBO) which jointly trains a denoising\nautoencoder with a discriminative multi-task Gaussian process head, enabling\ngradient-based optimization of multi-objective acquisition functions in the\nlatent space of the autoencoder. These acquisition functions allow LaMBO to\nbalance the explore-exploit trade-off over multiple design rounds, and to\nbalance objective tradeoffs by optimizing sequences at many different points on\nthe Pareto frontier. We evaluate LaMBO on a small-molecule task based on the\nZINC dataset and introduce a new large-molecule task targeting fluorescent\nproteins. In our experiments, LaMBO outperforms genetic optimizers and does not\nrequire a large pretraining corpus, demonstrating that Bayesian optimization is\npractical and effective for biological sequence design.\n","authors":["Samuel Stanton","Wesley Maddox","Nate Gruver","Phillip Maffettone","Emily Delaney","Peyton Greenside","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2203.12742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12738v1","updated":"2022-03-23T21:42:31Z","published":"2022-03-23T21:42:31Z","title":"Contextual Model Aggregation for Fast and Robust Federated Learning in\n  Edge Computing","summary":"  Federated learning is a prime candidate for distributed machine learning at\nthe network edge due to the low communication complexity and privacy protection\namong other attractive properties. However, existing algorithms face issues\nwith slow convergence and/or robustness of performance due to the considerable\nheterogeneity of data distribution, computation and communication capability at\nthe edge. In this work, we tackle both of these issues by focusing on the key\ncomponent of model aggregation in federated learning systems and studying\noptimal algorithms to perform this task. Particularly, we propose a contextual\naggregation scheme that achieves the optimal context-dependent bound on loss\nreduction in each round of optimization. The aforementioned context-dependent\nbound is derived from the particular participating devices in that round and an\nassumption on smoothness of the overall loss function. We show that this\naggregation leads to a definite reduction of loss function at every round.\nFurthermore, we can integrate our aggregation with many existing algorithms to\nobtain the contextual versions. Our experimental results demonstrate\nsignificant improvements in convergence speed and robustness of the contextual\nversions compared to the original algorithms. We also consider different\nvariants of the contextual aggregation and show robust performance even in the\nmost extreme settings.\n","authors":["Hung T. Nguyen","H. Vincent Poor","Mung Chiang"],"pdf_url":"https://arxiv.org/pdf/2203.12738v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2203.12720v1","updated":"2022-03-23T20:53:55Z","published":"2022-03-23T20:53:55Z","title":"Towards All-Purpose Domain Adaptation Under Confounding","summary":"  Current domain adaptation methods address the problems of covariate shift or\nlabel shift, but are not applicable to the setting where they occur\nsimultaneously and interact with each other. In this paper, we propose an\nassumption, confounded shift, to begin to address this problem. We also propose\na framework for this task, based on minimizing the expected divergence between\nthe source and target conditional distributions. Within this framework, we\npropose using the reverse KL divergence, demonstrating the use of both\nparametric linear Gaussian and nonparametric nonlinear Gaussian Process\nestimators of the conditional distribution. We also propose using the Maximum\nMean Discrepancy (MMD) within our framework. To make confounded domain\nadaptation with the MMD effective, we propose an intelligent dynamic strategy\nfor choosing the kernel bandwidth, which may be of independent interest even\noutside of the confounded shift context. Finally, we show that our approach is\nadvantageous on a variety of synthetic and real datasets.\n","authors":["Calvin McCarter"],"pdf_url":"https://arxiv.org/pdf/2203.12720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.08393v2","updated":"2022-03-23T20:52:50Z","published":"2021-06-15T19:46:53Z","title":"Spoofing Generalization: When Can't You Trust Proprietary Models?","summary":"  In this work, we study the computational complexity of determining whether a\nmachine learning model that perfectly fits the training data will generalizes\nto unseen data. In particular, we study the power of a malicious agent whose\ngoal is to construct a model g that fits its training data and nothing else,\nbut is indistinguishable from an accurate model f. We say that g strongly\nspoofs f if no polynomial-time algorithm can tell them apart. If instead we\nrestrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g\nc-weakly spoofs f. Our main results are\n  1. Under cryptographic assumptions, strong spoofing is possible and\n  2. For any c> 0, c-weak spoofing is possible unconditionally\n  While the assumption of a malicious agent is an extreme scenario (hopefully\ncompanies training large models are not malicious), we believe that it sheds\nlight on the inherent difficulties of blindly trusting large proprietary models\nor data.\n","authors":["Ankur Moitra","Elchanan Mossel","Colin Sandon"],"pdf_url":"https://arxiv.org/pdf/2106.08393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07561v3","updated":"2022-03-23T20:52:30Z","published":"2022-03-14T23:48:22Z","title":"Toward the Detection of Polyglot Files","summary":"  Standardized file formats play a key role in the development and use of\ncomputer software. However, it is possible to abuse standardized file formats\nby creating a file that is valid in multiple file formats. The resulting\npolyglot (many languages) file can confound file format identification,\nallowing elements of the file to evade analysis.This is especially problematic\nfor malware detection systems that rely on file format identification for\nfeature extraction. File format identification processes that depend on file\nsignatures can be easily evaded thanks to flexibility in the format\nspecifications of certain file formats. Although work has been done to identify\nfile formats using more comprehensive methods than file signatures, accurate\nidentification of polyglot files remains an open problem. Since malware\ndetection systems routinely perform file format-specific feature extraction,\npolyglot files need to be filtered out prior to ingestion by these systems.\nOtherwise, malicious content could pass through undetected. To address the\nproblem of polyglot detection we assembled a data set using the mitra tool. We\nthen evaluated the performance of the most commonly used file identification\ntool, file. Finally, we demonstrated the accuracy, precision, recall and F1\nscore of a range of machine and deep learning models. Malconv2 and Catboost\ndemonstrated the highest recall on our data set with 95.16% and 95.45%,\nrespectively. These models can be incorporated into a malware detector's file\nprocessing pipeline to filter out potentially malicious polyglots before file\nformat-dependent feature extraction takes place.\n","authors":["Luke Koch","Sean Oesch","Mary Adkisson","Sam Erwin","Brian Weber","Amul Chaulagain"],"pdf_url":"https://arxiv.org/pdf/2203.07561v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.09855v2","updated":"2022-03-23T20:48:33Z","published":"2021-09-20T21:40:12Z","title":"Reinforcement Learning for Finite-Horizon Restless Multi-Armed\n  Multi-Action Bandits","summary":"  We study a finite-horizon restless multi-armed bandit problem with multiple\nactions, dubbed R(MA)^2B. The state of each arm evolves according to a\ncontrolled Markov decision process (MDP), and the reward of pulling an arm\ndepends on both the current state of the corresponding MDP and the action\ntaken. The goal is to sequentially choose actions for arms so as to maximize\nthe expected value of the cumulative rewards collected. Since finding the\noptimal policy is typically intractable, we propose a computationally appealing\nindex policy which we call Occupancy-Measured-Reward Index Policy. Our policy\nis well-defined even if the underlying MDPs are not indexable. We prove that it\nis asymptotically optimal when the activation budget and number of arms are\nscaled up, while keeping their ratio as a constant. For the case when the\nsystem parameters are unknown, we develop a learning algorithm. Our learning\nalgorithm uses the principle of optimism in the face of uncertainty and further\nuses a generative model in order to fully exploit the structure of\nOccupancy-Measured-Reward Index Policy. We call it the R(MA)^2B-UCB algorithm.\nAs compared with the existing algorithms, R(MA)^2B-UCB performs close to an\noffline optimum policy, and also achieves a sub-linear regret with a low\ncomputational complexity. Experimental results show that R(MA)^2B-UCB\noutperforms the existing algorithms in both regret and run time.\n","authors":["Guojun Xiong","Jian Li","Rahul Singh"],"pdf_url":"https://arxiv.org/pdf/2109.09855v2.pdf","comment":"Appeared in AAAI 2022 with title \"Reinforcement Learning Augmented\n  Asymptotically Optimal Index Policy for Finite-Horizon Restless Bandits\""},{"id":"http://arxiv.org/abs/2203.12715v1","updated":"2022-03-23T20:38:48Z","published":"2022-03-23T20:38:48Z","title":"Predicting Multi-Antenna Frequency-Selective Channels via Meta-Learned\n  Linear Filters based on Long-Short Term Channel Decomposition","summary":"  An efficient data-driven prediction strategy for multi-antenna\nfrequency-selective channels must operate based on a small number of pilot\nsymbols. This paper proposes novel channel prediction algorithms that address\nthis goal by integrating transfer and meta-learning with a reduced-rank\nparametrization of the channel. The proposed methods optimize linear predictors\nby utilizing data from previous frames, which are generally characterized by\ndistinct propagation characteristics, in order to enable fast training on the\ntime slots of the current frame. The proposed predictors rely on a novel\nlong-short-term decomposition (LSTD) of the linear prediction model that\nleverages the disaggregation of the channel into long-term space-time\nsignatures and fading amplitudes. We first develop predictors for\nsingle-antenna frequency-flat channels based on transfer/meta-learned quadratic\nregularization. Then, we introduce transfer and meta-learning algorithms for\nLSTD-based prediction models that build on equilibrium propagation (EP) and\nalternating least squares (ALS). Numerical results under the 3GPP 5G standard\nchannel model demonstrate the impact of transfer and meta-learning on reducing\nthe number of pilots for channel prediction, as well as the merits of the\nproposed LSTD parametrization.\n","authors":["Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2203.12715v1.pdf","comment":"submitted for journal publication, subsumes (arXiv:2110.00414)"},{"id":"http://arxiv.org/abs/2007.05078v2","updated":"2022-03-23T20:21:47Z","published":"2020-07-09T21:37:13Z","title":"A Kernel-Based Approach to Non-Stationary Reinforcement Learning in\n  Metric Spaces","summary":"  In this work, we propose KeRNS: an algorithm for episodic reinforcement\nlearning in non-stationary Markov Decision Processes (MDPs) whose state-action\nset is endowed with a metric. Using a non-parametric model of the MDP built\nwith time-dependent kernels, we prove a regret bound that scales with the\ncovering dimension of the state-action space and the total variation of the MDP\nwith time, which quantifies its level of non-stationarity. Our method\ngeneralizes previous approaches based on sliding windows and exponential\ndiscounting used to handle changing environments. We further propose a\npractical implementation of KeRNS, we analyze its regret and validate it\nexperimentally.\n","authors":["Omar Darwiche Domingues","Pierre Ménard","Matteo Pirotta","Emilie Kaufmann","Michal Valko"],"pdf_url":"https://arxiv.org/pdf/2007.05078v2.pdf","comment":"Update following the publication in AISTATS 2021. Fixed typos and\n  lemma about runtime"},{"id":"http://arxiv.org/abs/2107.07508v2","updated":"2022-03-23T20:08:59Z","published":"2021-07-15T17:59:08Z","title":"USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization\n  Problems","summary":"  Real-world decision-making systems are often subject to uncertainties that\nhave to be resolved through observational data. Therefore, we are frequently\nconfronted with combinatorial optimization problems of which the objective\nfunction is unknown and thus has to be debunked using empirical evidence. In\ncontrast to the common practice that relies on a learning-and-optimization\nstrategy, we consider the regression between combinatorial spaces, aiming to\ninfer high-quality optimization solutions from samples of input-solution pairs\n-- without the need to learn the objective function. Our main deliverable is a\nuniversal solver that is able to handle abstract undetermined stochastic\ncombinatorial optimization problems. For learning foundations, we present\nlearning-error analysis under the PAC-Bayesian framework using a new\nmargin-based analysis. In empirical studies, we demonstrate our design using\nproof-of-concept experiments, and compare it with other methods that are\npotentially applicable. Overall, we obtain highly encouraging experimental\nresults for several classic combinatorial problems on both synthetic and\nreal-world datasets.\n","authors":["Guangmo Tong"],"pdf_url":"https://arxiv.org/pdf/2107.07508v2.pdf","comment":"NeurIPS 2021"},{"id":"http://arxiv.org/abs/2203.12710v1","updated":"2022-03-23T20:05:06Z","published":"2022-03-23T20:05:06Z","title":"The Challenges of Continuous Self-Supervised Learning","summary":"  Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks\nin representation learning - the need for human annotations. As a result, SSL\nholds the promise to learn representations from data in-the-wild, i.e., without\nthe need for finite and static datasets. Instead, true SSL algorithms should be\nable to exploit the continuous stream of data being generated on the internet\nor by agents exploring their environments. But do traditional self-supervised\nlearning approaches work in this setup? In this work, we investigate this\nquestion by conducting experiments on the continuous self-supervised learning\nproblem. While learning in the wild, we expect to see a continuous (infinite)\nnon-IID data stream that follows a non-stationary distribution of visual\nconcepts. The goal is to learn a representation that can be robust, adaptive\nyet not forgetful of concepts seen in the past. We show that a direct\napplication of current methods to such continuous setup is 1) inefficient both\ncomputationally and in the amount of data required, 2) leads to inferior\nrepresentations due to temporal correlations (non-IID data) in some sources of\nstreaming data and 3) exhibits signs of catastrophic forgetting when trained on\nsources with non-stationary data distributions. We propose the use of replay\nbuffers as an approach to alleviate the issues of inefficiency and temporal\ncorrelations. We further propose a novel method to enhance the replay buffer by\nmaintaining the least redundant samples. Minimum redundancy (MinRed) buffers\nallow us to learn effective representations even in the most challenging\nstreaming scenarios composed of sequential visual data obtained from a single\nembodied agent, and alleviates the problem of catastrophic forgetting when\nlearning from data with non-stationary semantic distributions.\n","authors":["Senthil Purushwalkam","Pedro Morgado","Abhinav Gupta"],"pdf_url":"https://arxiv.org/pdf/2203.12710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12701v1","updated":"2022-03-23T19:49:45Z","published":"2022-03-23T19:49:45Z","title":"On Understanding the Influence of Controllable Factors with a Feature\n  Attribution Algorithm: a Medical Case Study","summary":"  Feature attribution XAI algorithms enable their users to gain insight into\nthe underlying patterns of large datasets through their feature importance\ncalculation. Existing feature attribution algorithms treat all features in a\ndataset homogeneously, which may lead to misinterpretation of consequences of\nchanging feature values. In this work, we consider partitioning features into\ncontrollable and uncontrollable parts and propose the Controllable fActor\nFeature Attribution (CAFA) approach to compute the relative importance of\ncontrollable features. We carried out experiments applying CAFA to two existing\ndatasets and our own COVID-19 non-pharmaceutical control measures dataset.\nExperimental results show that with CAFA, we are able to exclude influences\nfrom uncontrollable features in our explanation while keeping the full dataset\nfor prediction.\n","authors":["Veera Raghava Reddy Kovvuri","Siyuan Liu","Monika Seisenberger","Berndt Müller","Xiuyi Fan"],"pdf_url":"https://arxiv.org/pdf/2203.12701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13207v2","updated":"2022-03-23T19:44:50Z","published":"2021-08-30T13:02:00Z","title":"Representation of binary classification trees with binary features by\n  quantum circuits","summary":"  We propose a quantum representation of binary classification trees with\nbinary features based on a probabilistic approach. By using the quantum\ncomputer as a processor for probability distributions, a probabilistic\ntraversal of the decision tree can be realized via measurements of a quantum\ncircuit. We describe how tree inductions and the prediction of class labels of\nquery data can be integrated into this framework. An on-demand sampling method\nenables predictions with a constant number of classical memory slots,\nindependent of the tree depth. We experimentally study our approach using both\na quantum computing simulator and actual IBM quantum hardware. To our\nknowledge, this is the first realization of a decision tree classifier on a\nquantum device.\n","authors":["Raoul Heese","Patricia Bickert","Astrid Elisa Niederle"],"pdf_url":"https://arxiv.org/pdf/2108.13207v2.pdf","comment":"43 pages, 20 figures, 3 tables"},{"id":"http://arxiv.org/abs/2203.12693v1","updated":"2022-03-23T19:36:19Z","published":"2022-03-23T19:36:19Z","title":"Enhancing Classifier Conservativeness and Robustness by Polynomiality","summary":"  We illustrate the detrimental effect, such as overconfident decisions, that\nexponential behavior can have in methods like classical LDA and logistic\nregression. We then show how polynomiality can remedy the situation. This,\namong others, leads purposefully to random-level performance in the tails, away\nfrom the bulk of the training data. A directly related, simple, yet important\ntechnical novelty we subsequently present is softRmax: a reasoned alternative\nto the standard softmax function employed in contemporary (deep) neural\nnetworks. It is derived through linking the standard softmax to Gaussian\nclass-conditional models, as employed in LDA, and replacing those by a\npolynomial alternative. We show that two aspects of softRmax, conservativeness\nand inherent gradient regularization, lead to robustness against adversarial\nattacks without gradient obfuscation.\n","authors":["Ziqi Wang","Marco Loog"],"pdf_url":"https://arxiv.org/pdf/2203.12693v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022"},{"id":"http://arxiv.org/abs/2203.12686v1","updated":"2022-03-23T19:17:22Z","published":"2022-03-23T19:17:22Z","title":"Possibility Before Utility: Learning And Using Hierarchical Affordances","summary":"  Reinforcement learning algorithms struggle on tasks with complex hierarchical\ndependency structures. Humans and other intelligent agents do not waste time\nassessing the utility of every high-level action in existence, but instead only\nconsider ones they deem possible in the first place. By focusing only on what\nis feasible, or \"afforded\", at the present moment, an agent can spend more time\nboth evaluating the utility of and acting on what matters. To this end, we\npresent Hierarchical Affordance Learning (HAL), a method that learns a model of\nhierarchical affordances in order to prune impossible subtasks for more\neffective learning. Existing works in hierarchical reinforcement learning\nprovide agents with structural representations of subtasks but are not\naffordance-aware, and by grounding our definition of hierarchical affordances\nin the present state, our approach is more flexible than the multitude of\napproaches that ground their subtask dependencies in a symbolic history. While\nthese logic-based methods often require complete knowledge of the subtask\nhierarchy, our approach is able to utilize incomplete and varying symbolic\nspecifications. Furthermore, we demonstrate that relative to\nnon-affordance-aware methods, HAL agents are better able to efficiently learn\ncomplex tasks, navigate environment stochasticity, and acquire diverse skills\nin the absence of extrinsic supervision -- all of which are hallmarks of human\nlearning.\n","authors":["Robby Costales","Shariq Iqbal","Fei Sha"],"pdf_url":"https://arxiv.org/pdf/2203.12686v1.pdf","comment":"ICLR 2022 camera-ready"},{"id":"http://arxiv.org/abs/2203.12679v1","updated":"2022-03-23T19:06:16Z","published":"2022-03-23T19:06:16Z","title":"Sample-efficient Iterative Lower Bound Optimization of Deep Reactive\n  Policies for Planning in Continuous MDPs","summary":"  Recent advances in deep learning have enabled optimization of deep reactive\npolicies (DRPs) for continuous MDP planning by encoding a parametric policy as\na deep neural network and exploiting automatic differentiation in an end-to-end\nmodel-based gradient descent framework. This approach has proven effective for\noptimizing DRPs in nonlinear continuous MDPs, but it requires a large number of\nsampled trajectories to learn effectively and can suffer from high variance in\nsolution quality. In this work, we revisit the overall model-based DRP\nobjective and instead take a minorization-maximization perspective to\niteratively optimize the DRP w.r.t. a locally tight lower-bounded objective.\nThis novel formulation of DRP learning as iterative lower bound optimization\n(ILBO) is particularly appealing because (i) each step is structurally easier\nto optimize than the overall objective, (ii) it guarantees a monotonically\nimproving objective under certain theoretical conditions, and (iii) it reuses\nsamples between iterations thus lowering sample complexity. Empirical\nevaluation confirms that ILBO is significantly more sample-efficient than the\nstate-of-the-art DRP planner and consistently produces better solution quality\nwith lower variance. We additionally demonstrate that ILBO generalizes well to\nnew problem instances (i.e., different initial states) without requiring\nretraining.\n","authors":["Siow Meng Low","Akshat Kumar","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2203.12679v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2203.11113v2","updated":"2022-03-23T16:36:25Z","published":"2022-03-21T16:41:35Z","title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces","summary":"  Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n","authors":["Jia-Xing Zhong","Kaichen Zhou","Qingyong Hu","Bing Wang","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2203.11113v2.pdf","comment":"To appear at CVPR 2022 (Source Code:\n  https://github.com/jx-zhong-for-academic-purpose/Kinet )"},{"id":"http://arxiv.org/abs/2203.12441v1","updated":"2022-03-23T14:28:08Z","published":"2022-03-23T14:28:08Z","title":"M-SENA: An Integrated Platform for Multimodal Sentiment Analysis","summary":"  M-SENA is an open-sourced platform for Multimodal Sentiment Analysis. It aims\nto facilitate advanced research by providing flexible toolkits, reliable\nbenchmarks, and intuitive demonstrations. The platform features a fully modular\nvideo sentiment analysis framework consisting of data management, feature\nextraction, model training, and result analysis modules. In this paper, we\nfirst illustrate the overall architecture of the M-SENA platform and then\nintroduce features of the core modules. Reliable baseline results of different\nmodality features and MSA benchmarks are also reported. Moreover, we use model\nevaluation and analysis tools provided by M-SENA to present intermediate\nrepresentation visualization, on-the-fly instance test, and generalization\nability test results. The source code of the platform is publicly available at\nhttps://github.com/thuiar/M-SENA.\n","authors":["Huisheng Mao","Ziqi Yuan","Hua Xu","Wenmeng Yu","Yihe Liu","Kai Gao"],"pdf_url":"https://arxiv.org/pdf/2203.12441v1.pdf","comment":"11 pages, 4 figures, to be published in ACL 2022 System Demonstration\n  Track"},{"id":"http://arxiv.org/abs/2203.12122v1","updated":"2022-03-23T01:31:17Z","published":"2022-03-23T01:31:17Z","title":"On Adversarial Robustness of Large-scale Audio Visual Learning","summary":"  As audio-visual systems are being deployed for safety-critical tasks such as\nsurveillance and malicious content filtering, their robustness remains an\nunder-studied area. Existing published work on robustness either does not scale\nto large-scale dataset, or does not deal with multiple modalities. This work\naims to study several key questions related to multi-modal learning through the\nlens of robustness: 1) Are multi-modal models necessarily more robust than\nuni-modal models? 2) How to efficiently measure the robustness of multi-modal\nlearning? 3) How to fuse different modalities to achieve a more robust\nmulti-modal model? To understand the robustness of the multi-modal model in a\nlarge-scale setting, we propose a density-based metric, and a convexity metric\nto efficiently measure the distribution of each modality in high-dimensional\nlatent space. Our work provides a theoretical intuition together with empirical\nevidence showing how multi-modal fusion affects adversarial robustness through\nthese metrics. We further devise a mix-up strategy based on our metrics to\nimprove the robustness of the trained model. Our experiments on AudioSet and\nKinetics-Sounds verify our hypothesis that multi-modal models are not\nnecessarily more robust than their uni-modal counterparts in the face of\nadversarial examples. We also observe our mix-up trained method could achieve\nas much protection as traditional adversarial training, offering a\ncomputationally cheap alternative. Implementation:\nhttps://github.com/lijuncheng16/AudioSetDoneRight\n","authors":["Juncheng B Li","Shuhui Qu","Xinjian Li"," Po-Yao"," Huang","Florian Metze"],"pdf_url":"https://arxiv.org/pdf/2203.12122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12736v1","updated":"2022-03-23T21:40:01Z","published":"2022-03-23T21:40:01Z","title":"An interactive music infilling interface for pop music composition","summary":"  Artificial intelligence (AI) has been widely applied to music generation\ntopics such as continuation, melody/harmony generation, genre transfer and\nmusic infilling application. Although with the burst interest to apply AI to\nmusic, there are still few interfaces for the musicians to take advantage of\nthe latest progress of the AI technology. This makes those tools less valuable\nin practice and harder to find its advantage/drawbacks without utilizing them\nin the real scenario. This work builds a max patch for interactive music\ninfilling application with different levels of control, including track\ndensity/polyphony/occupation rate and bar tonal tension control. The user can\nselect the melody/bass/harmony track as the infilling content up to 16 bars.\nThe infilling algorithm is based on the author's previous work, and the\ninterface sends/receives messages to the AI system hosted in the cloud. This\ninterface lowers the barrier of AI technology and can generate different\nvariations of the selected content. Those results can give several alternatives\nto the musicians' composition, and the interactive process realizes the value\nof the AI infilling system.\n","authors":["Rui Guo"],"pdf_url":"https://arxiv.org/pdf/2203.12736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12692v1","updated":"2022-03-23T19:28:20Z","published":"2022-03-23T19:28:20Z","title":"Affective Feedback Synthesis Towards Multimodal Text and Image Data","summary":"  In this paper, we have defined a novel task of affective feedback synthesis\nthat deals with generating feedback for input text & corresponding image in a\nsimilar way as humans respond towards the multimodal data. A feedback synthesis\nsystem has been proposed and trained using ground-truth human comments along\nwith image-text input. We have also constructed a large-scale dataset\nconsisting of image, text, Twitter user comments, and the number of likes for\nthe comments by crawling the news articles through Twitter feeds. The proposed\nsystem extracts textual features using a transformer-based textual encoder\nwhile the visual features have been extracted using a Faster region-based\nconvolutional neural networks model. The textual and visual features have been\nconcatenated to construct the multimodal features using which the decoder\nsynthesizes the feedback. We have compared the results of the proposed system\nwith the baseline models using quantitative and qualitative measures. The\ngenerated feedbacks have been analyzed using automatic and human evaluation.\nThey have been found to be semantically similar to the ground-truth comments\nand relevant to the given text-image input.\n","authors":["Puneet Kumar","Gaurav Bhat","Omkar Ingle","Daksh Goyal","Balasubramanian Raman"],"pdf_url":"https://arxiv.org/pdf/2203.12692v1.pdf","comment":"Submitted to ACM Transactions on Multimedia Computing,\n  Communications, and Applications"}]},"2022-03-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2203.13240v1","updated":"2022-03-24T17:50:46Z","published":"2022-03-24T17:50:46Z","title":"Token Dropping for Efficient BERT Pretraining","summary":"  Transformer-based models generally allocate the same amount of computation\nfor each token in a given sequence. We develop a simple but effective \"token\ndropping\" method to accelerate the pretraining of transformer models, such as\nBERT, without degrading its performance on downstream tasks. In short, we drop\nunimportant tokens starting from an intermediate layer in the model to make the\nmodel focus on important tokens; the dropped tokens are later picked up by the\nlast layer of the model so that the model still produces full-length sequences.\nWe leverage the already built-in masked language modeling (MLM) loss to\nidentify unimportant tokens with practically no computational overhead. In our\nexperiments, this simple approach reduces the pretraining cost of BERT by 25%\nwhile achieving similar overall fine-tuning performance on standard downstream\ntasks.\n","authors":["Le Hou","Richard Yuanzhe Pang","Tianyi Zhou","Yuexin Wu","Xinying Song","Xiaodan Song","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.13240v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2112.06825v2","updated":"2022-03-24T17:33:07Z","published":"2021-12-13T17:35:26Z","title":"VL-Adapter: Parameter-Efficient Transfer Learning for\n  Vision-and-Language Tasks","summary":"  Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.\n","authors":["Yi-Lin Sung","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2112.06825v2.pdf","comment":"CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)"},{"id":"http://arxiv.org/abs/2203.13226v1","updated":"2022-03-24T17:31:46Z","published":"2022-03-24T17:31:46Z","title":"SMARAGD: Synthesized sMatch for Accurate and Rapid AMR Graph Distance","summary":"  The semantic similarity of graph-based meaning representations, such as\nAbstract Meaning Representation (AMR), is typically assessed using graph\nmatching algorithms, such as SMATCH (Cai and Knight, 2013). However, SMATCH\nsuffers from NP-completeness, making its large-scale application, e.g., for AMR\nclustering or semantic search, infeasible. To mitigate this issue, we propose\nSMARAGD (Synthesized sMatch for accurate and rapid AMR graph distance). We show\nthe potential of neural networks to approximate the SMATCH scores and graph\nalignments, i) in linear time using a machine translation framework to predict\nthe alignments, or ii) in constant time using a Siamese CNN to directly predict\nSMATCH scores. We show that the approximation error can be substantially\nreduced by applying data augmentation and AMR graph anonymization.\n","authors":["Juri Opitz","Philipp Meier","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2203.13226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13224v1","updated":"2022-03-24T17:31:26Z","published":"2022-03-24T17:31:26Z","title":"Language Models that Seek for Knowledge: Modular Search & Generation for\n  Dialogue and Prompt Completion","summary":"  Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine->Knowledge->Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.\n","authors":["Kurt Shuster","Mojtaba Komeili","Leonard Adolphs","Stephen Roller","Arthur Szlam","Jason Weston"],"pdf_url":"https://arxiv.org/pdf/2203.13224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08222v2","updated":"2022-03-24T17:26:00Z","published":"2021-10-15T17:34:35Z","title":"DialFact: A Benchmark for Fact-Checking in Dialogue","summary":"  Fact-checking is an essential tool to mitigate the spread of misinformation\nand disinformation. We introduce the task of fact-checking in dialogue, which\nis a relatively unexplored area. We construct DialFact, a testing benchmark\ndataset of 22,245 annotated conversational claims, paired with pieces of\nevidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable\nclaim detection task distinguishes whether a response carries verifiable\nfactual information; 2) Evidence retrieval task retrieves the most relevant\nWikipedia snippets as evidence; 3) Claim verification task predicts a dialogue\nresponse to be supported, refuted, or not enough information. We found that\nexisting fact-checking models trained on non-dialogue data like FEVER fail to\nperform well on our task, and thus, we propose a simple yet data-efficient\nsolution to effectively improve fact-checking performance in dialogue. We point\nout unique challenges in DialFact such as handling the colloquialisms,\ncoreferences and retrieval ambiguities in the error analysis to shed light on\nfuture research in this direction.\n","authors":["Prakhar Gupta","Chien-Sheng Wu","Wenhao Liu","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2110.08222v2.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2203.13209v1","updated":"2022-03-24T17:09:23Z","published":"2022-03-24T17:09:23Z","title":"Direct parsing to sentiment graphs","summary":"  This paper demonstrates how a graph-based semantic parser can be applied to\nthe task of structured sentiment analysis, directly predicting sentiment graphs\nfrom text. We advance the state of the art on 4 out of 5 standard benchmark\nsets. We release the source code, models and predictions.\n","authors":["David Samuel","Jeremy Barnes","Robin Kurtz","Stephan Oepen","Lilja Øvrelid","Erik Velldal"],"pdf_url":"https://arxiv.org/pdf/2203.13209v1.pdf","comment":"Accepted to ACL 2022"},{"id":"http://arxiv.org/abs/2203.13176v1","updated":"2022-03-24T16:52:07Z","published":"2022-03-24T16:52:07Z","title":"Emergence of hierarchical reference systems in multi-agent communication","summary":"  In natural language, referencing objects at different levels of specificity\nis a fundamental pragmatic mechanism for efficient communication in context. We\ndevelop a novel communication game, the hierarchical reference game, to study\nthe emergence of such reference systems in artificial agents. We consider a\nsimplified world, in which concepts are abstractions over a set of primitive\nattributes (e.g., color, style, shape). Depending on how many attributes are\ncombined, concepts are more general (\"circle\") or more specific (\"red dotted\ncircle\"). Based on the context, the agents have to communicate at different\nlevels of this hierarchy. Our results show, that the agents learn to play the\ngame successfully and can even generalize to novel concepts. To achieve\nabstraction, they use implicit (omitting irrelevant information) and explicit\n(indicating that attributes are irrelevant) strategies. In addition, the\ncompositional structure underlying the concept hierarchy is reflected in the\nemergent protocols, indicating that the need to develop hierarchical reference\nsystems supports the emergence of compositionality.\n","authors":["Xenia Ohmer","Marko Duda","Elia Bruni"],"pdf_url":"https://arxiv.org/pdf/2203.13176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.08694v2","updated":"2022-03-24T16:31:21Z","published":"2021-06-16T10:57:24Z","title":"On the proper role of linguistically-oriented deep net analysis in\n  linguistic theorizing","summary":"  A lively research field has recently emerged that uses experimental methods\nto probe the linguistic behavior of modern deep networks. While work in this\ntradition often reports intriguing results about the grammatical skills of deep\nnets, it is not clear what their implications for linguistic theorizing should\nbe. As a consequence, linguistically-oriented deep net analysis has had very\nlittle impact on linguistics at large. In this chapter, I suggest that deep\nnetworks should be treated as theories making explicit predictions about the\nacceptability of linguistic utterances. I argue that, if we overcome some\nobstacles standing in the way of seriously pursuing this idea, we will gain a\npowerful new theoretical tool, complementary to mainstream algebraic\napproaches.\n","authors":["Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2106.08694v2.pdf","comment":"To appear in collective volume on Algebraic Systems and the\n  Representation of Linguistic Knowledge, editor: Shalom Lappin, Taylor &\n  Francis"},{"id":"http://arxiv.org/abs/2103.12407v4","updated":"2022-03-24T16:24:54Z","published":"2021-03-23T09:17:22Z","title":"Detecting Hate Speech with GPT-3","summary":"  Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist. We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 can identify sexist or racist text with an average\naccuracy between 55 per cent and 67 per cent, depending on the category of text\nand type of learning. With few-shot learning, the model's accuracy can be as\nhigh as 85 per cent. Large language models have a role to play in hate speech\ndetection, and with further development they could eventually be used to\ncounter hate speech.\n","authors":["Ke-Li Chiu","Annie Collins","Rohan Alexander"],"pdf_url":"https://arxiv.org/pdf/2103.12407v4.pdf","comment":"29 pages, 1 figure, 23 tables 24 March 2022: Re-submission changes\n  the modelling to occur multiple times and adds standard errors"},{"id":"http://arxiv.org/abs/2203.13151v1","updated":"2022-03-24T16:12:21Z","published":"2022-03-24T16:12:21Z","title":"Multi-armed bandits for online optimization of language model\n  pre-training: the use case of dynamic masking","summary":"  Transformer-based language models (TLMs) provide state-of-the-art performance\nin many modern natural language processing applications. TLM training is\nconducted in two phases. First, the model is pre-trained over large volumes of\ntext to minimize a generic objective function, such as the Masked Language\nModel (MLM). Second, the model is fine-tuned in specific downstream tasks.\nPre-training requires large volumes of data and high computational resources,\nwhile introducing many still unresolved design choices. For instance, selecting\nhyperparameters for language model pre-training is often carried out based on\nheuristics or grid-based searches. In this work, we propose a multi-armed\nbandit-based online optimization framework for the sequential selection of\npre-training hyperparameters to optimize language model performance. We pose\nthe pre-training procedure as a sequential decision-making task, where at each\npre-training step, an agent must determine what hyperparameters to use towards\noptimizing the pre-training objective. We propose a Thompson sampling bandit\nalgorithm, based on a surrogate Gaussian process reward model of the MLM\npre-training objective, for its sequential minimization. We empirically show\nhow the proposed Gaussian process based Thompson sampling pre-trains robust and\nwell-performing language models. Namely, by sequentially selecting masking\nhyperparameters of the TLM, we achieve satisfactory performance in less epochs,\nnot only in terms of the pre-training MLM objective, but in diverse downstream\nfine-tuning tasks. The proposed bandit-based technique provides an automated\nhyperparameter selection method for pre-training TLMs of interest to\npractitioners. In addition, our results indicate that, instead of MLM\npre-training with fixed masking probabilities, sequentially adapting the\nmasking hyperparameters improves both pre-training loss and downstream task\nmetrics.\n","authors":["Iñigo Urteaga","Moulay-Zaïdane Draïdia","Tomer Lancewicki","Shahram Khadivi"],"pdf_url":"https://arxiv.org/pdf/2203.13151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13131v1","updated":"2022-03-24T15:44:50Z","published":"2022-03-24T15:44:50Z","title":"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors","summary":"  Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.\n","authors":["Oran Gafni","Adam Polyak","Oron Ashual","Shelly Sheynin","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2203.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13112v1","updated":"2022-03-24T15:11:06Z","published":"2022-03-24T15:11:06Z","title":"minicons: Enabling Flexible Behavioral and Representational Analyses of\n  Transformer Language Models","summary":"  We present minicons, an open source library that provides a standard API for\nresearchers interested in conducting behavioral and representational analyses\nof transformer-based language models (LMs). Specifically, minicons enables\nresearchers to apply analysis methods at two levels: (1) at the prediction\nlevel -- by providing functions to efficiently extract word/sentence level\nprobabilities; and (2) at the representational level -- by also facilitating\nefficient extraction of word/phrase level vectors from one or more layers. In\nthis paper, we describe the library and apply it to two motivating case\nstudies: One focusing on the learning dynamics of the BERT architecture on\nrelative grammatical judgments, and the other on benchmarking 23 different LMs\non zero-shot abductive reasoning. minicons is available at\nhttps://github.com/kanishkamisra/minicons\n","authors":["Kanishka Misra"],"pdf_url":"https://arxiv.org/pdf/2203.13112v1.pdf","comment":"To be submitted; Code to reproduce experiments can be found on\n  https://github.com/kanishkamisra/minicons-experiments"},{"id":"http://arxiv.org/abs/2203.13088v1","updated":"2022-03-24T14:28:07Z","published":"2022-03-24T14:28:07Z","title":"Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized\n  Late Interactions using Enhanced Reduction","summary":"  Recent progress in neural information retrieval has demonstrated large gains\nin effectiveness, while often sacrificing the efficiency and interpretability\nof the neural model compared to classical approaches. This paper proposes\nColBERTer, a neural retrieval model using contextualized late interaction\n(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,\nColBERTer's reductions dramatically lower ColBERT's storage requirements while\nsimultaneously improving the interpretability of its token-matching scores. To\nthis end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and\noptional lexical matching components into one model. For its multi-vector\ncomponent, ColBERTer reduces the number of stored vectors per document by\nlearning unique whole-word representations for the terms in each document and\nlearning to identify and remove word representations that are not essential to\neffective scoring. We employ an explicit multi-task, multi-stage training to\nfacilitate using very small vector dimensions. Results on the MS MARCO and\nTREC-DL collection show that ColBERTer can reduce the storage footprint by up\nto 2.5x, while maintaining effectiveness. With just one dimension per token in\nits smallest setting, ColBERTer achieves index storage parity with the\nplaintext size, with very strong effectiveness results. Finally, we demonstrate\nColBERTer's robustness on seven high-quality out-of-domain collections,\nyielding statistically significant gains over traditional retrieval baselines.\n","authors":["Sebastian Hofstätter","Omar Khattab","Sophia Althammer","Mete Sertkan","Allan Hanbury"],"pdf_url":"https://arxiv.org/pdf/2203.13088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13064v1","updated":"2022-03-24T13:18:36Z","published":"2022-03-24T13:18:36Z","title":"Ensembling and Knowledge Distilling of Large Sequence Taggers for\n  Grammatical Error Correction","summary":"  In this paper, we investigate improvements to the GEC sequence tagging\narchitecture with a focus on ensembling of recent cutting-edge\nTransformer-based encoders in Large configurations. We encourage ensembling\nmodels by majority votes on span-level edits because this approach is tolerant\nto the model architecture and vocabulary size. Our best ensemble achieves a new\nSOTA result with an $F_{0.5}$ score of 76.05 on BEA-2019 (test), even without\npre-training on synthetic datasets. In addition, we perform knowledge\ndistillation with a trained ensemble to generate new synthetic training\ndatasets, \"Troy-Blogs\" and \"Troy-1BW\". Our best single sequence tagging model\nthat is pretrained on the generated Troy-datasets in combination with the\npublicly available synthetic PIE dataset achieves a near-SOTA (To the best of\nour knowledge, our best single model gives way only to much heavier T5 model\nresult with an $F_{0.5}$ score of 73.21 on BEA-2019 (test). The code, datasets,\nand trained models are publicly available).\n","authors":["Maksym Tarnavskyi","Artem Chernodub","Kostiantyn Omelianchuk"],"pdf_url":"https://arxiv.org/pdf/2203.13064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12998v1","updated":"2022-03-24T11:45:44Z","published":"2022-03-24T11:45:44Z","title":"Kratt: Developing an Automatic Subject Indexing Tool for The National\n  Library of Estonia","summary":"  Manual subject indexing in libraries is a time-consuming and costly process\nand the quality of the assigned subjects is affected by the cataloguer's\nknowledge on the specific topics contained in the book. Trying to solve these\nissues, we exploited the opportunities arising from artificial intelligence to\ndevelop Kratt: a prototype of an automatic subject indexing tool. Kratt is able\nto subject index a book independent of its extent and genre with a set of\nkeywords present in the Estonian Subject Thesaurus. It takes Kratt\napproximately 1 minute to subject index a book, outperforming humans 10-15\ntimes. Although the resulting keywords were not considered satisfactory by the\ncataloguers, the ratings of a small sample of regular library users showed more\npromise. We also argue that the results can be enhanced by including a bigger\ncorpus for training the model and applying more careful preprocessing\ntechniques.\n","authors":["Marit Asula","Jane Makke","Linda Freienthal","Hele-Andra Kuulmets","Raul Sirel"],"pdf_url":"https://arxiv.org/pdf/2203.12998v1.pdf","comment":"This is a preprint version. It has 12 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2110.07474v2","updated":"2022-03-24T11:36:37Z","published":"2021-10-14T15:48:03Z","title":"MReD: A Meta-Review Dataset for Controllable Text Generation","summary":"  When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.\n","authors":["Chenhui Shen","Liying Cheng","Ran Zhou","Lidong Bing","Yang You","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2110.07474v2.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.12990v1","updated":"2022-03-24T11:29:20Z","published":"2022-03-24T11:29:20Z","title":"Generating Scientific Claims for Zero-Shot Scientific Fact Checking","summary":"  Automated scientific fact checking is difficult due to the complexity of\nscientific language and a lack of significant amounts of training data, as\nannotation requires domain expertise. To address this challenge, we propose\nscientific claim generation, the task of generating one or more atomic and\nverifiable claims from scientific sentences, and demonstrate its usefulness in\nzero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new\nsupervised method for generating claims supported by the literature, as well as\nKBIN, a novel method for generating claim negations. Additionally, we adapt an\nexisting unsupervised entity-centric method of claim generation to biomedical\nclaims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking\ndemonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN,\nachieve up to 90% performance of fully supervised models trained on manually\nannotated claims and evidence. A rigorous evaluation study demonstrates\nsignificant improvement in generated claim and negation quality over existing\nbaselines\n","authors":["Dustin Wright","David Wadden","Kyle Lo","Bailey Kuehl","Arman Cohan","Isabelle Augenstein","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12990v1.pdf","comment":"Accepted to ACL 2022; 13 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2203.12971v1","updated":"2022-03-24T10:21:07Z","published":"2022-03-24T10:21:07Z","title":"Probing for Labeled Dependency Trees","summary":"  Probing has become an important tool for analyzing representations in Natural\nLanguage Processing (NLP). For graphical NLP tasks such as dependency parsing,\nlinear probes are currently limited to extracting undirected or unlabeled parse\ntrees which do not capture the full task. This work introduces DepProbe, a\nlinear probe which can extract labeled and directed dependency parse trees from\nembeddings while using fewer parameters and compute than prior methods.\nLeveraging its full task coverage and lightweight parametrization, we\ninvestigate its predictive power for selecting the best transfer language for\ntraining a full biaffine attention parser. Across 13 languages, our proposed\nmethod identifies the best source treebank 94% of the time, outperforming\ncompetitive baselines and prior work. Finally, we analyze the informativeness\nof task-specific subspaces in contextual embeddings as well as which benefits a\nfull parser's non-linear parametrization provides.\n","authors":["Max Müller-Eberstein","Rob van der Goot","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2203.12971v1.pdf","comment":"Accepted at ACL 2022 (Main Conference)"},{"id":"http://arxiv.org/abs/2109.10294v4","updated":"2022-03-24T10:02:00Z","published":"2021-09-21T16:13:29Z","title":"DeepSTL -- From English Requirements to Signal Temporal Logic","summary":"  Formal methods provide very powerful tools and techniques for the design and\nanalysis of complex systems. Their practical application remains however\nlimited, due to the widely accepted belief that formal methods require\nextensive expertise and a steep learning curve. Writing correct formal\nspecifications in form of logical formulas is still considered to be a\ndifficult and error prone task.\n  In this paper we propose DeepSTL, a tool and technique for the translation of\ninformal requirements, given as free English sentences, into Signal Temporal\nLogic (STL), a formal specification language for cyber-physical systems, used\nboth by academia and advanced research labs in industry. A major challenge to\ndevise such a translator is the lack of publicly available informal\nrequirements and formal specifications. We propose a two-step workflow to\naddress this challenge. We first design a grammar-based generation technique of\nsynthetic data, where each output is a random STL formula and its associated\nset of possible English translations. In the second step, we use a\nstate-of-the-art transformer-based neural translation technique, to train an\naccurate attentional translator of English to STL. The experimental results\nshow high translation quality for patterns of English requirements that have\nbeen well trained, making this workflow promising to be extended for processing\nmore complex translation tasks.\n","authors":["Jie He","Ezio Bartocci","Dejan Ničković","Haris Isakovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2109.10294v4.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2203.12949v1","updated":"2022-03-24T09:24:39Z","published":"2022-03-24T09:24:39Z","title":"Duality-Induced Regularizer for Semantic Matching Knowledge Graph\n  Embeddings","summary":"  Semantic matching models -- which assume that entities with similar semantics\nhave similar embeddings -- have shown great power in knowledge graph embeddings\n(KGE). Many existing semantic matching models use inner products in embedding\nspaces to measure the plausibility of triples and quadruples in static and\ntemporal knowledge graphs. However, vectors that have the same inner products\nwith another vector can still be orthogonal to each other, which implies that\nentities with similar semantics may have dissimilar embeddings. This property\nof inner products significantly limits the performance of semantic matching\nmodels. To address this challenge, we propose a novel regularizer -- namely,\nDUality-induced RegulArizer (DURA) -- which effectively encourages the entities\nwith similar semantics to have similar embeddings. The major novelty of DURA is\nbased on the observation that, for an existing semantic matching KGE model\n(primal), there is often another distance based KGE model (dual) closely\nassociated with it, which can be used as effective constraints for entity\nembeddings. Experiments demonstrate that DURA consistently and significantly\nimproves the performance of state-of-the-art semantic matching models on both\nstatic and temporal knowledge graph benchmarks.\n","authors":["Jie Wang","Zhanqiu Zhang","Zhihao Shi","Jianyu Cai","Shuiwang Ji","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2203.12949v1.pdf","comment":"Accepted to TPAMI. This work is a journal extension of our NeurIPS'20\n  paper arXiv:2011.05816"},{"id":"http://arxiv.org/abs/2203.12942v1","updated":"2022-03-24T09:08:05Z","published":"2022-03-24T09:08:05Z","title":"Generating Data to Mitigate Spurious Correlations in Natural Language\n  Inference Datasets","summary":"  Natural language processing models often exploit spurious correlations\nbetween task-independent features and labels in datasets to perform well only\nwithin the distributions they are trained on, while not generalising to\ndifferent task distributions. We propose to tackle this problem by generating a\ndebiased version of a dataset, which can then be used to train a debiased,\noff-the-shelf model, by simply replacing its training data. Our approach\nconsists of 1) a method for training data generators to generate high-quality,\nlabel-consistent data samples; and 2) a filtering mechanism for removing data\npoints that contribute to spurious correlations, measured in terms of\nz-statistics. We generate debiased versions of the SNLI and MNLI datasets, and\nwe evaluate on a large suite of debiased, out-of-distribution, and adversarial\ntest sets. Results show that models trained on our debiased datasets generalise\nbetter than those trained on the original datasets in all settings. On the\nmajority of the datasets, our method outperforms or performs comparably to\nprevious state-of-the-art debiasing strategies, and when combined with an\northogonal technique, product-of-experts, it improves further and outperforms\nprevious best results of SNLI-hard and MNLI-hard.\n","authors":["Yuxiang Wu","Matt Gardner","Pontus Stenetorp","Pradeep Dasigi"],"pdf_url":"https://arxiv.org/pdf/2203.12942v1.pdf","comment":"Accepted to ACL 2022 main conference"},{"id":"http://arxiv.org/abs/2203.12940v1","updated":"2022-03-24T09:04:52Z","published":"2022-03-24T09:04:52Z","title":"mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot\n  Filling","summary":"  Zero-shot slot filling has received considerable attention to cope with the\nproblem of limited available data for the target domain. One of the important\nfactors in zero-shot learning is to make the model learn generalized and\nreliable representations. For this purpose, we present mcBERT, which stands for\nmomentum contrastive learning with BERT, to develop a robust zero-shot slot\nfilling model. mcBERT uses BERT to initialize the two encoders, the query\nencoder and key encoder, and is trained by applying momentum contrastive\nlearning. Our experimental results on the SNIPS benchmark show that mcBERT\nsubstantially outperforms the previous models, recording a new\nstate-of-the-art. Besides, we also show that each component composing mcBERT\ncontributes to the performance improvement.\n","authors":["Seong-Hwan Heo","WonKee Lee","Jong-Hyeok Lee"],"pdf_url":"https://arxiv.org/pdf/2203.12940v1.pdf","comment":"Submitted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2203.10430v3","updated":"2022-03-24T08:56:02Z","published":"2022-03-20T02:28:25Z","title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation\n  in Mandarin","summary":"  Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.\n","authors":["Yi-Chang Chen","Yu-Chuan Chang","Yen-Cheng Chang","Yi-Ren Yeh"],"pdf_url":"https://arxiv.org/pdf/2203.10430v3.pdf","comment":"submitted to Insterspeech 2022"},{"id":"http://arxiv.org/abs/2203.12926v1","updated":"2022-03-24T08:16:04Z","published":"2022-03-24T08:16:04Z","title":"Multitasking Framework for Unsupervised Simple Definition Generation","summary":"  The definition generation task can help language learners by providing\nexplanations for unfamiliar words. This task has attracted much attention in\nrecent years. We propose a novel task of Simple Definition Generation (SDG) to\nhelp language learners and low literacy readers. A significant challenge of\nthis task is the lack of learner's dictionaries in many languages, and\ntherefore the lack of data for supervised training. We explore this task and\npropose a multitasking framework SimpDefiner that only requires a standard\ndictionary with complex definitions and a corpus containing arbitrary simple\ntexts. We disentangle the complexity factors from the text by carefully\ndesigning a parameter sharing scheme between two decoders. By jointly training\nthese components, the framework can generate both complex and simple\ndefinitions simultaneously. We demonstrate that the framework can generate\nrelevant, simple definitions for the target words through automatic and manual\nevaluations on English and Chinese datasets. Our method outperforms the\nbaseline model by a 1.77 SARI score on the English dataset, and raises the\nproportion of the low level (HSK level 1-3) words in Chinese definitions by\n3.87%.\n","authors":["Cunliang Kong","Yun Chen","Hengyuan Zhang","Liner Yang","Erhong Yang"],"pdf_url":"https://arxiv.org/pdf/2203.12926v1.pdf","comment":"Accepted by ACL 2022 (main conference)"},{"id":"http://arxiv.org/abs/2106.15231v3","updated":"2022-03-24T08:13:32Z","published":"2021-06-29T10:27:01Z","title":"Exploring the Efficacy of Automatically Generated Counterfactuals for\n  Sentiment Analysis","summary":"  While state-of-the-art NLP models have been achieving the excellent\nperformance of a wide range of tasks in recent years, important questions are\nbeing raised about their robustness and their underlying sensitivity to\nsystematic biases that may exist in their training and test data. Such issues\ncome to be manifest in performance problems when faced with out-of-distribution\ndata in the field. One recent solution has been to use counterfactually\naugmented datasets in order to reduce any reliance on spurious patterns that\nmay exist in the original data. Producing high-quality augmented data can be\ncostly and time-consuming as it usually needs to involve human feedback and\ncrowdsourcing efforts. In this work, we propose an alternative by describing\nand evaluating an approach to automatically generating counterfactual data for\ndata augmentation and explanation. A comprehensive evaluation on several\ndifferent datasets and using a variety of state-of-the-art benchmarks\ndemonstrate how our approach can achieve significant improvements in model\nperformance when compared to models training on the original data and even when\ncompared to models trained with the benefit of human-generated augmented data.\n","authors":["Linyi Yang","Jiazheng Li","Pádraig Cunningham","Yue Zhang","Barry Smyth","Ruihai Dong"],"pdf_url":"https://arxiv.org/pdf/2106.15231v3.pdf","comment":"Accepted to ACL-21"},{"id":"http://arxiv.org/abs/2203.12918v1","updated":"2022-03-24T08:12:57Z","published":"2022-03-24T08:12:57Z","title":"A Rationale-Centric Framework for Human-in-the-loop Machine Learning","summary":"  We present a novel rationale-centric framework with human-in-the-loop --\nRationales-centric Double-robustness Learning (RDL) -- to boost model\nout-of-distribution performance in few-shot learning scenarios. By using static\nsemi-factual generation and dynamic human-intervened correction, RDL exploits\nrationales (i.e. phrases that cause the prediction), human interventions and\nsemi-factual augmentations to decouple spurious associations and bias models\ntowards generally applicable underlying distributions, which enables fast and\naccurate generalisation. Experimental results show that RDL leads to\nsignificant prediction benefits on both in-distribution and out-of-distribution\ntests compared to many state-of-the-art benchmarks -- especially for few-shot\nlearning scenarios. We also perform extensive ablation studies to support\nin-depth analyses of each component in our framework.\n","authors":["Jinghui Lu","Linyi Yang","Brian Mac Namee","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.12918v1.pdf","comment":"Accepted to ACL 2022"},{"id":"http://arxiv.org/abs/2203.12907v1","updated":"2022-03-24T07:50:41Z","published":"2022-03-24T07:50:41Z","title":"Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named\n  Entity Recognition","summary":"  Named entity recognition (NER) is the process of recognising and classifying\nimportant information (entities) in text. Proper nouns, such as a person's\nname, an organization's name, or a location's name, are examples of entities.\nThe NER is one of the important modules in applications like human resources,\ncustomer support, search engines, content classification, and academia. In this\nwork, we consider NER for low-resource Indian languages like Hindi and Marathi.\nThe transformer-based models have been widely used for NER tasks. We consider\ndifferent variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark\nthem on publicly available Hindi and Marathi NER datasets. We provide an\nexhaustive comparison of different monolingual and multilingual\ntransformer-based models and establish simple baselines currently missing in\nthe literature. We show that the monolingual MahaRoBERTa model performs the\nbest for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for\nHindi NER. We also perform cross-language evaluation and present mixed\nobservations.\n","authors":["Onkar Litake","Maithili Sabane","Parth Patil","Aparna Ranade","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2203.12907v1.pdf","comment":"Accepted at ICMISC 2022"},{"id":"http://arxiv.org/abs/2203.12906v1","updated":"2022-03-24T07:50:25Z","published":"2022-03-24T07:50:25Z","title":"Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some\n  benchmarks","summary":"  The Donate Speech campaign has so far succeeded in gathering approximately\n3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta\n(Donate Speech) corpus. The corpus includes over twenty thousand speakers from\nall the regions of Finland and from all age brackets. The primary goals of the\ncollection were to create a representative, large-scale resource to study\nspontaneous spoken Finnish and to accelerate the development of language\ntechnology and speech-based services. In this paper, we present the collection\nprocess and the collected corpus, and showcase its versatility through multiple\nuse cases. The evaluated use cases include: automatic speech recognition of\nspontaneous speech, detection of age, gender, dialect and topic and metadata\nanalysis. We provide benchmarks for the use cases, as well down loadable,\ntrained baseline systems with open-source code for reproducibility. One further\nuse case is to verify the metadata and transcripts given in this corpus itself,\nand to suggest artificial metadata and transcripts for the part of the corpus\nwhere it is missing.\n","authors":["Anssi Moisio","Dejan Porjazovski","Aku Rouhe","Yaroslav Getman","Anja Virkkunen","Tamás Grósz","Krister Lindén","Mikko Kurimo"],"pdf_url":"https://arxiv.org/pdf/2203.12906v1.pdf","comment":"Submitted to Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2203.12886v1","updated":"2022-03-24T07:15:24Z","published":"2022-03-24T07:15:24Z","title":"Automatic Speech recognition for Speech Assessment of Preschool Children","summary":"  The acoustic and linguistic features of preschool speech are investigated in\nthis study to design an automated speech recognition (ASR) system. Acoustic\nfluctuation has been highlighted as a significant barrier to developing\nhigh-performance ASR applications for youngsters. Because of the epidemic,\npreschool speech assessment should be conducted online. Accordingly, there is a\nneed for an automatic speech recognition system. We were confronted with new\nchallenges in our cognitive system, including converting meaningless words from\nspeech to text and recognizing word sequence. After testing and experimenting\nwith several models we obtained a 3.1\\% phoneme error rate in Persian. Wav2Vec\n2.0 is a paradigm that could be used to build a robust end-to-end speech\nrecognition system.\n","authors":["Amirhossein Abaskohi","Fatemeh Mortazavi","Hadi Moradi"],"pdf_url":"https://arxiv.org/pdf/2203.12886v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.12881v1","updated":"2022-03-24T06:48:56Z","published":"2022-03-24T06:48:56Z","title":"Can Unsupervised Knowledge Transfer from Social Discussions Help\n  Argument Mining?","summary":"  Identifying argument components from unstructured texts and predicting the\nrelationships expressed among them are two primary steps of argument mining.\nThe intrinsic complexity of these tasks demands powerful learning models. While\npretrained Transformer-based Language Models (LM) have been shown to provide\nstate-of-the-art results over different NLP tasks, the scarcity of manually\nannotated data and the highly domain-dependent nature of argumentation restrict\nthe capabilities of such models. In this work, we propose a novel transfer\nlearning strategy to overcome these challenges. We utilize argumentation-rich\nsocial discussions from the ChangeMyView subreddit as a source of unsupervised,\nargumentative discourse-aware knowledge by finetuning pretrained LMs on a\nselectively masked language modeling task. Furthermore, we introduce a novel\nprompt-based strategy for inter-component relation prediction that compliments\nour proposed finetuning method while leveraging on the discourse context.\nExhaustive experiments show the generalization capability of our method on\nthese two tasks over within-domain as well as out-of-domain datasets,\noutperforming several existing and employed strong baselines.\n","authors":["Subhabrata Dutta","Jeevesh Juneja","Dipankar Das","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2203.12881v1.pdf","comment":"Accepted in ACL 2022"},{"id":"http://arxiv.org/abs/2203.12865v1","updated":"2022-03-24T06:05:28Z","published":"2022-03-24T06:05:28Z","title":"Multilingual CheckList: Generation and Evaluation","summary":"  The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation\nof NLP systems has revealed high failure rates for basic capabilities for\nmultiple state-of-the-art and commercial models. However, the CheckList\ncreation process is manual which creates a bottleneck towards creation of\nmultilingual CheckLists catering 100s of languages. In this work, we explore\nmultiple approaches to generate and evaluate the quality of Multilingual\nCheckList. We device an algorithm -- Automated Multilingual Checklist\nGeneration (AMCG) for automatically transferring a CheckList from a source to a\ntarget language that relies on a reasonable machine translation system. We then\ncompare the CheckList generated by AMCG with CheckLists generated with\ndifferent levels of human intervention. Through in-depth crosslingual\nexperiments between English and Hindi, and broad multilingual experiments\nspanning 11 languages, we show that the automatic approach can provide accurate\nestimates of failure rates of a model across capabilities, as would a\nhuman-verified CheckList, and better than CheckLists generated by humans from\nscratch.\n","authors":["Karthikeyan K","Shaily Bhatt","Pankaj Singh","Somak Aditya","Sandipan Dandapat","Sunayana Sitaram","Monojit Choudhary"],"pdf_url":"https://arxiv.org/pdf/2203.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.12373v2","updated":"2022-03-24T04:59:12Z","published":"2021-06-23T13:12:40Z","title":"PALRACE: Reading Comprehension Dataset with Human Data and Labeled\n  Rationales","summary":"  Pre-trained language models achieves high performance on machine reading\ncomprehension (MRC) tasks but the results are hard to explain. An appealing\napproach to make models explainable is to provide rationales for its decision.\nTo investigate whether human rationales can further improve current models and\nto facilitate supervised learning of human rationales, here we present PALRACE\n(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for\n800 passages selected from the RACE dataset. We further classified the question\nto each passage into 6 types. Each passage was read by at least 26 human\nreaders, who labeled their rationales to answer the question. It is\ndemonstrated that models such as RoBERTa-large outperforms human readers in all\n6 types of questions, including inference questions, but its performance can be\nfurther improved when having access to the human rationales. Simpler models and\npre-trained models that are not fine-tuned based on the task benefit more from\nhuman rationales, and their performance can be boosted by more than 30% by\nrationales. With access to human rationales, a simple model based on the GloVe\nword embedding can reach the performance of BERT-base.\n","authors":["Jiajie Zou","Yuran Zhang","Peiqing Jin","Cheng Luo","Xunyi Pan","Nai Ding"],"pdf_url":"https://arxiv.org/pdf/2106.12373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.05917v2","updated":"2022-03-24T04:49:39Z","published":"2021-12-11T05:32:09Z","title":"Show and Write: Entity-aware Article Generation with Image Information","summary":"  Many vision-language applications contain long articles of text paired with\nimages (e.g., news or Wikipedia articles). Prior work learning to encode and/or\ngenerate these articles has primarily focused on understanding the article\nitself and some related metadata like the title or date it was written.\nHowever, the images and their captions or alt-text often contain crucial\ninformation such as named entities that are difficult to be correctly\nrecognized and predicted by language models. To address this shortcoming, this\npaper introduces an ENtity-aware article Generation method with Image\niNformation, ENGIN, to incorporate an article's image information into language\nmodels. ENGIN represents articles that can be conditioned on metadata used by\nprior work and information such as captions and named entities extracted from\nimages. Our key contribution is a novel Entity-aware mechanism to help our\nmodel better recognize and predict the entity names in articles. We perform\nexperiments on three public datasets, GoodNews, VisualNews, and WikiText.\nQuantitative results show that our approach improves generated article\nperplexity by 4-5 points over the base models. Qualitative results demonstrate\nthe text generated by ENGIN is more consistent with embedded article images. We\nalso perform article quality annotation experiments on the generated articles\nto validate that our model produces higher-quality articles. Finally, we\ninvestigate the effect ENGIN has on methods that automatically detect\nmachine-generated articles.\n","authors":["Zhongping Zhang","Yiwen Gu","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2112.05917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10750v2","updated":"2022-03-24T03:57:17Z","published":"2022-03-21T06:42:44Z","title":"WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses","summary":"  In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. Both quantitative and qualitative evaluation results demonstrate\nthe effectiveness of WeSinger in terms of accuracy and naturalness, and\nWeSinger achieves state-of-the-art performance on the public corpus Opencpop.\nSome synthesized singing samples are available online\n(https://zzw922cn.github.io/WeSinger/).\n","authors":["Zewang Zhang","Yibin Zheng","Xinhui Li","Li Lu"],"pdf_url":"https://arxiv.org/pdf/2203.10750v2.pdf","comment":"Submitted to InterSpeech2022"},{"id":"http://arxiv.org/abs/2203.12257v2","updated":"2022-03-24T03:27:52Z","published":"2022-03-23T08:07:32Z","title":"IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument\n  Mining Tasks","summary":"  Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.\n","authors":["Liying Cheng","Lidong Bing","Ruidan He","Qian Yu","Yan Zhang","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2203.12257v2.pdf","comment":"11 pages, 3 figures, accepted by ACL 2022"},{"id":"http://arxiv.org/abs/2203.12815v1","updated":"2022-03-24T02:33:30Z","published":"2022-03-24T02:33:30Z","title":"Revisiting the Effects of Leakage on Dependency Parsing","summary":"  Recent work by S{\\o}gaard (2020) showed that, treebank size aside, overlap\nbetween training and test graphs (termed leakage) explains more of the observed\nvariation in dependency parsing performance than other explanations. In this\nwork we revisit this claim, testing it on more models and languages. We find\nthat it only holds for zero-shot cross-lingual settings. We then propose a more\nfine-grained measure of such leakage which, unlike the original measure, not\nonly explains but also correlates with observed performance variation. Code and\ndata are available here: https://github.com/miriamwanner/reu-nlp-project\n","authors":["Nathaniel Krasner","Miriam Wanner","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2203.12815v1.pdf","comment":"to be presented at ACL'22 Findings"},{"id":"http://arxiv.org/abs/2011.02320v4","updated":"2022-03-24T02:32:15Z","published":"2020-11-04T14:39:41Z","title":"PCP Theorems, SETH and More: Towards Proving Sub-linear Time\n  Inapproximability","summary":"  In this paper we propose the PCP-like theorem for sub-linear time\ninapproximability. Abboud et al. have devised the distributed PCP framework for\nsub-quadratic time inapproximability. We show that the distributed PCP theorem\ncan be generalized for proving arbitrary polynomial time inapproximability, but\nfails in the linear case. We prove the sub-linear PCP theorem by adapting from\nan MA-protocol for the Set Containment problem, and show how to use the theorem\nto prove both existing and new inapproximability results, exhibiting the power\nof the sub-linear PCP theorem. Considering the emerging research works on\nsub-linear time algorithms, the sub-linear PCP theorem is important in guiding\nthe research in sub-linear time approximation algorithms.\n","authors":["Hengzhao Ma","Jianzhong Li"],"pdf_url":"https://arxiv.org/pdf/2011.02320v4.pdf","comment":"This paper is an old version of another paper submitted to arxiv,\n  with id 2107.01520. Moreover, this paper contains mistakes. Theorem 5.1 in\n  this paper is wrong. The reason is that Merlin can make Alice believe q\\in S\n  by sending another q'\\ne q but q'\\in S. Another player Bob should be included\n  in the communication protocol to avoid this situation. The paper 2107.01520\n  fixed this problem"},{"id":"http://arxiv.org/abs/2203.12813v1","updated":"2022-03-24T02:24:39Z","published":"2022-03-24T02:24:39Z","title":"Disentangleing Content and Fine-grained Prosody Information via Hybrid\n  ASR Bottleneck Features for Voice Conversion","summary":"  Non-parallel data voice conversion (VC) have achieved considerable\nbreakthroughs recently through introducing bottleneck features (BNFs) extracted\nby the automatic speech recognition(ASR) model. However, selection of BNFs have\na significant impact on VC result. For example, when extracting BNFs from ASR\ntrained with Cross Entropy loss (CE-BNFs) and feeding into neural network to\ntrain a VC system, the timbre similarity of converted speech is significantly\ndegraded. If BNFs are extracted from ASR trained using Connectionist Temporal\nClassification loss (CTC-BNFs), the naturalness of the converted speech may\ndecrease. This phenomenon is caused by the difference of information contained\nin BNFs. In this paper, we proposed an any-to-one VC method using hybrid\nbottleneck features extracted from CTC-BNFs and CE-BNFs to complement each\nother advantages. Gradient reversal layer and instance normalization were used\nto extract prosody information from CE-BNFs and content information from\nCTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate\nhigh-quality waveform. Experimental results show that our proposed method\nachieves higher similarity, naturalness, quality than baseline method and\nreveals the differences between the information contained in CE-BNFs and\nCTC-BNFs as well as the influence they have on the converted speech.\n","authors":["Xintao Zhao","Feng Liu","Changhe Song","Zhiyong Wu","Shiyin Kang","Deyi Tuo","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2203.12813v1.pdf","comment":"Accepted by ICASSP 2022"},{"id":"http://arxiv.org/abs/2102.00225v7","updated":"2022-03-24T01:18:01Z","published":"2021-01-30T13:13:50Z","title":"Learning From Human Correction For Data-Centric Deep Learning","summary":"  In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% accuracy is trained on the corrected dataset, which improve the baseline\nfrom 83.3% to 91.7%.\n","authors":["Tong Guo"],"pdf_url":"https://arxiv.org/pdf/2102.00225v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12788v1","updated":"2022-03-24T01:09:46Z","published":"2022-03-24T01:09:46Z","title":"Evaluating Distributional Distortion in Neural Language Modeling","summary":"  A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n","authors":["Benjamin LeBrun","Alessandro Sordoni","Timothy J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2203.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12781v1","updated":"2022-03-24T00:36:59Z","published":"2022-03-24T00:36:59Z","title":"Classifying Cyber-Risky Clinical Notes by Employing Natural Language\n  Processing","summary":"  Clinical notes, which can be embedded into electronic medical records,\ndocument patient care delivery and summarize interactions between healthcare\nproviders and patients. These clinical notes directly inform patient care and\ncan also indirectly inform research and quality/safety metrics, among other\nindirect metrics. Recently, some states within the United States of America\nrequire patients to have open access to their clinical notes to improve the\nexchange of patient information for patient care. Thus, developing methods to\nassess the cyber risks of clinical notes before sharing and exchanging data is\ncritical. While existing natural language processing techniques are geared to\nde-identify clinical notes, to the best of our knowledge, few have focused on\nclassifying sensitive-information risk, which is a fundamental step toward\ndeveloping effective, widespread protection of patient health information. To\nbridge this gap, this research investigates methods for identifying\nsecurity/privacy risks within clinical notes. The classification either can be\nused upstream to identify areas within notes that likely contain sensitive\ninformation or downstream to improve the identification of clinical notes that\nhave not been entirely de-identified. We develop several models using unigram\nand word2vec features with different classifiers to categorize sentence risk.\nExperiments on i2b2 de-identification dataset show that the SVM classifier\nusing word2vec features obtained a maximum F1-score of 0.792. Future research\ninvolves articulation and differentiation of risk in terms of different global\nregulatory requirements.\n","authors":["Suzanna Schmeelk","Martins Samuel Dogo","Yifan Peng","Braja Gopal Patra"],"pdf_url":"https://arxiv.org/pdf/2203.12781v1.pdf","comment":"7 pages, 4 figures, published in Proceedings of the 55th Hawaii\n  International Conference on System Sciences"},{"id":"http://arxiv.org/abs/2203.08411v2","updated":"2022-03-24T00:17:27Z","published":"2022-03-16T06:02:02Z","title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document\n  Information Extraction","summary":"  Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.\n","authors":["Chen-Yu Lee","Chun-Liang Li","Timothy Dozat","Vincent Perot","Guolong Su","Nan Hua","Joshua Ainslie","Renshen Wang","Yasuhisa Fujii","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2203.08411v2.pdf","comment":"Accepted to ACL 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2203.13254v1","updated":"2022-03-24T17:59:49Z","published":"2022-03-24T17:59:49Z","title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for\n  Monocular Object Pose Estimation","summary":"  Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, so that\n2D-3D point correspondences can be partly learned by backpropagating the\ngradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D\npoints from scratch fails to converge with existing approaches, since the\ndeterministic pose is inherently non-differentiable. In this paper, we propose\nthe EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,\nwhich outputs a distribution of pose on the SE(3) manifold, essentially\nbringing categorical Softmax to the continuous domain. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle unifies the existing approaches and\nresembles the attention mechanism. EPro-PnP significantly outperforms\ncompetitive baselines, closing the gap between PnP-based method and the\ntask-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D\nobject detection benchmarks.\n","authors":["Hansheng Chen","Pichao Wang","Fan Wang","Wei Tian","Lu Xiong","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2203.13254v1.pdf","comment":"Accepted at CVPR 2022"},{"id":"http://arxiv.org/abs/2203.13253v1","updated":"2022-03-24T17:59:20Z","published":"2022-03-24T17:59:20Z","title":"Video Instance Segmentation via Multi-scale Spatio-temporal Split\n  Attention Transformer","summary":"  State-of-the-art transformer-based video instance segmentation (VIS)\napproaches typically utilize either single-scale spatio-temporal features or\nper-frame multi-scale features during the attention computations. We argue that\nsuch an attention computation ignores the multi-scale spatio-temporal feature\nrelationships that are crucial to tackle target appearance deformations in\nvideos. To address this issue, we propose a transformer-based VIS framework,\nnamed MS-STS VIS, that comprises a novel multi-scale spatio-temporal split\n(MS-STS) attention module in the encoder. The proposed MS-STS module\neffectively captures spatio-temporal feature relationships at multiple scales\nacross frames in a video. We further introduce an attention block in the\ndecoder to enhance the temporal consistency of the detected instances in\ndifferent frames of a video. Moreover, an auxiliary discriminator is introduced\nduring training to ensure better foreground-background separability within the\nmulti-scale spatio-temporal feature space. We conduct extensive experiments on\ntwo benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves\nstate-of-the-art performance on both benchmarks. When using the ResNet50\nbackbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best\nreported results in literature by 2.7 % and by 4.8 % at higher overlap\nthreshold of AP_75, while being comparable in model size and speed on\nYoutube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS\nachieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models\nare available at https://github.com/OmkarThawakar/MSSTS-VIS.\n","authors":["Omkar Thawakar","Sanath Narayan","Jiale Cao","Hisham Cholakkal","Rao Muhammad Anwer","Muhammad Haris Khan","Salman Khan","Michael Felsberg","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2203.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13251v1","updated":"2022-03-24T17:58:54Z","published":"2022-03-24T17:58:54Z","title":"Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient\n  Dexterous Manipulation","summary":"  Optimizing behaviors for dexterous manipulation has been a longstanding\nchallenge in robotics, with a variety of methods from model-based control to\nmodel-free reinforcement learning having been previously explored in\nliterature. Perhaps one of the most powerful techniques to learn complex\nmanipulation strategies is imitation learning. However, collecting and learning\nfrom demonstrations in dexterous manipulation is quite challenging. The\ncomplex, high-dimensional action-space involved with multi-finger control often\nleads to poor sample efficiency of learning-based methods. In this work, we\npropose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning\nframework for dexterous manipulation. DIME only requires a single RGB camera to\nobserve a human operator and teleoperate our robotic hand. Once demonstrations\nare collected, DIME employs standard imitation learning methods to train\ndexterous manipulation policies. On both simulation and real robot benchmarks\nwe demonstrate that DIME can be used to solve complex, in-hand manipulation\ntasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro\nhand. Our framework along with pre-collected demonstrations is publicly\navailable at https://nyu-robot-learning.github.io/dime.\n","authors":["Sridhar Pandian Arunachalam","Sneha Silwal","Ben Evans","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2203.13251v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2203.13250v1","updated":"2022-03-24T17:58:04Z","published":"2022-03-24T17:58:04Z","title":"Global Tracking Transformers","summary":"  We present a novel transformer-based architecture for global multi-object\ntracking. Our network takes a short sequence of frames as input and produces\nglobal trajectories for all objects. The core component is a global tracking\ntransformer that operates on objects from all frames in the sequence. The\ntransformer encodes object features from all frames, and uses trajectory\nqueries to group them into trajectories. The trajectory queries are object\nfeatures from a single frame and naturally produce unique trajectories. Our\nglobal tracking transformer does not require intermediate pairwise grouping or\ncombinatorial association, and can be jointly trained with an object detector.\nIt achieves competitive performance on the popular MOT17 benchmark, with 75.3\nMOTA and 59.1 HOTA. More importantly, our framework seamlessly integrates into\nstate-of-the-art large-vocabulary detectors to track any objects. Experiments\non the challenging TAO dataset show that our framework consistently improves\nupon baselines that are based on pairwise association, outperforming published\nworks by a significant 7.7 tracking mAP. Code is available at\nhttps://github.com/xingyizhou/GTR.\n","authors":["Xingyi Zhou","Tianwei Yin","Vladlen Koltun","Phillip Krähenbühl"],"pdf_url":"https://arxiv.org/pdf/2203.13250v1.pdf","comment":"CVPR 2022. Code is available at https://github.com/xingyizhou/GTR"},{"id":"http://arxiv.org/abs/2203.13249v1","updated":"2022-03-24T17:57:29Z","published":"2022-03-24T17:57:29Z","title":"BigDetection: A Large-scale Benchmark for Improved Object Detector\n  Pre-training","summary":"  Multiple datasets and open challenges for object detection have been\nintroduced in recent years. To build more general and powerful object detection\nsystems, in this paper, we construct a new large-scale benchmark termed\nBigDetection. Our goal is to simply leverage the training data from existing\ndatasets (LVIS, OpenImages and Object365) with carefully designed principles,\nand curate a larger dataset for improved detector pre-training. Specifically,\nwe generate a new taxonomy which unifies the heterogeneous label spaces from\ndifferent sources. Our BigDetection dataset has 600 object categories and\ncontains over 3.4M training images with 36M bounding boxes. It is much larger\nin multiple dimensions than previous benchmarks, which offers both\nopportunities and challenges. Extensive experiments demonstrate its validity as\na new benchmark for evaluating different object detection methods, and its\neffectiveness as a pre-training dataset.\n","authors":["Likun Cai","Zhi Zhang","Yi Zhu","Li Zhang","Mu Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2203.13249v1.pdf","comment":"Technical report, code is released at\n  https://github.com/amazon-research/bigdetection"},{"id":"http://arxiv.org/abs/2203.13248v1","updated":"2022-03-24T17:57:11Z","published":"2022-03-24T17:57:11Z","title":"Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer","summary":"  Recent studies on StyleGAN show high performance on artistic portrait\ngeneration by transfer learning with limited data. In this paper, we explore\nmore challenging exemplar-based high-resolution portrait style transfer by\nintroducing a novel DualStyleGAN with flexible control of dual styles of the\noriginal face domain and the extended artistic portrait domain. Different from\nStyleGAN, DualStyleGAN provides a natural way of style transfer by\ncharacterizing the content and style of a portrait with an intrinsic style path\nand a new extrinsic style path, respectively. The delicately designed extrinsic\nstyle path enables our model to modulate both the color and complex structural\nstyles hierarchically to precisely pastiche the style example. Furthermore, a\nnovel progressive fine-tuning scheme is introduced to smoothly transform the\ngenerative space of the model to the target domain, even with the above\nmodifications on the network architecture. Experiments demonstrate the\nsuperiority of DualStyleGAN over state-of-the-art methods in high-quality\nportrait style transfer and flexible style control.\n","authors":["Shuai Yang","Liming Jiang","Ziwei Liu","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2203.13248v1.pdf","comment":"CVPR 2022. Code: https://github.com/williamyang1991/DualStyleGAN\n  Project page: https://www.mmlab-ntu.com/project/dualstylegan/"},{"id":"http://arxiv.org/abs/2203.13241v1","updated":"2022-03-24T17:51:02Z","published":"2022-03-24T17:51:02Z","title":"VRNet: Learning the Rectified Virtual Corresponding Points for 3D Point\n  Cloud Registration","summary":"  3D point cloud registration is fragile to outliers, which are labeled as the\npoints without corresponding points. To handle this problem, a widely adopted\nstrategy is to estimate the relative pose based only on some accurate\ncorrespondences, which is achieved by building correspondences on the\nidentified inliers or by selecting reliable ones. However, these approaches are\nusually complicated and time-consuming. By contrast, the virtual point-based\nmethods learn the virtual corresponding points (VCPs) for all source points\nuniformly without distinguishing the outliers and the inliers. Although this\nstrategy is time-efficient, the learned VCPs usually exhibit serious collapse\ndegeneration due to insufficient supervision and the inherent distribution\nlimitation. In this paper, we propose to exploit the best of both worlds and\npresent a novel robust 3D point cloud registration framework. We follow the\nidea of the virtual point-based methods but learn a new type of virtual points\ncalled rectified virtual corresponding points (RCPs), which are defined as the\npoint set with the same shape as the source and with the same pose as the\ntarget. Hence, a pair of consistent point clouds, i.e. source and RCPs, is\nformed by rectifying VCPs to RCPs (VRNet), through which reliable\ncorrespondences between source and RCPs can be accurately obtained. Since the\nrelative pose between source and RCPs is the same as the relative pose between\nsource and target, the input point clouds can be registered naturally.\nSpecifically, we first construct the initial VCPs by using an estimated soft\nmatching matrix to perform a weighted average on the target points. Then, we\ndesign a correction-walk module to learn an offset to rectify VCPs to RCPs,\nwhich effectively breaks the distribution limitation of VCPs. Finally, we\ndevelop a hybrid loss function to enforce the shape and geometry structure\nconsistency ...\n","authors":["Zhiyuan Zhang","Jiadai Sun","Yuchao Dai","Bin Fan","Mingyi He"],"pdf_url":"https://arxiv.org/pdf/2203.13241v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology"},{"id":"http://arxiv.org/abs/2203.13239v1","updated":"2022-03-24T17:50:19Z","published":"2022-03-24T17:50:19Z","title":"A Representation Separation Perspective to Correspondences-free\n  Unsupervised 3D Point Cloud Registration","summary":"  3D point cloud registration in remote sensing field has been greatly advanced\nby deep learning based methods, where the rigid transformation is either\ndirectly regressed from the two point clouds (correspondences-free approaches)\nor computed from the learned correspondences (correspondences-based\napproaches). Existing correspondences-free methods generally learn the holistic\nrepresentation of the entire point cloud, which is fragile for partial and\nnoisy point clouds. In this paper, we propose a correspondences-free\nunsupervised point cloud registration (UPCR) method from the representation\nseparation perspective. First, we model the input point cloud as a combination\nof pose-invariant representation and pose-related representation. Second, the\npose-related representation is used to learn the relative pose wrt a \"latent\ncanonical shape\" for the source and target point clouds respectively. Third,\nthe rigid transformation is obtained from the above two learned relative poses.\nOur method not only filters out the disturbance in pose-invariant\nrepresentation but also is robust to partial-to-partial point clouds or noise.\nExperiments on benchmark datasets demonstrate that our unsupervised method\nachieves comparable if not better performance than state-of-the-art supervised\nregistration methods.\n","authors":["Zhiyuan Zhang","Jiadai Sun","Yuchao Dai","Dingfu Zhou","Xibin Song","Mingyi He"],"pdf_url":"https://arxiv.org/pdf/2203.13239v1.pdf","comment":"Accepted by IEEE Geoscience and Remote Sensing Letters"},{"id":"http://arxiv.org/abs/2203.13238v1","updated":"2022-03-24T17:49:38Z","published":"2022-03-24T17:49:38Z","title":"Open-set Recognition via Augmentation-based Similarity Learning","summary":"  The primary assumption of conventional supervised learning or classification\nis that the test samples are drawn from the same distribution as the training\nsamples, which is called closed set learning or classification. In many\npractical scenarios, this is not the case because there are unknowns or unseen\nclass samples in the test data, which is called the open set scenario, and the\nunknowns need to be detected. This problem is referred to as the open set\nrecognition problem and is important in safety-critical applications. We\npropose to detect unknowns (or unseen class samples) through learning pairwise\nsimilarities. The proposed method works in two steps. It first learns a closed\nset classifier using the seen classes that have appeared in training and then\nlearns how to compare seen classes with pseudo-unseen (automatically generated\nunseen class samples). The pseudo-unseen generation is carried out by\nperforming distribution shifting augmentations on the seen or training samples.\nWe call our method OPG (Open set recognition based on Pseudo unseen data\nGeneration). The experimental evaluation shows that the learned\nsimilarity-based features can successfully distinguish seen from unseen in\nbenchmark datasets for open set recognition.\n","authors":["Sepideh Esmaeilpour","Lei shu","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2203.13238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13235v1","updated":"2022-03-24T17:47:56Z","published":"2022-03-24T17:47:56Z","title":"Facial Expression Recognition based on Multi-head Cross Attention\n  Network","summary":"  Facial expression in-the-wild is essential for various interactive computing\ndomains. In this paper, we proposed an extended version of DAN model to address\nthe VA estimation and facial expression challenges introduced in ABAW 2022. Our\nmethod produced preliminary results of 0.44 of mean CCC value for the VA\nestimation task, and 0.33 of the average F1 score for the expression\nclassification task.\n","authors":["Jae-Yeop Jeong","Yeong-Gi Hong","Daun Kim","Yuchul Jung","Jin-Woo Jeong"],"pdf_url":"https://arxiv.org/pdf/2203.13235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.06825v2","updated":"2022-03-24T17:33:07Z","published":"2021-12-13T17:35:26Z","title":"VL-Adapter: Parameter-Efficient Transfer Learning for\n  Vision-and-Language Tasks","summary":"  Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.\n","authors":["Yi-Lin Sung","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2112.06825v2.pdf","comment":"CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)"},{"id":"http://arxiv.org/abs/2203.13215v1","updated":"2022-03-24T17:11:31Z","published":"2022-03-24T17:11:31Z","title":"Neural Neighbor Style Transfer","summary":"  We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers\nstate-of-the-art quality, generalization, and competitive efficiency for\nartistic style transfer. Our approach is based on explicitly replacing neural\nfeatures extracted from the content input (to be stylized) with those from a\nstyle exemplar, then synthesizing the final output based on these rearranged\nfeatures. While the spirit of our approach is similar to prior work, we show\nthat our design decisions dramatically improve the final visual quality.\n","authors":["Nicholas Kolkin","Michal Kucera","Sylvain Paris","Daniel Sykora","Eli Shechtman","Greg Shakhnarovich"],"pdf_url":"https://arxiv.org/pdf/2203.13215v1.pdf","comment":"Code for NNST-Opt available at\n  https://github.com/nkolkin13/NeuralNeighborStyleTransfer"},{"id":"http://arxiv.org/abs/2203.13214v1","updated":"2022-03-24T17:10:26Z","published":"2022-03-24T17:10:26Z","title":"A Perturbation Constrained Adversarial Attack for Evaluating the\n  Robustness of Optical Flow","summary":"  Recent optical flow methods are almost exclusively judged in terms of\naccuracy, while analyzing their robustness is often neglected. Although\nadversarial attacks offer a useful tool to perform such an analysis, current\nattacks on optical flow methods rather focus on real-world attacking scenarios\nthan on a worst case robustness assessment. Hence, in this work, we propose a\nnovel adversarial attack - the Perturbation Constrained Flow Attack (PCFA) -\nthat emphasizes destructivity over applicability as a real-world attack. More\nprecisely, PCFA is a global attack that optimizes adversarial perturbations to\nshift the predicted flow towards a specified target flow, while keeping the L2\nnorm of the perturbation below a chosen bound. Our experiments not only\ndemonstrate PCFA's applicability in white- and black-box settings, but also\nshow that it finds stronger adversarial samples for optical flow than previous\nattacking frameworks. Moreover, based on these strong samples, we provide the\nfirst common ranking of optical flow methods in the literature considering both\nprediction quality and adversarial robustness, indicating that high quality\nmethods are not necessarily robust. Our source code will be publicly available.\n","authors":["Jenny Schmalfuss","Philipp Scholze","Andrés Bruhn"],"pdf_url":"https://arxiv.org/pdf/2203.13214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13185v1","updated":"2022-03-24T17:02:43Z","published":"2022-03-24T17:02:43Z","title":"Quantum Motion Segmentation","summary":"  Motion segmentation is a challenging problem that seeks to identify\nindependent motions in two or several input images. This paper introduces the\nfirst algorithm for motion segmentation that relies on adiabatic quantum\noptimization of the objective function. The proposed method achieves on-par\nperformance with the state of the art on problem instances which can be mapped\nto modern quantum annealers.\n","authors":["Federica Arrigoni","Willi Menapace","Marcel Seelbach Benkner","Elisa Ricci","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2203.13185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.11795v2","updated":"2022-03-24T16:59:34Z","published":"2021-06-20T20:12:41Z","title":"DeepMesh: Differentiable Iso-Surface Extraction","summary":"  Geometric Deep Learning has recently made striking progress with the advent\nof continuous deep implicit fields. They allow for detailed modeling of\nwatertight surfaces of arbitrary topology while not relying on a 3D Euclidean\ngrid, resulting in a learnable parameterization that is unlimited in\nresolution.\n  Unfortunately, these methods are often unsuitable for applications that\nrequire an explicit mesh-based surface representation because converting an\nimplicit field to such a representation relies on the Marching Cubes algorithm,\nwhich cannot be differentiated with respect to the underlying implicit field.\n  In this work, we remove this limitation and introduce a differentiable way to\nproduce explicit surface mesh representations from Deep Implicit Fields. Our\nkey insight is that by reasoning on how implicit field perturbations impact\nlocal surface geometry, one can ultimately differentiate the 3D location of\nsurface samples with respect to the underlying deep implicit field. We exploit\nthis to define DeepMesh - an end-to-end differentiable mesh representation that\ncan vary its topology.\n  We validate our theoretical insight through several applications: Single view\n3D Reconstruction via Differentiable Rendering, Physically-Driven Shape\nOptimization, Full Scene 3D Reconstruction from Scans and End-to-End Training.\nIn all cases our end-to-end differentiable parameterization gives us an edge\nover state-of-the-art algorithms.\n","authors":["Benoit Guillard","Edoardo Remelli","Artem Lukoianov","Stephan R. Richter","Timur Bagautdinov","Pierre Baque","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2106.11795v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2006.03997"},{"id":"http://arxiv.org/abs/2203.13167v1","updated":"2022-03-24T16:40:36Z","published":"2022-03-24T16:40:36Z","title":"Towards Exemplar-Free Continual Learning in Vision Transformers: an\n  Account of Attention, Functional and Weight Regularization","summary":"  In this paper, we investigate the continual learning of Vision Transformers\n(ViT) for the challenging exemplar-free scenario, with special focus on how to\nefficiently distill the knowledge of its crucial self-attention mechanism\n(SAM). Our work takes an initial step towards a surgical investigation of SAM\nfor designing coherent continual learning methods in ViTs. We first carry out\nan evaluation of established continual learning regularization techniques. We\nthen examine the effect of regularization when applied to two key enablers of\nSAM: (a) the contextualized embedding layers, for their ability to capture\nwell-scaled representations with respect to the values, and (b) the prescaled\nattention maps, for carrying value-independent global contextual information.\nWe depict the perks of each distilling strategy on two image recognition\nbenchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall\naccuracy, (b) helps enhance the rigidity by maintaining competitive\nperformances. Furthermore, we identify the limitation imposed by the symmetric\nnature of regularization losses. To alleviate this, we propose an asymmetric\nvariant and apply it to the pooled output distillation (POD) loss adapted for\nViTs. Our experiments confirm that introducing asymmetry to POD boosts its\nplasticity while retaining stability across (a) and (b). Moreover, we\nacknowledge low forgetting measures for all the compared methods, indicating\nthat ViTs might be naturally inclined continual learner\n","authors":["Francesco Pelosin","Saurav Jha","Andrea Torsello","Bogdan Raducanu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2203.13167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13166v1","updated":"2022-03-24T16:38:54Z","published":"2022-03-24T16:38:54Z","title":"Self-supervised Video-centralised Transformer for Video Face Clustering","summary":"  This paper presents a novel method for face clustering in videos using a\nvideo-centralised transformer. Previous works often employed contrastive\nlearning to learn frame-level representation and used average pooling to\naggregate the features along the temporal dimension. This approach may not\nfully capture the complicated video dynamics. In addition, despite the recent\nprogress in video-based contrastive learning, few have attempted to learn a\nself-supervised clustering-friendly face representation that benefits the video\nface clustering task. To overcome these limitations, our method employs a\ntransformer to directly learn video-level representations that can better\nreflect the temporally-varying property of faces in videos, while we also\npropose a video-centralised self-supervised framework to train the transformer\nmodel. We also investigate face clustering in egocentric videos, a\nfast-emerging field that has not been studied yet in works related to face\nclustering. To this end, we present and release the first large-scale\negocentric video face clustering dataset named EasyCom-Clustering. We evaluate\nour proposed method on both the widely used Big Bang Theory (BBT) dataset and\nthe new EasyCom-Clustering dataset. Results show the performance of our\nvideo-centralised transformer has surpassed all previous state-of-the-art\nmethods on both benchmarks, exhibiting a self-attentive understanding of face\nvideos.\n","authors":["Yujiang Wang","Mingzhi Dong","Jie Shen","Yiming Luo","Yiming Lin","Pingchuan Ma","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2203.13166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08368v2","updated":"2022-03-24T16:36:47Z","published":"2022-03-16T03:23:50Z","title":"Mixed-Precision Neural Network Quantization via Learned Layer-wise\n  Importance","summary":"  The exponentially large discrete search space in mixed-precision quantization\n(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous\nworks usually resort to iterative search methods on the training set, which\nconsume hundreds or even thousands of GPU-hours. In this study, we reveal that\nsome unique learnable parameters in quantization, namely the scale factors in\nthe quantizer, can serve as importance indicators of a layer, reflecting the\ncontribution of that layer to the final accuracy at certain bit-widths. These\nimportance indicators naturally perceive the numerical transformation during\nquantization-aware training, which can precisely and correctly provide\nquantization sensitivity metrics of layers. However, a deep network always\ncontains hundreds of such indicators, and training them one by one would lead\nto an excessive time cost. To overcome this issue, we propose a joint training\nscheme that can obtain all indicators at once. It considerably speeds up the\nindicators training process by parallelizing the original sequential training\nprocesses. With these learned importance indicators, we formulate the MPQ\nsearch problem as a one-time integer linear programming (ILP) problem. That\navoids the iterative search and significantly reduces search time without\nlimiting the bit-width search space. For example, MPQ search on ResNet18 with\nour indicators takes only 0.06 seconds. Also, extensive experiments show our\napproach can achieve SOTA accuracy on ImageNet for far-ranging models with\nvarious constraints (e.g., BitOps, compress rate).\n","authors":["Chen Tang","Kai Ouyang","Zhi Wang","Yifei Zhu","Yaowei Wang","Wen Ji","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.08368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.06016v2","updated":"2022-03-24T16:35:49Z","published":"2021-11-11T01:58:44Z","title":"Synthetic Document Generator for Annotation-free Layout Recognition","summary":"  Analyzing the layout of a document to identify headers, sections, tables,\nfigures etc. is critical to understanding its content. Deep learning based\napproaches for detecting the layout structure of document images have been\npromising. However, these methods require a large number of annotated examples\nduring training, which are both expensive and time consuming to obtain. We\ndescribe here a synthetic document generator that automatically produces\nrealistic documents with labels for spatial positions, extents and categories\nof the layout elements. The proposed generative process treats every physical\ncomponent of a document as a random variable and models their intrinsic\ndependencies using a Bayesian Network graph. Our hierarchical formulation using\nstochastic templates allow parameter sharing between documents for retaining\nbroad themes and yet the distributional characteristics produces visually\nunique samples, thereby capturing complex and diverse layouts. We empirically\nillustrate that a deep layout detection model trained purely on the synthetic\ndocuments can match the performance of a model that uses real documents.\n","authors":["Natraj Raman","Sameena Shah","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2111.06016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13161v1","updated":"2022-03-24T16:33:29Z","published":"2022-03-24T16:33:29Z","title":"Learning Hierarchical Cross-Modal Association for Co-Speech Gesture\n  Generation","summary":"  Generating speech-consistent body and gesture movements is a long-standing\nproblem in virtual avatar creation. Previous studies often synthesize pose\nmovement in a holistic manner, where poses of all joints are generated\nsimultaneously. Such a straightforward pipeline fails to generate fine-grained\nco-speech gestures. One observation is that the hierarchical semantics in\nspeech and the hierarchical structures of human gestures can be naturally\ndescribed into multiple granularities and associated together. To fully utilize\nthe rich connections between speech audio and human gestures, we propose a\nnovel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech\ngesture generation. In HA2G, a Hierarchical Audio Learner extracts audio\nrepresentations across semantic granularities. A Hierarchical Pose Inferer\nsubsequently renders the entire human pose gradually in a hierarchical manner.\nTo enhance the quality of synthesized gestures, we develop a contrastive\nlearning strategy based on audio-text alignment for better audio\nrepresentations. Extensive experiments and human evaluation demonstrate that\nthe proposed method renders realistic co-speech gestures and outperforms\nprevious methods in a clear margin. Project page:\nhttps://alvinliu0.github.io/projects/HA2G\n","authors":["Xian Liu","Qianyi Wu","Hang Zhou","Yinghao Xu","Rui Qian","Xinyi Lin","Xiaowei Zhou","Wayne Wu","Bo Dai","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.13161v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2022. Camera-Ready Version, 19 Pages"},{"id":"http://arxiv.org/abs/2203.13148v1","updated":"2022-03-24T16:06:31Z","published":"2022-03-24T16:06:31Z","title":"Physics-based Learning of Parameterized Thermodynamics from Real-time\n  Thermography","summary":"  Progress in automatic control of thermal processes has long been limited by\nthe difficulty of obtaining high-fidelity thermodynamic models. Traditionally,\nin complex thermodynamic systems, it is often infeasible to estimate the\nthermophysical parameters of spatiotemporally varying processes, forcing the\nadoption of model-free control architectures. This comes at the cost of losing\nany robustness guarantees, and implies a need for extensive real-life testing.\nIn recent years, however, infrared cameras and other thermographic equipment\nhave become readily applicable to these processes, allowing for a real-time,\nnon-invasive means of sensing the thermal state of a process. In this work, we\npresent a novel physics-based approach to learning a thermal process's dynamics\ndirectly from such real-time thermographic data, while focusing attention on\nregions with high thermal activity. We call this process, which applies to any\nhigher-dimensional scalar field, attention-based noise robust averaging (ANRA).\nGiven a partial-differential equation model structure, we show that our\napproach is robust against noise, and can be used to initialize optimization\nroutines to further refine parameter estimates. We demonstrate our method on\nseveral simulation examples, as well as by applying it to electrosurgical\nthermal response data on in vivo porcine skin tissue.\n","authors":["Hamza El-Kebir","Joseph Bentsman"],"pdf_url":"https://arxiv.org/pdf/2203.13148v1.pdf","comment":"Submitted to IEEE International Conference in Image Processing (ICIP)\n  2022"},{"id":"http://arxiv.org/abs/2112.10982v3","updated":"2022-03-24T15:49:07Z","published":"2021-12-21T04:44:57Z","title":"Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning","summary":"  Generalized few-shot semantic segmentation was introduced to move beyond only\nevaluating few-shot segmentation models on novel classes to include testing\ntheir ability to remember base classes. While the current state-of-the-art\napproach is based on meta-learning, it performs poorly and saturates in\nlearning after observing only a few shots. We propose the first fine-tuning\nsolution, and demonstrate that it addresses the saturation problem while\nachieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We\nalso show that it outperforms existing methods, whether fine-tuning multiple\nfinal layers or only the final layer. Finally, we present a triplet loss\nregularization that shows how to redistribute the balance of performance\nbetween novel and base categories so that there is a smaller gap between them.\n","authors":["Josh Myers-Dean","Yinan Zhao","Brian Price","Scott Cohen","Danna Gurari"],"pdf_url":"https://arxiv.org/pdf/2112.10982v3.pdf","comment":"Includes supplementary materials"},{"id":"http://arxiv.org/abs/2203.13131v1","updated":"2022-03-24T15:44:50Z","published":"2022-03-24T15:44:50Z","title":"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors","summary":"  Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.\n","authors":["Oran Gafni","Adam Polyak","Oron Ashual","Shelly Sheynin","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2203.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13122v1","updated":"2022-03-24T15:30:48Z","published":"2022-03-24T15:30:48Z","title":"Moving Window Regression: A Novel Approach to Ordinal Regression","summary":"  A novel ordinal regression algorithm, called moving window regression (MWR),\nis proposed in this paper. First, we propose the notion of relative rank\n($\\rho$-rank), which is a new order representation scheme for input and\nreference instances. Second, we develop global and local relative regressors\n($\\rho$-regressors) to predict $\\rho$-ranks within entire and specific rank\nranges, respectively. Third, we refine an initial rank estimate iteratively by\nselecting two reference instances to form a search window and then estimating\nthe $\\rho$-rank within the window. Extensive experiments results show that the\nproposed algorithm achieves the state-of-the-art performances on various\nbenchmark datasets for facial age estimation and historical color image\nclassification. The codes are available at https://github.com/nhshin-mcl/MWR.\n","authors":["Nyeong-Ho Shin","Seon-Ho Lee","Chang-Su Kim"],"pdf_url":"https://arxiv.org/pdf/2203.13122v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2112.09490v3","updated":"2022-03-24T15:28:39Z","published":"2021-12-17T13:00:37Z","title":"Visual Microfossil Identification via Deep Metric Learning","summary":"  We apply deep metric learning for the first time to the problem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualization options for\nhuman experts and cannot be applied to open-set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualization of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning outperforms all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Keycode, network weights, and data splits are\npublished with this paper for full reproducibility.\n","authors":["Tayfun Karaderi","Tilo Burghardt","Allison Y. Hsiang","Jacob Ramaer","Daniela N. Schmidt"],"pdf_url":"https://arxiv.org/pdf/2112.09490v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13120v1","updated":"2022-03-24T15:24:38Z","published":"2022-03-24T15:24:38Z","title":"Feature visualization for convolutional neural network models trained on\n  neuroimaging data","summary":"  A major prerequisite for the application of machine learning models in\nclinical decision making is trust and interpretability. Current explainability\nstudies in the neuroimaging community have mostly focused on explaining\nindividual decisions of trained models, e.g. obtained by a convolutional neural\nnetwork (CNN). Using attribution methods such as layer-wise relevance\npropagation or SHAP heatmaps can be created that highlight which regions of an\ninput are more relevant for the decision than others. While this allows the\ndetection of potential data set biases and can be used as a guide for a human\nexpert, it does not allow an understanding of the underlying principles the\nmodel has learned. In this study, we instead show, to the best of our\nknowledge, for the first time results using feature visualization of\nneuroimaging CNNs. Particularly, we have trained CNNs for different tasks\nincluding sex classification and artificial lesion classification based on\nstructural magnetic resonance imaging (MRI) data. We have then iteratively\ngenerated images that maximally activate specific neurons, in order to\nvisualize the patterns they respond to. To improve the visualizations we\ncompared several regularization strategies. The resulting images reveal the\nlearned concepts of the artificial lesions, including their shapes, but remain\nhard to interpret for abstract features in the sex classification task.\n","authors":["Fabian Eitel","Anna Melkonyan","Kerstin Ritter"],"pdf_url":"https://arxiv.org/pdf/2203.13120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13118v1","updated":"2022-03-24T15:18:57Z","published":"2022-03-24T15:18:57Z","title":"X-ray Dissectography Improves Lung Nodule Detection","summary":"  Although radiographs are the most frequently used worldwide due to their\ncost-effectiveness and widespread accessibility, the structural superposition\nalong the x-ray paths often renders suspicious or concerning lung nodules\ndifficult to detect. In this study, we apply \"X-ray dissectography\" to dissect\nlungs digitally from a few radiographic projections, suppress the interference\nof irrelevant structures, and improve lung nodule detectability. For this\npurpose, a collaborative detection network is designed to localize lung nodules\nin 2D dissected projections and 3D physical space. Our experimental results\nshow that our approach can significantly improve the average precision by 20+%\nin comparison with the common baseline that detects lung nodules from original\nprojections using a popular detection network. Potentially, this approach could\nhelp re-design the current X-ray imaging protocols and workflows and improve\nthe diagnostic performance of chest radiographs in lung diseases.\n","authors":["Chuang Niu","Giridhar Dasegowda","Pingkun Yan","Mannudeep K. Kalra","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2203.13118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13116v1","updated":"2022-03-24T15:16:05Z","published":"2022-03-24T15:16:05Z","title":"Egocentric Prediction of Action Target in 3D","summary":"  We are interested in anticipating as early as possible the target location of\na person's object manipulation action in a 3D workspace from egocentric vision.\nIt is important in fields like human-robot collaboration, but has not yet\nreceived enough attention from vision and learning communities. To stimulate\nmore research on this challenging egocentric vision task, we propose a large\nmultimodality dataset of more than 1 million frames of RGB-D and IMU streams,\nand provide evaluation metrics based on our high-quality 2D and 3D labels from\nsemi-automatic annotation. Meanwhile, we design baseline methods using\nrecurrent neural networks and conduct various ablation studies to validate\ntheir effectiveness. Our results demonstrate that this new task is worthy of\nfurther study by researchers in robotics, vision, and learning communities.\n","authors":["Yiming Li","Ziang Cao","Andrew Liang","Benjamin Liang","Luoyao Chen","Hang Zhao","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2203.13116v1.pdf","comment":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR)"},{"id":"http://arxiv.org/abs/2203.13104v1","updated":"2022-03-24T14:54:15Z","published":"2022-03-24T14:54:15Z","title":"R-DFCIL: Relation-Guided Representation Learning for Data-Free Class\n  Incremental Learning","summary":"  Class-Incremental Learning (CIL) struggles with catastrophic forgetting when\nlearning new knowledge, and Data-Free CIL (DFCIL) is even more challenging\nwithout access to the training data of previous classes. Though recent DFCIL\nworks introduce techniques such as model inversion to synthesize data for\nprevious classes, they fail to overcome forgetting due to the severe domain gap\nbetween the synthetic and real data. To address this issue, this paper proposes\nrelation-guided representation learning (RRL) for DFCIL, dubbed R-DFCIL. In\nRRL, we introduce relational knowledge distillation to flexibly transfer the\nstructural relation of new data from the old model to the current model. Our\nRRL-boosted DFCIL can guide the current model to learn representations of new\nclasses better compatible with representations of previous classes, which\ngreatly reduces forgetting while improving plasticity. To avoid the mutual\ninterference between representation and classifier learning, we employ local\nrather than global classification loss during RRL. After RRL, the\nclassification head is fine-tuned with global class-balanced classification\nloss to address the data imbalance issue as well as learn the decision boundary\nbetween new and previous classes. Extensive experiments on CIFAR100,\nTiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL significantly\nsurpasses previous approaches and achieves a new state-of-the-art performance\nfor DFCIL.\n","authors":["Qiankun Gao","Chen Zhao","Bernard Ghanem","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.13104v1.pdf","comment":"22 pages, 6 figures, submitted to the ECCV for possible publication"},{"id":"http://arxiv.org/abs/2203.13097v1","updated":"2022-03-24T14:44:56Z","published":"2022-03-24T14:44:56Z","title":"IA-FaceS, Bidirectional Method, Disentangled Attribute Manipulation,\n  Flexible Component Editing","summary":"  Semantic face editing has achieved substantial progress in recent years.\nKnown as a growingly popular method, latent space manipulation performs face\nediting by changing the latent code of an input face to liberate users from\npainting skills. However, previous latent space manipulation methods usually\nencode an entire face into a single low-dimensional embedding, which constrains\nthe reconstruction capacity and the control flexibility of facial components,\nsuch as eyes and nose. This paper proposes IA-FaceS as a bidirectional method\nfor disentangled face attribute manipulation as well as flexible, controllable\ncomponent editing without the need for segmentation masks or sketches in the\noriginal image. To strike a balance between the reconstruction capacity and the\ncontrol flexibility, the encoder is designed as a multi-head structure to yield\nembeddings for reconstruction and control, respectively: a high-dimensional\ntensor with spatial properties for consistent reconstruction and four\nlow-dimensional facial component embeddings for semantic face editing.\nManipulating the separate component embeddings can help achieve disentangled\nattribute manipulation and flexible control of facial components. To further\ndisentangle the highly-correlated components, a component adaptive modulation\n(CAM) module is proposed for the decoder. The semantic single-eye editing is\ndeveloped for the first time without any input visual guidance, such as\nsegmentation masks or sketches. According to the experimental results, IA-FaceS\nestablishes a good balance between maintaining image details and performing\nflexible face manipulation. Both quantitative and qualitative results indicate\nthat the proposed method outperforms the other techniques in reconstruction,\nface attribute manipulation, and component transfer.\n","authors":["Wenjing Huang","Shikui Tu","Lei Xu"],"pdf_url":"https://arxiv.org/pdf/2203.13097v1.pdf","comment":"68 pages, 33 figures"},{"id":"http://arxiv.org/abs/2203.13093v1","updated":"2022-03-24T14:36:18Z","published":"2022-03-24T14:36:18Z","title":"A Preliminary Research on Space Situational Awareness Based on Event\n  Cameras","summary":"  Event camera is a new type of sensor that is different from traditional\ncameras. Each pixel is triggered asynchronously by an event. The trigger event\nis the change of the brightness irradiated on the pixel. If the increment or\ndecrement is higher than a certain threshold, the event is output. Compared\nwith traditional cameras, event cameras have the advantages of high temporal\nresolution, low latency, high dynamic range, low bandwidth and low power\nconsumption. We carried out a series of observation experiments in a simulated\nspace lighting environment. The experimental results show that the event camera\ncan give full play to the above advantages in space situational awareness. This\narticle first introduces the basic principles of the event camera, then\nanalyzes its advantages and disadvantages, then introduces the observation\nexperiment and analyzes the experimental results, and finally, a workflow of\nspace situational awareness based on event cameras is given.\n","authors":["Kun Xiao","Pengju Li","Guohui Wang","Zhi Li","Yi Chen","Yongfeng Xie","Yuqiang Fang"],"pdf_url":"https://arxiv.org/pdf/2203.13093v1.pdf","comment":"arXiv admin note: text overlap with arXiv:1904.08405 by other authors"},{"id":"http://arxiv.org/abs/2202.13657v2","updated":"2022-03-24T14:32:41Z","published":"2022-02-28T10:01:22Z","title":"Avalanche RL: a Continual Reinforcement Learning Library","summary":"  Continual Reinforcement Learning (CRL) is a challenging setting where an\nagent learns to interact with an environment that is constantly changing over\ntime (the stream of experiences). In this paper, we describe Avalanche RL, a\nlibrary for Continual Reinforcement Learning which allows to easily train\nagents on a continuous stream of tasks. Avalanche RL is based on PyTorch and\nsupports any OpenAI Gym environment. Its design is based on Avalanche, one of\nthe more popular continual learning libraries, which allow us to reuse a large\nnumber of continual learning strategies and improve the interaction between\nreinforcement learning and continual learning researchers. Additionally, we\npropose Continual Habitat-Lab, a novel benchmark and a high-level library which\nenables the usage of the photorealistic simulator Habitat-Sim for CRL research.\nOverall, Avalanche RL attempts to unify under a common framework continual\nreinforcement learning applications, which we hope will foster the growth of\nthe field.\n","authors":["Nicolò Lucchesi","Antonio Carta","Vincenzo Lomonaco","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2202.13657v2.pdf","comment":"Presented at the 21st International Conference on Image Analysis and\n  Processing (ICIAP 2021)"},{"id":"http://arxiv.org/abs/2203.13090v1","updated":"2022-03-24T14:29:34Z","published":"2022-03-24T14:29:34Z","title":"AziNorm: Exploiting the Radial Symmetry of Point Cloud for\n  Azimuth-Normalized 3D Perception","summary":"  Studying the inherent symmetry of data is of great importance in machine\nlearning. Point cloud, the most important data format for 3D environmental\nperception, is naturally endowed with strong radial symmetry. In this work, we\nexploit this radial symmetry via a divide-and-conquer strategy to boost 3D\nperception performance and ease optimization. We propose Azimuth Normalization\n(AziNorm), which normalizes the point clouds along the radial direction and\neliminates the variability brought by the difference of azimuth. AziNorm can be\nflexibly incorporated into most LiDAR-based perception methods. To validate its\neffectiveness and generalization ability, we apply AziNorm in both object\ndetection and semantic segmentation. For detection, we integrate AziNorm into\ntwo representative detection methods, the one-stage SECOND detector and the\nstate-of-the-art two-stage PV-RCNN detector. Experiments on Waymo Open Dataset\ndemonstrate that AziNorm improves SECOND and PV-RCNN by 7.03 mAPH and 3.01 mAPH\nrespectively. For segmentation, we integrate AziNorm into KPConv. On\nSemanticKitti dataset, AziNorm improves KPConv by 1.6/1.1 mIoU on val/test set.\nBesides, AziNorm remarkably improves data efficiency and accelerates\nconvergence, reducing the requirement of data amounts or training epochs by an\norder of magnitude. SECOND w/ AziNorm can significantly outperform fully\ntrained vanilla SECOND, even trained with only 10% data or 10% epochs. Code and\nmodels are available at https://github.com/hustvl/AziNorm.\n","authors":["Shaoyu Chen","Xinggang Wang","Tianheng Cheng","Wenqiang Zhang","Qian Zhang","Chang Huang","Wenyu Liu"],"pdf_url":"https://arxiv.org/pdf/2203.13090v1.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2105.11111v4","updated":"2022-03-24T14:17:16Z","published":"2021-05-24T06:18:23Z","title":"Oriented RepPoints for Aerial Object Detection","summary":"  In contrast to the generic object, aerial targets are often non-axis aligned\nwith arbitrary orientations having the cluttered surroundings. Unlike the\nmainstreamed approaches regressing the bounding box orientations, this paper\nproposes an effective adaptive points learning approach to aerial object\ndetection by taking advantage of the adaptive points representation, which is\nable to capture the geometric information of the arbitrary-oriented instances.\nTo this end, three oriented conversion functions are presented to facilitate\nthe classification and localization with accurate orientation. Moreover, we\npropose an effective quality assessment and sample assignment scheme for\nadaptive points learning toward choosing the representative oriented reppoints\nsamples during training, which is able to capture the non-axis aligned features\nfrom adjacent objects or background noises. A spatial constraint is introduced\nto penalize the outlier points for roust adaptive learning. Experimental\nresults on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD\nand DIOR-R, demonstrate the efficacy of our proposed approach. The source code\nis availabel at: https://github.com/LiWentomng/OrientedRepPoints.\n","authors":["Wentong Li","Yijie Chen","Kaixuan Hu","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2105.11111v4.pdf","comment":"10 pages, 4 figures, Accepted by CVPR2022"},{"id":"http://arxiv.org/abs/2112.10652v2","updated":"2022-03-24T13:56:32Z","published":"2021-12-20T16:21:09Z","title":"HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D\n  Medical Image Segmentation using HyperNet","summary":"  Semantic segmentation of 3D medical images is a challenging task due to the\nhigh variability of the shape and pattern of objects (such as organs or\ntumors). Given the recent success of deep learning in medical image\nsegmentation, Neural Architecture Search (NAS) has been introduced to find\nhigh-performance 3D segmentation network architectures. However, because of the\nmassive computational requirements of 3D data and the discrete optimization\nnature of architecture search, previous NAS methods require a long search time\nor necessary continuous relaxation, and commonly lead to sub-optimal network\narchitectures. While one-shot NAS can potentially address these disadvantages,\nits application in the segmentation domain has not been well studied in the\nexpansive multi-scale multi-path search space. To enable one-shot NAS for\nmedical image segmentation, our method, named HyperSegNAS, introduces a\nHyperNet to assist super-net training by incorporating architecture topology\ninformation. Such a HyperNet can be removed once the super-net is trained and\nintroduces no overhead during architecture search. We show that HyperSegNAS\nyields better performing and more intuitive architectures compared to the\nprevious state-of-the-art (SOTA) segmentation networks; furthermore, it can\nquickly and accurately find good architecture candidates under different\ncomputing constraints. Our method is evaluated on public datasets from the\nMedical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.\n","authors":["Cheng Peng","Andriy Myronenko","Ali Hatamizadeh","Vish Nath","Md Mahfuzur Rahman Siddiquee","Yufan He","Daguang Xu","Rama Chellappa","Dong Yang"],"pdf_url":"https://arxiv.org/pdf/2112.10652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13072v1","updated":"2022-03-24T13:50:48Z","published":"2022-03-24T13:50:48Z","title":"Multitask Emotion Recognition Model with Knowledge Distillation and Task\n  Discriminator","summary":"  Due to the collection of big data and the development of deep learning,\nresearch to predict human emotions in the wild is being actively conducted. We\ndesigned a multi-task model using ABAW dataset to predict valence-arousal,\nexpression, and action unit through audio data and face images at in real\nworld. We trained model from the incomplete label by applying the knowledge\ndistillation technique. The teacher model was trained as a supervised learning\nmethod, and the student model was trained by using the output of the teacher\nmodel as a soft label. As a result we achieved 2.40 in Multi Task Learning task\nvalidation dataset.\n","authors":["Euiseok Jeong","Geesung Oh","Sejoon Lim"],"pdf_url":"https://arxiv.org/pdf/2203.13072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05151v4","updated":"2022-03-24T13:49:48Z","published":"2022-03-10T04:46:51Z","title":"Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity","summary":"  Current adversarial attack research reveals the vulnerability of\nlearning-based classifiers against carefully crafted perturbations. However,\nmost existing attack methods have inherent limitations in cross-dataset\ngeneralization as they rely on a classification layer with a closed set of\ncategories. Furthermore, the perturbations generated by these methods may\nappear in regions easily perceptible to the human visual system (HVS). To\ncircumvent the former problem, we propose a novel algorithm that attacks\nsemantic similarity on feature representations. In this way, we are able to\nfool classifiers without limiting attacks to a specific dataset. For\nimperceptibility, we introduce the low-frequency constraint to limit\nperturbations within high-frequency components, ensuring perceptual similarity\nbetween adversarial examples and originals. Extensive experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online\nplatforms indicate that our attack can yield misleading and transferable\nadversarial examples across architectures and datasets. Additionally,\nvisualization results and quantitative performance (in terms of four different\nmetrics) show that the proposed algorithm generates more imperceptible\nperturbations than the state-of-the-art methods. Code is made available at.\n","authors":["Cheng Luo","Qinliang Lin","Weicheng Xie","Bizhu Wu","Jinheng Xie","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2203.05151v4.pdf","comment":"CVPR 2022 conference (accepted), 18 pages, 17 figure"},{"id":"http://arxiv.org/abs/2203.13068v1","updated":"2022-03-24T13:46:25Z","published":"2022-03-24T13:46:25Z","title":"SIFT and SURF based feature extraction for the anomaly detection","summary":"  In this paper, we suggest a way, how to use SIFT and SURF algorithms to\nextract the image features for anomaly detection. We use those feature vectors\nto train various classifiers on a real-world dataset in the semi -supervised\n(with a small number of faulty samples) manner with a large number of\nclassifiers and in the one-class (with no faulty samples) manner using the SVDD\nand SVM classifier. We prove, that the SIFT and SURF algorithms could be used\nas feature extractors, that they could be used to train a semi-supervised and\none-class classifier with an accuracy around 89\\% and that the performance of\nthe one-class classifier could be comparable to the semi-supervised one. We\nalso made our dataset and source code publicly available.\n","authors":["Simon Bilik","Karel Horak"],"pdf_url":"https://arxiv.org/pdf/2203.13068v1.pdf","comment":"28th Conference STUDENT EEICT 2022, Brno University of Technology"},{"id":"http://arxiv.org/abs/2203.02284v2","updated":"2022-03-24T13:17:49Z","published":"2022-03-03T01:00:26Z","title":"Nuclei instance segmentation and classification in histopathology images\n  with StarDist","summary":"  Instance segmentation and classification of nuclei is an important task in\ncomputational pathology. We show that StarDist, a deep learning based nuclei\nsegmentation method originally developed for fluorescence microscopy, can be\nextended and successfully applied to histopathology images. This is\nsubstantiated by conducting experiments on the Lizard dataset, and through\nentering the Colon Nuclei Identification and Counting (CoNIC) challenge 2022.\nAt the end of the preliminary test phase of CoNIC, our approach ranked first on\nthe leaderboard for the segmentation and classification task.\n","authors":["Martin Weigert","Uwe Schmidt"],"pdf_url":"https://arxiv.org/pdf/2203.02284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13055v1","updated":"2022-03-24T13:06:43Z","published":"2022-03-24T13:06:43Z","title":"Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic\n  Memory","summary":"  Driving 3D characters to dance following a piece of music is highly\nchallenging due to the spatial constraints applied to poses by choreography\nnorms. In addition, the generated dance sequence also needs to maintain\ntemporal coherency with different music genres. To tackle these challenges, we\npropose a novel music-to-dance framework, Bailando, with two powerful\ncomponents: 1) a choreographic memory that learns to summarize meaningful\ndancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic\nGenerative Pre-trained Transformer (GPT) that composes these units to a fluent\ndance coherent to the music. With the learned choreographic memory, dance\ngeneration is realized on the quantized units that meet high choreography\nstandards, such that the generated dancing sequences are confined within the\nspatial constraints. To achieve synchronized alignment between diverse motion\ntempos and music beats, we introduce an actor-critic-based reinforcement\nlearning scheme to the GPT with a newly-designed beat-align reward function.\nExtensive experiments on the standard benchmark demonstrate that our proposed\nframework achieves state-of-the-art performance both qualitatively and\nquantitatively. Notably, the learned choreographic memory is shown to discover\nhuman-interpretable dancing-style poses in an unsupervised manner.\n","authors":["Li Siyao","Weijiang Yu","Tianpei Gu","Chunze Lin","Quan Wang","Chen Qian","Chen Change Loy","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2203.13055v1.pdf","comment":"Accpted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.13052v1","updated":"2022-03-24T13:01:53Z","published":"2022-03-24T13:01:53Z","title":"Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial\n  Expression Recognition","summary":"  Facial expression recognition plays an important role in human-computer\ninteraction. In this paper, we propose the Coarse-to-Fine Cascaded networks\nwith Smooth Predicting (CFC-SP) to improve the performance of facial expression\nrecognition. CFC-SP contains two core components, namely Coarse-to-Fine\nCascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups\nseveral similar emotions to form a rough category, and then employs a network\nto conduct a coarse but accurate classification. Later, Then, an additional\nnetwork for these grouped emotions is further used to obtain fine-grained\npredictions. For SP, it improves the recognition capability of the model by\ncapturing both universal and unique effective features. To be specific, the\nuniversal features denote the general characteristic of facial emotions and the\nunique features denote the specific characteristic of each facial expression.\nExperiments on Aff-Wild2 show the effectiveness of the proposed CFSP.\n","authors":["Fanglei Xue","Zichang Tan","Yu Zhu","Zhongsong Ma","Guodong Guo"],"pdf_url":"https://arxiv.org/pdf/2203.13052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13049v1","updated":"2022-03-24T12:55:23Z","published":"2022-03-24T12:55:23Z","title":"Compositional Temporal Grounding with Structured Variational Cross-Graph\n  Correspondence Learning","summary":"  Temporal grounding in videos aims to localize one target video segment that\nsemantically corresponds to a given query sentence. Thanks to the semantic\ndiversity of natural language descriptions, temporal grounding allows activity\ngrounding beyond pre-defined classes and has received increasing attention in\nrecent years. The semantic diversity is rooted in the principle of\ncompositionality in linguistics, where novel semantics can be systematically\ndescribed by combining known words in novel ways (compositional\ngeneralization). However, current temporal grounding datasets do not\nspecifically test for the compositional generalizability. To systematically\nmeasure the compositional generalizability of temporal grounding models, we\nintroduce a new Compositional Temporal Grounding task and construct two new\ndataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the\nstate-of-the-art methods on our new dataset splits, we empirically find that\nthey fail to generalize to queries with novel combinations of seen words. To\ntackle this challenge, we propose a variational cross-graph reasoning framework\nthat explicitly decomposes video and language into multiple structured\nhierarchies and learns fine-grained semantic correspondence among them.\nExperiments illustrate the superior compositional generalizability of our\napproach. The repository of this work is at https://github.com/YYJMJC/\nCompositional-Temporal-Grounding.\n","authors":["Juncheng Li","Junlin Xie","Long Qian","Linchao Zhu","Siliang Tang","Fei Wu","Yi Yang","Yueting Zhuang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2203.13049v1.pdf","comment":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2022"},{"id":"http://arxiv.org/abs/2010.00361v2","updated":"2022-03-24T12:55:15Z","published":"2020-10-01T12:46:38Z","title":"Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue","summary":"  A goal-oriented visual dialogue involves multi-turn interactions between two\nagents, Questioner and Oracle. During which, the answer given by Oracle is of\ngreat significance, as it provides golden response to what Questioner concerns.\nBased on the answer, Questioner updates its belief on target visual content and\nfurther raises another question. Notably, different answers drive into\ndifferent visual beliefs and future questions. However, existing methods always\nindiscriminately encode answers after much longer questions, resulting in a\nweak utilization of answers. In this paper, we propose an Answer-Driven Visual\nState Estimator (ADVSE) to impose the effects of different answers on visual\nstates. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture\nthe answer-driven effect on visual attention by sharpening question-related\nattention and adjusting it by answer-based logical operation at each turn. Then\nbased on the focusing attention, we get the visual state estimation by\nConditional Visual Information Fusion (CVIF), where overall information and\ndifference information are fused conditioning on the question-answer state. We\nevaluate the proposed ADVSE to both question generator and guesser tasks on the\nlarge-scale GuessWhat?! dataset and achieve the state-of-the-art performances\non both tasks. The qualitative results indicate that the ADVSE boosts the agent\nto generate highly efficient questions and obtains reliable visual attentions\nduring the reasonable question generation and guess processes.\n","authors":["Zipeng Xu","Fangxiang Feng","Xiaojie Wang","Yushu Yang","Huixing Jiang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2010.00361v2.pdf","comment":"Accepted at ACM International Conference on Multimedia (ACM MM 2020)"},{"id":"http://arxiv.org/abs/2111.13333v2","updated":"2022-03-24T12:52:41Z","published":"2021-11-26T06:49:26Z","title":"Predict, Prevent, and Evaluate: Disentangled Text-Driven Image\n  Manipulation Empowered by Pre-Trained Vision-Language Model","summary":"  To achieve disentangled image manipulation, previous works depend heavily on\nmanual annotation. Meanwhile, the available manipulations are limited to a\npre-defined set the models were trained for. We propose a novel framework,\ni.e., Predict, Prevent, and Evaluate (PPE), for disentangled text-driven image\nmanipulation that requires little manual annotation while being applicable to a\nwide variety of manipulations. Our method approaches the targets by deeply\nexploiting the power of the large-scale pre-trained vision-language model CLIP.\nConcretely, we firstly Predict the possibly entangled attributes for a given\ntext command. Then, based on the predicted attributes, we introduce an\nentanglement loss to Prevent entanglements during training. Finally, we propose\na new evaluation metric to Evaluate the disentangled image manipulation. We\nverify the effectiveness of our method on the challenging face editing task.\nExtensive experiments show that the proposed PPE framework achieves much better\nquantitative and qualitative results than the up-to-date StyleCLIP baseline.\n","authors":["Zipeng Xu","Tianwei Lin","Hao Tang","Fu Li","Dongliang He","Nicu Sebe","Radu Timofte","Luc Van Gool","Errui Ding"],"pdf_url":"https://arxiv.org/pdf/2111.13333v2.pdf","comment":"To appear in CVPR 2022"},{"id":"http://arxiv.org/abs/2203.13048v1","updated":"2022-03-24T12:51:10Z","published":"2022-03-24T12:51:10Z","title":"A Simulation Benchmark for Vision-based Autonomous Navigation","summary":"  This work introduces a simulator benchmark for vision-based autonomous\nnavigation. The simulator offers control over real world variables such as the\nenvironment, time of day, weather and traffic. The benchmark includes a modular\nintegration of different components of a full autonomous visual navigation\nstack. In the experimental part of the paper, state-of-the-art visual\nlocalization methods are evaluated as a part of the stack in realistic\nnavigation tasks. To the authors' best knowledge, the proposed benchmark is the\nfirst to study modern visual localization methods as part of a full autonomous\nvisual navigation stack.\n","authors":["Lauri Suomela","Atakan Dag","Harry Edelman","Joni-Kristian Kämäräinen"],"pdf_url":"https://arxiv.org/pdf/2203.13048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13046v1","updated":"2022-03-24T12:50:02Z","published":"2022-03-24T12:50:02Z","title":"Facial Action Unit Recognition With Multi-models Ensembling","summary":"  The Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition gives\nAffective Computing a large promotion. In this paper, we present our method of\nAU challenge in this Competition. We use improved IResnet100 as backbone. Then\nwe train AU dataset in Aff-Wild2 on three pertained models pretrained by our\nprivate au and expression dataset, and Glint360K respectively. Finally, we\nensemble the results of our models. We achieved F1 score (macro) 0.731 on AU\nvalidation set.\n","authors":["Wenqiang Jiang","Yannan Wu","Fengsheng Qiao","Liyu Meng","Yuanyuan Deng","Chuanhe Liu"],"pdf_url":"https://arxiv.org/pdf/2203.13046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.04386v4","updated":"2022-03-24T12:46:15Z","published":"2021-09-09T16:17:38Z","title":"ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions","summary":"  An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends also on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct and Pserf. Experiments suggest that the proposed functions improve the\nnetwork performance significantly compared to the widely used activations like\nReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and\n5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in\nCIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet\nV2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.\n","authors":["Koushik Biswas","Sandeep Kumar","Shilpak Banerjee","Ashish Kumar Pandey"],"pdf_url":"https://arxiv.org/pdf/2109.04386v4.pdf","comment":"AAAI 2022"},{"id":"http://arxiv.org/abs/2203.13038v1","updated":"2022-03-24T12:33:58Z","published":"2022-03-24T12:33:58Z","title":"Interpretable Prediction of Pulmonary Hypertension in Newborns using\n  Echocardiograms","summary":"  Pulmonary hypertension (PH) in newborns and infants is a complex condition\nassociated with several pulmonary, cardiac, and systemic diseases contributing\nto morbidity and mortality. Therefore, accurate and early detection of PH is\ncrucial for successful management. Using echocardiography, the primary\ndiagnostic tool in pediatrics, human assessment is both time-consuming and\nexpertise-demanding, raising the need for an automated approach. In this work,\nwe present an interpretable multi-view video-based deep learning approach to\npredict PH for a cohort of 194 newborns using echocardiograms. We use\nspatio-temporal convolutional architectures for the prediction of PH from each\nview, and aggregate the predictions of the different views using majority\nvoting. To the best of our knowledge, this is the first work for an automated\nassessment of PH in newborns using echocardiograms. Our results show a mean\nF1-score of 0.84 for severity prediction and 0.92 for binary detection using\n10-fold cross-validation. We complement our predictions with saliency maps and\nshow that the learned model focuses on clinically relevant cardiac structures,\nmotivating its usage in clinical practice.\n","authors":["Hanna Ragnarsdottir","Laura Manduchi","Holger Michel","Fabian Laumer","Sven Wellmann","Ece Ozkan","Julia Vogt"],"pdf_url":"https://arxiv.org/pdf/2203.13038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13032v1","updated":"2022-03-24T12:23:07Z","published":"2022-03-24T12:23:07Z","title":"Multi-modal Emotion Estimation for in-the-wild Videos","summary":"  In this paper, we briefly introduce our submission to the Valence-Arousal\nEstimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)\ncompetition. Our method utilizes the multi-modal information, i.e., the visual\nand audio information, and employs a temporal encoder to model the temporal\ncontext in the videos. Besides, a smooth processor is applied to get more\nreasonable predictions, and a model ensemble strategy is used to improve the\nperformance of our proposed method. The experiment results show that our method\nachieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation\nset of the Aff-Wild2 dataset, which prove the effectiveness of our proposed\nmethod.\n","authors":["Liyu Meng","Yuchen Liu","Xiaolong Liu","Zhaopei Huang","Wenqiang Jiang","Tenggan Zhang","Yuanyuan Deng","Ruichen Li","Yannan Wu","Jinming Zhao","Fengsheng Qiao","Qin Jin","Chuanhe Liu"],"pdf_url":"https://arxiv.org/pdf/2203.13032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.09136v2","updated":"2022-03-24T12:19:20Z","published":"2021-03-16T15:30:20Z","title":"QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small\n  Object Detection","summary":"  While general object detection with deep learning has achieved great success\nin the past few years, the performance and efficiency of detecting small\nobjects are far from satisfactory. The most common and effective way to promote\nsmall object detection is to use high-resolution images or feature maps.\nHowever, both approaches induce costly computation since the computational cost\ngrows squarely as the size of images and features increases. To get the best of\ntwo worlds, we propose QueryDet that uses a novel query mechanism to accelerate\nthe inference speed of feature-pyramid based object detectors. The pipeline\ncomposes two steps: it first predicts the coarse locations of small objects on\nlow-resolution features and then computes the accurate detection results using\nhigh-resolution features sparsely guided by those coarse positions. In this\nway, we can not only harvest the benefit of high-resolution feature maps but\nalso avoid useless computation for the background area. On the popular COCO\ndataset, the proposed method improves the detection mAP by 1.0 and mAP-small by\n2.0, and the high-resolution inference speed is improved to 3.0x on average. On\nVisDrone dataset, which contains more small objects, we create a new\nstate-of-the-art while gaining a 2.3x high-resolution acceleration on average.\nCode is available at https://github.com/ChenhongyiYang/QueryDet-PyTorch.\n","authors":["Chenhongyi Yang","Zehao Huang","Naiyan Wang"],"pdf_url":"https://arxiv.org/pdf/2103.09136v2.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.13031v1","updated":"2022-03-24T12:18:06Z","published":"2022-03-24T12:18:06Z","title":"Continuous Emotion Recognition using Visual-audio-linguistic\n  information: A Technical Report for ABAW3","summary":"  We propose a cross-modal co-attention model for continuous emotion\nrecognition using visual-audio-linguistic information. The model consists of\nfour blocks. The visual, audio, and linguistic blocks are used to learn the\nspatial-temporal features of the multimodal input. A co-attention block is\ndesigned to fuse the learned enbeddings with the multihead co-attention\nmechanism. The visual encoding from the visual block is concatenated with the\nattention feature to emphasize the visual information. To make full use of the\ndata and alleviate over-fitting, the cross-validation is carried out on the\ntraining and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. The achieved CCC on\nvalidation set is 0.450 for valence and 0.651 for arousal, which significantly\noutperforms the baseline method with the corresponding CCC of 0.310 and 0.170,\nrespectively. The code is available at https://github.com/sucv/ABAW3.\n","authors":["Su Zhang","Ruyi An","Yi Ding","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2203.13031v1.pdf","comment":"5 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:2107.01175"},{"id":"http://arxiv.org/abs/2203.13009v1","updated":"2022-03-24T11:59:28Z","published":"2022-03-24T11:59:28Z","title":"CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image\n  Denoising by Disentangling Noise from Image","summary":"  Recently, significant progress has been made on image denoising with strong\nsupervision from large-scale datasets. However, obtaining well-aligned\nnoisy-clean training image pairs for each specific scenario is complicated and\ncostly in practice. Consequently, applying a conventional supervised denoising\nnetwork on in-the-wild noisy inputs is not straightforward. Although several\nstudies have challenged this problem without strong supervision, they rely on\nless practical assumptions and cannot be applied to practical situations\ndirectly. To address the aforementioned challenges, we propose a novel and\npowerful self-supervised denoising method called CVF-SID based on a Cyclic\nmulti-Variate Function (CVF) module and a self-supervised image disentangling\n(SID) framework. The CVF module can output multiple decomposed variables of the\ninput and take a combination of the outputs back as an input in a cyclic\nmanner. Our CVF-SID can disentangle a clean image and noise maps from the input\nby leveraging various self-supervised loss terms. Unlike several methods that\nonly consider the signal-independent noise models, we also deal with\nsignal-dependent noise components for real-world applications. Furthermore, we\ndo not rely on any prior assumptions about the underlying noise distribution,\nmaking CVF-SID more generalizable toward realistic noise. Extensive experiments\non real-world datasets show that CVF-SID achieves state-of-the-art\nself-supervised image denoising performance and is comparable to other existing\napproaches. The code is publicly available from\nhttps://github.com/Reyhanehne/CVF-SID_PyTorch .\n","authors":["Reyhaneh Neshatavar","Mohsen Yavartanoo","Sanghyun Son","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2203.13009v1.pdf","comment":"Published at CVPR 2022"},{"id":"http://arxiv.org/abs/2203.13006v1","updated":"2022-03-24T11:54:59Z","published":"2022-03-24T11:54:59Z","title":"Compound Domain Generalization via Meta-Knowledge Encoding","summary":"  Domain generalization (DG) aims to improve the generalization performance for\nan unseen target domain by using the knowledge of multiple seen source domains.\nMainstream DG methods typically assume that the domain label of each source\nsample is known a priori, which is challenged to be satisfied in many\nreal-world applications. In this paper, we study a practical problem of\ncompound DG, which relaxes the discrete domain assumption to the mixed source\ndomains setting. On the other hand, current DG algorithms prioritize the focus\non semantic invariance across domains (one-vs-one), while paying less attention\nto the holistic semantic structure (many-vs-many). Such holistic semantic\nstructure, referred to as meta-knowledge here, is crucial for learning\ngeneralizable representations. To this end, we present Compound Domain\nGeneralization via Meta-Knowledge Encoding (COMEN), a general approach to\nautomatically discover and model latent domains in two steps. Firstly, we\nintroduce Style-induced Domain-specific Normalization (SDNorm) to re-normalize\nthe multi-modal underlying distributions, thereby dividing the mixture of\nsource domains into latent clusters. Secondly, we harness the prototype\nrepresentations, the centroids of classes, to perform relational modeling in\nthe embedding space with two parallel and complementary modules, which\nexplicitly encode the semantic structure for the out-of-distribution\ngeneralization. Experiments on four standard DG benchmarks reveal that COMEN\nexceeds the state-of-the-art performance without the need of domain\nsupervision.\n","authors":["Chaoqi Chen","Jiongcheng Li","Xiaoguang Han","Xiaoqing Liu","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2203.13006v1.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12999v1","updated":"2022-03-24T11:47:11Z","published":"2022-03-24T11:47:11Z","title":"A Deep-Discrete Learning Framework for Spherical Surface Registration","summary":"  Cortical surface registration is a fundamental tool for neuroimaging analysis\nthat has been shown to improve the alignment of functional regions relative to\nvolumetric approaches. Classically, image registration is performed by\noptimizing a complex objective similarity function, leading to long run times.\nThis contributes to a convention for aligning all data to a global average\nreference frame that poorly reflects the underlying cortical heterogeneity. In\nthis paper, we propose a novel unsupervised learning-based framework that\nconverts registration to a multi-label classification problem, where each point\nin a low-resolution control grid deforms to one of fixed, finite number of\nendpoints. This is learned using a spherical geometric deep learning\narchitecture, in an end-to-end unsupervised way, with regularization imposed\nusing a deep Conditional Random Field (CRF). Experiments show that our proposed\nframework performs competitively, in terms of similarity and areal distortion,\nrelative to the most popular classical surface registration algorithms and\ngenerates smoother deformations than other learning-based surface registration\nmethods, even in subjects with atypical cortical morphology.\n","authors":["Mohamed A. Suliman","Logan Z. J. Williams","Abdulah Fawaz","Emma C. Robinson"],"pdf_url":"https://arxiv.org/pdf/2203.12999v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2108.00358v3","updated":"2022-03-24T11:44:50Z","published":"2021-08-01T03:46:48Z","title":"Applications of Artificial Neural Networks in Microorganism Image\n  Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to\n  Popular Convolutional Neural Network and Potential Visual Transformer","summary":"  Microorganisms are widely distributed in the human daily living environment.\nThey play an essential role in environmental pollution control, disease\nprevention and treatment, and food and drug production. The analysis of\nmicroorganisms is essential for making full use of different microorganisms.\nThe conventional analysis methods are laborious and time-consuming. Therefore,\nthe automatic image analysis based on artificial neural networks is introduced\nto optimize it. However, the automatic microorganism image analysis faces many\nchallenges, such as the requirement of a robust algorithm caused by various\napplication occasions, insignificant features and easy under-segmentation\ncaused by the image characteristic, and various analysis tasks. Therefore, we\nconduct this review to comprehensively discuss the characteristics of\nmicroorganism image analysis based on artificial neural networks. In this\nreview, the background and motivation are introduced first. Then, the\ndevelopment of artificial neural networks and representative networks are\npresented. After that, the papers related to microorganism image analysis based\non classical and deep neural networks are reviewed from the perspectives of\ndifferent tasks. In the end, the methodology analysis and potential direction\nare discussed.\n","authors":["Jinghua Zhang","Chen Li","Yimin Yin","Jiawei Zhang","Marcin Grzegorzek"],"pdf_url":"https://arxiv.org/pdf/2108.00358v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12997v1","updated":"2022-03-24T11:41:16Z","published":"2022-03-24T11:41:16Z","title":"Hierarchical Nearest Neighbor Graph Embedding for Efficient\n  Dimensionality Reduction","summary":"  Dimensionality reduction is crucial both for visualization and preprocessing\nhigh dimensional data for machine learning. We introduce a novel method based\non a hierarchy built on 1-nearest neighbor graphs in the original space which\nis used to preserve the grouping properties of the data distribution on\nmultiple levels. The core of the proposal is an optimization-free projection\nthat is competitive with the latest versions of t-SNE and UMAP in performance\nand visualization quality while being an order of magnitude faster in run-time.\nFurthermore, its interpretable mechanics, the ability to project new data, and\nthe natural separation of data clusters in visualizations make it a general\npurpose unsupervised dimension reduction technique. In the paper, we argue\nabout the soundness of the proposed method and evaluate it on a diverse\ncollection of datasets with sizes varying from 1K to 11M samples and dimensions\nfrom 28 to 16K. We perform comparisons with other state-of-the-art methods on\nmultiple metrics and target dimensions highlighting its efficiency and\nperformance. Code is available at https://github.com/koulakis/h-nne\n","authors":["M. Saquib Sarfraz","Marios Koulakis","Constantin Seibold","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2203.12997v1.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.11991v2","updated":"2022-03-24T11:39:35Z","published":"2022-03-22T18:37:11Z","title":"Joint Feature Learning and Relation Modeling for Tracking: A One-Stream\n  Framework","summary":"  The current popular two-stream, two-stage tracking framework extracts the\ntemplate and the search region features separately and then performs relation\nmodeling, thus the extracted features lack the awareness of the target and have\nlimited target-background discriminability. To tackle the above issue, we\npropose a novel one-stream tracking (OSTrack) framework that unifies feature\nlearning and relation modeling by bridging the template-search image pairs with\nbidirectional information flows. In this way, discriminative target-oriented\nfeatures can be dynamically extracted by mutual guidance. Since no extra heavy\nrelation modeling module is needed and the implementation is highly\nparallelized, the proposed tracker runs at a fast speed. To further improve the\ninference efficiency, an in-network candidate early elimination module is\nproposed based on the strong similarity prior calculated in the one-stream\nframework. As a unified framework, OSTrack achieves state-of-the-art\nperformance on multiple benchmarks, in particular, it shows impressive results\non the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving\nthe existing best result (SwinTrack) by 4.3%. Besides, our method maintains a\ngood performance-speed trade-off and shows faster convergence. The code and\nmodels will be available at https://github.com/botaoye/OSTrack.\n","authors":["Botao Ye","Hong Chang","Bingpeng Ma","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2203.11991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12985v1","updated":"2022-03-24T11:19:04Z","published":"2022-03-24T11:19:04Z","title":"Learning Disentangled Representation for One-shot Progressive Face\n  Swapping","summary":"  Although face swapping has attracted much attention in recent years, it\nremains a challenging problem. The existing methods leverage a large number of\ndata samples to explore the intrinsic properties of face swapping without\ntaking into account the semantic information of face images. Moreover, the\nrepresentation of the identity information tends to be fixed, leading to\nsuboptimal face swapping. In this paper, we present a simple yet efficient\nmethod named FaceSwapper, for one-shot face swapping based on Generative\nAdversarial Networks. Our method consists of a disentangled representation\nmodule and a semantic-guided fusion module. The disentangled representation\nmodule is composed of an attribute encoder and an identity encoder, which aims\nto achieve the disentanglement of the identity and the attribute information.\nThe identity encoder is more flexible and the attribute encoder contains more\ndetails of the attributes than its competitors. Benefiting from the\ndisentangled representation, FaceSwapper can swap face images progressively. In\naddition, semantic information is introduced into the semantic-guided fusion\nmodule to control the swapped area and model the pose and expression more\naccurately. The experimental results show that our method achieves\nstate-of-the-art results on benchmark datasets with fewer training samples. Our\ncode is publicly available at https://github.com/liqi-casia/FaceSwapper.\n","authors":["Qi Li","Weining Wang","Chengzhong Xu","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2203.12985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.00567v6","updated":"2022-03-24T11:16:22Z","published":"2021-04-01T15:48:01Z","title":"Text to Image Generation with Semantic-Spatial Aware GAN","summary":"  Text-to-image synthesis (T2I) aims to generate photo-realistic images which\nare semantically consistent with the text descriptions. Existing methods are\nusually built upon conditional generative adversarial networks (GANs) and\ninitialize an image from noise with sentence embedding, and then refine the\nfeatures with fine-grained word embedding iteratively. A close inspection of\ntheir generated images reveals a major limitation: even though the generated\nimage holistically matches the description, individual image regions or parts\nof somethings are often not recognizable or consistent with words in the\nsentence, e.g. \"a white crown\". To address this problem, we propose a novel\nframework Semantic-Spatial Aware GAN for synthesizing images from input text.\nConcretely, we introduce a simple and effective Semantic-Spatial Aware block,\nwhich (1) learns semantic-adaptive transformation conditioned on text to\neffectively fuse text features and image features, and (2) learns a semantic\nmask in a weakly-supervised way that depends on the current text-image fusion\nprocess in order to guide the transformation spatially. Experiments on the\nchallenging COCO and CUB bird datasets demonstrate the advantage of our method\nover the recent state-of-the-art approaches, regarding both visual fidelity and\nalignment with input text description.\n","authors":["Kai Hu","Wentong Liao","Michael Ying Yang","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2104.00567v6.pdf","comment":"arXiv admin note: text overlap with arXiv:1711.10485 by other authors"},{"id":"http://arxiv.org/abs/1804.03547v3","updated":"2022-03-24T11:04:37Z","published":"2018-04-10T14:07:45Z","title":"A real-time and unsupervised face Re-Identification system for\n  Human-Robot Interaction","summary":"  In the context of Human-Robot Interaction (HRI), face Re-Identification (face\nRe-ID) aims to verify if certain detected faces have already been observed by\nrobots. The ability of distinguishing between different users is crucial in\nsocial robots as it will enable the robot to tailor the interaction strategy\ntoward the users' individual preferences. So far face recognition research has\nachieved great success, however little attention has been paid to the realistic\napplications of Face Re-ID in social robots. In this paper, we present an\neffective and unsupervised face Re-ID system which simultaneously re-identifies\nmultiple faces for HRI. This Re-ID system employs Deep Convolutional Neural\nNetworks to extract features, and an online clustering algorithm to determine\nthe face's ID. Its performance is evaluated on two datasets: the TERESA video\ndataset collected by the TERESA robot, and the YouTube Face Dataset (YTF\nDataset). We demonstrate that the optimised combination of techniques achieves\nan overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on\nYTF dataset. We have implemented the proposed method into a software module in\nthe HCI^2 Framework for it to be further integrated into the TERESA robot, and\nhas achieved real-time performance at 10~26 Frames per second.\n","authors":["Yujiang Wang","Jie Shen","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/1804.03547v3.pdf","comment":"Code implementation in Python is available at:\n  https://github.com/ibug-group/face_reid"},{"id":"http://arxiv.org/abs/2203.12979v1","updated":"2022-03-24T10:55:17Z","published":"2022-03-24T10:55:17Z","title":"Is Geometry Enough for Matching in Visual Localization?","summary":"  In this paper, we propose to go beyond the well-established approach to\nvision-based localization that relies on visual descriptor matching between a\nquery image and a 3D point cloud. While matching keypoints via visual\ndescriptors makes localization highly accurate, it has significant storage\ndemands, raises privacy concerns and increases map maintenance complexity. To\nelegantly address those practical challenges for large-scale localization, we\npresent GoMatch, an alternative to visual-based matching that solely relies on\ngeometric information for matching image keypoints to maps, represented as sets\nof bearing vectors. Our novel bearing vectors representation of 3D points,\nsignificantly relieves the cross-domain challenge in geometric-based matching\nthat prevented prior work to tackle localization in a realistic environment.\nWith additional careful architecture design, GoMatch improves over prior\ngeometric-based matching work with a reduction of ($10.67m, 95.7^{\\circ}$) and\n($1.43m$, $34.7^{\\circ}$) in average median pose errors on Cambridge Landmarks\nand 7-Scenes, while requiring as little as $1.5/1.7\\%$ of storage capacity in\ncomparison to the best visual-based matching methods. This confirms its\npotential and feasibility for real-world localization and opens the door to\nfuture efforts in advancing city-scale visual localization methods that do not\nrequire storing visual descriptors.\n","authors":["Qunjie Zhou","Sergio Agostinho","Aljosa Osep","Laura Leal-Taixe"],"pdf_url":"https://arxiv.org/pdf/2203.12979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12976v1","updated":"2022-03-24T10:43:56Z","published":"2022-03-24T10:43:56Z","title":"Focus-and-Detect: A Small Object Detection Framework for Aerial Images","summary":"  Despite recent advances, object detection in aerial images is still a\nchallenging task. Specific problems in aerial images makes the detection\nproblem harder, such as small objects, densely packed objects, objects in\ndifferent sizes and with different orientations. To address small object\ndetection problem, we propose a two-stage object detection framework called\n\"Focus-and-Detect\". The first stage which consists of an object detector\nnetwork supervised by a Gaussian Mixture Model, generates clusters of objects\nconstituting the focused regions. The second stage, which is also an object\ndetector network, predicts objects within the focal regions. Incomplete Box\nSuppression (IBS) method is also proposed to overcome the truncation effect of\nregion search approach. Results indicate that the proposed two-stage framework\nachieves an AP score of 42.06 on VisDrone validation dataset, surpassing all\nother state-of-the-art small object detection methods reported in the\nliterature, to the best of authors' knowledge.\n","authors":["Onur Can Koyun","Reyhan Kevser Keser","İbrahim Batuhan Akkaya","Behçet Uğur Töreyin"],"pdf_url":"https://arxiv.org/pdf/2203.12976v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.11834v2","updated":"2022-03-24T10:30:14Z","published":"2022-03-22T16:01:04Z","title":"Improving Generalization in Federated Learning by Seeking Flat Minima","summary":"  Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2203.11834v2.pdf","comment":"Removed axessibility package for smaller output PDF"},{"id":"http://arxiv.org/abs/2202.11539v2","updated":"2022-03-24T09:38:24Z","published":"2022-02-23T14:27:36Z","title":"Self-Supervised Transformers for Unsupervised Object Discovery using\n  Normalized Cut","summary":"  Transformers trained with self-supervised learning using self-distillation\nloss (DINO) have been shown to produce attention maps that highlight salient\nforeground objects. In this paper, we demonstrate a graph-based approach that\nuses the self-supervised transformer features to discover an object from an\nimage. Visual tokens are viewed as nodes in a weighted graph with edges\nrepresenting a connectivity score based on the similarity of tokens. Foreground\nobjects can then be segmented using a normalized graph-cut to group\nself-similar regions. We solve the graph-cut problem using spectral clustering\nwith generalized eigen-decomposition and show that the second smallest\neigenvector provides a cutting solution since its absolute value indicates the\nlikelihood that a token belongs to a foreground object. Despite its simplicity,\nthis approach significantly boosts the performance of unsupervised object\ndiscovery: we improve over the recent state of the art LOST by a margin of\n6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The\nperformance can be further improved by adding a second stage class-agnostic\ndetector (CAD). Our proposed method can be easily extended to unsupervised\nsaliency detection and weakly supervised object detection. For unsupervised\nsaliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,\nDUT-OMRON respectively compared to previous state of the art. For weakly\nsupervised object detection, we achieve competitive performance on CUB and\nImageNet.\n","authors":["Yangtao Wang","Xi Shen","Shell Hu","Yuan Yuan","James Crowley","Dominique Vaufreydaz"],"pdf_url":"https://arxiv.org/pdf/2202.11539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.00207v3","updated":"2022-03-24T09:15:14Z","published":"2021-10-30T08:39:55Z","title":"PatchFormer: An Efficient Point Transformer with Patch Attention","summary":"  The point cloud learning community witnesses a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have achieved top accuracy\non the major learning benchmarks. However, existing point Transformers are\ncomputationally expensive since they need to generate a large attention map,\nwhich has quadratic complexity (both in space and time) with respect to input\nsize. To solve this shortcoming, we introduce Patch ATtention (PAT) to\nadaptively learn a much smaller set of bases upon which the attention maps are\ncomputed. By a weighted summation upon these bases, PAT not only captures the\nglobal shape context but also achieves linear complexity to input size. In\naddition, we propose a lightweight Multi-Scale aTtention (MST) block to build\nattentions among features of different scales, providing the model with\nmulti-scale features. Equipped with the PAT and MST, we construct our neural\narchitecture called PatchFormer that integrates both modules into a joint\nframework for point cloud learning. Extensive experiments demonstrate that our\nnetwork achieves comparable accuracy on general point cloud learning tasks with\n9.2x speed-up than previous point Transformers.\n","authors":["Zhang Cheng","Haocheng Wan","Xinyi Shen","Zizhao Wu"],"pdf_url":"https://arxiv.org/pdf/2111.00207v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2203.12944v1","updated":"2022-03-24T09:09:00Z","published":"2022-03-24T09:09:00Z","title":"Transformers Meet Visual Learning Understanding: A Comprehensive Review","summary":"  Dynamic attention mechanism and global modeling ability make Transformer show\nstrong feature learning ability. In recent years, Transformer has become\ncomparable to CNNs methods in computer vision. This review mainly investigates\nthe current research progress of Transformer in image and video applications,\nwhich makes a comprehensive overview of Transformer in visual learning\nunderstanding. First, the attention mechanism is reviewed, which plays an\nessential part in Transformer. And then, the visual Transformer model and the\nprinciple of each module are introduced. Thirdly, the existing\nTransformer-based models are investigated, and their performance is compared in\nvisual learning understanding applications. Three image tasks and two video\ntasks of computer vision are investigated. The former mainly includes image\nclassification, object detection, and image segmentation. The latter contains\nobject tracking and video classification. It is significant for comparing\ndifferent models' performance in various tasks on several public benchmark data\nsets. Finally, ten general problems are summarized, and the developing\nprospects of the visual Transformer are given in this review.\n","authors":["Yuting Yang","Licheng Jiao","Xu Liu","Fang Liu","Shuyuan Yang","Zhixi Feng","Xu Tang"],"pdf_url":"https://arxiv.org/pdf/2203.12944v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2010.11929,\n  arXiv:1706.03762 by other authors"},{"id":"http://arxiv.org/abs/2103.08573v4","updated":"2022-03-24T09:01:27Z","published":"2021-03-15T17:40:25Z","title":"RoRD: Rotation-Robust Descriptors and Orthographic Views for Local\n  Feature Matching","summary":"  The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.\n","authors":["Udit Singh Parihar","Aniket Gujarathi","Kinal Mehta","Satyajit Tourani","Sourav Garg","Michael Milford","K. Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2103.08573v4.pdf","comment":"Accepted to IROS 2021. Project Page:\n  https://uditsinghparihar.github.io/RoRD/"},{"id":"http://arxiv.org/abs/2112.05496v2","updated":"2022-03-24T08:39:32Z","published":"2021-12-10T12:58:17Z","title":"Graph-based Generative Face Anonymisation with Pose Preservation","summary":"  We propose AnonyGAN, a GAN-based solution for face anonymisation which\nreplaces the visual information corresponding to a source identity with a\ncondition identity provided as any single image. With the goal to maintain the\ngeometric attributes of the source face, i.e., the facial pose and expression,\nand to promote more natural face generation, we propose to exploit a Bipartite\nGraph to explicitly model the relations between the facial landmarks of the\nsource identity and the ones of the condition identity through a deep model. We\nfurther propose a landmark attention model to relax the manual selection of\nfacial landmarks, allowing the network to weight the landmarks for the best\nvisual naturalness and pose preservation. Finally, to facilitate the appearance\nlearning, we propose a hybrid training strategy to address the challenge caused\nby the lack of direct pixel-level supervision. We evaluate our method and its\nvariants on two public datasets, CelebA and LFW, in terms of visual\nnaturalness, facial pose preservation and of its impacts on face detection and\nre-identification. We prove that AnonyGAN significantly outperforms the\nstate-of-the-art methods in terms of visual naturalness, face detection and\npose preservation.\n","authors":["Nicola Dall'Asen","Yiming Wang","Hao Tang","Luca Zanella","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2112.05496v2.pdf","comment":"21st International Conference on Image analysis and Processing"},{"id":"http://arxiv.org/abs/2203.12929v1","updated":"2022-03-24T08:21:41Z","published":"2022-03-24T08:21:41Z","title":"Towards Escaping from Language Bias and OCR Error: Semantics-Centered\n  Text Visual Question Answering","summary":"  Texts in scene images convey critical information for scene understanding and\nreasoning. The abilities of reading and reasoning matter for the model in the\ntext-based visual question answering (TextVQA) process. However, current\nTextVQA models do not center on the text and suffer from several limitations.\nThe model is easily dominated by language biases and optical character\nrecognition (OCR) errors due to the absence of semantic guidance in the answer\nprediction process. In this paper, we propose a novel Semantics-Centered\nNetwork (SC-Net) that consists of an instance-level contrastive semantic\nprediction module (ICSP) and a semantics-centered transformer module (SCT).\nEquipped with the two modules, the semantics-centered model can resist the\nlanguage biases and the accumulated errors from OCR. Extensive experiments on\nTextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net\nsurpasses previous works with a noticeable margin and is more reasonable for\nthe TextVQA task.\n","authors":["Chengyang Fang","Gangyan Zeng","Yu Zhou","Daiqing Wu","Can Ma","Dayong Hu","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12928v1","updated":"2022-03-24T08:21:28Z","published":"2022-03-24T08:21:28Z","title":"The Fixed Sub-Center: A Better Way to Capture Data Complexity","summary":"  Treating class with a single center may hardly capture data distribution\ncomplexities. Using multiple sub-centers is an alternative way to address this\nproblem. However, highly correlated sub-classes, the classifier's parameters\ngrow linearly with the number of classes, and lack of intra-class compactness\nare three typical issues that need to be addressed in existing multi-subclass\nmethods. To this end, we propose to use Fixed Sub-Center (F-SC), which allows\nthe model to create more discrepant sub-centers while saving memory and cutting\ncomputational costs considerably. The F-SC specifically, first samples a class\ncenter Ui for each class from a uniform distribution, and then generates a\nnormal distribution for each class, where the mean is equal to Ui. Finally, the\nsub-centers are sampled based on the normal distribution corresponding to each\nclass, and the sub-centers are fixed during the training process avoiding the\noverhead of gradient calculation. Moreover, F-SC penalizes the Euclidean\ndistance between the samples and their corresponding sub-centers, it helps\nremain intra-compactness. The experimental results show that F-SC significantly\nimproves the accuracy of both image classification and fine-grained recognition\ntasks.\n","authors":["Zhemin Zhang","Xun Gong"],"pdf_url":"https://arxiv.org/pdf/2203.12928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09775v2","updated":"2022-03-24T08:17:52Z","published":"2022-03-18T07:41:48Z","title":"ContrastMask: Contrastive Learning to Segment Every Thing","summary":"  Partially-supervised instance segmentation is a task which requests\nsegmenting objects from novel unseen categories via learning on limited seen\ncategories with annotated masks thus eliminating demands of heavy annotation\nburden. The key to addressing this task is to build an effective class-agnostic\nmask segmentation model. Unlike previous methods that learn such models only on\nseen categories, in this paper, we propose a new method, named ContrastMask,\nwhich learns a mask segmentation model on both seen and unseen categories under\na unified pixel-level contrastive learning framework. In this framework,\nannotated masks of seen categories and pseudo masks of unseen categories serve\nas a prior for contrastive learning, where features from the mask regions\n(foreground) are pulled together, and are contrasted against those from the\nbackground, and vice versa. Through this framework, feature discrimination\nbetween foreground and background is largely improved, facilitating learning of\nthe class-agnostic mask segmentation model. Exhaustive experiments on the COCO\ndataset demonstrate the superiority of our method, which outperforms previous\nstate-of-the-arts.\n","authors":["Xuehui Wang","Kai Zhao","Ruixin Zhang","Shouhong Ding","Yan Wang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2203.09775v2.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12919v1","updated":"2022-03-24T08:13:26Z","published":"2022-03-24T08:13:26Z","title":"Learning Dense Correspondence from Synthetic Environments","summary":"  Estimation of human shape and pose from a single image is a challenging task.\nIt is an even more difficult problem to map the identified human shape onto a\n3D human model. Existing methods map manually labelled human pixels in real 2D\nimages onto the 3D surface, which is prone to human error, and the sparsity of\navailable annotated data often leads to sub-optimal results. We propose to\nsolve the problem of data scarcity by training 2D-3D human mapping algorithms\nusing automatically generated synthetic data for which exact and dense 2D-3D\ncorrespondence is known. Such a learning strategy using synthetic environments\nhas a high generalisation potential towards real-world data. Using different\ncamera parameter variations, background and lighting settings, we created\nprecise ground truth data that constitutes a wider distribution. We evaluate\nthe performance of models trained on synthetic using the COCO dataset and\nvalidation framework. Results show that training 2D-3D mapping network models\non synthetic data is a viable alternative to using real data.\n","authors":["Mithun Lal","Anthony Paproki","Nariman Habili","Lars Petersson","Olivier Salvado","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2203.12919v1.pdf","comment":"Submitted to ICIP 2022"},{"id":"http://arxiv.org/abs/2203.12917v1","updated":"2022-03-24T08:12:43Z","published":"2022-03-24T08:12:43Z","title":"WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point\n  Cloud Generation","summary":"  We propose WarpingGAN, an effective and efficient 3D point cloud generation\nnetwork. Unlike existing methods that generate point clouds by directly\nlearning the mapping functions between latent codes and 3D shapes, Warping-GAN\nlearns a unified local-warping function to warp multiple identical pre-defined\npriors (i.e., sets of points uniformly distributed on regular 3D grids) into 3D\nshapes driven by local structure-aware semantics. In addition, we also\ningeniously utilize the principle of the discriminator and tailor a stitching\nloss to eliminate the gaps between different partitions of a generated shape\ncorresponding to different priors for boosting quality. Owing to the novel\ngenerating mechanism, WarpingGAN, a single lightweight network after one-time\ntraining, is capable of efficiently generating uniformly distributed 3D point\nclouds with various resolutions. Extensive experimental results demonstrate the\nsuperiority of our WarpingGAN over state-of-the-art methods in terms of\nquantitative metrics, visual quality, and efficiency. The source code is\npublicly available at https://github.com/yztang4/WarpingGAN.git.\n","authors":["Yingzhi Tang","Yue Qian","Qijian Zhang","Yiming Zeng","Junhui Hou","Xuefei Zhe"],"pdf_url":"https://arxiv.org/pdf/2203.12917v1.pdf","comment":"This paper has been accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12915v1","updated":"2022-03-24T08:10:13Z","published":"2022-03-24T08:10:13Z","title":"NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep\n  Neural Networks","summary":"  Deep learning has recently been widely applied to many applications across\ndifferent domains, e.g., image classification and audio recognition. However,\nthe quality of Deep Neural Networks (DNNs) still raises concerns in the\npractical operational environment, which calls for systematic testing,\nespecially in safety-critical scenarios. Inspired by software testing, a number\nof structural coverage criteria are designed and proposed to measure the test\nadequacy of DNNs. However, due to the blackbox nature of DNN, the existing\nstructural coverage criteria are difficult to interpret, making it hard to\nunderstand the underlying principles of these criteria. The relationship\nbetween the structural coverage and the decision logic of DNNs is unknown.\nMoreover, recent studies have further revealed the non-existence of correlation\nbetween the structural coverage and DNN defect detection, which further posts\nconcerns on what a suitable DNN testing criterion should be.\n  In this paper, we propose the interpretable coverage criteria through\nconstructing the decision structure of a DNN. Mirroring the control flow graph\nof the traditional program, we first extract a decision graph from a DNN based\non its interpretation, where a path of the decision graph represents a decision\nlogic of the DNN. Based on the control flow and data flow of the decision\ngraph, we propose two variants of path coverage to measure the adequacy of the\ntest cases in exercising the decision logic. The higher the path coverage, the\nmore diverse decision logic the DNN is expected to be explored. Our large-scale\nevaluation results demonstrate that: the path in the decision graph is\neffective in characterizing the decision of the DNN, and the proposed coverage\ncriteria are also sensitive with errors including natural errors and\nadversarial examples, and strongly correlated with the output impartiality.\n","authors":["Xiaofei Xie","Tianlin Li","Jian Wang","Lei Ma","Qing Guo","Felix Juefei-Xu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2203.12915v1.pdf","comment":"27 pages. Accepted to ACM Transactions on Software Engineering and\n  Methodology (TOSEM), 2022"},{"id":"http://arxiv.org/abs/2112.06476v2","updated":"2022-03-24T08:07:25Z","published":"2021-12-13T08:17:15Z","title":"gACSON software for automated segmentation and morphology analyses of\n  myelinated axons in 3D electron microscopy","summary":"  Background and Objective: Advances in electron microscopy (EM) now allow\nthree-dimensional (3D) imaging of hundreds of micrometers of tissue with\nnanometer-scale resolution, providing new opportunities to study the\nultrastructure of the brain. In this work, we introduce a freely available\nMatlab-based gACSON software for visualization, segmentation, assessment, and\nmorphology analysis of myelinated axons in 3D-EM volumes of brain tissue\nsamples. Methods: The software is equipped with a graphical user interface\n(GUI). It automatically segments the intra-axonal space of myelinated axons and\ntheir corresponding myelin sheaths and allows manual segmentation,\nproofreading, and interactive correction of the segmented components. gACSON\nanalyzes the morphology of myelinated axons, such as axonal diameter, axonal\neccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of\nthe software by segmenting and analyzing myelinated axons in six 3D-EM volumes\nof rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).\nOur results suggest that the equivalent diameter of myelinated axons in\nsomatosensory cortex was decreased in TBI animals five months after the injury.\nConclusions: Our results indicate that gACSON is a valuable tool for\nvisualization, segmentation, assessment, and morphology analysis of myelinated\naxons in 3D-EM volumes. It is freely available at\nhttps://github.com/AndreaBehan/g-ACSON under the MIT license.\n","authors":["Andrea Behanova","Ali Abdollahzadeh","Ilya Belevich","Eija Jokitalo","Alejandra Sierra","Jussi Tohka"],"pdf_url":"https://arxiv.org/pdf/2112.06476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.05341v2","updated":"2022-03-24T08:05:49Z","published":"2021-12-10T05:21:18Z","title":"Hyperdimensional Feature Fusion for Interpretable Out-Of-Distribution\n  Detection","summary":"  We introduce powerful ideas from Hyperdimensional Computing into the\nchallenging field of Out-of-Distribution (OOD) detection. In contrast to most\nexisting work that performs OOD detection based on only a single layer of a\nneural network, we use similarity-preserving semi-orthogonal projection\nmatrices to project the feature maps from multiple layers into a common vector\nspace. By repeatedly applying the bundling operation $\\oplus$, we create\nexpressive class-specific descriptor vectors for all in-distribution classes.\nAt test time, a simple and efficient cosine similarity calculation between\ndescriptor vectors consistently identifies OOD samples with better performance\nthan the current state-of-the-art. We show that the hyperdimensional fusion of\nmultiple network layers is critical to achieve best general performance.\n","authors":["Samuel Wilson","Tobias Fischer","Niko Sünderhauf","Feras Dayoub"],"pdf_url":"https://arxiv.org/pdf/2112.05341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00680v3","updated":"2022-03-24T07:59:50Z","published":"2022-03-01T18:59:01Z","title":"CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D\n  Point Cloud Understanding","summary":"  Manual annotation of large-scale point cloud dataset for varying tasks such\nas 3D object classification, segmentation and detection is often laborious\nowing to the irregular structure of point clouds. Self-supervised learning,\nwhich operates without any human labeling, is a promising approach to address\nthis issue. We observe in the real world that humans are capable of mapping the\nvisual concepts learnt from 2D images to understand the 3D world. Encouraged by\nthis insight, we propose CrossPoint, a simple cross-modal contrastive learning\napproach to learn transferable 3D point cloud representations. It enables a\n3D-2D correspondence of objects by maximizing agreement between point clouds\nand the corresponding rendered 2D image in the invariant space, while\nencouraging invariance to transformations in the point cloud modality. Our\njoint training objective combines the feature correspondences within and across\nmodalities, thus ensembles a rich learning signal from both 3D point cloud and\n2D image modalities in a self-supervised fashion. Experimental results show\nthat our approach outperforms the previous unsupervised learning methods on a\ndiverse range of downstream tasks including 3D object classification and\nsegmentation. Further, the ablation studies validate the potency of our\napproach for a better point cloud understanding. Code and pretrained models are\navailable at http://github.com/MohamedAfham/CrossPoint.\n","authors":["Mohamed Afham","Isuru Dissanayake","Dinithi Dissanayake","Amaya Dharmasiri","Kanchana Thilakarathna","Ranga Rodrigo"],"pdf_url":"https://arxiv.org/pdf/2203.00680v3.pdf","comment":"CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12909v1","updated":"2022-03-24T07:57:20Z","published":"2022-03-24T07:57:20Z","title":"Neural Reflectance for Shape Recovery with Shadow Handling","summary":"  This paper aims at recovering the shape of a scene with unknown,\nnon-Lambertian, and possibly spatially-varying surface materials. When the\nshape of the object is highly complex and that shadows cast on the surface, the\ntask becomes very challenging. To overcome these challenges, we propose a\ncoordinate-based deep MLP (multilayer perceptron) to parameterize both the\nunknown 3D shape and the unknown reflectance at every surface point. This\nnetwork is able to leverage the observed photometric variance and shadows on\nthe surface, and recover both surface shape and general non-Lambertian\nreflectance. We explicitly predict cast shadows, mitigating possible artifacts\non these shadowing regions, leading to higher estimation accuracy. Our\nframework is entirely self-supervised, in the sense that it requires neither\nground truth shape nor BRDF. Tests on real-world images demonstrate that our\nmethod outperform existing methods by a significant margin. Thanks to the small\nsize of the MLP-net, our method is an order of magnitude faster than previous\nCNN-based methods.\n","authors":["Junxuan Li","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2203.12909v1.pdf","comment":"Accepted to CVPR 2022. Codes available in\n  https://github.com/junxuan-li/Neural-Reflectance-PS"},{"id":"http://arxiv.org/abs/2203.12905v1","updated":"2022-03-24T07:49:33Z","published":"2022-03-24T07:49:33Z","title":"Privileged Attribution Constrained Deep Networks for Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) is crucial in many research domains\nbecause it enables machines to better understand human behaviours. FER methods\nface the problems of relatively small datasets and noisy data that don't allow\nclassical networks to generalize well. To alleviate these issues, we guide the\nmodel to concentrate on specific facial areas like the eyes, the mouth or the\neyebrows, which we argue are decisive to recognise facial expressions. We\npropose the Privileged Attribution Loss (PAL), a method that directs the\nattention of the model towards the most salient facial regions by encouraging\nits attribution maps to correspond to a heatmap formed by facial landmarks.\nFurthermore, we introduce several channel strategies that allow the model to\nhave more degrees of freedom. The proposed method is independent of the\nbackbone architecture and doesn't need additional semantic information at test\ntime. Finally, experimental results show that the proposed PAL method\noutperforms current state-of-the-art methods on both RAF-DB and AffectNet.\n","authors":["Jules Bonnard","Arnaud Dapogny","Ferdinand Dhombres","Kévin Bailly"],"pdf_url":"https://arxiv.org/pdf/2203.12905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12899v1","updated":"2022-03-24T07:36:21Z","published":"2022-03-24T07:36:21Z","title":"Expression Classification using Concatenation of Deep Neural Network for\n  the 3rd ABAW3 Competition","summary":"  For computers to recognize human emotions, expression classification is an\nequally important problem in the human-computer interaction area. In the 3rd\nAffective Behavior Analysis In-The-Wild competition, the task of expression\nclassification includes 8 classes including 6 basic expressions of human faces\nfrom videos. In this paper, we perform combination representation from RegNet,\nAttention module, and Transformer Encoder for the expression classification\ntask. We achieve 35.87 \\% for F1-score on the validation set of Aff-Wild2\ndataset. This result shows the effectiveness of the proposed architecture.\n","authors":["Kim Ngan Phan","Hong-Hai Nguyen","Van-Thong Huynh","Soo-Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2203.12899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12893v1","updated":"2022-03-24T07:26:29Z","published":"2022-03-24T07:26:29Z","title":"FAMLP: A Frequency-Aware MLP-Like Architecture For Domain Generalization","summary":"  MLP-like models built entirely upon multi-layer perceptrons have recently\nbeen revisited, exhibiting the comparable performance with transformers. It is\none of most promising architectures due to the excellent trade-off between\nnetwork capability and efficiency in the large-scale recognition tasks.\nHowever, its generalization performance to heterogeneous tasks is inferior to\nother architectures (e.g., CNNs and transformers) due to the extensive\nretention of domain information. To address this problem, we propose a novel\nfrequency-aware MLP architecture, in which the domain-specific features are\nfiltered out in the transformed frequency domain, augmenting the invariant\ndescriptor for label prediction. Specifically, we design an adaptive Fourier\nfilter layer, in which a learnable frequency filter is utilized to adjust the\namplitude distribution by optimizing both the real and imaginary parts. A\nlow-rank enhancement module is further proposed to rectify the filtered\nfeatures by adding the low-frequency components from SVD decomposition.\nFinally, a momentum update strategy is utilized to stabilize the optimization\nto fluctuation of model parameters and inputs by the output distillation with\nweighted historical states. To our best knowledge, we are the first to propose\na MLP-like backbone for domain generalization. Extensive experiments on three\nbenchmarks demonstrate significant generalization performance, outperforming\nthe state-of-the-art methods by a margin of 3%, 4% and 9%, respectively.\n","authors":["Kecheng Zheng","Yang Cao","Kai Zhu","Ruijing Zhao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2203.12893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12892v1","updated":"2022-03-24T07:26:11Z","published":"2022-03-24T07:26:11Z","title":"Making Heads or Tails: Towards Semantically Consistent Visual\n  Counterfactuals","summary":"  A visual counterfactual explanation replaces image regions in a query image\nwith regions from a distractor image such that the system's decision on the\ntransformed image changes to the distractor class. In this work, we present a\nnovel framework for computing visual counterfactual explanations based on two\nkey ideas. First, we enforce that the \\textit{replaced} and \\textit{replacer}\nregions contain the same semantic part, resulting in more semantically\nconsistent explanations. Second, we use multiple distractor images in a\ncomputationally efficient way and obtain more discriminative explanations with\nfewer region replacements. Our approach is $\\mathbf{27\\%}$ more semantically\nconsistent and an order of magnitude faster than a competing method on three\nfine-grained image recognition datasets. We highlight the utility of our\ncounterfactuals over existing works through machine teaching experiments where\nwe teach humans to classify different bird species. We also complement our\nexplanations with the vocabulary of parts and attributes that contributed the\nmost to the system's decision. In this task as well, we obtain state-of-the-art\nresults when using our counterfactual explanations relative to existing works,\nreinforcing the importance of semantically consistent explanations.\n","authors":["Simon Vandenhende","Dhruv Mahajan","Filip Radenovic","Deepti Ghadiyaram"],"pdf_url":"https://arxiv.org/pdf/2203.12892v1.pdf","comment":"Tech report. We plan to make code available"},{"id":"http://arxiv.org/abs/2203.02261v2","updated":"2022-03-24T07:25:57Z","published":"2022-03-04T12:18:23Z","title":"Class-Aware Contrastive Semi-Supervised Learning","summary":"  Pseudo-label-based semi-supervised learning (SSL) has achieved great success\non raw data utilization. However, its training procedure suffers from\nconfirmation bias due to the noise contained in self-generated artificial\nlabels. Moreover, the model's judgment becomes noisier in real-world\napplications with extensive out-of-distribution data. To address this issue, we\npropose a general method named Class-aware Contrastive Semi-Supervised Learning\n(CCSSL), which is a drop-in helper to improve the pseudo-label quality and\nenhance the model's robustness in the real-world setting. Rather than treating\nreal-world data as a union set, our method separately handles reliable\nin-distribution data with class-wise clustering for blending into downstream\ntasks and noisy out-of-distribution data with image-wise contrastive for better\ngeneralization. Furthermore, by applying target re-weighting, we successfully\nemphasize clean label learning and simultaneously reduce noisy label learning.\nDespite its simplicity, our proposed CCSSL has significant performance\nimprovements over the state-of-the-art SSL methods on the standard datasets\nCIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, we improve\nFixMatch by 9.80% and CoMatch by 3.18%. Code is available\nhttps://github.com/TencentYoutuResearch/Classification-SemiCLS.\n","authors":["Fan Yang","Kai Wu","Shuyi Zhang","Guannan Jiang","Yong Liu","Feng Zheng","Wei Zhang","Chengjie Wang","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2203.02261v2.pdf","comment":"cvpr2022 accepted, half more page for adding rebuttal Infos"},{"id":"http://arxiv.org/abs/2203.12891v1","updated":"2022-03-24T07:25:23Z","published":"2022-03-24T07:25:23Z","title":"An Ensemble Approach for Facial Expression Analysis in Video","summary":"  Human emotions recognization contributes to the development of human-computer\ninteraction. The machines understanding human emotions in the real world will\nsignificantly contribute to life in the future. This paper will introduce the\nAffective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. The paper\nfocuses on solving the problem of the valence-arousal estimation and action\nunit detection. For valence-arousal estimation, we conducted two stages:\ncreating new features from multimodel and temporal learning to predict\nvalence-arousal. First, we make new features; the Gated Recurrent Unit (GRU)\nand Transformer are combined using a Regular Networks (RegNet) feature, which\nis extracted from the image. The next step is the GRU combined with Local\nAttention to predict valence-arousal. The Concordance Correlation Coefficient\n(CCC) was used to evaluate the model.\n","authors":["Hong-Hai Nguyen","Van-Thong Huynh","Soo-Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2203.12891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12873v1","updated":"2022-03-24T06:30:47Z","published":"2022-03-24T06:30:47Z","title":"Weakly-Supervised End-to-End CAD Retrieval to Scan Objects","summary":"  CAD model retrieval to real-world scene observations has shown strong promise\nas a basis for 3D perception of objects and a clean, lightweight mesh-based\nscene representation; however, current approaches to retrieve CAD models to a\nquery scan rely on expensive manual annotations of 1:1 associations of CAD-scan\nobjects, which typically contain strong lower-level geometric differences. We\nthus propose a new weakly-supervised approach to retrieve semantically and\nstructurally similar CAD models to a query 3D scanned scene without requiring\nany CAD-scan associations, and only object detection information as oriented\nbounding boxes. Our approach leverages a fully-differentiable top-$k$ retrieval\nlayer, enabling end-to-end training guided by geometric and perceptual\nsimilarity of the top retrieved CAD models to the scan queries. We demonstrate\nthat our weakly-supervised approach can outperform fully-supervised retrieval\nmethods on challenging real-world ScanNet scans, and maintain robustness for\nunseen class categories, achieving significantly improved performance over\nfully-supervised state of the art in zero-shot CAD retrieval.\n","authors":["Tim Beyer","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2203.12873v1.pdf","comment":"Accompanying video at https://youtu.be/3bCUMxpscdQ"},{"id":"http://arxiv.org/abs/2203.12872v1","updated":"2022-03-24T06:28:07Z","published":"2022-03-24T06:28:07Z","title":"Intrinsic Bias Identification on Medical Image Datasets","summary":"  Machine learning based medical image analysis highly depends on datasets.\nBiases in the dataset can be learned by the model and degrade the\ngeneralizability of the applications. There are studies on debiased models.\nHowever, scientists and practitioners are difficult to identify implicit biases\nin the datasets, which causes lack of reliable unbias test datasets to valid\nmodels. To tackle this issue, we first define the data intrinsic bias\nattribute, and then propose a novel bias identification framework for medical\nimage datasets. The framework contains two major components, KlotskiNet and\nBias Discriminant Direction Analysis(bdda), where KlostkiNet is to build the\nmapping which makes backgrounds to distinguish positive and negative samples\nand bdda provides a theoretical solution on determining bias attributes.\nExperimental results on three datasets show the effectiveness of the bias\nattributes discovered by the framework.\n","authors":["Shijie Zhang","Lanjun Wang","Lian Ding","Senhua Zhu","Dandan Tu"],"pdf_url":"https://arxiv.org/pdf/2203.12872v1.pdf","comment":"19pages, 12 figures"},{"id":"http://arxiv.org/abs/2203.12870v1","updated":"2022-03-24T06:24:55Z","published":"2022-03-24T06:24:55Z","title":"RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust\n  Correspondence Field Estimation and Pose Optimization","summary":"  Direct estimating the 6-DoF object pose from a single color image is\nchallenging, and post-refinement is generally needed to achieve high-precision\nestimation. In this paper, we propose a framework based on a recurrent neural\nnetwork (RNN) for object pose refinement, which is robust to erroneous initial\nposes and occlusions. During the recurrent iterations, object pose refinement\nis formulated as a non-linear least squares problem based on the estimated\ncorrespondence field (between a rendered image and the observed image). The\nproblem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm\nfor end-toend training. The correspondence field estimation and pose refinement\nare conducted alternatively in each iteration to recover accurate object poses.\nFurthermore, to improve the robustness to occlusions, we introduce a\nconsistencycheck mechanism based on the learned descriptors of the 3D model and\nobserved 2D image, which downweights the unreliable correspondences during pose\noptimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and\nYCB-Video datasets validate the effectiveness of our method and demonstrate\nstate-of-the-art performance.\n","authors":["Yan Xu","Junyi Lin","Guofeng Zhang","Xiaogang Wang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2203.12870v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12868v1","updated":"2022-03-24T06:22:33Z","published":"2022-03-24T06:22:33Z","title":"DyRep: Bootstrapping Training with Dynamic Re-parameterization","summary":"  Structural re-parameterization (Rep) methods achieve noticeable improvements\non simple VGG-style networks. Despite the prevalence, current Rep methods\nsimply re-parameterize all operations into an augmented network, including\nthose that rarely contribute to the model's performance. As such, the price to\npay is an expensive computational overhead to manipulate these unnecessary\nbehaviors. To eliminate the above caveats, we aim to bootstrap the training\nwith minimal cost by devising a dynamic re-parameterization (DyRep) method,\nwhich encodes Rep technique into the training process that dynamically evolves\nthe network structures. Concretely, our proposal adaptively finds the\noperations which contribute most to the loss in the network, and applies Rep to\nenhance their representational capacity. Besides, to suppress the noisy and\nredundant operations introduced by Rep, we devise a de-parameterization\ntechnique for a more compact re-parameterization. With this regard, DyRep is\nmore efficient than Rep since it smoothly evolves the given network instead of\nconstructing an over-parameterized network. Experimental results demonstrate\nour effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\\%$\non ImageNet and reduces $22\\%$ runtime over the baseline. Code is available at:\nhttps://github.com/hunto/DyRep.\n","authors":["Tao Huang","Shan You","Bohan Zhang","Yuxuan Du","Fei Wang","Chen Qian","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2203.12868v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2112.00322v2","updated":"2022-03-24T06:12:39Z","published":"2021-12-01T07:28:52Z","title":"FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection","summary":"  Recently, promising applications in robotics and augmented reality have\nattracted considerable attention to 3D object detection from point clouds. In\nthis paper, we present FCAF3D - a first-in-class fully convolutional\nanchor-free indoor 3D object detection method. It is a simple yet effective\nmethod that uses a voxel representation of a point cloud and processes voxels\nwith sparse convolutions. FCAF3D can handle large-scale scenes with minimal\nruntime through a single fully convolutional feed-forward pass. Existing 3D\nobject detection methods make prior assumptions on the geometry of objects, and\nwe argue that it limits their generalization ability. To get rid of any prior\nassumptions, we propose a novel parametrization of oriented bounding boxes that\nallows obtaining better results in a purely data-driven way. The proposed\nmethod achieves state-of-the-art 3D object detection results in terms of\nmAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The\ncode and models are available at https://github.com/samsunglabs/fcaf3d.\n","authors":["Danila Rukhovich","Anna Vorontsova","Anton Konushin"],"pdf_url":"https://arxiv.org/pdf/2112.00322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12861v1","updated":"2022-03-24T05:56:30Z","published":"2022-03-24T05:56:30Z","title":"Transformer Compressed Sensing via Global Image Tokens","summary":"  Convolutional neural networks (CNN) have demonstrated outstanding Compressed\nSensing (CS) performance compared to traditional, hand-crafted methods.\nHowever, they are broadly limited in terms of generalisability, inductive bias\nand difficulty to model long distance relationships. Transformer neural\nnetworks (TNN) overcome such issues by implementing an attention mechanism\ndesigned to capture dependencies between inputs. However, high-resolution tasks\ntypically require vision Transformers (ViT) to decompose an image into\npatch-based tokens, limiting inputs to inherently local contexts. We propose a\nnovel image decomposition that naturally embeds images into low-resolution\ninputs. These Kaleidoscope tokens (KD) provide a mechanism for global\nattention, at the same computational cost as a patch-based approach. To\nshowcase this development, we replace CNN components in a well-known CS-MRI\nneural network with TNN blocks and demonstrate the improvements afforded by KD.\nWe also propose an ensemble of image tokens, which enhance overall image\nquality and reduces model size. Supplementary material is available:\nhttps://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git\n","authors":["Marlon Bran Lorenzana","Craig Engstrom","Shekhar S. Chandra"],"pdf_url":"https://arxiv.org/pdf/2203.12861v1.pdf","comment":"4 Pages, 4 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2203.12856v1","updated":"2022-03-24T05:38:07Z","published":"2022-03-24T05:38:07Z","title":"Beyond Fixation: Dynamic Window Visual Transformer","summary":"  Recently, a surge of interest in visual transformers is to reduce the\ncomputational cost by limiting the calculation of self-attention to a local\nwindow. Most current work uses a fixed single-scale window for modeling by\ndefault, ignoring the impact of window size on model performance. However, this\nmay limit the modeling potential of these window-based models for multi-scale\ninformation. In this paper, we propose a novel method, named Dynamic Window\nVision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT\ngoes beyond the model that employs a fixed single window setting. To the best\nof our knowledge, we are the first to use dynamic multi-scale windows to\nexplore the upper limit of the effect of window settings on model performance.\nIn DW-ViT, multi-scale information is obtained by assigning windows of\ndifferent sizes to different head groups of window multi-head self-attention.\nThen, the information is dynamically fused by assigning different weights to\nthe multi-scale window branches. We conducted a detailed performance evaluation\non three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related\nstate-of-the-art (SoTA) methods, DW-ViT obtains the best performance.\nSpecifically, compared with the current SoTA Swin Transformers\n\\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements\non all three datasets with similar parameters and computational costs. In\naddition, DW-ViT exhibits good scalability and can be easily inserted into any\nwindow-based visual transformers.\n","authors":["Pengzhen Ren","Changlin Li","Guangrun Wang","Yun Xiao","Qing Du Xiaodan Liang Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2203.12856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11799v2","updated":"2022-03-24T05:35:09Z","published":"2022-03-22T15:04:37Z","title":"AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric\n  PD and Blind-Spot Network","summary":"  Blind-spot network (BSN) and its variants have made significant advances in\nself-supervised denoising. Nevertheless, they are still bound to synthetic\nnoisy inputs due to less practical assumptions like pixel-wise independent\nnoise. Hence, it is challenging to deal with spatially correlated real-world\nnoise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has\nbeen proposed to remove the spatial correlation of real-world noise. However,\nit is not trivial to integrate PD and BSN directly, which prevents the fully\nself-supervised denoising model on real-world images. We propose an Asymmetric\nPD (AP) to address this issue, which introduces different PD stride factors for\ntraining and inference. We systematically demonstrate that the proposed AP can\nresolve inherent trade-offs caused by specific PD stride factors and make BSN\napplicable to practical scenarios. To this end, we develop AP-BSN, a\nstate-of-the-art self-supervised denoising method for real-world sRGB images.\nWe further propose random-replacing refinement, which significantly improves\nthe performance of our AP-BSN without any additional parameters. Extensive\nstudies demonstrate that our method outperforms the other self-supervised and\neven unpaired denoising methods by a large margin, without using any additional\nknowledge, e.g., noise level, regarding the underlying unknown noise.\n","authors":["Wooseok Lee","Sanghyun Son","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2203.11799v2.pdf","comment":"Accepted to CVPR2022"},{"id":"http://arxiv.org/abs/2112.09081v4","updated":"2022-03-24T05:35:02Z","published":"2021-12-16T18:05:48Z","title":"CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic\n  Data","summary":"  We present a visual localization system that learns to estimate camera poses\nin the real world with the help of synthetic data. Despite significant progress\nin recent years, most learning-based approaches to visual localization target\nat a single domain and require a dense database of geo-tagged images to\nfunction well. To mitigate the data scarcity issue and improve the scalability\nof the neural localization models, we introduce TOPO-DataGen, a versatile\nsynthetic data generation tool that traverses smoothly between the real and\nvirtual world, hinged on the geographic camera viewpoint. New large-scale\nsim-to-real benchmark datasets are proposed to showcase and evaluate the\nutility of the said synthetic data. Our experiments reveal that synthetic data\ngenerically enhances the neural network performance on real data. Furthermore,\nwe introduce CrossLoc, a cross-modal visual representation learning approach to\npose estimation that makes full use of the scene coordinate ground truth via\nself-supervision. Without any extra data, CrossLoc significantly outperforms\nthe state-of-the-art methods and achieves substantially higher real-data sample\nefficiency. Our code and datasets are all available at\nhttps://crossloc.github.io/.\n","authors":["Qi Yan","Jianhao Zheng","Simon Reding","Shanci Li","Iordan Doytchinov"],"pdf_url":"https://arxiv.org/pdf/2112.09081v4.pdf","comment":"CVPR 2022. Project page: https://crossloc.github.io/"},{"id":"http://arxiv.org/abs/2109.02973v4","updated":"2022-03-24T05:29:25Z","published":"2021-09-07T10:00:45Z","title":"Unpaired Deep Image Deraining Using Dual Contrastive Learning","summary":"  Learning single image deraining (SID) networks from an unpaired set of clean\nand rainy images is practical and valuable as acquiring paired real-world data\nis almost infeasible. However, without the paired data as the supervision,\nlearning a SID network is challenging. Moreover, simply using existing unpaired\nlearning methods (e.g., unpaired adversarial learning and cycle-consistency\nconstraints) in the SID task is insufficient to learn the underlying\nrelationship from rainy inputs to clean outputs as there exists significant\ndomain gap between the rainy and clean images. In this paper, we develop an\neffective unpaired SID adversarial framework which explores mutual properties\nof the unpaired exemplars by a dual contrastive learning manner in a deep\nfeature space, named as DCD-GAN. The proposed method mainly consists of two\ncooperative branches: Bidirectional Translation Branch (BTB) and Contrastive\nGuidance Branch (CGB). Specifically, BTB exploits full advantage of the\ncirculatory architecture of adversarial consistency to generate abundant\nexemplar pairs and excavates latent feature distributions between two domains\nby equipping it with bidirectional mapping. Simultaneously, CGB implicitly\nconstrains the embeddings of different exemplars in the deep feature space by\nencouraging the similar feature distributions closer while pushing the\ndissimilar further away, in order to better facilitate rain removal and help\nimage restoration. Extensive experiments demonstrate that our method performs\nfavorably against existing unpaired deraining approaches on both synthetic and\nreal-world datasets, and generates comparable results against several\nfully-supervised or semi-supervised models.\n","authors":["Xiang Chen","Jinshan Pan","Kui Jiang","Yufeng Li","Yufeng Huang","Caihua Kong","Longgang Dai","Zhentao Fan"],"pdf_url":"https://arxiv.org/pdf/2109.02973v4.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12853v1","updated":"2022-03-24T05:29:09Z","published":"2022-03-24T05:29:09Z","title":"Direct evaluation of progression or regression of disease burden in\n  brain metastatic disease with Deep Neuroevolution","summary":"  Purpose: A core component of advancing cancer treatment research is assessing\nresponse to therapy. Doing so by hand, for example as per RECIST or RANO\ncriteria, is tedious, time-consuming, and can miss important tumor response\ninformation; most notably, they exclude non-target lesions. We wish to assess\nchange in a holistic fashion that includes all lesions, obtaining simple,\ninformative, and automated assessments of tumor progression or regression. Due\nto often low patient enrolments in clinical trials, we wish to make response\nassessments with small training sets. Deep neuroevolution (DNE) can produce\nradiology artificial intelligence (AI) that performs well on small training\nsets. Here we use DNE for function approximation that predicts progression\nversus regression of metastatic brain disease.\n  Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training\nset. Half of these pairs, separated in time, qualified as disease progression,\nwhile the other 25 images constituted regression. We trained the parameters of\na relatively small CNN via mutations that consisted of random CNN weight\nadjustments and mutation fitness. We then incorporated the best mutations into\nthe next generations CNN, repeating this process for approximately 50,000\ngenerations. We applied the CNNs to our training set, as well as a separate\ntesting set with the same class balance of 25 progression and 25 regression\nimages.\n  Results: DNE achieved monotonic convergence to 100% training set accuracy.\nDNE also converged monotonically to 100% testing set accuracy.\n  Conclusion: DNE can accurately classify brain-metastatic disease progression\nversus regression. Future work will extend the input from 2D image slices to\nfull 3D volumes, and include the category of no change. We believe that an\napproach such as our could ultimately provide a useful adjunct to RANO/RECIST\nassessment.\n","authors":["Joseph Stember","Robert Young","Hrithwik Shalu"],"pdf_url":"https://arxiv.org/pdf/2203.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12613v2","updated":"2022-03-24T05:28:17Z","published":"2022-03-23T17:58:56Z","title":"A Hybrid Mesh-neural Representation for 3D Transparent Object\n  Reconstruction","summary":"  We propose a novel method to reconstruct the 3D shapes of transparent objects\nusing hand-held captured images under natural light conditions. It combines the\nadvantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid\nrepresentation, to simplify the capture setting used in recent contributions.\nAfter obtaining an initial shape through the multi-view silhouettes, we\nintroduce surface-based local MLPs to encode the vertex displacement field\n(VDF) for the reconstruction of surface details. The design of local MLPs\nallows to represent the VDF in a piece-wise manner using two layer MLP\nnetworks, which is beneficial to the optimization algorithm. Defining local\nMLPs on the surface instead of the volume also reduces the searching space.\nSuch a hybrid representation enables us to relax the ray-pixel correspondences\nthat represent the light path constraint to our designed ray-cell\ncorrespondences, which significantly simplifies the implementation of\nsingle-image based environment matting algorithm. We evaluate our\nrepresentation and reconstruction algorithm on several transparent objects with\nground truth models. Our experiments show that our method can produce\nhigh-quality reconstruction results superior to state-of-the-art methods using\na simplified data acquisition setup.\n","authors":["Jiamin Xu","Zihan Zhu","Hujun Bao","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2203.12613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.10990v2","updated":"2022-03-24T05:26:47Z","published":"2021-11-22T05:07:36Z","title":"Imperceptible Transfer Attack and Defense on 3D Point Cloud\n  Classification","summary":"  Although many efforts have been made into attack and defense on the 2D image\ndomain in recent years, few methods explore the vulnerability of 3D models.\nExisting 3D attackers generally perform point-wise perturbation over point\nclouds, resulting in deformed structures or outliers, which is easily\nperceivable by humans. Moreover, their adversarial examples are generated under\nthe white-box setting, which frequently suffers from low success rates when\ntransferred to attack remote black-box models. In this paper, we study 3D point\ncloud attacks from two new and challenging perspectives by proposing a novel\nImperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the\nperturbation direction of each point along its normal vector of the\nneighborhood surface, leading to generated examples with similar geometric\nproperties and thus enhancing the imperceptibility. 2) Transferability: we\ndevelop an adversarial transformation model to generate the most harmful\ndistortions and enforce the adversarial examples to resist it, improving their\ntransferability to unknown black-box models. Further, we propose to train more\nrobust black-box 3D models to defend against such ITA attacks by learning more\ndiscriminative point cloud representations. Extensive evaluations demonstrate\nthat our ITA attack is more imperceptible and transferable than\nstate-of-the-arts and validate the superiority of our defense strategy.\n","authors":["Daizong Liu","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2111.10990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12849v1","updated":"2022-03-24T05:12:54Z","published":"2022-03-24T05:12:54Z","title":"Semantic Image Manipulation with Background-guided Internal Learning","summary":"  Image manipulation has attracted a lot of interest due to its wide range of\napplications. Prior work modifies images either from low-level manipulation,\nsuch as image inpainting or through manual edits via paintbrushes and\nscribbles, or from high-level manipulation, employing deep generative networks\nto output an image conditioned on high-level semantic input. In this study, we\npropose Semantic Image Manipulation with Background-guided Internal Learning\n(SIMBIL), which combines high-level and low-level manipulation. Specifically,\nusers can edit an image at the semantic level by applying changes on a scene\ngraph. Then our model manipulates the image at the pixel level according to the\nmodified scene graph. There are two major advantages of our approach. First,\nhigh-level manipulation of scene graphs requires less manual effort from the\nuser compared to manipulating raw image pixels. Second, our low-level internal\nlearning approach is scalable to images of various sizes without reliance on\nexternal visual datasets for training. We outperform the state-of-the-art in a\nquantitative and qualitative evaluation on the CLEVR and Visual Genome\ndatasets. Experiments show 8 points improvement on FID scores (CLEVR) and 27%\nimprovement on user evaluation (Visual Genome), demonstrating the effectiveness\nof our approach.\n","authors":["Zhongping Zhang","Huiwen He","Bryan A. Plummer","Zhenyu Liao","Huayan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12848v1","updated":"2022-03-24T05:06:46Z","published":"2022-03-24T05:06:46Z","title":"Keypoints Tracking via Transformer Networks","summary":"  In this thesis, we propose a pioneering work on sparse keypoints tracking\nacross images using transformer networks. While deep learning-based keypoints\nmatching have been widely investigated using graph neural networks - and more\nrecently transformer networks, they remain relatively too slow to operate in\nreal-time and are particularly sensitive to the poor repeatability of the\nkeypoints detectors. In order to address these shortcomings, we propose to\nstudy the particular case of real-time and robust keypoints tracking.\nSpecifically, we propose a novel architecture which ensures a fast and robust\nestimation of the keypoints tracking between successive images of a video\nsequence. Our method takes advantage of a recent breakthrough in computer\nvision, namely, visual transformer networks. Our method consists of two\nsuccessive stages, a coarse matching followed by a fine localization of the\nkeypoints' correspondences prediction. Through various experiments, we\ndemonstrate that our approach achieves competitive results and demonstrates\nhigh robustness against adverse conditions, such as illumination change,\nocclusion and viewpoint differences.\n","authors":["Oleksii Nasypanyi","Francois Rameau"],"pdf_url":"https://arxiv.org/pdf/2203.12848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12845v1","updated":"2022-03-24T04:55:21Z","published":"2022-03-24T04:55:21Z","title":"Multiple Emotion Descriptors Estimation at the ABAW3 Challenge","summary":"  To describe complex emotional states, psychologists have proposed multiple\nemotion descriptors: sparse descriptors like facial action units; continuous\ndescriptors like valence and arousal; and discrete class descriptors like\nhappiness and anger. According to Ekman and Friesen, 1969, facial action units\nare sign vehicles that convey the emotion message, while discrete or continuous\nemotion descriptors are the messages perceived and expressed by human.\n  In this paper, we designed an architecture for multiple emotion descriptors\nestimation in participating the ABAW3 Challenge. Based on the theory of Ekman\nand Friesen, 1969, we designed distinct architectures to measure the sign\nvehicles (i.e., facial action units) and the message (i.e., discrete emotions,\nvalence and arousal) given their different properties. The quantitative\nexperiments on the ABAW3 challenge dataset has shown the superior performance\nof our approach over two baseline models.\n","authors":["Didan Deng"],"pdf_url":"https://arxiv.org/pdf/2203.12845v1.pdf","comment":"The technical report for our multi-task approach in the ABAW3\n  Challenge"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2203.13088v1","updated":"2022-03-24T14:28:07Z","published":"2022-03-24T14:28:07Z","title":"Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized\n  Late Interactions using Enhanced Reduction","summary":"  Recent progress in neural information retrieval has demonstrated large gains\nin effectiveness, while often sacrificing the efficiency and interpretability\nof the neural model compared to classical approaches. This paper proposes\nColBERTer, a neural retrieval model using contextualized late interaction\n(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,\nColBERTer's reductions dramatically lower ColBERT's storage requirements while\nsimultaneously improving the interpretability of its token-matching scores. To\nthis end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and\noptional lexical matching components into one model. For its multi-vector\ncomponent, ColBERTer reduces the number of stored vectors per document by\nlearning unique whole-word representations for the terms in each document and\nlearning to identify and remove word representations that are not essential to\neffective scoring. We employ an explicit multi-task, multi-stage training to\nfacilitate using very small vector dimensions. Results on the MS MARCO and\nTREC-DL collection show that ColBERTer can reduce the storage footprint by up\nto 2.5x, while maintaining effectiveness. With just one dimension per token in\nits smallest setting, ColBERTer achieves index storage parity with the\nplaintext size, with very strong effectiveness results. Finally, we demonstrate\nColBERTer's robustness on seven high-quality out-of-domain collections,\nyielding statistically significant gains over traditional retrieval baselines.\n","authors":["Sebastian Hofstätter","Omar Khattab","Sophia Althammer","Mete Sertkan","Allan Hanbury"],"pdf_url":"https://arxiv.org/pdf/2203.13088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.10665v2","updated":"2022-03-24T13:51:09Z","published":"2021-09-22T11:52:38Z","title":"A Survey on Reinforcement Learning for Recommender Systems","summary":"  Recommender systems have been widely applied in different real-life scenarios\nto help us find useful information. In particular, Reinforcement Learning (RL)\nbased recommender systems have become an emerging research topic in recent\nyears. Empirical results show that RL-based recommendation methods often\nsurpass most of supervised learning methods, owing to the interactive nature\nand autonomous learning ability. Nevertheless, there are various challenges of\napplying RL in recommender systems. To understand the challenges and relevant\nsolutions, there should be a reference for researchers and practitioners\nworking on RL-based recommender systems. To this end, we firstly provide a\nthorough overview, comparisons, and summarization of RL approaches applied in\nfour typical recommendation scenarios, including interactive recommendation,\nconversational recommendatin, sequential recommendation, and explainable\nrecommendation. We further systematically analyze the challenges and relevant\nsolutions on the basis of existing literature. Finally, under discussion for\nopen issues of RL and its limitations of recommender systems, we highlight some\npotential research directions in this field.\n","authors":["Yuanguo Lin","Yong Liu","Fan Lin","Lixin Zou","Pengcheng Wu","Wenhua Zeng","Huanhuan Chen","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2109.10665v2.pdf","comment":"21 pages, 3 figures"},{"id":"http://arxiv.org/abs/2203.13040v1","updated":"2022-03-24T12:41:04Z","published":"2022-03-24T12:41:04Z","title":"Semantic system for searching of employees","summary":"  Many people have stress to leave their job and start a new one because of the\nnew environment and not enough knowledge about the culture and structure about\nthe new organization they are going to work in. New employees in company\nnormally need to integrate in their working place environment quicker to start\ndoing their job. That makes them ask a lot of questions to their colleagues and\nsometimes their colleagues are too busy to answer those questions. In the\nliterature is defined that this problem could be solved when new employees use\ndigital system for information as the proposed system for searching of\ninformation. Furthermore, the quality of the returned results from the\nsearching system is defined as a standard for the efficiency of the searching\nsystems. Because of this, it is proposed a semantic system for searching\ninformation of employees in a company that will help to better orient new\nemployees in a company, to know the position and the function of each employee\nin the company\n","authors":["Mariya Evtimova-Gardair","Tasho Tashev"],"pdf_url":"https://arxiv.org/pdf/2203.13040v1.pdf","comment":"132-136 pages, 9 figures,International Conference on Applied Physics,\n  Simulation and Computing (APSAC)Croatia, Dubrovnik, Sept.28-29 2018"},{"id":"http://arxiv.org/abs/2201.07620v2","updated":"2022-03-24T08:58:55Z","published":"2022-01-19T14:25:55Z","title":"Validating Simulations of User Query Variants","summary":"  System-oriented IR evaluations are limited to rather abstract understandings\nof real user behavior. As a solution, simulating user interactions provides a\ncost-efficient way to support system-oriented experiments with more realistic\ndirectives when no interaction logs are available. While there are several user\nmodels for simulated clicks or result list interactions, very few attempts have\nbeen made towards query simulations, and it has not been investigated if these\ncan reproduce properties of real queries. In this work, we validate simulated\nuser query variants with the help of TREC test collections in reference to real\nuser queries that were made for the corresponding topics. Besides, we introduce\na simple yet effective method that gives better reproductions of real queries\nthan the established methods. Our evaluation framework validates the\nsimulations regarding the retrieval performance, reproducibility of topic score\ndistributions, shared task utility, effort and effect, and query term\nsimilarity when compared with real user query variants. While the retrieval\neffectiveness and statistical properties of the topic score distributions as\nwell as economic aspects are close to that of real queries, it is still\nchallenging to simulate exact term matches and later query reformulations.\n","authors":["Timo Breuer","Norbert Fuhr","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2201.07620v2.pdf","comment":"Accepted at ECIR22"},{"id":"http://arxiv.org/abs/2203.12800v1","updated":"2022-03-24T02:04:33Z","published":"2022-03-24T02:04:33Z","title":"Predicting the longevity of resources shared in scientific publications","summary":"  Research has shown that most resources shared in articles (e.g., URLs to code\nor data) are not kept up to date and mostly disappear from the web after some\nyears (Zeng et al., 2019). Little is known about the factors that differentiate\nand predict the longevity of these resources. This article explores a range of\nexplanatory features related to the publication venue, authors, references, and\nwhere the resource is shared. We analyze an extensive repository of\npublications and, through web archival services, reconstruct how they looked at\ndifferent time points. We discover that the most important factors are related\nto where and how the resource is shared, and surprisingly little is explained\nby the author's reputation or prestige of the journal. By examining the places\nwhere long-lasting resources are shared, we suggest that it is critical to\ndisseminate and create standards with modern technologies. Finally, we discuss\nimplications for reproducibility and recognizing scientific datasets as\nfirst-class citizens.\n","authors":["Daniel E. Acuna","Jian Jian","Tong Zeng","Lizhen Liang","Han Zhuang"],"pdf_url":"https://arxiv.org/pdf/2203.12800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.09663v2","updated":"2022-03-24T01:59:44Z","published":"2021-10-19T00:12:25Z","title":"EILEEN: A recommendation system for scientific publications and grants","summary":"  Finding relevant scientific articles is crucial for advancing knowledge.\nRecommendation systems are helpful for such purpose, although they have only\nbeen applied to science recently. This article describes EILEEN (Exploratory\nInnovator of LitEraturE Networks), a recommendation system for scientific\npublications and grants with open source code and datasets. We describe\nEILEEN's architecture for ingesting and processing documents and modeling the\nrecommendation system and keyphrase estimator. Using a unique dataset of log-in\nuser behavior, we validate our recommendation system against Latent Semantic\nAnalysis (LSA) and the standard ranking from Elasticsearch (Lucene scoring). We\nfind that a learning-to-rank with Random Forest achieves an AUC of 0.9,\nsignificantly outperforming both baselines. Our results suggest that we can\nsubstantially improve science recommendations and learn about scientists'\nbehavior through their search behavior. We make our system available through\neileen.io\n","authors":["Daniel E. Acuna","Kartik Nagre","Priya Matnani"],"pdf_url":"https://arxiv.org/pdf/2110.09663v2.pdf","comment":"16 pages, 3 figures, 2 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2203.12592v2","updated":"2022-03-24T17:59:01Z","published":"2022-03-23T17:54:20Z","title":"Your Policy Regularizer is Secretly an Adversary","summary":"  Policy regularization methods such as maximum entropy regularization are\nwidely used in reinforcement learning to improve the robustness of a learned\npolicy. In this paper, we show how this robustness arises from hedging against\nworst-case perturbations of the reward function, which are chosen from a\nlimited set by an imagined adversary. Using convex duality, we characterize\nthis robust set of adversarial reward perturbations under KL and\nalpha-divergence regularization, which includes Shannon and Tsallis entropy\nregularization as special cases. Importantly, generalization guarantees can be\ngiven within this robust set. We provide detailed discussion of the worst-case\nreward perturbations, and present intuitive empirical examples to illustrate\nthis robustness and its relationship with generalization. Finally, we discuss\nhow our analysis complements and extends previous results on adversarial reward\nrobustness and path consistency optimality conditions.\n","authors":["Rob Brekelmans","Tim Genewein","Jordi Grau-Moya","Grégoire Delétang","Markus Kunesch","Shane Legg","Pedro Ortega"],"pdf_url":"https://arxiv.org/pdf/2203.12592v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2203.13251v1","updated":"2022-03-24T17:58:54Z","published":"2022-03-24T17:58:54Z","title":"Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient\n  Dexterous Manipulation","summary":"  Optimizing behaviors for dexterous manipulation has been a longstanding\nchallenge in robotics, with a variety of methods from model-based control to\nmodel-free reinforcement learning having been previously explored in\nliterature. Perhaps one of the most powerful techniques to learn complex\nmanipulation strategies is imitation learning. However, collecting and learning\nfrom demonstrations in dexterous manipulation is quite challenging. The\ncomplex, high-dimensional action-space involved with multi-finger control often\nleads to poor sample efficiency of learning-based methods. In this work, we\npropose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning\nframework for dexterous manipulation. DIME only requires a single RGB camera to\nobserve a human operator and teleoperate our robotic hand. Once demonstrations\nare collected, DIME employs standard imitation learning methods to train\ndexterous manipulation policies. On both simulation and real robot benchmarks\nwe demonstrate that DIME can be used to solve complex, in-hand manipulation\ntasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro\nhand. Our framework along with pre-collected demonstrations is publicly\navailable at https://nyu-robot-learning.github.io/dime.\n","authors":["Sridhar Pandian Arunachalam","Sneha Silwal","Ben Evans","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2203.13251v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2203.13248v1","updated":"2022-03-24T17:57:11Z","published":"2022-03-24T17:57:11Z","title":"Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer","summary":"  Recent studies on StyleGAN show high performance on artistic portrait\ngeneration by transfer learning with limited data. In this paper, we explore\nmore challenging exemplar-based high-resolution portrait style transfer by\nintroducing a novel DualStyleGAN with flexible control of dual styles of the\noriginal face domain and the extended artistic portrait domain. Different from\nStyleGAN, DualStyleGAN provides a natural way of style transfer by\ncharacterizing the content and style of a portrait with an intrinsic style path\nand a new extrinsic style path, respectively. The delicately designed extrinsic\nstyle path enables our model to modulate both the color and complex structural\nstyles hierarchically to precisely pastiche the style example. Furthermore, a\nnovel progressive fine-tuning scheme is introduced to smoothly transform the\ngenerative space of the model to the target domain, even with the above\nmodifications on the network architecture. Experiments demonstrate the\nsuperiority of DualStyleGAN over state-of-the-art methods in high-quality\nportrait style transfer and flexible style control.\n","authors":["Shuai Yang","Liming Jiang","Ziwei Liu","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2203.13248v1.pdf","comment":"CVPR 2022. Code: https://github.com/williamyang1991/DualStyleGAN\n  Project page: https://www.mmlab-ntu.com/project/dualstylegan/"},{"id":"http://arxiv.org/abs/2203.13240v1","updated":"2022-03-24T17:50:46Z","published":"2022-03-24T17:50:46Z","title":"Token Dropping for Efficient BERT Pretraining","summary":"  Transformer-based models generally allocate the same amount of computation\nfor each token in a given sequence. We develop a simple but effective \"token\ndropping\" method to accelerate the pretraining of transformer models, such as\nBERT, without degrading its performance on downstream tasks. In short, we drop\nunimportant tokens starting from an intermediate layer in the model to make the\nmodel focus on important tokens; the dropped tokens are later picked up by the\nlast layer of the model so that the model still produces full-length sequences.\nWe leverage the already built-in masked language modeling (MLM) loss to\nidentify unimportant tokens with practically no computational overhead. In our\nexperiments, this simple approach reduces the pretraining cost of BERT by 25%\nwhile achieving similar overall fine-tuning performance on standard downstream\ntasks.\n","authors":["Le Hou","Richard Yuanzhe Pang","Tianyi Zhou","Yuexin Wu","Xinying Song","Xiaodan Song","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.13240v1.pdf","comment":"ACL 2022"},{"id":"http://arxiv.org/abs/2112.06825v2","updated":"2022-03-24T17:33:07Z","published":"2021-12-13T17:35:26Z","title":"VL-Adapter: Parameter-Efficient Transfer Learning for\n  Vision-and-Language Tasks","summary":"  Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.\n","authors":["Yi-Lin Sung","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2112.06825v2.pdf","comment":"CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)"},{"id":"http://arxiv.org/abs/2203.13225v1","updated":"2022-03-24T17:31:43Z","published":"2022-03-24T17:31:43Z","title":"Distributionally Robust Optimization via Ball Oracle Acceleration","summary":"  We develop and analyze algorithms for distributionally robust optimization\n(DRO) of convex losses. In particular, we consider group-structured and bounded\n$f$-divergence uncertainty sets. Our approach relies on an accelerated method\nthat queries a ball optimization oracle, i.e., a subroutine that minimizes the\nobjective within a small ball around the query point. Our main contribution is\nefficient implementations of this oracle for DRO objectives. For DRO with $N$\nnon-smooth loss functions, the resulting algorithms find an $\\epsilon$-accurate\nsolution with $\\widetilde{O}\\left(N\\epsilon^{-2/3} + \\epsilon^{-2}\\right)$\nfirst-order oracle queries to individual loss functions. Compared to existing\nalgorithms for this problem, we improve complexity by a factor of up to\n$\\epsilon^{-4/3}$.\n","authors":["Yair Carmon","Danielle Hausler"],"pdf_url":"https://arxiv.org/pdf/2203.13225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12622v1","updated":"2022-03-24T17:11:36Z","published":"2022-03-24T17:11:36Z","title":"Are Evolutionary Algorithms Safe Optimizers?","summary":"  We consider a type of constrained optimization problem, where the violation\nof a constraint leads to an irrevocable loss, such as breakage of a valuable\nexperimental resource/platform or loss of human life. Such problems are\nreferred to as safe optimization problems (SafeOPs). While SafeOPs have\nreceived attention in the machine learning community in recent years, there was\nlittle interest in the evolutionary computation (EC) community despite some\nearly attempts between 2009 and 2011. Moreover, there is a lack of acceptable\nguidelines on how to benchmark different algorithms for SafeOPs, an area where\nthe EC community has significant experience in. Driven by the need for more\nefficient algorithms and benchmark guidelines for SafeOPs, the objective of\nthis paper is to reignite the interest of this problem class in the EC\ncommunity. To achieve this we (i) provide a formal definition of SafeOPs and\ncontrast it to other types of optimization problems that the EC community is\nfamiliar with, (ii) investigate the impact of key SafeOP parameters on the\nperformance of selected safe optimization algorithms, (iii) benchmark EC\nagainst state-of-the-art safe optimization algorithms from the machine learning\ncommunity, and (iv) provide an open-source Python framework to replicate and\nextend our work.\n","authors":["Youngmin Kim","Richard Allmendinger","Manuel López-Ibáñez"],"pdf_url":"https://arxiv.org/pdf/2203.12622v1.pdf","comment":"Accepted for GECCO 2022. 8 pages, excluding references, accompanied\n  by supplementary material (4 pages, excluding references). 6 figures (and 6\n  figures in supplementary material also)"},{"id":"http://arxiv.org/abs/2106.02575v5","updated":"2022-03-24T16:50:47Z","published":"2021-06-04T16:17:21Z","title":"Optimal Rates of (Locally) Differentially Private Heavy-tailed\n  Multi-Armed Bandits","summary":"  In this paper we investigate the problem of stochastic multi-armed bandits\n(MAB) in the (local) differential privacy (DP/LDP) model. Unlike previous\nresults that assume bounded/sub-Gaussian reward distributions, we focus on the\nsetting where each arm's reward distribution only has $(1+v)$-th moment with\nsome $v\\in (0, 1]$. In the first part, we study the problem in the central\n$\\epsilon$-DP model. We first provide a near-optimal result by developing a\nprivate and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the\nresult via a private and robust version of the Successive Elimination (SE)\nalgorithm. Finally, we establish the lower bound to show that the\ninstance-dependent regret of our improved algorithm is optimal. In the second\npart, we study the problem in the $\\epsilon$-LDP model. We propose an algorithm\nthat can be seen as locally private and robust version of SE algorithm, which\nprovably achieves (near) optimal rates for both instance-dependent and\ninstance-independent regret. Our results reveal differences between the problem\nof private MAB with bounded/sub-Gaussian rewards and heavy-tailed rewards. To\nachieve these (near) optimal rates, we develop several new hard instances and\nprivate robust estimators as byproducts, which might be used to other related\nproblems. Finally, experiments also support our theoretical findings and show\nthe effectiveness of our algorithms.\n","authors":["Youming Tao","Yulian Wu","Peng Zhao","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2106.02575v5.pdf","comment":"Accepted for oral presentation at AISTATS 2022. A preliminary version\n  of this paper was presented at the CCS 2021 workshop Privacy Preserving\n  Machine Learning (PPML'21). In this version, we fixed some typos"},{"id":"http://arxiv.org/abs/2203.13167v1","updated":"2022-03-24T16:40:36Z","published":"2022-03-24T16:40:36Z","title":"Towards Exemplar-Free Continual Learning in Vision Transformers: an\n  Account of Attention, Functional and Weight Regularization","summary":"  In this paper, we investigate the continual learning of Vision Transformers\n(ViT) for the challenging exemplar-free scenario, with special focus on how to\nefficiently distill the knowledge of its crucial self-attention mechanism\n(SAM). Our work takes an initial step towards a surgical investigation of SAM\nfor designing coherent continual learning methods in ViTs. We first carry out\nan evaluation of established continual learning regularization techniques. We\nthen examine the effect of regularization when applied to two key enablers of\nSAM: (a) the contextualized embedding layers, for their ability to capture\nwell-scaled representations with respect to the values, and (b) the prescaled\nattention maps, for carrying value-independent global contextual information.\nWe depict the perks of each distilling strategy on two image recognition\nbenchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall\naccuracy, (b) helps enhance the rigidity by maintaining competitive\nperformances. Furthermore, we identify the limitation imposed by the symmetric\nnature of regularization losses. To alleviate this, we propose an asymmetric\nvariant and apply it to the pooled output distillation (POD) loss adapted for\nViTs. Our experiments confirm that introducing asymmetry to POD boosts its\nplasticity while retaining stability across (a) and (b). Moreover, we\nacknowledge low forgetting measures for all the compared methods, indicating\nthat ViTs might be naturally inclined continual learner\n","authors":["Francesco Pelosin","Saurav Jha","Andrea Torsello","Bogdan Raducanu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2203.13167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.03820v2","updated":"2022-03-24T16:37:58Z","published":"2021-06-07T17:35:54Z","title":"Accurate Shapley Values for explaining tree-based models","summary":"  Although Shapley Values (SV) are widely used in explainable AI, they can be\npoorly understood and estimated, implying that their analysis may lead to\nspurious inferences and explanations. As a starting point, we remind an\ninvariance principle for SV and derive the correct approach for computing the\nSV of categorical variables that are particularly sensitive to the encoding\nused. In the case of tree-based models, we introduce two estimators of Shapley\nValues that exploit the tree structure efficiently and are more accurate than\nstate-of-the-art methods. Simulations and comparisons are performed with\nstate-of-the-art algorithms and show the practical gain of our approach.\nFinally, we discuss the ability of SV to provide reliable local explanations.\nWe also provide a Python package that computes our estimators at\nhttps://github.com/salimamoukou/acv00.\n","authors":["Salim I. Amoukou","Nicolas J-B. Brunel","Tangi Salaün"],"pdf_url":"https://arxiv.org/pdf/2106.03820v2.pdf","comment":"Accepted at the 25th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), 2022. V2: The section on Active\n  Shapley Values has been removed in this updated version"},{"id":"http://arxiv.org/abs/2203.08368v2","updated":"2022-03-24T16:36:47Z","published":"2022-03-16T03:23:50Z","title":"Mixed-Precision Neural Network Quantization via Learned Layer-wise\n  Importance","summary":"  The exponentially large discrete search space in mixed-precision quantization\n(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous\nworks usually resort to iterative search methods on the training set, which\nconsume hundreds or even thousands of GPU-hours. In this study, we reveal that\nsome unique learnable parameters in quantization, namely the scale factors in\nthe quantizer, can serve as importance indicators of a layer, reflecting the\ncontribution of that layer to the final accuracy at certain bit-widths. These\nimportance indicators naturally perceive the numerical transformation during\nquantization-aware training, which can precisely and correctly provide\nquantization sensitivity metrics of layers. However, a deep network always\ncontains hundreds of such indicators, and training them one by one would lead\nto an excessive time cost. To overcome this issue, we propose a joint training\nscheme that can obtain all indicators at once. It considerably speeds up the\nindicators training process by parallelizing the original sequential training\nprocesses. With these learned importance indicators, we formulate the MPQ\nsearch problem as a one-time integer linear programming (ILP) problem. That\navoids the iterative search and significantly reduces search time without\nlimiting the bit-width search space. For example, MPQ search on ResNet18 with\nour indicators takes only 0.06 seconds. Also, extensive experiments show our\napproach can achieve SOTA accuracy on ImageNet for far-ranging models with\nvarious constraints (e.g., BitOps, compress rate).\n","authors":["Chen Tang","Kai Ouyang","Zhi Wang","Yifei Zhu","Yaowei Wang","Wen Ji","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.08368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13154v1","updated":"2022-03-24T16:19:19Z","published":"2022-03-24T16:19:19Z","title":"Addressing Missing Sources with Adversarial Support-Matching","summary":"  When trained on diverse labeled data, machine learning models have proven\nthemselves to be a powerful tool in all facets of society. However, due to\nbudget limitations, deliberate or non-deliberate censorship, and other problems\nduring data collection and curation, the labeled training set might exhibit a\nsystematic shortage of data for certain groups. We investigate a scenario in\nwhich the absence of certain data is linked to the second level of a two-level\nhierarchy in the data. Inspired by the idea of protected groups from\nalgorithmic fairness, we refer to the partitions carved by this second level as\n\"subgroups\"; we refer to combinations of subgroups and classes, or leaves of\nthe hierarchy, as \"sources\". To characterize the problem, we introduce the\nconcept of classes with incomplete subgroup support. The representational bias\nin the training set can give rise to spurious correlations between the classes\nand the subgroups which render standard classification models ungeneralizable\nto unseen sources. To overcome this bias, we make use of an additional, diverse\nbut unlabeled dataset, called the \"deployment set\", to learn a representation\nthat is invariant to subgroup. This is done by adversarially matching the\nsupport of the training and deployment sets in representation space. In order\nto learn the desired invariance, it is paramount that the sets of samples\nobserved by the discriminator are balanced by class; this is easily achieved\nfor the training set, but requires using semi-supervised clustering for the\ndeployment set. We demonstrate the effectiveness of our method with experiments\non several datasets and variants of the problem.\n","authors":["Thomas Kehrenberg","Myles Bartlett","Viktoriia Sharmanska","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2203.13154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.04302v2","updated":"2022-03-24T16:14:07Z","published":"2022-02-09T06:28:37Z","title":"On the Implicit Bias of Gradient Descent for Temporal Extrapolation","summary":"  When using recurrent neural networks (RNNs) it is common practice to apply\ntrained models to sequences longer than those seen in training. This\n\"extrapolating\" usage deviates from the traditional statistical learning setup\nwhere guarantees are provided under the assumption that train and test\ndistributions are identical. Here we set out to understand when RNNs can\nextrapolate, focusing on a simple case where the data generating distribution\nis memoryless. We first show that even with infinite training data, there exist\nRNN models that interpolate perfectly (i.e., they fit the training data) yet\nextrapolate poorly to longer sequences. We then show that if gradient descent\nis used for training, learning will converge to perfect extrapolation under\ncertain assumptions on initialization. Our results complement recent studies on\nthe implicit bias of gradient descent, showing that it plays a key role in\nextrapolation when learning temporal prediction models.\n","authors":["Edo Cohen-Karlik","Avichai Ben David","Nadav Cohen","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2202.04302v2.pdf","comment":"8 pages, 5 figures (plus appendix), AISTATS2022"},{"id":"http://arxiv.org/abs/2203.13151v1","updated":"2022-03-24T16:12:21Z","published":"2022-03-24T16:12:21Z","title":"Multi-armed bandits for online optimization of language model\n  pre-training: the use case of dynamic masking","summary":"  Transformer-based language models (TLMs) provide state-of-the-art performance\nin many modern natural language processing applications. TLM training is\nconducted in two phases. First, the model is pre-trained over large volumes of\ntext to minimize a generic objective function, such as the Masked Language\nModel (MLM). Second, the model is fine-tuned in specific downstream tasks.\nPre-training requires large volumes of data and high computational resources,\nwhile introducing many still unresolved design choices. For instance, selecting\nhyperparameters for language model pre-training is often carried out based on\nheuristics or grid-based searches. In this work, we propose a multi-armed\nbandit-based online optimization framework for the sequential selection of\npre-training hyperparameters to optimize language model performance. We pose\nthe pre-training procedure as a sequential decision-making task, where at each\npre-training step, an agent must determine what hyperparameters to use towards\noptimizing the pre-training objective. We propose a Thompson sampling bandit\nalgorithm, based on a surrogate Gaussian process reward model of the MLM\npre-training objective, for its sequential minimization. We empirically show\nhow the proposed Gaussian process based Thompson sampling pre-trains robust and\nwell-performing language models. Namely, by sequentially selecting masking\nhyperparameters of the TLM, we achieve satisfactory performance in less epochs,\nnot only in terms of the pre-training MLM objective, but in diverse downstream\nfine-tuning tasks. The proposed bandit-based technique provides an automated\nhyperparameter selection method for pre-training TLMs of interest to\npractitioners. In addition, our results indicate that, instead of MLM\npre-training with fixed masking probabilities, sequentially adapting the\nmasking hyperparameters improves both pre-training loss and downstream task\nmetrics.\n","authors":["Iñigo Urteaga","Moulay-Zaïdane Draïdia","Tomer Lancewicki","Shahram Khadivi"],"pdf_url":"https://arxiv.org/pdf/2203.13151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.10982v3","updated":"2022-03-24T15:49:07Z","published":"2021-12-21T04:44:57Z","title":"Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning","summary":"  Generalized few-shot semantic segmentation was introduced to move beyond only\nevaluating few-shot segmentation models on novel classes to include testing\ntheir ability to remember base classes. While the current state-of-the-art\napproach is based on meta-learning, it performs poorly and saturates in\nlearning after observing only a few shots. We propose the first fine-tuning\nsolution, and demonstrate that it addresses the saturation problem while\nachieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We\nalso show that it outperforms existing methods, whether fine-tuning multiple\nfinal layers or only the final layer. Finally, we present a triplet loss\nregularization that shows how to redistribute the balance of performance\nbetween novel and base categories so that there is a smaller gap between them.\n","authors":["Josh Myers-Dean","Yinan Zhao","Brian Price","Scott Cohen","Danna Gurari"],"pdf_url":"https://arxiv.org/pdf/2112.10982v3.pdf","comment":"Includes supplementary materials"},{"id":"http://arxiv.org/abs/2203.13131v1","updated":"2022-03-24T15:44:50Z","published":"2022-03-24T15:44:50Z","title":"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors","summary":"  Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.\n","authors":["Oran Gafni","Adam Polyak","Oron Ashual","Shelly Sheynin","Devi Parikh","Yaniv Taigman"],"pdf_url":"https://arxiv.org/pdf/2203.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09490v3","updated":"2022-03-24T15:28:39Z","published":"2021-12-17T13:00:37Z","title":"Visual Microfossil Identification via Deep Metric Learning","summary":"  We apply deep metric learning for the first time to the problem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualization options for\nhuman experts and cannot be applied to open-set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualization of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning outperforms all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Keycode, network weights, and data splits are\npublished with this paper for full reproducibility.\n","authors":["Tayfun Karaderi","Tilo Burghardt","Allison Y. Hsiang","Jacob Ramaer","Daniela N. Schmidt"],"pdf_url":"https://arxiv.org/pdf/2112.09490v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.00656v2","updated":"2022-03-24T15:26:04Z","published":"2021-07-01T18:00:00Z","title":"Shared Data and Algorithms for Deep Learning in Fundamental Physics","summary":"  We introduce a Python package that provides simply and unified access to a\ncollection of datasets from fundamental physics research - including particle\nphysics, astroparticle physics, and hadron- and nuclear physics - for\nsupervised machine learning studies. The datasets contain hadronic top quarks,\ncosmic-ray induced air showers, phase transitions in hadronic matter, and\ngenerator-level histories. While public datasets from multiple fundamental\nphysics disciplines already exist, the common interface and provided reference\nmodels simplify future work on cross-disciplinary machine learning and transfer\nlearning in fundamental physics. We discuss the design and structure and line\nout how additional datasets can be submitted for inclusion.\n  As showcase application, we present a simple yet flexible graph-based neural\nnetwork architecture that can easily be applied to a wide range of supervised\nlearning tasks. We show that our approach reaches performance close to\ndedicated methods on all datasets. To simplify adaptation for various problems,\nwe provide easy-to-follow instructions on how graph-based representations of\ndata structures, relevant for fundamental physics, can be constructed and\nprovide code implementations for several of them. Implementations are also\nprovided for our proposed method and all reference algorithms.\n","authors":["Lisa Benato","Erik Buhmann","Martin Erdmann","Peter Fackeldey","Jonas Glombitza","Nikolai Hartmann","Gregor Kasieczka","William Korcari","Thomas Kuhr","Jan Steinheimer","Horst Stöcker","Tilman Plehn","Kai Zhou"],"pdf_url":"https://arxiv.org/pdf/2107.00656v2.pdf","comment":"14 pages, 3 figures, 5 tables - Version accepted by Computing and\n  Software for Big Science"},{"id":"http://arxiv.org/abs/2202.00100v4","updated":"2022-03-24T15:07:16Z","published":"2022-01-31T21:36:16Z","title":"Calibration of P-values for calibration and for deviation of a\n  subpopulation from the full population","summary":"  The author's recent research papers, \"Cumulative deviation of a subpopulation\nfrom the full population\" and \"A graphical method of cumulative differences\nbetween two subpopulations\" (both published in volume 8 of Springer's\nopen-access \"Journal of Big Data\" during 2021), propose graphical methods and\nsummary statistics, without extensively calibrating formal significance tests.\nThe summary metrics and methods can measure the calibration of probabilistic\npredictions and can assess differences in responses between a subpopulation and\nthe full population while controlling for a covariate or score via conditioning\non it. These recently published papers construct significance tests based on\nthe scalar summary statistics, but only sketch how to calibrate the attained\nsignificance levels (also known as \"P-values\") for the tests. The present\narticle reviews and synthesizes work spanning many decades in order to detail\nhow to calibrate the P-values. The present paper presents computationally\nefficient, easily implemented numerical methods for evaluating properly\ncalibrated P-values, together with rigorous mathematical proofs guaranteeing\ntheir accuracy, and illustrates and validates the methods with open-source\nsoftware and numerical examples.\n","authors":["Mark Tygert"],"pdf_url":"https://arxiv.org/pdf/2202.00100v4.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.13110v1","updated":"2022-03-24T15:06:01Z","published":"2022-03-24T15:06:01Z","title":"Position Tracking using Likelihood Modeling of Channel Features with\n  Gaussian Processes","summary":"  Recent localization frameworks exploit spatial information of complex channel\nmeasurements (CMs) to estimate accurate positions even in multipath propagation\nscenarios. State-of-the art CM fingerprinting(FP)-based methods employ\nconvolutional neural networks (CNN) to extract the spatial information.\nHowever, they need spatially dense data sets (associated with high acquisition\nand maintenance efforts) to work well -- which is rarely the case in practical\napplications. If such data is not available (or its quality is low), we cannot\ncompensate the performance degradation of CNN-based FP as they do not provide\nstatistical position estimates, which prevents a fusion with other sources of\ninformation on the observation level.\n  We propose a novel localization framework that adapts well to sparse datasets\nthat only contain CMs of specific areas within the environment with strong\nmultipath propagation. Our framework compresses CMs into informative features\nto unravel spatial information. It then regresses Gaussian processes (GPs) for\neach of them, which imply statistical observation models based on\ndistance-dependent covariance kernels. Our framework combines the trained GPs\nwith line-of-sight ranges and a dynamics model in a particle filter. Our\nmeasurements show that our approach outperforms state-of-the-art CNN\nfingerprinting (0.52 m vs. 1.3 m MAE) on spatially sparse data collected in a\nrealistic industrial indoor environment.\n","authors":["Sebastian Kram","Christopher Kraus","Tobias Feigl","Maximilian Stahlke","Jörg Robert","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2203.13110v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.13108v1","updated":"2022-03-24T15:05:32Z","published":"2022-03-24T15:05:32Z","title":"Explainable Artificial Intelligence for Exhaust Gas Temperature of\n  Turbofan Engines","summary":"  Data-driven modeling is an imperative tool in various industrial\napplications, including many applications in the sectors of aeronautics and\ncommercial aviation. These models are in charge of providing key insights, such\nas which parameters are important on a specific measured outcome or which\nparameter values we should expect to observe given a set of input parameters.\nAt the same time, however, these models rely heavily on assumptions (e.g.,\nstationarity) or are \"black box\" (e.g., deep neural networks), meaning that\nthey lack interpretability of their internal working and can be viewed only in\nterms of their inputs and outputs. An interpretable alternative to the \"black\nbox\" models and with considerably less assumptions is symbolic regression (SR).\nSR searches for the optimal model structure while simultaneously optimizing the\nmodel's parameters without relying on an a-priori model structure. In this\nwork, we apply SR on real-life exhaust gas temperature (EGT) data, collected at\nhigh frequencies through the entire flight, in order to uncover meaningful\nalgebraic relationships between the EGT and other measurable engine parameters.\nThe experimental results exhibit promising model accuracy, as well as\nexplainability returning an absolute difference of 3{\\deg}C compared to the\nground truth and demonstrating consistency from an engineering perspective.\n","authors":["Marios Kefalas","Juan de Santiago Rojo Jr.","Asteris Apostolidis","Dirk van den Herik","Bas van Stein","Thomas Bäck"],"pdf_url":"https://arxiv.org/pdf/2203.13108v1.pdf","comment":"Main paper: 20 pages, 4 figures. Supplemental material: 18 pages, 30\n  figures. Published"},{"id":"http://arxiv.org/abs/2103.13822v3","updated":"2022-03-24T14:58:49Z","published":"2021-03-24T03:25:14Z","title":"FedCor: Correlation-Based Active Client Selection Strategy for\n  Heterogeneous Federated Learning","summary":"  Client-wise data heterogeneity is one of the major issues that hinder\neffective training in federated learning (FL). Since the data distribution on\neach client may vary dramatically, the client selection strategy can\nsignificantly influence the convergence rate of the FL process. Active client\nselection strategies are popularly proposed in recent studies. However, they\nneglect the loss correlations between the clients and achieve only marginal\nimprovement compared to the uniform selection strategy. In this work, we\npropose FedCor -- an FL framework built on a correlation-based client selection\nstrategy, to boost the convergence rate of FL. Specifically, we first model the\nloss correlations between the clients with a Gaussian Process (GP). Based on\nthe GP model, we derive a client selection strategy with a significant\nreduction of expected global loss in each round. Besides, we develop an\nefficient GP training method with a low communication overhead in the FL\nscenario by utilizing the covariance stationarity. Our experimental results\nshow that compared to the state-of-the-art method, FedCorr can improve the\nconvergence rates by $34\\%\\sim 99\\%$ and $26\\%\\sim 51\\%$ on FMNIST and\nCIFAR-10, respectively.\n","authors":["Minxue Tang","Xuefei Ning","Yitu Wang","Jingwei Sun","Yu Wang","Hai Li","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2103.13822v3.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2006.16745v3","updated":"2022-03-24T14:39:51Z","published":"2020-06-30T13:01:06Z","title":"On the Applicability of ML Fairness Notions","summary":"  Fairness emerged as an important requirement to guarantee that Machine\nLearning (ML) predictive systems do not discriminate against specific\nindividuals or entire sub-populations, in particular, minorities. Given the\ninherent subjectivity of viewing the concept of fairness, several notions of\nfairness have been introduced in the literature. This paper is a survey that\nillustrates the subtleties between fairness notions through a large number of\nexamples and scenarios. In addition, unlike other surveys in the literature, it\naddresses the question of: which notion of fairness is most suited to a given\nreal-world scenario and why? Our attempt to answer this question consists in\n(1) identifying the set of fairness-related characteristics of the real-world\nscenario at hand, (2) analyzing the behavior of each fairness notion, and then\n(3) fitting these two elements to recommend the most suitable fairness notion\nin every specific setup. The results are summarized in a decision diagram that\ncan be used by practitioners and policymakers to navigate the relatively large\ncatalog of ML.\n","authors":["Karima Makhlouf","Sami Zhioua","Catuscia Palamidessi"],"pdf_url":"https://arxiv.org/pdf/2006.16745v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13657v2","updated":"2022-03-24T14:32:41Z","published":"2022-02-28T10:01:22Z","title":"Avalanche RL: a Continual Reinforcement Learning Library","summary":"  Continual Reinforcement Learning (CRL) is a challenging setting where an\nagent learns to interact with an environment that is constantly changing over\ntime (the stream of experiences). In this paper, we describe Avalanche RL, a\nlibrary for Continual Reinforcement Learning which allows to easily train\nagents on a continuous stream of tasks. Avalanche RL is based on PyTorch and\nsupports any OpenAI Gym environment. Its design is based on Avalanche, one of\nthe more popular continual learning libraries, which allow us to reuse a large\nnumber of continual learning strategies and improve the interaction between\nreinforcement learning and continual learning researchers. Additionally, we\npropose Continual Habitat-Lab, a novel benchmark and a high-level library which\nenables the usage of the photorealistic simulator Habitat-Sim for CRL research.\nOverall, Avalanche RL attempts to unify under a common framework continual\nreinforcement learning applications, which we hope will foster the growth of\nthe field.\n","authors":["Nicolò Lucchesi","Antonio Carta","Vincenzo Lomonaco","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2202.13657v2.pdf","comment":"Presented at the 21st International Conference on Image Analysis and\n  Processing (ICIAP 2021)"},{"id":"http://arxiv.org/abs/2203.13088v1","updated":"2022-03-24T14:28:07Z","published":"2022-03-24T14:28:07Z","title":"Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized\n  Late Interactions using Enhanced Reduction","summary":"  Recent progress in neural information retrieval has demonstrated large gains\nin effectiveness, while often sacrificing the efficiency and interpretability\nof the neural model compared to classical approaches. This paper proposes\nColBERTer, a neural retrieval model using contextualized late interaction\n(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,\nColBERTer's reductions dramatically lower ColBERT's storage requirements while\nsimultaneously improving the interpretability of its token-matching scores. To\nthis end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and\noptional lexical matching components into one model. For its multi-vector\ncomponent, ColBERTer reduces the number of stored vectors per document by\nlearning unique whole-word representations for the terms in each document and\nlearning to identify and remove word representations that are not essential to\neffective scoring. We employ an explicit multi-task, multi-stage training to\nfacilitate using very small vector dimensions. Results on the MS MARCO and\nTREC-DL collection show that ColBERTer can reduce the storage footprint by up\nto 2.5x, while maintaining effectiveness. With just one dimension per token in\nits smallest setting, ColBERTer achieves index storage parity with the\nplaintext size, with very strong effectiveness results. Finally, we demonstrate\nColBERTer's robustness on seven high-quality out-of-domain collections,\nyielding statistically significant gains over traditional retrieval baselines.\n","authors":["Sebastian Hofstätter","Omar Khattab","Sophia Althammer","Mete Sertkan","Allan Hanbury"],"pdf_url":"https://arxiv.org/pdf/2203.13088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13086v1","updated":"2022-03-24T14:25:51Z","published":"2022-03-24T14:25:51Z","title":"HiFi++: a Unified Framework for Neural Vocoding, Bandwidth Extension and\n  Speech Enhancement","summary":"  Generative adversarial networks have recently demonstrated outstanding\nperformance in neural vocoding outperforming best autoregressive and flow-based\nmodels. In this paper, we show that this success can be extended to other tasks\nof conditional audio generation. In particular, building upon HiFi vocoders, we\npropose a novel HiFi++ general framework for neural vocoding, bandwidth\nextension, and speech enhancement. We show that with the improved generator\narchitecture and simplified multi-discriminator training, HiFi++ performs on\npar with the state-of-the-art in these tasks while spending significantly less\nmemory and computational resources. The effectiveness of our approach is\nvalidated through a series of extensive experiments.\n","authors":["Pavel Andreev","Aibek Alanov","Oleg Ivanov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2203.13086v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2203.13085v1","updated":"2022-03-24T14:25:15Z","published":"2022-03-24T14:25:15Z","title":"Locally Asynchronous Stochastic Gradient Descent for Decentralised Deep\n  Learning","summary":"  Distributed training algorithms of deep neural networks show impressive\nconvergence speedup properties on very large problems. However, they inherently\nsuffer from communication related slowdowns and communication topology becomes\na crucial design choice. Common approaches supported by most machine learning\nframeworks are: 1) Synchronous decentralized algorithms relying on a\npeer-to-peer All Reduce topology that is sensitive to stragglers and\ncommunication delays. 2) Asynchronous centralised algorithms with a server\nbased topology that is prone to communication bottleneck. Researchers also\nsuggested asynchronous decentralized algorithms designed to avoid the\nbottleneck and speedup training, however, those commonly use inexact sparse\naveraging that may lead to a degradation in accuracy. In this paper, we propose\nLocal Asynchronous SGD (LASGD), an asynchronous decentralized algorithm that\nrelies on All Reduce for model synchronization.\n  We empirically validate LASGD's performance on image classification tasks on\nthe ImageNet dataset. Our experiments demonstrate that LASGD accelerates\ntraining compared to SGD and state of the art gossip based approaches.\n","authors":["Tomer Avidor","Nadav Tal Israel"],"pdf_url":"https://arxiv.org/pdf/2203.13085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13084v1","updated":"2022-03-24T14:20:27Z","published":"2022-03-24T14:20:27Z","title":"The Dutch Draw: Constructing a Universal Baseline for Binary Prediction\n  Models","summary":"  Novel prediction methods should always be compared to a baseline to know how\nwell they perform. Without this frame of reference, the performance score of a\nmodel is basically meaningless. What does it mean when a model achieves an\n$F_1$ of 0.8 on a test set? A proper baseline is needed to evaluate the\n`goodness' of a performance score. Comparing with the latest state-of-the-art\nmodel is usually insightful. However, being state-of-the-art can change rapidly\nwhen newer models are developed. Contrary to an advanced model, a simple dummy\nclassifier could be used. However, the latter could be beaten too easily,\nmaking the comparison less valuable. This paper presents a universal baseline\nmethod for all binary classification models, named the Dutch Draw (DD). This\napproach weighs simple classifiers and determines the best classifier to use as\na baseline. We theoretically derive the DD baseline for many commonly used\nevaluation measures and show that in most situations it reduces to (almost)\nalways predicting either zero or one. Summarizing, the DD baseline is: (1)\ngeneral, as it is applicable to all binary classification problems; (2) simple,\nas it is quickly determined without training or parameter-tuning; (3)\ninformative, as insightful conclusions can be drawn from the results. The DD\nbaseline serves two purposes. First, to enable comparisons across research\npapers by this robust and universal baseline. Secondly, to provide a sanity\ncheck during the development process of a prediction model. It is a major\nwarning sign when a model is outperformed by the DD baseline.\n","authors":["Etienne van de Bijl","Jan Klein","Joris Pries","Sandjai Bhulai","Mark Hoogendoorn","Rob van der Mei"],"pdf_url":"https://arxiv.org/pdf/2203.13084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.04260v2","updated":"2022-03-24T13:55:32Z","published":"2021-09-09T13:30:31Z","title":"Online Enhanced Semantic Hashing: Towards Effective and Efficient\n  Retrieval for Streaming Multi-Modal Data","summary":"  With the vigorous development of multimedia equipment and applications,\nefficient retrieval of large-scale multi-modal data has become a trendy\nresearch topic. Thereinto, hashing has become a prevalent choice due to its\nretrieval efficiency and low storage cost. Although multi-modal hashing has\ndrawn lots of attention in recent years, there still remain some problems. The\nfirst point is that existing methods are mainly designed in batch mode and not\nable to efficiently handle streaming multi-modal data. The second point is that\nall existing online multi-modal hashing methods fail to effectively handle\nunseen new classes which come continuously with streaming data chunks. In this\npaper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).\nWe design novel semantic-enhanced representation for data, which could help\nhandle the new coming classes, and thereby construct the enhanced semantic\nobjective function. An efficient and effective discrete online optimization\nalgorithm is further proposed for OASIS. Extensive experiments show that our\nmethod can exceed the state-of-the-art models. For good reproducibility and\nbenefiting the community, our code and data are already available in\nsupplementary material and will be made publicly available.\n","authors":["Xiao-Ming Wu","Xin Luo","Yu-Wei Zhan","Chen-Lu Ding","Zhen-Duo Chen","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2109.04260v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.10973v2","updated":"2022-03-24T13:51:57Z","published":"2022-03-21T13:33:37Z","title":"A Local Convergence Theory for the Stochastic Gradient Descent Method in\n  Non-Convex Optimization With Non-isolated Local Minima","summary":"  Non-convex loss functions arise frequently in modern machine learning, and\nfor the theoretical analysis of stochastic optimization methods, the presence\nof non-isolated minima presents a unique challenge that has remained\nunder-explored. In this paper, we study the local convergence of the stochastic\ngradient descent method to non-isolated global minima. Under mild assumptions,\nwe estimate the probability for the iterations to stay near the minima by\nadopting the notion of stochastic stability. After establishing such stability,\nwe present the lower bound complexity in terms of various error criteria for a\ngiven error tolerance $\\epsilon$ and a failure probability $\\gamma$.\n","authors":["Taehee Ko","Xiantao Li"],"pdf_url":"https://arxiv.org/pdf/2203.10973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.11906v4","updated":"2022-03-24T13:30:15Z","published":"2021-01-28T10:13:15Z","title":"Development of a Vertex Finding Algorithm using Recurrent Neural Network","summary":"  Deep learning is a rapidly-evolving technology with possibility to\nsignificantly improve physics reach of collider experiments. In this study we\ndeveloped a novel algorithm of vertex finding for future lepton colliders such\nas the International Linear Collider. We deploy two networks; one is simple\nfully-connected layers to look for vertex seeds from track pairs, and the other\nis a customized Recurrent Neural Network with an attention mechanism and an\nencoder-decoder structure to associate tracks to the vertex seeds. The\nperformance of the vertex finder is compared with the standard ILC\nreconstruction algorithm.\n","authors":["Kiichi Goto","Taikan Suehara","Tamaki Yoshioka","Masakazu Kurata","Hajime Nagahara","Yuta Nakashima","Noriko Takemura","Masako Iwasaki"],"pdf_url":"https://arxiv.org/pdf/2101.11906v4.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2203.13060v1","updated":"2022-03-24T13:12:23Z","published":"2022-03-24T13:12:23Z","title":"SwiftAgg+: Achieving Asymptotically Optimal Communication Load in Secure\n  Aggregation for Federated Learning","summary":"  We propose SwiftAgg+, a novel secure aggregation protocol for federated\nlearning systems, where a central server aggregates local models of\n$N\\in\\mathbb{N}$ distributed users, each of size $L \\in \\mathbb{N}$, trained on\ntheir local data, in a privacy-preserving manner. SwiftAgg+ can significantly\nreduce the communication overheads without any compromise on security, and\nachieve the optimum communication load within a diminishing gap. Specifically,\nin presence of at most $D$ dropout users, SwiftAgg+ achieves average per-user\ncommunication load of $(1+\\mathcal{O}(\\frac{1}{N}))L$ and the server\ncommunication load of $(1+\\mathcal{O}(\\frac{1}{N}))L$, with a worst-case\ninformation-theoretic security guarantee, against any subset of up to $T$\nsemi-honest users who may also collude with the curious server. The proposed\nSwiftAgg+ has also a flexibility to reduce the number of active communication\nlinks at the cost of increasing the the communication load between the users\nand the server. In particular, for any $K\\in\\mathbb{N}$, SwiftAgg+ can achieve\nthe uplink communication load of $(1+\\frac{T}{K})L$, and per-user communication\nload of up to $(1-\\frac{1}{N})(1+\\frac{T+D}{K})L$, where the number of\npair-wise active connections in the network is $\\frac{N}{2}(K+T+D+1)$.\n","authors":["Tayyebeh Jahani-Nezhad","Mohammad Ali Maddah-Ali","Songze Li","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2203.13060v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2202.04169"},{"id":"http://arxiv.org/abs/2109.04386v4","updated":"2022-03-24T12:46:15Z","published":"2021-09-09T16:17:38Z","title":"ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions","summary":"  An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends also on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct and Pserf. Experiments suggest that the proposed functions improve the\nnetwork performance significantly compared to the widely used activations like\nReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and\n5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in\nCIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet\nV2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.\n","authors":["Koushik Biswas","Sandeep Kumar","Shilpak Banerjee","Ashish Kumar Pandey"],"pdf_url":"https://arxiv.org/pdf/2109.04386v4.pdf","comment":"AAAI 2022"},{"id":"http://arxiv.org/abs/2203.13038v1","updated":"2022-03-24T12:33:58Z","published":"2022-03-24T12:33:58Z","title":"Interpretable Prediction of Pulmonary Hypertension in Newborns using\n  Echocardiograms","summary":"  Pulmonary hypertension (PH) in newborns and infants is a complex condition\nassociated with several pulmonary, cardiac, and systemic diseases contributing\nto morbidity and mortality. Therefore, accurate and early detection of PH is\ncrucial for successful management. Using echocardiography, the primary\ndiagnostic tool in pediatrics, human assessment is both time-consuming and\nexpertise-demanding, raising the need for an automated approach. In this work,\nwe present an interpretable multi-view video-based deep learning approach to\npredict PH for a cohort of 194 newborns using echocardiograms. We use\nspatio-temporal convolutional architectures for the prediction of PH from each\nview, and aggregate the predictions of the different views using majority\nvoting. To the best of our knowledge, this is the first work for an automated\nassessment of PH in newborns using echocardiograms. Our results show a mean\nF1-score of 0.84 for severity prediction and 0.92 for binary detection using\n10-fold cross-validation. We complement our predictions with saliency maps and\nshow that the learned model focuses on clinically relevant cardiac structures,\nmotivating its usage in clinical practice.\n","authors":["Hanna Ragnarsdottir","Laura Manduchi","Holger Michel","Fabian Laumer","Sven Wellmann","Ece Ozkan","Julia Vogt"],"pdf_url":"https://arxiv.org/pdf/2203.13038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.04307v2","updated":"2022-03-24T12:22:35Z","published":"2021-02-08T16:10:50Z","title":"Learning Optimal Strategies for Temporal Tasks in Stochastic Games","summary":"  Synthesis from linear temporal logic (LTL) specifications provides assured\ncontrollers for autonomous systems operating in stochastic and potentially\nadversarial environments. Automatic synthesis tools, however, require a model\nof the environment to construct controllers. In this work, we introduce a\nmodel-free reinforcement learning (RL) approach that derives controllers from\ngiven LTL specifications even when the environment is completely unknown. We\nmodel the problem of satisfying the LTL specifications as a stochastic game\n(SG) between the controller and the adversarial environment; we then learn\noptimal controller strategies that maximize the probability of satisfying the\nLTL specifications against the worst-case environment behavior. We first\nconstruct a product game using the deterministic parity automaton (DPA)\ntranslated from the given LTL specification. By deriving distinct rewards and\ndiscount factors from the acceptance condition of the DPA, we reduce the\nmaximization of the worst-case probability of satisfying the LTL specification\ninto the maximization of a discounted reward objective in the product game;\nthis allows for the use of model-free RL algorithms to learn an optimal\ncontroller strategy. To deal with the common scalability problems when the\nnumber of colors defining the acceptance condition of the DPA is large, we\npropose a lazy color generation method where distinct rewards and discount\nfactors are utilized only when needed, and an approximate method where the\ncontroller eventually focuses on only one color. In several case studies, we\nshow that our approach is scalable to a wide range of LTL formulas,\nsignificantly outperforming existing methods for learning controllers from LTL\nspecifications in SGs.\n","authors":["Alper Kamil Bozkurt","Yu Wang","Michael M. Zavlanos","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2102.04307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.00998v3","updated":"2022-03-24T12:08:23Z","published":"2021-09-02T14:51:16Z","title":"Waveform Learning for Next-Generation Wireless Communication Systems","summary":"  We propose a learning-based method for the joint design of a transmit and\nreceive filter, the constellation geometry and associated bit labeling, as well\nas a neural network (NN)-based detector. The method maximizes an achievable\ninformation rate, while simultaneously satisfying constraints on the adjacent\nchannel leakage ratio (ACLR) and peak-to-average power ratio (PAPR). This\nallows control of the tradeoff between spectral containment, peak power, and\ncommunication rate. Evaluation on an additive white Gaussian noise (AWGN)\nchannel shows significant reduction of ACLR and PAPR compared to a conventional\nbaseline relying on quadrature amplitude modulation (QAM) and\nroot-raised-cosine (RRC), without significant loss of information rate. When\nconsidering a 3rd Generation Partnership Project (3GPP) multipath channel, the\nlearned waveform and neural receiver enable competitive or higher rates than an\northogonal frequency division multiplexing (OFDM) baseline, while reducing the\nACLR by 10 dB and the PAPR by 2 dB. The proposed method incurs no additional\ncomplexity on the transmitter side and might be an attractive tool for waveform\ndesign of beyond-5G systems.\n","authors":["Fayçal Ait Aoudia","Jakob Hoydis"],"pdf_url":"https://arxiv.org/pdf/2109.00998v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13004v1","updated":"2022-03-24T11:52:43Z","published":"2022-03-24T11:52:43Z","title":"Using Orientation to Distinguish Overlapping Chromosomes","summary":"  A difficult step in the process of karyotyping is segmenting chromosomes that\ntouch or overlap. In an attempt to automate the process, previous studies\nturned to Deep Learning methods, with some formulating the task as a semantic\nsegmentation problem. These models treat separate chromosome instances as\nsemantic classes, which we show to be problematic, since it is uncertain which\nchromosome should be classed as #1 and #2. Assigning class labels based on\ncomparison rules, such as the shorter/longer chromosome alleviates, but does\nnot fully resolve the issue. Instead, we separate the chromosome instances in a\nsecond stage, predicting the orientation of the chromosomes by the model and\nuse it as one of the key distinguishing factors of the chromosomes. We\ndemonstrate this method to be effective. Furthermore, we introduce a novel\nDouble-Angle representation that a neural network can use to predict the\norientation. The representation maps any direction and its reverse to the same\npoint. Lastly, we present a new expanded synthetic dataset, which is based on\nPommier's dataset, but addresses its issues with insufficient separation\nbetween its training and testing sets.\n","authors":["Daniel Kluvanec","Thomas B. Phillips","Kenneth J. W. McCaffrey","Noura Al Moubayed"],"pdf_url":"https://arxiv.org/pdf/2203.13004v1.pdf","comment":"Conference for Health, Inference, and Learning (CHIL) 2022 - Invited\n  non-archival presentation"},{"id":"http://arxiv.org/abs/2203.12999v1","updated":"2022-03-24T11:47:11Z","published":"2022-03-24T11:47:11Z","title":"A Deep-Discrete Learning Framework for Spherical Surface Registration","summary":"  Cortical surface registration is a fundamental tool for neuroimaging analysis\nthat has been shown to improve the alignment of functional regions relative to\nvolumetric approaches. Classically, image registration is performed by\noptimizing a complex objective similarity function, leading to long run times.\nThis contributes to a convention for aligning all data to a global average\nreference frame that poorly reflects the underlying cortical heterogeneity. In\nthis paper, we propose a novel unsupervised learning-based framework that\nconverts registration to a multi-label classification problem, where each point\nin a low-resolution control grid deforms to one of fixed, finite number of\nendpoints. This is learned using a spherical geometric deep learning\narchitecture, in an end-to-end unsupervised way, with regularization imposed\nusing a deep Conditional Random Field (CRF). Experiments show that our proposed\nframework performs competitively, in terms of similarity and areal distortion,\nrelative to the most popular classical surface registration algorithms and\ngenerates smoother deformations than other learning-based surface registration\nmethods, even in subjects with atypical cortical morphology.\n","authors":["Mohamed A. Suliman","Logan Z. J. Williams","Abdulah Fawaz","Emma C. Robinson"],"pdf_url":"https://arxiv.org/pdf/2203.12999v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2104.00567v6","updated":"2022-03-24T11:16:22Z","published":"2021-04-01T15:48:01Z","title":"Text to Image Generation with Semantic-Spatial Aware GAN","summary":"  Text-to-image synthesis (T2I) aims to generate photo-realistic images which\nare semantically consistent with the text descriptions. Existing methods are\nusually built upon conditional generative adversarial networks (GANs) and\ninitialize an image from noise with sentence embedding, and then refine the\nfeatures with fine-grained word embedding iteratively. A close inspection of\ntheir generated images reveals a major limitation: even though the generated\nimage holistically matches the description, individual image regions or parts\nof somethings are often not recognizable or consistent with words in the\nsentence, e.g. \"a white crown\". To address this problem, we propose a novel\nframework Semantic-Spatial Aware GAN for synthesizing images from input text.\nConcretely, we introduce a simple and effective Semantic-Spatial Aware block,\nwhich (1) learns semantic-adaptive transformation conditioned on text to\neffectively fuse text features and image features, and (2) learns a semantic\nmask in a weakly-supervised way that depends on the current text-image fusion\nprocess in order to guide the transformation spatially. Experiments on the\nchallenging COCO and CUB bird datasets demonstrate the advantage of our method\nover the recent state-of-the-art approaches, regarding both visual fidelity and\nalignment with input text description.\n","authors":["Kai Hu","Wentong Liao","Michael Ying Yang","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2104.00567v6.pdf","comment":"arXiv admin note: text overlap with arXiv:1711.10485 by other authors"},{"id":"http://arxiv.org/abs/2110.09916v2","updated":"2022-03-24T11:14:21Z","published":"2021-10-19T12:27:02Z","title":"Identification of high order closure terms from fully kinetic\n  simulations using machine learning","summary":"  Simulations of large-scale plasma systems are typically based on a fluid\napproximation approach. These models construct a moment-based system of\nequations that approximate the particle-based physics as a fluid, but as a\nresult lack the small-scale physical processes available to fully kinetic\nmodels. Traditionally, empirical closure relations are used to close the\nmoment-based system of equations, which typically approximate the pressure\ntensor or heat flux. The more accurate the closure relation, the stronger the\nsimulation approaches kinetic-based results. In this paper, new closure terms\nare constructed using machine learning techniques. Two different machine\nlearning models, a multi-layer perceptron and a gradient boosting regressor,\nsynthesize a local closure relation for the pressure tensor and heat flux\nvector from fully kinetic simulations of a 2D magnetic reconnection problem.\nThe models are compared to an existing closure relation for the pressure\ntensor, and the applicability of the models is discussed. The initial results\nshow that the models can capture the diagonal components of the pressure tensor\naccurately, and show promising results for the heat flux, opening the way for\nnew experiments in multi-scale modeling. We find that the sampling of the\npoints used to train both models play a capital role in their accuracy.\n","authors":["Brecht Laperre","Jorge Amaya","Sara Jamal","Giovanni Lapenta"],"pdf_url":"https://arxiv.org/pdf/2110.09916v2.pdf","comment":"37 pages, 12 figures, 2 tables"},{"id":"http://arxiv.org/abs/2203.12978v1","updated":"2022-03-24T10:50:05Z","published":"2022-03-24T10:50:05Z","title":"Effective Explanations for Entity Resolution Models","summary":"  Entity resolution (ER) aims at matching records that refer to the same\nreal-world entity. Although widely studied for the last 50 years, ER still\nrepresents a challenging data management problem, and several recent works have\nstarted to investigate the opportunity of applying deep learning (DL)\ntechniques to solve this problem. In this paper, we study the fundamental\nproblem of explainability of the DL solution for ER. Understanding the matching\npredictions of an ER solution is indeed crucial to assess the trustworthiness\nof the DL model and to discover its biases. We treat the DL model as a black\nbox classifier and - while previous approaches to provide explanations for DL\npredictions are agnostic to the classification task. we propose the CERTA\napproach that is aware of the semantics of the ER problem. Our approach\nproduces both saliency explanations, which associate each attribute with a\nsaliency score, and counterfactual explanations, which provide examples of\nvalues that can flip the prediction. CERTA builds on a probabilistic framework\nthat aims at computing the explanations evaluating the outcomes produced by\nusing perturbed copies of the input records. We experimentally evaluate CERTA's\nexplanations of state-of-the-art ER solutions based on DL models using publicly\navailable datasets, and demonstrate the effectiveness of CERTA over recently\nproposed methods for this problem.\n","authors":["Tommaso Teofili","Donatella Firmani","Nick Koudas","Vincenzo Martello","Paolo Merialdo","Divesh Srivastava"],"pdf_url":"https://arxiv.org/pdf/2203.12978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11834v2","updated":"2022-03-24T10:30:14Z","published":"2022-03-22T16:01:04Z","title":"Improving Generalization in Federated Learning by Seeking Flat Minima","summary":"  Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n","authors":["Debora Caldarola","Barbara Caputo","Marco Ciccone"],"pdf_url":"https://arxiv.org/pdf/2203.11834v2.pdf","comment":"Removed axessibility package for smaller output PDF"},{"id":"http://arxiv.org/abs/2203.12967v1","updated":"2022-03-24T10:15:50Z","published":"2022-03-24T10:15:50Z","title":"Extended critical regimes of deep neural networks","summary":"  Deep neural networks (DNNs) have been successfully applied to many real-world\nproblems, but a complete understanding of their dynamical and computational\nprinciples is still lacking. Conventional theoretical frameworks for analysing\nDNNs often assume random networks with coupling weights obeying Gaussian\nstatistics. However, non-Gaussian, heavy-tailed coupling is a ubiquitous\nphenomenon in DNNs. Here, by weaving together theories of heavy-tailed random\nmatrices and non-equilibrium statistical physics, we develop a new type of mean\nfield theory for DNNs which predicts that heavy-tailed weights enable the\nemergence of an extended critical regime without fine-tuning parameters. In\nthis extended critical regime, DNNs exhibit rich and complex propagation\ndynamics across layers. We further elucidate that the extended criticality\nendows DNNs with profound computational advantages: balancing the contraction\nas well as expansion of internal neural representations and speeding up\ntraining processes, hence providing a theoretical guide for the design of\nefficient neural architectures.\n","authors":["Cheng Kevin Qu","Asem Wardak","Pulin Gong"],"pdf_url":"https://arxiv.org/pdf/2203.12967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12369v2","updated":"2022-03-24T10:03:35Z","published":"2022-03-23T12:42:28Z","title":"MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data","summary":"  Training of speech enhancement systems often does not incorporate knowledge\nof human perception and thus can lead to unnatural sounding results.\nIncorporating psychoacoustically motivated speech perception metrics as part of\nmodel training via a predictor network has recently gained interest. However,\nthe performance of such predictors is limited by the distribution of metric\nscores that appear in the training data. In this work, we propose MetricGAN+/-\n(an extension of MetricGAN+, one such metric-motivated system) which introduces\nan additional network - a \"de-generator\" which attempts to improve the\nrobustness of the prediction network (and by extension of the generator) by\nensuring observation of a wider range of metric scores in training.\nExperimental results on the VoiceBank-DEMAND dataset show relative improvement\nin PESQ score of 3.8% (3.05 vs 3.22 PESQ score), as well as better\ngeneralisation to unseen noise and speech.\n","authors":["George Close","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2203.12369v2.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2203.12964v1","updated":"2022-03-24T10:03:01Z","published":"2022-03-24T10:03:01Z","title":"Knowledge Removal in Sampling-based Bayesian Inference","summary":"  The right to be forgotten has been legislated in many countries, but its\nenforcement in the AI industry would cause unbearable costs. When single data\ndeletion requests come, companies may need to delete the whole models learned\nwith massive resources. Existing works propose methods to remove knowledge\nlearned from data for explicitly parameterized models, which however are not\nappliable to the sampling-based Bayesian inference, i.e., Markov chain Monte\nCarlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we\npropose the first machine unlearning algorithm for MCMC. We first convert the\nMCMC unlearning problem into an explicit optimization problem. Based on this\nproblem conversion, an {\\it MCMC influence function} is designed to provably\ncharacterize the learned knowledge from data, which then delivers the MCMC\nunlearning algorithm. Theoretical analysis shows that MCMC unlearning would not\ncompromise the generalizability of the MCMC models. Experiments on Gaussian\nmixture models and Bayesian neural networks confirm the effectiveness of the\nproposed algorithm. The code is available at\n\\url{https://github.com/fshp971/mcmc-unlearning}.\n","authors":["Shaopeng Fu","Fengxiang He","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2203.12964v1.pdf","comment":"In International Conference on Learning Representations, 2022"},{"id":"http://arxiv.org/abs/2203.08007v2","updated":"2022-03-24T09:42:27Z","published":"2022-03-15T15:44:20Z","title":"Data Smells in Public Datasets","summary":"  The adoption of Artificial Intelligence (AI) in high-stakes domains such as\nhealthcare, wildlife preservation, autonomous driving and criminal justice\nsystem calls for a data-centric approach to AI. Data scientists spend the\nmajority of their time studying and wrangling the data, yet tools to aid them\nwith data analysis are lacking. This study identifies the recurrent data\nquality issues in public datasets. Analogous to code smells, we introduce a\nnovel catalogue of data smells that can be used to indicate early signs of\nproblems or technical debt in machine learning systems. To understand the\nprevalence of data quality issues in datasets, we analyse 25 public datasets\nand identify 14 data smells.\n","authors":["Arumoy Shome","Luis Cruz","Arie van Deursen"],"pdf_url":"https://arxiv.org/pdf/2203.08007v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12948v1","updated":"2022-03-24T09:24:29Z","published":"2022-03-24T09:24:29Z","title":"Personalized incentives as feedback design in generalized Nash\n  equilibrium problems","summary":"  We investigate both stationary and time-varying, nonmonotone generalized Nash\nequilibrium problems that exhibit symmetric interactions among the agents,\nwhich are known to be potential. As may happen in practical cases, however, we\nenvision a scenario in which the formal expression of the underlying potential\nfunction is not available, and we design a semi-decentralized Nash equilibrium\nseeking algorithm. In the proposed two-layer scheme, a coordinator iteratively\nintegrates the (possibly noisy and sporadic) agents' feedback to learn the\npseudo-gradients of the agents, and then design personalized incentives for\nthem. On their side, the agents receive those personalized incentives, compute\na solution to an extended game, and then return feedback measurements to the\ncoordinator. In the stationary setting, our algorithm returns a Nash\nequilibrium in case the coordinator is endowed with standard learning policies,\nwhile it returns a Nash equilibrium up to a constant, yet adjustable, error in\nthe time-varying case. As a motivating application, we consider the ridehailing\nservice provided by several companies with mobility as a service orchestration,\nnecessary to both handle competition among firms and avoid traffic congestion,\nwhich is also adopted to run numerical experiments verifying our results.\n","authors":["Filippo Fabiani","Andrea Simonetto","Paul J. Goulart"],"pdf_url":"https://arxiv.org/pdf/2203.12948v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.03854"},{"id":"http://arxiv.org/abs/2203.12940v1","updated":"2022-03-24T09:04:52Z","published":"2022-03-24T09:04:52Z","title":"mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot\n  Filling","summary":"  Zero-shot slot filling has received considerable attention to cope with the\nproblem of limited available data for the target domain. One of the important\nfactors in zero-shot learning is to make the model learn generalized and\nreliable representations. For this purpose, we present mcBERT, which stands for\nmomentum contrastive learning with BERT, to develop a robust zero-shot slot\nfilling model. mcBERT uses BERT to initialize the two encoders, the query\nencoder and key encoder, and is trained by applying momentum contrastive\nlearning. Our experimental results on the SNIPS benchmark show that mcBERT\nsubstantially outperforms the previous models, recording a new\nstate-of-the-art. Besides, we also show that each component composing mcBERT\ncontributes to the performance improvement.\n","authors":["Seong-Hwan Heo","WonKee Lee","Jong-Hyeok Lee"],"pdf_url":"https://arxiv.org/pdf/2203.12940v1.pdf","comment":"Submitted to INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2203.12932v1","updated":"2022-03-24T08:37:26Z","published":"2022-03-24T08:37:26Z","title":"Bioformers: Embedding Transformers for Ultra-Low Power sEMG-based\n  Gesture Recognition","summary":"  Human-machine interaction is gaining traction in rehabilitation tasks, such\nas controlling prosthetic hands or robotic arms. Gesture recognition exploiting\nsurface electromyographic (sEMG) signals is one of the most promising\napproaches, given that sEMG signal acquisition is non-invasive and is directly\nrelated to muscle contraction. However, the analysis of these signals still\npresents many challenges since similar gestures result in similar muscle\ncontractions. Thus the resulting signal shapes are almost identical, leading to\nlow classification accuracy. To tackle this challenge, complex neural networks\nare employed, which require large memory footprints, consume relatively high\nenergy and limit the maximum battery life of devices used for classification.\nThis work addresses this problem with the introduction of the Bioformers. This\nnew family of ultra-small attention-based architectures approaches\nstate-of-the-art performance while reducing the number of parameters and\noperations of 4.9X. Additionally, by introducing a new inter-subjects\npre-training, we improve the accuracy of our best Bioformer by 3.39%, matching\nstate-of-the-art accuracy without any additional inference cost. Deploying our\nbest performing Bioformer on a Parallel, Ultra-Low Power (PULP) microcontroller\nunit (MCU), the GreenWaves GAP8, we achieve an inference latency and energy of\n2.72 ms and 0.14 mJ, respectively, 8.0X lower than the previous\nstate-of-the-art neural network, while occupying just 94.2 kB of memory.\n","authors":["Alessio Burrello","Francesco Bianco Morghet","Moritz Scherer","Simone Benatti","Luca Benini","Enrico Macii","Massimo Poncino","Daniele Jahier Pagliari"],"pdf_url":"https://arxiv.org/pdf/2203.12932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12377v2","updated":"2022-03-24T08:29:17Z","published":"2022-03-23T12:52:49Z","title":"Dynamically-Scaled Deep Canonical Correlation Analysis","summary":"  Canonical Correlation Analysis (CCA) is a method for feature extraction of\ntwo views by finding maximally correlated linear projections of them. Several\nvariants of CCA have been introduced in the literature, in particular, variants\nbased on deep neural networks for learning highly correlated nonlinear\ntransformations of two views. As these models are parameterized conventionally,\ntheir learnable parameters remain independent of the inputs after the training\nprocess, which may limit their capacity for learning highly correlated\nrepresentations. We introduce a novel dynamic scaling method for training an\ninput-dependent canonical correlation model. In our deep-CCA models, the\nparameters of the last layer are scaled by a second neural network that is\nconditioned on the model's input, resulting in a parameterization that is\ndependent on the input samples. We evaluate our model on multiple datasets and\ndemonstrate that the learned representations are more correlated in comparison\nto the conventionally-parameterized CCA-based models and also obtain preferable\nretrieval results. Our code is available at\nhttps://github.com/tomerfr/DynamicallyScaledDeepCCA.\n","authors":["Tomer Friedlander","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2203.12377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.11099v2","updated":"2022-03-24T08:19:32Z","published":"2021-12-21T11:00:16Z","title":"High pressure hydrogen by machine learning and quantum Monte Carlo","summary":"  We have developed a technique combining the accuracy of quantum Monte Carlo\nin describing the electron correlation with the efficiency of a Machine\nLearning Potential (MLP). We use kernel regression in combination with SOAP\n(Smooth Overlap of Atomic Position) features, implemented here in a very\nefficient way. The key ingredients are: i) a sparsification technique, based on\nfarthest point sampling, ensuring generality and transferability of our MLPs\nand ii) the so called $\\Delta$-learning, allowing a small training data set, a\nfundamental property for highly accurate but computationally demanding\ncalculations, such as the ones based on quantum Monte Carlo. As the first\napplication we present a benchmark study of the liquid-liquid transition of\nhigh-pressure hydrogen and show the quality of our MLP, by emphasizing the\nimportance of high accuracy for this very debated subject, where experiments\nare difficult in the lab, and theory is still far from being conclusive.\n","authors":["Andrea Tirelli","Giacomo Tenti","Kousuke Nakano","Sandro Sorella"],"pdf_url":"https://arxiv.org/pdf/2112.11099v2.pdf","comment":"revised exposition, performed more validation tests. Comments\n  welcome!"},{"id":"http://arxiv.org/abs/2203.12925v1","updated":"2022-03-24T08:15:17Z","published":"2022-03-24T08:15:17Z","title":"TCN Mapping Optimization for Ultra-Low Power Time-Series Edge Inference","summary":"  Temporal Convolutional Networks (TCNs) are emerging lightweight Deep Learning\nmodels for Time Series analysis. We introduce an automated exploration approach\nand a library of optimized kernels to map TCNs on Parallel Ultra-Low Power\n(PULP) microcontrollers. Our approach minimizes latency and energy by\nexploiting a layer tiling optimizer to jointly find the tiling dimensions and\nselect among alternative implementations of the causal and dilated\n1D-convolution operations at the core of TCNs. We benchmark our approach on a\ncommercial PULP device, achieving up to 103X lower latency and 20.3X lower\nenergy than the Cube-AI toolkit executed on the STM32L4 and from 2.9X to 26.6X\nlower energy compared to commercial closed-source and academic open-source\napproaches on the same hardware target.\n","authors":["Alessio Burrello","Alberto Dequino","Daniele Jahier Pagliari","Francesco Conti","Marcello Zanghieri","Enrico Macii","Luca Benini","Massimo Poncino"],"pdf_url":"https://arxiv.org/pdf/2203.12925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12922v1","updated":"2022-03-24T08:14:12Z","published":"2022-03-24T08:14:12Z","title":"Horizon-Free Reinforcement Learning in Polynomial Time: the Power of\n  Stationary Policies","summary":"  This paper gives the first polynomial-time algorithm for tabular Markov\nDecision Processes (MDP) that enjoys a regret bound \\emph{independent on the\nplanning horizon}. Specifically, we consider tabular MDP with $S$ states, $A$\nactions, a planning horizon $H$, total reward bounded by $1$, and the agent\nplays for $K$ episodes. We design an algorithm that achieves an\n$O\\left(\\mathrm{poly}(S,A,\\log K)\\sqrt{K}\\right)$ regret in contrast to\nexisting bounds which either has an additional $\\mathrm{polylog}(H)$\ndependency~\\citep{zhang2020reinforcement} or has an exponential dependency on\n$S$~\\citep{li2021settling}. Our result relies on a sequence of new structural\nlemmas establishing the approximation power, stability, and concentration\nproperty of stationary policies, which can have applications in other problems\nrelated to Markov chains.\n","authors":["Zihan Zhang","Xiangyang Ji","Simon S. Du"],"pdf_url":"https://arxiv.org/pdf/2203.12922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12921v1","updated":"2022-03-24T08:13:56Z","published":"2022-03-24T08:13:56Z","title":"Rubik's Cube Operator: A Plug And Play Permutation Module for Better\n  Arranging High Dimensional Industrial Data in Deep Convolutional Processes","summary":"  The convolutional neural network (CNN) has been widely applied to process the\nindustrial data based tensor input, which integrates data records of\ndistributed industrial systems from the spatial, temporal, and system dynamics\naspects. However, unlike images, information in the industrial data based\ntensor is not necessarily spatially ordered. Thus, directly applying CNN is\nineffective. To tackle such issue, we propose a plug and play module, the\nRubik's Cube Operator (RCO), to adaptively permutate the data organization of\nthe industrial data based tensor to an optimal or suboptimal order of\nattributes before being processed by CNNs, which can be updated with subsequent\nCNNs together via the gradient-based optimizer. The proposed RCO maintains K\nbinary and right stochastic permutation matrices to permutate attributes of K\naxes of the input industrial data based tensor. A novel learning process is\nproposed to enable learning permutation matrices from data, where the\nGumbel-Softmax is employed to reparameterize elements of permutation matrices,\nand the soft regularization loss is proposed and added to the task-specific\nloss to ensure the feature diversity of the permuted data. We verify the\neffectiveness of the proposed RCO via considering two representative learning\ntasks processing industrial data via CNNs, the wind power prediction (WPP) and\nthe wind speed prediction (WSP) from the renewable energy domain. Computational\nexperiments are conducted based on four datasets collected from different wind\nfarms and the results demonstrate that the proposed RCO can improve the\nperformance of CNN based networks significantly.\n","authors":["Luoxiao Yang","Zhong Zheng","Zijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.12921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12919v1","updated":"2022-03-24T08:13:26Z","published":"2022-03-24T08:13:26Z","title":"Learning Dense Correspondence from Synthetic Environments","summary":"  Estimation of human shape and pose from a single image is a challenging task.\nIt is an even more difficult problem to map the identified human shape onto a\n3D human model. Existing methods map manually labelled human pixels in real 2D\nimages onto the 3D surface, which is prone to human error, and the sparsity of\navailable annotated data often leads to sub-optimal results. We propose to\nsolve the problem of data scarcity by training 2D-3D human mapping algorithms\nusing automatically generated synthetic data for which exact and dense 2D-3D\ncorrespondence is known. Such a learning strategy using synthetic environments\nhas a high generalisation potential towards real-world data. Using different\ncamera parameter variations, background and lighting settings, we created\nprecise ground truth data that constitutes a wider distribution. We evaluate\nthe performance of models trained on synthetic using the COCO dataset and\nvalidation framework. Results show that training 2D-3D mapping network models\non synthetic data is a viable alternative to using real data.\n","authors":["Mithun Lal","Anthony Paproki","Nariman Habili","Lars Petersson","Olivier Salvado","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2203.12919v1.pdf","comment":"Submitted to ICIP 2022"},{"id":"http://arxiv.org/abs/2203.12915v1","updated":"2022-03-24T08:10:13Z","published":"2022-03-24T08:10:13Z","title":"NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep\n  Neural Networks","summary":"  Deep learning has recently been widely applied to many applications across\ndifferent domains, e.g., image classification and audio recognition. However,\nthe quality of Deep Neural Networks (DNNs) still raises concerns in the\npractical operational environment, which calls for systematic testing,\nespecially in safety-critical scenarios. Inspired by software testing, a number\nof structural coverage criteria are designed and proposed to measure the test\nadequacy of DNNs. However, due to the blackbox nature of DNN, the existing\nstructural coverage criteria are difficult to interpret, making it hard to\nunderstand the underlying principles of these criteria. The relationship\nbetween the structural coverage and the decision logic of DNNs is unknown.\nMoreover, recent studies have further revealed the non-existence of correlation\nbetween the structural coverage and DNN defect detection, which further posts\nconcerns on what a suitable DNN testing criterion should be.\n  In this paper, we propose the interpretable coverage criteria through\nconstructing the decision structure of a DNN. Mirroring the control flow graph\nof the traditional program, we first extract a decision graph from a DNN based\non its interpretation, where a path of the decision graph represents a decision\nlogic of the DNN. Based on the control flow and data flow of the decision\ngraph, we propose two variants of path coverage to measure the adequacy of the\ntest cases in exercising the decision logic. The higher the path coverage, the\nmore diverse decision logic the DNN is expected to be explored. Our large-scale\nevaluation results demonstrate that: the path in the decision graph is\neffective in characterizing the decision of the DNN, and the proposed coverage\ncriteria are also sensitive with errors including natural errors and\nadversarial examples, and strongly correlated with the output impartiality.\n","authors":["Xiaofei Xie","Tianlin Li","Jian Wang","Lei Ma","Qing Guo","Felix Juefei-Xu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2203.12915v1.pdf","comment":"27 pages. Accepted to ACM Transactions on Software Engineering and\n  Methodology (TOSEM), 2022"},{"id":"http://arxiv.org/abs/2112.06476v2","updated":"2022-03-24T08:07:25Z","published":"2021-12-13T08:17:15Z","title":"gACSON software for automated segmentation and morphology analyses of\n  myelinated axons in 3D electron microscopy","summary":"  Background and Objective: Advances in electron microscopy (EM) now allow\nthree-dimensional (3D) imaging of hundreds of micrometers of tissue with\nnanometer-scale resolution, providing new opportunities to study the\nultrastructure of the brain. In this work, we introduce a freely available\nMatlab-based gACSON software for visualization, segmentation, assessment, and\nmorphology analysis of myelinated axons in 3D-EM volumes of brain tissue\nsamples. Methods: The software is equipped with a graphical user interface\n(GUI). It automatically segments the intra-axonal space of myelinated axons and\ntheir corresponding myelin sheaths and allows manual segmentation,\nproofreading, and interactive correction of the segmented components. gACSON\nanalyzes the morphology of myelinated axons, such as axonal diameter, axonal\neccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of\nthe software by segmenting and analyzing myelinated axons in six 3D-EM volumes\nof rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).\nOur results suggest that the equivalent diameter of myelinated axons in\nsomatosensory cortex was decreased in TBI animals five months after the injury.\nConclusions: Our results indicate that gACSON is a valuable tool for\nvisualization, segmentation, assessment, and morphology analysis of myelinated\naxons in 3D-EM volumes. It is freely available at\nhttps://github.com/AndreaBehan/g-ACSON under the MIT license.\n","authors":["Andrea Behanova","Ali Abdollahzadeh","Ilya Belevich","Eija Jokitalo","Alejandra Sierra","Jussi Tohka"],"pdf_url":"https://arxiv.org/pdf/2112.06476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12907v1","updated":"2022-03-24T07:50:41Z","published":"2022-03-24T07:50:41Z","title":"Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named\n  Entity Recognition","summary":"  Named entity recognition (NER) is the process of recognising and classifying\nimportant information (entities) in text. Proper nouns, such as a person's\nname, an organization's name, or a location's name, are examples of entities.\nThe NER is one of the important modules in applications like human resources,\ncustomer support, search engines, content classification, and academia. In this\nwork, we consider NER for low-resource Indian languages like Hindi and Marathi.\nThe transformer-based models have been widely used for NER tasks. We consider\ndifferent variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark\nthem on publicly available Hindi and Marathi NER datasets. We provide an\nexhaustive comparison of different monolingual and multilingual\ntransformer-based models and establish simple baselines currently missing in\nthe literature. We show that the monolingual MahaRoBERTa model performs the\nbest for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for\nHindi NER. We also perform cross-language evaluation and present mixed\nobservations.\n","authors":["Onkar Litake","Maithili Sabane","Parth Patil","Aparna Ranade","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2203.12907v1.pdf","comment":"Accepted at ICMISC 2022"},{"id":"http://arxiv.org/abs/2203.12881v1","updated":"2022-03-24T06:48:56Z","published":"2022-03-24T06:48:56Z","title":"Can Unsupervised Knowledge Transfer from Social Discussions Help\n  Argument Mining?","summary":"  Identifying argument components from unstructured texts and predicting the\nrelationships expressed among them are two primary steps of argument mining.\nThe intrinsic complexity of these tasks demands powerful learning models. While\npretrained Transformer-based Language Models (LM) have been shown to provide\nstate-of-the-art results over different NLP tasks, the scarcity of manually\nannotated data and the highly domain-dependent nature of argumentation restrict\nthe capabilities of such models. In this work, we propose a novel transfer\nlearning strategy to overcome these challenges. We utilize argumentation-rich\nsocial discussions from the ChangeMyView subreddit as a source of unsupervised,\nargumentative discourse-aware knowledge by finetuning pretrained LMs on a\nselectively masked language modeling task. Furthermore, we introduce a novel\nprompt-based strategy for inter-component relation prediction that compliments\nour proposed finetuning method while leveraging on the discourse context.\nExhaustive experiments show the generalization capability of our method on\nthese two tasks over within-domain as well as out-of-domain datasets,\noutperforming several existing and employed strong baselines.\n","authors":["Subhabrata Dutta","Jeevesh Juneja","Dipankar Das","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2203.12881v1.pdf","comment":"Accepted in ACL 2022"},{"id":"http://arxiv.org/abs/2203.12868v1","updated":"2022-03-24T06:22:33Z","published":"2022-03-24T06:22:33Z","title":"DyRep: Bootstrapping Training with Dynamic Re-parameterization","summary":"  Structural re-parameterization (Rep) methods achieve noticeable improvements\non simple VGG-style networks. Despite the prevalence, current Rep methods\nsimply re-parameterize all operations into an augmented network, including\nthose that rarely contribute to the model's performance. As such, the price to\npay is an expensive computational overhead to manipulate these unnecessary\nbehaviors. To eliminate the above caveats, we aim to bootstrap the training\nwith minimal cost by devising a dynamic re-parameterization (DyRep) method,\nwhich encodes Rep technique into the training process that dynamically evolves\nthe network structures. Concretely, our proposal adaptively finds the\noperations which contribute most to the loss in the network, and applies Rep to\nenhance their representational capacity. Besides, to suppress the noisy and\nredundant operations introduced by Rep, we devise a de-parameterization\ntechnique for a more compact re-parameterization. With this regard, DyRep is\nmore efficient than Rep since it smoothly evolves the given network instead of\nconstructing an over-parameterized network. Experimental results demonstrate\nour effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\\%$\non ImageNet and reduces $22\\%$ runtime over the baseline. Code is available at:\nhttps://github.com/hunto/DyRep.\n","authors":["Tao Huang","Shan You","Bohan Zhang","Yuxuan Du","Fei Wang","Chen Qian","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2203.12868v1.pdf","comment":"Accepted to CVPR 2022"},{"id":"http://arxiv.org/abs/2203.12865v1","updated":"2022-03-24T06:05:28Z","published":"2022-03-24T06:05:28Z","title":"Multilingual CheckList: Generation and Evaluation","summary":"  The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation\nof NLP systems has revealed high failure rates for basic capabilities for\nmultiple state-of-the-art and commercial models. However, the CheckList\ncreation process is manual which creates a bottleneck towards creation of\nmultilingual CheckLists catering 100s of languages. In this work, we explore\nmultiple approaches to generate and evaluate the quality of Multilingual\nCheckList. We device an algorithm -- Automated Multilingual Checklist\nGeneration (AMCG) for automatically transferring a CheckList from a source to a\ntarget language that relies on a reasonable machine translation system. We then\ncompare the CheckList generated by AMCG with CheckLists generated with\ndifferent levels of human intervention. Through in-depth crosslingual\nexperiments between English and Hindi, and broad multilingual experiments\nspanning 11 languages, we show that the automatic approach can provide accurate\nestimates of failure rates of a model across capabilities, as would a\nhuman-verified CheckList, and better than CheckLists generated by humans from\nscratch.\n","authors":["Karthikeyan K","Shaily Bhatt","Pankaj Singh","Somak Aditya","Sandipan Dandapat","Sunayana Sitaram","Monojit Choudhary"],"pdf_url":"https://arxiv.org/pdf/2203.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12864v1","updated":"2022-03-24T06:03:42Z","published":"2022-03-24T06:03:42Z","title":"Kullback-Leibler control for discrete-time nonlinear systems on\n  continuous spaces","summary":"  Kullback-Leibler (KL) control enables efficient numerical methods for\nnonlinear optimal control problems. The crucial assumption of KL control is the\nfull controllability of the transition distribution. However, this assumption\nis often violated when the dynamics evolves in a continuous space.\nConsequently, applying KL control to problems with continuous spaces requires\nsome approximation, which leads to the lost of the optimality. To avoid such\napproximation, in this paper, we reformulate the KL control problem for\ncontinuous spaces so that it does not require unrealistic assumptions. The key\ndifference between the original and reformulated KL control is that the former\nmeasures the control effort by KL divergence between controlled and\nuncontrolled transition distributions while the latter replaces the\nuncontrolled transition by a noise-driven transition. We show that the\nreformulated KL control admits efficient numerical algorithms like the original\none without unreasonable assumptions. Specifically, the associated value\nfunction can be computed by using a Monte Carlo method based on its path\nintegral representation.\n","authors":["Kaito Ito","Kenji Kashima"],"pdf_url":"https://arxiv.org/pdf/2203.12864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12861v1","updated":"2022-03-24T05:56:30Z","published":"2022-03-24T05:56:30Z","title":"Transformer Compressed Sensing via Global Image Tokens","summary":"  Convolutional neural networks (CNN) have demonstrated outstanding Compressed\nSensing (CS) performance compared to traditional, hand-crafted methods.\nHowever, they are broadly limited in terms of generalisability, inductive bias\nand difficulty to model long distance relationships. Transformer neural\nnetworks (TNN) overcome such issues by implementing an attention mechanism\ndesigned to capture dependencies between inputs. However, high-resolution tasks\ntypically require vision Transformers (ViT) to decompose an image into\npatch-based tokens, limiting inputs to inherently local contexts. We propose a\nnovel image decomposition that naturally embeds images into low-resolution\ninputs. These Kaleidoscope tokens (KD) provide a mechanism for global\nattention, at the same computational cost as a patch-based approach. To\nshowcase this development, we replace CNN components in a well-known CS-MRI\nneural network with TNN blocks and demonstrate the improvements afforded by KD.\nWe also propose an ensemble of image tokens, which enhance overall image\nquality and reduces model size. Supplementary material is available:\nhttps://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git\n","authors":["Marlon Bran Lorenzana","Craig Engstrom","Shekhar S. Chandra"],"pdf_url":"https://arxiv.org/pdf/2203.12861v1.pdf","comment":"4 Pages, 4 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2203.12856v1","updated":"2022-03-24T05:38:07Z","published":"2022-03-24T05:38:07Z","title":"Beyond Fixation: Dynamic Window Visual Transformer","summary":"  Recently, a surge of interest in visual transformers is to reduce the\ncomputational cost by limiting the calculation of self-attention to a local\nwindow. Most current work uses a fixed single-scale window for modeling by\ndefault, ignoring the impact of window size on model performance. However, this\nmay limit the modeling potential of these window-based models for multi-scale\ninformation. In this paper, we propose a novel method, named Dynamic Window\nVision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT\ngoes beyond the model that employs a fixed single window setting. To the best\nof our knowledge, we are the first to use dynamic multi-scale windows to\nexplore the upper limit of the effect of window settings on model performance.\nIn DW-ViT, multi-scale information is obtained by assigning windows of\ndifferent sizes to different head groups of window multi-head self-attention.\nThen, the information is dynamically fused by assigning different weights to\nthe multi-scale window branches. We conducted a detailed performance evaluation\non three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related\nstate-of-the-art (SoTA) methods, DW-ViT obtains the best performance.\nSpecifically, compared with the current SoTA Swin Transformers\n\\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements\non all three datasets with similar parameters and computational costs. In\naddition, DW-ViT exhibits good scalability and can be easily inserted into any\nwindow-based visual transformers.\n","authors":["Pengzhen Ren","Changlin Li","Guangrun Wang","Yun Xiao","Qing Du Xiaodan Liang Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2203.12856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12853v1","updated":"2022-03-24T05:29:09Z","published":"2022-03-24T05:29:09Z","title":"Direct evaluation of progression or regression of disease burden in\n  brain metastatic disease with Deep Neuroevolution","summary":"  Purpose: A core component of advancing cancer treatment research is assessing\nresponse to therapy. Doing so by hand, for example as per RECIST or RANO\ncriteria, is tedious, time-consuming, and can miss important tumor response\ninformation; most notably, they exclude non-target lesions. We wish to assess\nchange in a holistic fashion that includes all lesions, obtaining simple,\ninformative, and automated assessments of tumor progression or regression. Due\nto often low patient enrolments in clinical trials, we wish to make response\nassessments with small training sets. Deep neuroevolution (DNE) can produce\nradiology artificial intelligence (AI) that performs well on small training\nsets. Here we use DNE for function approximation that predicts progression\nversus regression of metastatic brain disease.\n  Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training\nset. Half of these pairs, separated in time, qualified as disease progression,\nwhile the other 25 images constituted regression. We trained the parameters of\na relatively small CNN via mutations that consisted of random CNN weight\nadjustments and mutation fitness. We then incorporated the best mutations into\nthe next generations CNN, repeating this process for approximately 50,000\ngenerations. We applied the CNNs to our training set, as well as a separate\ntesting set with the same class balance of 25 progression and 25 regression\nimages.\n  Results: DNE achieved monotonic convergence to 100% training set accuracy.\nDNE also converged monotonically to 100% testing set accuracy.\n  Conclusion: DNE can accurately classify brain-metastatic disease progression\nversus regression. Future work will extend the input from 2D image slices to\nfull 3D volumes, and include the category of no change. We believe that an\napproach such as our could ultimately provide a useful adjunct to RANO/RECIST\nassessment.\n","authors":["Joseph Stember","Robert Young","Hrithwik Shalu"],"pdf_url":"https://arxiv.org/pdf/2203.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.09028v3","updated":"2022-03-24T05:09:54Z","published":"2022-02-18T05:21:28Z","title":"On the Implicit Bias Towards Minimal Depth of Deep Neural Networks","summary":"  We study the implicit bias of gradient based training methods to favor\nlow-depth solutions when training deep neural networks. Recent results in the\nliterature suggest that penultimate layer representations learned by a\nclassifier over multiple classes exhibit a clustering property, called neural\ncollapse. We demonstrate empirically that the neural collapse property extends\nbeyond the penultimate layer and tends to emerge in intermediate layers as\nwell. In this regards, we hypothesize that gradient based methods are\nimplicitly biased towards selecting neural networks of minimal depth for\nachieving this clustering property.\n","authors":["Tomer Galanti","Liane Galanti"],"pdf_url":"https://arxiv.org/pdf/2202.09028v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13792v3","updated":"2022-03-24T04:14:56Z","published":"2021-11-27T01:54:45Z","title":"LAFITE: Towards Language-Free Training for Text-to-Image Generation","summary":"  One of the major challenges in training text-to-image generation models is\nthe need of a large number of high-quality image-text pairs. While image\nsamples are often easily accessible, the associated text descriptions typically\nrequire careful human captioning, which is particularly time- and\ncost-consuming. In this paper, we propose the first work to train text-to-image\ngeneration models without any text data. Our method leverages the well-aligned\nmulti-modal semantic space of the powerful pre-trained CLIP model: the\nrequirement of text-conditioning is seamlessly alleviated via generating text\nfeatures from image features. Extensive experiments are conducted to illustrate\nthe effectiveness of the proposed method. We obtain state-of-the-art results in\nthe standard text-to-image generation tasks. Importantly, the proposed\nlanguage-free model outperforms most existing models trained with full\nimage-text pairs. Furthermore, our method can be applied in fine-tuning\npre-trained models, which saves both training time and cost in training\ntext-to-image generation models. Our pre-trained model obtains competitive\nresults in zero-shot text-to-image generation on the MS-COCO dataset, yet with\naround only 1% of the model size and training data size relative to the\nrecently proposed large DALL-E model.\n","authors":["Yufan Zhou","Ruiyi Zhang","Changyou Chen","Chunyuan Li","Chris Tensmeyer","Tong Yu","Jiuxiang Gu","Jinhui Xu","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2111.13792v3.pdf","comment":"Accepted by CVPR 2022, https://github.com/drboog/Lafite"},{"id":"http://arxiv.org/abs/2203.12836v1","updated":"2022-03-24T03:49:04Z","published":"2022-03-24T03:49:04Z","title":"Risk Consistent Multi-Class Learning from Label Proportions","summary":"  This study addresses a multiclass learning from label proportions (MCLLP)\nsetting in which training instances are provided in bags and only the\nproportion of each class within the bags is provided. Most existing MCLLP\nmethods impose bag-wise constraints on the prediction of instances or assign\nthem pseudo-labels; however, none of these methods have a theoretical\nconsistency. To solve this problem, a risk-consistent method is proposed for\ninstance classification using the empirical risk minimization framework, and\nits estimation error bound is derived. An approximation method is proposed for\nthe proposed risk estimator, to apply it to large bags, by diverting the\nconstraints on bags in existing research. The proposed method can be applied to\nany deep model or loss and is compatible with stochastic optimization.\nExperiments are conducted on benchmarks to verify the effectiveness of the\nproposed method.\n","authors":["Ryoma Kobayashi","Yusuke Mukuta","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2203.12836v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2203.12831v1","updated":"2022-03-24T03:31:18Z","published":"2022-03-24T03:31:18Z","title":"LHNN: Lattice Hypergraph Neural Network for VLSI Congestion Prediction","summary":"  Precise congestion prediction from a placement solution plays a crucial role\nin circuit placement. This work proposes the lattice hypergraph (LH-graph), a\nnovel graph formulation for circuits, which preserves netlist data during the\nwhole learning process, and enables the congestion information propagated\ngeometrically and topologically. Based on the formulation, we further developed\na heterogeneous graph neural network architecture LHNN, jointing the routing\ndemand regression to support the congestion spot classification. LHNN\nconstantly achieves more than 35% improvements compared with U-nets and Pix2Pix\non the F1 score. We expect our work shall highlight essential procedures using\nmachine learning for congestion prediction.\n","authors":["Bowen Wang","Guibao Shen","Dong Li","Jianye Hao","Wulong Liu","Yu Huang","Hongzhong Wu","Yibo Lin","Guangyong Chen","Pheng Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2203.12831v1.pdf","comment":"Accepted as a conference paper in DAC 2022; 6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2110.00603v2","updated":"2022-03-24T03:09:24Z","published":"2021-10-01T18:18:13Z","title":"Algorithm Fairness in AI for Medicine and Healthcare","summary":"  In the current development and deployment of many artificial intelligence\n(AI) systems in healthcare, algorithm fairness is a challenging problem in\ndelivering equitable care. Recent evaluation of AI models stratified across\nrace sub-populations have revealed inequalities in how patients are diagnosed,\ngiven treatments, and billed for healthcare costs. In this perspective article,\nwe summarize the intersectional field of fairness in machine learning through\nthe context of current issues in healthcare, outline how algorithmic biases\n(e.g. - image acquisition, genetic variation, intra-observer labeling\nvariability) arise in current clinical workflows and their resulting healthcare\ndisparities. Lastly, we also review emerging technology for mitigating bias via\nfederated learning, disentanglement, and model explainability, and their role\nin AI-SaMD development.\n","authors":["Richard J. Chen","Tiffany Y. Chen","Jana Lipkova","Judy J. Wang","Drew F. K. Williamson","Ming Y. Lu","Sharifa Sahai","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2110.00603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12821v1","updated":"2022-03-24T02:58:36Z","published":"2022-03-24T02:58:36Z","title":"GraphCoCo: Graph Complementary Contrastive Learning","summary":"  Graph Contrastive Learning (GCL) has shown promising performance in graph\nrepresentation learning (GRL) without the supervision of manual annotations.\nGCL can generate graph-level embeddings by maximizing the Mutual Information\n(MI) between different augmented views of the same graph (positive pairs).\nHowever, we identify an obstacle that the optimization of InfoNCE loss only\nconcentrates on a few embeddings dimensions, limiting the distinguishability of\nembeddings in downstream graph classification tasks. This paper proposes an\neffective graph complementary contrastive learning approach named GraphCoCo to\ntackle the above issue. Specifically, we set the embedding of the first\naugmented view as the anchor embedding to localize \"highlighted\" dimensions\n(i.e., the dimensions contribute most in similarity measurement). Then remove\nthese dimensions in the embeddings of the second augmented view to discover\nneglected complementary representations. Therefore, the combination of anchor\nand complementary embeddings significantly improves the performance in\ndownstream tasks. Comprehensive experiments on various benchmark datasets are\nconducted to demonstrate the effectiveness of GraphCoCo, and the results show\nthat our model outperforms the state-of-the-art methods. Source code will be\nmade publicly available.\n","authors":["Jiawei Sun","Junchi Yan","Chentao Wu","Yue Ding","Ruoxin Chen","Xiang Yu","Xinyu Lu","Jie Li"],"pdf_url":"https://arxiv.org/pdf/2203.12821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.11528v3","updated":"2022-03-24T02:47:43Z","published":"2022-03-22T08:04:38Z","title":"Out-of-distribution Generalization with Causal Invariant Transformations","summary":"  In real-world applications, it is important and desirable to learn a model\nthat performs well on out-of-distribution (OOD) data. Recently, causality has\nbecome a powerful tool to tackle the OOD generalization problem, with the idea\nresting on the causal mechanism that is invariant across domains of interest.\nTo leverage the generally unknown causal mechanism, existing works assume a\nlinear form of causal feature or require sufficiently many and diverse training\ndomains, which are usually restrictive in practice. In this work, we obviate\nthese assumptions and tackle the OOD problem without explicitly recovering the\ncausal feature. Our approach is based on transformations that modify the\nnon-causal feature but leave the causal part unchanged, which can be either\nobtained from prior knowledge or learned from the training data in the\nmulti-domain scenario. Under the setting of invariant causal mechanism, we\ntheoretically show that if all such transformations are available, then we can\nlearn a minimax optimal model across the domains using only single domain data.\nNoticing that knowing a complete set of these causal invariant transformations\nmay be impractical, we further show that it suffices to know only a subset of\nthese transformations. Based on the theoretical findings, a regularized\ntraining procedure is proposed to improve the OOD generalization capability.\nExtensive experimental results on both synthetic and real datasets verify the\neffectiveness of the proposed algorithm, even with only a few causal invariant\ntransformations.\n","authors":["Ruoyu Wang","Mingyang Yi","Zhitang Chen","Shengyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.11528v3.pdf","comment":"accepted by cvpr2022"},{"id":"http://arxiv.org/abs/2108.04462v2","updated":"2022-03-24T02:29:35Z","published":"2021-08-10T06:13:05Z","title":"Deep Reinforcement Learning for Demand Driven Services in Logistics and\n  Transportation Systems: A Survey","summary":"  Recent technology development brings the booming of numerous new\nDemand-Driven Services (DDS) into urban lives, including ridesharing, on-demand\ndelivery, express systems and warehousing. In DDS, a service loop is an\nelemental structure, including its service worker, the service providers and\ncorresponding service targets. The service workers should transport either\nhumans or parcels from the providers to the target locations. Various planning\ntasks within DDS can thus be classified into two individual stages: 1)\nDispatching, which is to form service loops from demand/supply distributions,\nand 2)Routing, which is to decide specific serving orders within the\nconstructed loops. Generating high-quality strategies in both stages is\nimportant to develop DDS but faces several challenging. Meanwhile, deep\nreinforcement learning (DRL) has been developed rapidly in recent years. It is\na powerful tool to solve these problems since DRL can learn a parametric model\nwithout relying on too many problem-based assumptions and optimize long-term\neffect by learning sequential decisions. In this survey, we first define DDS,\nthen highlight common applications and important decision/control problems\nwithin. For each problem, we comprehensively introduce the existing DRL\nsolutions. We also introduce open simulation environments for development and\nevaluation of DDS applications. Finally, we analyze remaining challenges and\ndiscuss further research opportunities in DRL solutions for DDS.\n","authors":["Zefang Zong","Tao Feng","Tong Xia","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2108.04462v2.pdf","comment":"21 pages. survey preprint"},{"id":"http://arxiv.org/abs/2203.12803v1","updated":"2022-03-24T02:09:41Z","published":"2022-03-24T02:09:41Z","title":"When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning\n  Framework in Classification of Medical Images on Limited Data: A COVID-19\n  Case Study","summary":"  COVID-19 pandemic has spread rapidly and caused a shortage of global medical\nresources. The efficiency of COVID-19 diagnosis has become highly significant.\nAs deep learning and convolutional neural network (CNN) has been widely\nutilized and been verified in analyzing medical images, it has become a\npowerful tool for computer-assisted diagnosis. However, there are two most\nsignificant challenges in medical image classification with the help of deep\nlearning and neural networks, one of them is the difficulty of acquiring enough\nsamples, which may lead to model overfitting. Privacy concerns mainly bring the\nother challenge since medical-related records are often deemed patients'\nprivate information and protected by laws such as GDPR and HIPPA. Federated\nlearning can ensure the model training is decentralized on different devices\nand no data is shared among them, which guarantees privacy. However, with data\nlocated on different devices, the accessible data of each device could be\nlimited. Since transfer learning has been verified in dealing with limited data\nwith good performance, therefore, in this paper, We made a trial to implement\nfederated learning and transfer learning techniques using CNNs to classify\nCOVID-19 using lung CT scans. We also explored the impact of dataset\ndistribution at the client-side in federated learning and the number of\ntraining epochs a model is trained. Finally, we obtained very high performance\nwith federated learning, demonstrating our success in leveraging accuracy and\nprivacy.\n","authors":["Alexandros Shikun Zhang","Naomi Fengqi Li"],"pdf_url":"https://arxiv.org/pdf/2203.12803v1.pdf","comment":"11 pages, 11 figures, preprint"},{"id":"http://arxiv.org/abs/2203.12088v2","updated":"2022-03-24T02:05:48Z","published":"2022-03-22T22:51:22Z","title":"Deep Portrait Delighting","summary":"  We present a deep neural network for removing undesirable shading features\nfrom an unconstrained portrait image, recovering the underlying texture. Our\ntraining scheme incorporates three regularization strategies: masked loss, to\nemphasize high-frequency shading features; soft-shadow loss, which improves\nsensitivity to subtle changes in lighting; and shading-offset estimation, to\nsupervise separation of shading and texture. Our method demonstrates improved\ndelighting quality and generalization when compared with the state-of-the-art.\nWe further demonstrate how our delighting method can enhance the performance of\nlight-sensitive computer vision tasks such as face relighting and semantic\nparsing, allowing them to handle extreme lighting conditions.\n","authors":["Joshua Weir","Junhong Zhao","Andrew Chalmers","Taehyun Rhee"],"pdf_url":"https://arxiv.org/pdf/2203.12088v2.pdf","comment":"25 pages, 16 figures"},{"id":"http://arxiv.org/abs/2203.12798v1","updated":"2022-03-24T01:43:13Z","published":"2022-03-24T01:43:13Z","title":"DPar2: Fast and Scalable PARAFAC2 Decomposition for Irregular Dense\n  Tensors","summary":"  Given an irregular dense tensor, how can we efficiently analyze it? An\nirregular tensor is a collection of matrices whose columns have the same size\nand rows have different sizes from each other. PARAFAC2 decomposition is a\nfundamental tool to deal with an irregular tensor in applications including\nphenotype discovery and trend analysis. Although several PARAFAC2 decomposition\nmethods exist, their efficiency is limited for irregular dense tensors due to\nthe expensive computations involved with the tensor. In this paper, we propose\nDPar2, a fast and scalable PARAFAC2 decomposition method for irregular dense\ntensors. DPar2 achieves high efficiency by effectively compressing each slice\nmatrix of a given irregular tensor, careful reordering of computations with the\ncompression results, and exploiting the irregularity of the tensor. Extensive\nexperiments show that DPar2 is up to 6.0x faster than competitors on real-world\nirregular tensors while achieving comparable accuracy. In addition, DPar2 is\nscalable with respect to the tensor size and target rank.\n","authors":["Jun-Gi Jang","U Kang"],"pdf_url":"https://arxiv.org/pdf/2203.12798v1.pdf","comment":"14 pages, 11 figures. To appear at the 38th IEEE International\n  Conference on Data Engineering (ICDE '22)"},{"id":"http://arxiv.org/abs/2002.09478v6","updated":"2022-03-24T01:29:19Z","published":"2020-02-21T02:44:56Z","title":"On the Search for Feedback in Reinforcement Learning","summary":"  The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical\nsystem is equivalent to the search for an optimal feedback law utilizing the\nsimulations/ rollouts of the dynamical system. Most RL techniques search over a\ncomplex global nonlinear feedback parametrization making them suffer from high\ntraining times as well as variance. Instead, we advocate searching over a local\nfeedback representation consisting of an open-loop sequence, and an associated\noptimal linear feedback law completely determined by the open-loop. We show\nthat this alternate approach results in highly efficient training, the answers\nobtained are repeatable and hence reliable, and the resulting closed\nperformance is superior to global state-of-the-art RL techniques. Finally, if\nwe replan, whenever required, which is feasible due to the fast and reliable\nlocal solution, it allows us to recover global optimality of the resulting\nfeedback law.\n","authors":["Ran Wang","Karthikeya S. Parunandi","Aayushman Sharma","Raman Goyal","Suman Chakravorty"],"pdf_url":"https://arxiv.org/pdf/2002.09478v6.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:1904.08361"},{"id":"http://arxiv.org/abs/2203.12786v1","updated":"2022-03-24T01:04:17Z","published":"2022-03-24T01:04:17Z","title":"Bellman Residual Orthogonalization for Offline Reinforcement Learning","summary":"  We introduce a new reinforcement learning principle that approximates the\nBellman equations by enforcing their validity only along an user-defined space\nof test functions. Focusing on applications to model-free offline RL with\nfunction approximation, we exploit this principle to derive confidence\nintervals for off-policy evaluation, as well as to optimize over policies\nwithin a prescribed policy class. We prove an oracle inequality on our policy\noptimization procedure in terms of a trade-off between the value and\nuncertainty of an arbitrary comparator policy. Different choices of test\nfunction spaces allow us to tackle different problems within a common\nframework. We characterize the loss of efficiency in moving from on-policy to\noff-policy data using our procedures, and establish connections to\nconcentrability coefficients studied in past work. We examine in depth the\nimplementation of our methods with linear function approximation, and provide\ntheoretical guarantees with polynomial-time implementations even when Bellman\nclosure does not hold.\n","authors":["Andrea Zanette","Martin J. Wainwright"],"pdf_url":"https://arxiv.org/pdf/2203.12786v1.pdf","comment":"Initial Submission"},{"id":"http://arxiv.org/abs/2109.10135v2","updated":"2022-03-24T00:50:17Z","published":"2021-09-21T12:41:02Z","title":"Self-supervised Representation Learning for Reliable Robotic Monitoring\n  of Fruit Anomalies","summary":"  Data augmentation can be a simple yet powerful tool for autonomous robots to\nfully utilise available data for selfsupervised identification of atypical\nscenes or objects. State-of-the-art augmentation methods arbitrarily embed\n\"structural\" peculiarity on typical images so that classifying these artefacts\ncan provide guidance for learning representations for the detection of\nanomalous visual signals. In this paper, however, we argue that learning such\nstructure-sensitive representations can be a suboptimal approach to some\nclasses of anomaly (e.g., unhealthy fruits) which could be better recognised by\na different type of visual element such as \"colour\". We thus propose Channel\nRandomisation as a novel data augmentation method for restricting neural\nnetworks to learn encoding of \"colour irregularity\" whilst predicting\nchannel-randomised images to ultimately build reliable fruit-monitoring robots\nidentifying atypical fruit qualities. Our experiments show that (1) this\ncolour-based alternative can better learn representations for consistently\naccurate identification of fruit anomalies in various fruit species, and also,\n(2) unlike other methods, the validation accuracy can be utilised as a\ncriterion for early stopping of training in practice due to positive\ncorrelation between the performance in the self-supervised\ncolour-differentiation task and the subsequent detection rate of actual\nanomalous fruits. Also, the proposed approach is evaluated on a new\nagricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images\ngathered by a mobile robot, which we share online to encourage active\nagri-robotics research.\n","authors":["Taeyeong Choi","Owen Would","Adrian Salazar-Gomez","Grzegorz Cielniak"],"pdf_url":"https://arxiv.org/pdf/2109.10135v2.pdf","comment":"Accepted to ICRA2022; Codes and data are all available online"},{"id":"http://arxiv.org/abs/2203.08411v2","updated":"2022-03-24T00:17:27Z","published":"2022-03-16T06:02:02Z","title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document\n  Information Extraction","summary":"  Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.\n","authors":["Chen-Yu Lee","Chun-Liang Li","Timothy Dozat","Vincent Perot","Guolong Su","Nan Hua","Joshua Ainslie","Renshen Wang","Yasuhisa Fujii","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2203.08411v2.pdf","comment":"Accepted to ACL 2022"}],"Multimedia":[{"id":"http://arxiv.org/abs/2109.04260v2","updated":"2022-03-24T13:55:32Z","published":"2021-09-09T13:30:31Z","title":"Online Enhanced Semantic Hashing: Towards Effective and Efficient\n  Retrieval for Streaming Multi-Modal Data","summary":"  With the vigorous development of multimedia equipment and applications,\nefficient retrieval of large-scale multi-modal data has become a trendy\nresearch topic. Thereinto, hashing has become a prevalent choice due to its\nretrieval efficiency and low storage cost. Although multi-modal hashing has\ndrawn lots of attention in recent years, there still remain some problems. The\nfirst point is that existing methods are mainly designed in batch mode and not\nable to efficiently handle streaming multi-modal data. The second point is that\nall existing online multi-modal hashing methods fail to effectively handle\nunseen new classes which come continuously with streaming data chunks. In this\npaper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).\nWe design novel semantic-enhanced representation for data, which could help\nhandle the new coming classes, and thereby construct the enhanced semantic\nobjective function. An efficient and effective discrete online optimization\nalgorithm is further proposed for OASIS. Extensive experiments show that our\nmethod can exceed the state-of-the-art models. For good reproducibility and\nbenefiting the community, our code and data are already available in\nsupplementary material and will be made publicly available.\n","authors":["Xiao-Ming Wu","Xin Luo","Yu-Wei Zhan","Chen-Lu Ding","Zhen-Duo Chen","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2109.04260v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2203.13031v1","updated":"2022-03-24T12:18:06Z","published":"2022-03-24T12:18:06Z","title":"Continuous Emotion Recognition using Visual-audio-linguistic\n  information: A Technical Report for ABAW3","summary":"  We propose a cross-modal co-attention model for continuous emotion\nrecognition using visual-audio-linguistic information. The model consists of\nfour blocks. The visual, audio, and linguistic blocks are used to learn the\nspatial-temporal features of the multimodal input. A co-attention block is\ndesigned to fuse the learned enbeddings with the multihead co-attention\nmechanism. The visual encoding from the visual block is concatenated with the\nattention feature to emphasize the visual information. To make full use of the\ndata and alleviate over-fitting, the cross-validation is carried out on the\ntraining and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. The achieved CCC on\nvalidation set is 0.450 for valence and 0.651 for arousal, which significantly\noutperforms the baseline method with the corresponding CCC of 0.310 and 0.170,\nrespectively. The code is available at https://github.com/sucv/ABAW3.\n","authors":["Su Zhang","Ruyi An","Yi Ding","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2203.13031v1.pdf","comment":"5 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:2107.01175"},{"id":"http://arxiv.org/abs/2203.13010v1","updated":"2022-03-24T12:00:04Z","published":"2022-03-24T12:00:04Z","title":"Score difficulty analysis for piano performance education based on\n  fingering","summary":"  In this paper, we introduce score difficulty classification as a sub-task of\nmusic information retrieval (MIR), which may be used in music education\ntechnologies, for personalised curriculum generation, and score retrieval. We\nintroduce a novel dataset for our task, Mikrokosmos-difficulty, containing 147\npiano pieces in symbolic representation and the corresponding difficulty labels\nderived by its composer B\\'ela Bart\\'ok and the publishers. As part of our\nmethodology, we propose piano technique feature representations based on\ndifferent piano fingering algorithms. We use these features as input for two\nclassifiers: a Gated Recurrent Unit neural network (GRU) with attention\nmechanism and gradient-boosted trees trained on score segments. We show that\nfor our dataset fingering based features perform better than a simple baseline\nconsidering solely the notes in the score. Furthermore, the GRU with attention\nmechanism classifier surpasses the gradient-boosted trees. Our proposed models\nare interpretable and are capable of generating difficulty feedback both\nlocally, on short term segments, and globally, for whole pieces. Code,\ndatasets, models, and an online demo are made available for reproducibility\n","authors":["Pedro Ramoneda","Nazif Can Tamer","Vsevolod Eremenko","Xavier Serra","Marius Miron"],"pdf_url":"https://arxiv.org/pdf/2203.13010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1709.00944v4","updated":"2022-03-24T08:39:52Z","published":"2017-09-01T14:17:53Z","title":"Audio-Visual Speech Enhancement using Multimodal Deep Convolutional\n  Neural Network","summary":"  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus on addressing audio information only. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. In the proposed AVDCNN SE model, audio\nand visual data are first processed using individual CNNs, and then, fused into\na joint network to generate enhanced speech at the output layer. The AVDCNN\nmodel is trained in an end-to-end manner, and parameters are jointly learned\nthrough back-propagation. We evaluate enhanced speech using five objective\ncriteria. Results show that the AVDCNN yields notably better performance,\ncompared with an audio-only CNN-based SE model and two conventional SE\napproaches, confirming the effectiveness of integrating visual information into\nthe SE process.\n","authors":["Jen-Cheng Hou","Syu-Siang Wang","Ying-Hui Lai","Yu Tsao","Hsiu-Wen Chang","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/1709.00944v4.pdf","comment":"This paper is the same as arXiv:1703.10893v2. Apologies for the\n  inconvenience"},{"id":"http://arxiv.org/abs/2203.12929v1","updated":"2022-03-24T08:21:41Z","published":"2022-03-24T08:21:41Z","title":"Towards Escaping from Language Bias and OCR Error: Semantics-Centered\n  Text Visual Question Answering","summary":"  Texts in scene images convey critical information for scene understanding and\nreasoning. The abilities of reading and reasoning matter for the model in the\ntext-based visual question answering (TextVQA) process. However, current\nTextVQA models do not center on the text and suffer from several limitations.\nThe model is easily dominated by language biases and optical character\nrecognition (OCR) errors due to the absence of semantic guidance in the answer\nprediction process. In this paper, we propose a novel Semantics-Centered\nNetwork (SC-Net) that consists of an instance-level contrastive semantic\nprediction module (ICSP) and a semantics-centered transformer module (SCT).\nEquipped with the two modules, the semantics-centered model can resist the\nlanguage biases and the accumulated errors from OCR. Extensive experiments on\nTextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net\nsurpasses previous works with a noticeable margin and is more reasonable for\nthe TextVQA task.\n","authors":["Chengyang Fang","Gangyan Zeng","Yu Zhou","Daiqing Wu","Can Ma","Dayong Hu","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2203.12929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12843v1","updated":"2022-03-24T04:44:51Z","published":"2022-03-24T04:44:51Z","title":"Steganalysis of Image with Adaptively Parametric Activation","summary":"  Steganalysis as a method to detect whether image contains se-cret message, is\na crucial study avoiding the imperils from abus-ing steganography. The point of\nsteganalysis is to detect the weak embedding signals which is hardly learned by\nconvolution-al layer and easily suppressed. In this paper, to enhance\nembed-ding signals, we study the insufficiencies of activation function,\nfilters and loss function from the aspects of reduce embedding signal loss and\nenhance embedding signal capture ability. Adap-tive Parametric Activation\nModule is designed to reserve nega-tive embedding signal. For embedding signal\ncapture ability enhancement, we add constraints on the high-pass filters to\nim-prove residual diversity which enables the filters extracts rich embedding\nsignals. Besides, a loss function based on contrastive learning is applied to\novercome the limitations of cross-entropy loss by maximum inter-class distance.\nIt helps the network make a distinction between embedding signals and semantic\nedges. We use images from BOSSbase 1.01 and make stegos by WOW and S-UNIWARD\nfor experiments. Compared to state-of-the-art methods, our method has a\ncompetitive performance.\n","authors":["Hai Su","Meiyin Han","Junle Liang","Songsen Yu"],"pdf_url":"https://arxiv.org/pdf/2203.12843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12829v1","updated":"2022-03-24T03:16:42Z","published":"2022-03-24T03:16:42Z","title":"AIMusicGuru: Music Assisted Human Pose Correction","summary":"  Pose Estimation techniques rely on visual cues available through observations\nrepresented in the form of pixels. But the performance is bounded by the frame\nrate of the video and struggles from motion blur, occlusions, and temporal\ncoherence. This issue is magnified when people are interacting with objects and\ninstruments, for example playing the violin. Standard approaches for\npostprocessing use interpolation and smoothing functions to filter noise and\nfill gaps, but they cannot model highly non-linear motion. We present a method\nthat leverages our understanding of the high degree of a causal relationship\nbetween the sound produced and the motion that produces them. We use the audio\nsignature to refine and predict accurate human body pose motion models. We\npropose MAPnet (Music Assisted Pose network) for generating a fine grain motion\nmodel from sparse input pose sequences but continuous audio. To accelerate\nfurther research in this domain, we also open-source MAPdat, a new multi-modal\ndataset of 3D violin playing motion with music. We perform a comparison of\ndifferent standard machine learning models and perform analysis on input\nmodalities, sampling techniques, and audio and motion features. Experiments on\nMAPdat suggest multi-modal approaches like ours as a promising direction for\ntasks previously approached with visual methods only. Our results show both\nqualitatively and quantitatively how audio can be combined with visual\nobservation to help improve any pose estimation methods.\n","authors":["Snehesh Shrestha","Cornelia Fermüller","Tianyu Huang","Pyone Thant Win","Adam Zukerman","Chethan M. Parameshwara","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2203.12829v1.pdf","comment":"10 pages, 7 figures, under review"}]}}